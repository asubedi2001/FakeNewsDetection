# -*- coding: utf-8 -*-
"""NLP Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10BQ1qzlQ3K0EkEDc8NVLCE8g8x8zTvgD
"""

import numpy as np
import pandas as pd
import re
import nltk
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, random_split
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('stopwords')
nltk.download('punkt_tab')
nltk.download('punkt')

peek = 10
def present_list_like(name, list_like, peek=peek):
    print(f"{name} peek:")
    print('  ' + '\n  '.join([str(seq) for seq in list_like[0:peek]]))

# read data from covid_lies.csv dataset into dataframe
df = pd.read_csv('/content/covid_lies.csv')
print("The dataset:")
df.info()
print("\nData peek:")
print(df.head(peek))
print()

# seperate out text data and labels
input_text = df['misconception'].to_numpy()
input_label = df['label'].to_numpy()
print("Unique labels:", np.unique(input_label))

def preprocess_text(text)->str:
    #Letter-level cleaning
    text = text.lower() #Lowering significantly reduces the number of possible tokens to deal with
    valid_asciis = {9, *range(32, 127)}
    text = ''.join(filter(lambda x: ord(x) in valid_asciis, text)) #Remove irrelevant text(emojis, special characters/symbols, etc)

    #Word/sequence-level cleaning
    text = re.sub(r'\s+', ' ', text) #Remove extra spaces
    text = re.sub(r'http\S+', '', text) #Remove URLs
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text

#Preprocess the text
for i in range(len(input_text)):
    input_text[i] = preprocess_text(input_text[i])

#Tokenize the text

input_tokens = [nltk.word_tokenize(text) for text in input_text]

# Split tokens based on non-alphanumeric characters
final_tokens = []
total_tokens = 0
for token_set in input_tokens:
    final_tkn_set = []
    for tk in token_set:
        sub_tkns = [c for c in re.split(r"(\W+)", tk) if c]
        total_tokens += len(sub_tkns)
        final_tkn_set += sub_tkns
    final_tokens.append(final_tkn_set)
present_list_like(f"Tokenized sentences({len(final_tokens)} sentences, {total_tokens} tokens)", final_tokens)

#Embed the tokens
from collections import Counter
# Map each token to its frequency in the dataset
flat_tokens = [word for token_set in final_tokens for word in token_set]
frequencies = Counter(flat_tokens)
token_to_idx = {word: idx+1 for idx, (word, _) in enumerate(frequencies.most_common())}
vocab_size = len(token_to_idx)
print(vocab_size, "unique tokens")
present_list_like("Unique tokens", list(token_to_idx.keys()))

# Embed the tokens
freq_indexed = [[token_to_idx[token] for token in token_set] for token_set in final_tokens]

# Make embeddings the same size
forced_idx_set_size = max(len(idxs) for idxs in freq_indexed)
freq_indexed = [
    idxs[:forced_idx_set_size] + [0]*(forced_idx_set_size - len(idxs))
    for idxs in freq_indexed
]
present_list_like(f"\nFinal Index Sets(Set_Size = {forced_idx_set_size}, {len(freq_indexed)} index sets)", freq_indexed)

print(Counter(input_label))

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
#indexed_tokens = torch.tensor(freq_indexed, dtype = torch.long)
labels_numeric, unique_labels = pd.factorize(input_label)
#labels_tensor = torch.tensor(labels_numeric, dtype = torch.long)

x = np.array(freq_indexed)
y = np.array(labels_numeric)

smote = SMOTE(random_state=42)
x_resampled, y_resampled = smote.fit_resample(x, y)

x_train, x_val, y_train, y_val = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)
x_train_tensor = torch.tensor(x_train, dtype=torch.long)
y_train_tensor = torch.tensor(y_train, dtype=torch.long)
x_val_tensor = torch.tensor(x_val, dtype=torch.long)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)

train_dataset = TensorDataset(x_train_tensor, y_train_tensor)
val_dataset = TensorDataset(x_val_tensor, y_val_tensor)

train_loader = DataLoader(train_dataset, batch_size = 32,shuffle = True)
val_loader = DataLoader(val_dataset, batch_size = 32, shuffle = False)

class MisInformationDetectionLSTM(nn.Module):

  def __init__(self, input_size, hidden_size, output_size, num_layers = 1, dropout = 0.2):

    super(MisInformationDetectionLSTM, self).__init__()
    self.embedding = nn.Embedding(vocab_size+1, embed_size, padding_idx=0)
    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first= True, dropout = dropout)
    self.fc = nn.Linear(hidden_size, output_size)
    self.softmax = nn.Softmax(dim = 1)

  def forward(self, x):
    embedded = self.embedding(x)
    _, (hidden, _) = self.lstm(embedded)
    output = self.fc(hidden[-1])
    return self.softmax(output)

vocab_size = len(token_to_idx)
embed_size = 128
input_size = embed_size
print(vocab_size)

hidden_size = 256
output_size = len(unique_labels)
num_layers = 3
dropout = 0.2
print(output_size)

model = MisInformationDetectionLSTM(input_size, hidden_size, output_size, num_layers, dropout)

counter_data = Counter(input_label)
class_counts = list(counter_data.values())
print(class_counts)
class_weights = 1./ torch.tensor(class_counts, dtype = torch.float)
print(class_weights)
#criterion = nn.CrossEntropyLoss(weight = class_weights)
criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.5, 1.2,1.0]))
optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)

def train_model(model, train_loader, val_loader, criterion, optimizer, epochs = 10):

  for epoch in range(epochs):

    model.train()
    total_loss = 0

    for inputs, labels in train_loader:

      optimizer.zero_grad()
      outputs = model(inputs)
      loss = criterion(outputs, labels)
      loss.backward()
      optimizer.step()
      total_loss+=loss.item()

    print(f'Epoch {epoch+1}, Training loss : {total_loss/ len(train_loader):.4f}')

    model.eval()

    correct, total = 0,0

    with torch.no_grad():

      for inputs, labels in val_loader:

        outputs = model(inputs)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct+= (predicted == labels).sum().item()
    print(f"Validation Accuracy: {correct / total* 100:.2f}%")

train_model(model, train_loader, val_loader, criterion, optimizer, epochs = 30)

from sklearn.metrics import classification_report, confusion_matrix

def evaluate_model(model, val_loader):
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            y_true.extend(labels.tolist())
            y_pred.extend(predicted.tolist())

    print(classification_report(y_true, y_pred, target_names = unique_labels))
    print(confusion_matrix(y_true, y_pred))

evaluate_model(model, val_loader)

