{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ozyb-vXSAMtT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import random\n",
        "import pprint\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "random.seed(184)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "cF0LGpIbKgBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b15c220-1901-4e03-cf20-b315319185d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peek = 20\n",
        "def present_list_like(name, list_like, peek=peek):\n",
        "    print(f\"{name} peek:\")\n",
        "    print('  ' + '\\n  '.join(\n",
        "        str(v) for v in random.choices(list_like, k=min(peek, len(list_like)))\n",
        "    ))"
      ],
      "metadata": {
        "id": "BCo7k9jXdxOe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5lNu54LNAMtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fd9267-54f1-4c2d-ee8d-56d6dd7626c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset columns(14 in total) peek:\n",
            "  subject\n",
            "  subject\n",
            "  context\n",
            "  false_counts\n",
            "  speaker\n",
            "  mostly_true_counts\n",
            "  pants_on_fire_counts\n",
            "  context\n",
            "  barely_true_counts\n",
            "  pants_on_fire_counts\n",
            "  mostly_true_counts\n",
            "  barely_true_counts\n",
            "  id\n",
            "  party_affiliation\n"
          ]
        }
      ],
      "source": [
        "columns = [\n",
        "    'id', 'label', 'claim', 'subject', 'speaker', 'speaker_job_title', 'state_info',\n",
        "    'party_affiliation', 'barely_true_counts', 'false_counts',\n",
        "    'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
        "]\n",
        "present_list_like(f\"Dataset columns({len(columns)} in total)\", columns, len(columns))\n",
        "def load_data(split):\n",
        "    df = pd.read_csv(f\"./data/{split}.tsv\", sep='\\t', names=columns).dropna()\n",
        "    print(\"The training dataset:\")\n",
        "    df.info()\n",
        "    print(\"\\nData peek:\")\n",
        "    print(df.head(peek))\n",
        "    print()\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pad_tkn = \"<PAD>\""
      ],
      "metadata": {
        "id": "8KEuwtbf8v0e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z8PtD24vAMtW"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(input_text, known_vector_size=None, token_to_idx={}):\n",
        "    def preprocess_text(text)->str:\n",
        "        #Letter-level cleaning\n",
        "        text = text.lower()\n",
        "        valid_asciis = {9, *range(32, 127)}\n",
        "        text = ''.join(filter(lambda x: ord(x) in valid_asciis, text))\n",
        "\n",
        "        #Word/sequence-level cleaning\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "        return text\n",
        "\n",
        "    #Preprocess the text\n",
        "    for i in range(len(input_text)):\n",
        "        input_text[i] = preprocess_text(input_text[i])\n",
        "\n",
        "\n",
        "    #Tokenize\n",
        "    final_tokens = input_tokens = [nltk.word_tokenize(text) for text in input_text]\n",
        "    total_tokens = sum(len(tkns) for tkns in final_tokens)\n",
        "\n",
        "    # Make all token sets the same length\n",
        "    forced_tkn_set_size = (\n",
        "        known_vector_size if known_vector_size\n",
        "        else int(np.percentile([len(tkns) for tkns in final_tokens], 80))\n",
        "    )\n",
        "    final_tokens = [\n",
        "        tkns[:forced_tkn_set_size] + [pad_tkn]*(forced_tkn_set_size - len(tkns))\n",
        "        for tkns in final_tokens\n",
        "    ]\n",
        "\n",
        "    # Present results\n",
        "    present_list_like(f\"Tokenized sentences({len(final_tokens)} sentences, {total_tokens} total tokens)\", final_tokens)\n",
        "\n",
        "\n",
        "    #Index the tokens\n",
        "    # Map each token to its frequency in the dataset\n",
        "    if not len(token_to_idx):\n",
        "        flat_tokens = [word for token_set in final_tokens for word in token_set]\n",
        "        frequencies = Counter(flat_tokens)\n",
        "        token_to_idx = {}\n",
        "        for idx, (word, _) in enumerate(frequencies.most_common()):\n",
        "            if idx >= 10000:\n",
        "                break\n",
        "            token_to_idx[word] = idx + 1\n",
        "        if pad_tkn not in token_to_idx:\n",
        "            token_to_idx[pad_tkn] = len(token_to_idx) + 1\n",
        "    vocab_size = len(token_to_idx)\n",
        "    print()\n",
        "    print(vocab_size, \"unique tokens\")\n",
        "    present_list_like(\"Unique tokens\", list(token_to_idx.keys()))\n",
        "\n",
        "    # Index the tokens\n",
        "    freq_indexed = [\n",
        "        [(token_to_idx[token] if token in token_to_idx else 0) for token in token_set]\n",
        "        for token_set in final_tokens\n",
        "    ]\n",
        "\n",
        "    # Present results\n",
        "    present_list_like(f\"\\nFinal Index Sets(Set_Size = {forced_tkn_set_size}, {len(freq_indexed)} index sets)\", freq_indexed)\n",
        "\n",
        "    return freq_indexed, token_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_freq_indexed_and_labels(split, known_vector_size=None, token_to_idx={}):\n",
        "    df = load_data(split)\n",
        "    input_text = df[\"claim\"].to_numpy()\n",
        "    input_labels = df[\"label\"].to_numpy()\n",
        "    freq_indexed, token_to_idx = tokenize_text(input_text, known_vector_size, token_to_idx)\n",
        "\n",
        "    return freq_indexed, token_to_idx, input_labels"
      ],
      "metadata": {
        "id": "L4ep_xTTR8ZA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Turn the data into tensors"
      ],
      "metadata": {
        "id": "CSG8ChPrXEac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def as_tensors(split, label_encoder=None, known_vector_size=None, token_to_idx={}):\n",
        "    freq_indexed, token_to_idx, input_labels = get_freq_indexed_and_labels(split, known_vector_size, token_to_idx)\n",
        "    X = torch.tensor(freq_indexed, dtype=torch.long)\n",
        "    label_encoder_existed = (type(label_encoder) != type(None))\n",
        "    label_encoder = (LabelEncoder() if not label_encoder_existed else label_encoder)\n",
        "    y = (\n",
        "        label_encoder.fit_transform(input_labels) if not label_encoder_existed\n",
        "        else label_encoder.transform(input_labels)\n",
        "    )\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "    print(f\"{split.upper()} SPLIT:\", X.size(0), \"overall samples:\", X.shape)\n",
        "\n",
        "    return X, token_to_idx, label_encoder, input_labels, y"
      ],
      "metadata": {
        "id": "DU41k5wmRzHk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, token_to_idx, label_encoder, train_input_labels, y_train = as_tensors(\"train\")\n",
        "train_vocab_size = len(token_to_idx)\n",
        "input_vector_size = X_train.shape[1]\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "X_test, token_to_idx_test, label_encoder_test, test_input_labels, y_test = as_tensors(\"test\")\n",
        "test_vocab_size = len(token_to_idx_test)\n",
        "input_vector_size_test = X_test.shape[1]\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "X_valid, token_to_idx_valid, label_encoder_valid, valid_input_labels, y_valid = as_tensors(\"valid\")\n",
        "valid_vocab_size = len(token_to_idx_valid)\n",
        "input_vector_size_valid = X_valid.shape[1]\n",
        "valid_dataset = TensorDataset(X_valid, y_valid)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV1XEzpgVvtQ",
        "outputId": "df2f8afe-1bd8-456e-aef9-4e77f475788e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6721 entries, 0 to 10239\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   id                    6721 non-null   object \n",
            " 1   label                 6721 non-null   object \n",
            " 2   claim                 6721 non-null   object \n",
            " 3   subject               6721 non-null   object \n",
            " 4   speaker               6721 non-null   object \n",
            " 5   speaker_job_title     6721 non-null   object \n",
            " 6   state_info            6721 non-null   object \n",
            " 7   party_affiliation     6721 non-null   object \n",
            " 8   barely_true_counts    6721 non-null   float64\n",
            " 9   false_counts          6721 non-null   float64\n",
            " 10  half_true_counts      6721 non-null   float64\n",
            " 11  mostly_true_counts    6721 non-null   float64\n",
            " 12  pants_on_fire_counts  6721 non-null   float64\n",
            " 13  context               6721 non-null   object \n",
            "dtypes: float64(5), object(9)\n",
            "memory usage: 787.6+ KB\n",
            "\n",
            "Data peek:\n",
            "            id        label  \\\n",
            "0    2635.json        false   \n",
            "1   10540.json    half-true   \n",
            "2     324.json  mostly-true   \n",
            "5   12465.json         true   \n",
            "7     153.json    half-true   \n",
            "9    9741.json  mostly-true   \n",
            "10   7115.json  mostly-true   \n",
            "11   4148.json    half-true   \n",
            "12   5947.json        false   \n",
            "14   8705.json  barely-true   \n",
            "15  10683.json    half-true   \n",
            "16    620.json         true   \n",
            "18  12372.json    half-true   \n",
            "19  12385.json  mostly-true   \n",
            "20  10173.json        false   \n",
            "22  12408.json  mostly-true   \n",
            "24   7057.json  barely-true   \n",
            "25  10215.json        false   \n",
            "26  12517.json  mostly-true   \n",
            "27   3910.json    half-true   \n",
            "\n",
            "                                                claim  \\\n",
            "0   Says the Annies List political group supports ...   \n",
            "1   When did the decline of coal start? It started...   \n",
            "2   Hillary Clinton agrees with John McCain \"by vo...   \n",
            "5   The Chicago Bears have had more starting quart...   \n",
            "7   I'm the only person on this stage who has work...   \n",
            "9   Says GOP primary opponents Glenn Grothman and ...   \n",
            "10  For the first time in history, the share of th...   \n",
            "11  Since 2000, nearly 12 million Americans have s...   \n",
            "12  When Mitt Romney was governor of Massachusetts...   \n",
            "14  Most of the (Affordable Care Act) has already ...   \n",
            "15  In this last election in November, ... 63 perc...   \n",
            "16  McCain opposed a requirement that the governme...   \n",
            "18  Water rates in Manila, Philippines, were raise...   \n",
            "19  Almost 100,000 people left Puerto Rico last year.   \n",
            "20  Women and men both are making less when you ad...   \n",
            "22  We just had the best year for the auto industr...   \n",
            "24  Says Mitt Romney wants to get rid of Planned P...   \n",
            "25              I dont know who (Jonathan Gruber) is.   \n",
            "26  Hate crimes against American Muslims and mosqu...   \n",
            "27  Rick Perry has never lost an election and rema...   \n",
            "\n",
            "                                        subject  \\\n",
            "0                                      abortion   \n",
            "1            energy,history,job-accomplishments   \n",
            "2                                foreign-policy   \n",
            "5                                     education   \n",
            "7                                        ethics   \n",
            "9     energy,message-machine-2014,voting-record   \n",
            "10                                    elections   \n",
            "11      economy,jobs,new-hampshire-2012,poverty   \n",
            "12                         history,state-budget   \n",
            "14                                  health-care   \n",
            "15                                    elections   \n",
            "16                               federal-budget   \n",
            "18    financial-regulation,foreign-policy,water   \n",
            "19                bankruptcy,economy,population   \n",
            "20                               economy,income   \n",
            "22                                      economy   \n",
            "24          abortion,federal-budget,health-care   \n",
            "25                                  health-care   \n",
            "26  crime,diversity,homeland-security,terrorism   \n",
            "27                         candidates-biography   \n",
            "\n",
            "                           speaker                  speaker_job_title  \\\n",
            "0                     dwayne-bohac               State representative   \n",
            "1                   scott-surovell                     State delegate   \n",
            "2                     barack-obama                          President   \n",
            "5                        robin-vos         Wisconsin Assembly speaker   \n",
            "7                     barack-obama                          President   \n",
            "9                    duey-stroebel               State representative   \n",
            "10                 robert-menendez                       U.S. Senator   \n",
            "11                        bernie-s                       U.S. Senator   \n",
            "12                     mitt-romney                    Former governor   \n",
            "14                     george-will                          Columnist   \n",
            "15                        bernie-s                       U.S. Senator   \n",
            "16                    barack-obama                          President   \n",
            "18                      gwen-moore  U.S. House member -- 4th District   \n",
            "19                        jack-lew                Treasury secretary    \n",
            "20               dennis-richardson               state representative   \n",
            "22                 hillary-clinton             Presidential candidate   \n",
            "24  planned-parenthood-action-fund                     Advocacy group   \n",
            "25                    nancy-pelosi              House Minority Leader   \n",
            "26                 hillary-clinton             Presidential candidate   \n",
            "27                      ted-nugent                           musician   \n",
            "\n",
            "           state_info party_affiliation  barely_true_counts  false_counts  \\\n",
            "0               Texas        republican                 0.0           1.0   \n",
            "1            Virginia          democrat                 0.0           0.0   \n",
            "2            Illinois          democrat                70.0          71.0   \n",
            "5           Wisconsin        republican                 0.0           3.0   \n",
            "7            Illinois          democrat                70.0          71.0   \n",
            "9           Wisconsin        republican                 0.0           0.0   \n",
            "10         New Jersey          democrat                 1.0           3.0   \n",
            "11            Vermont       independent                18.0          12.0   \n",
            "12      Massachusetts        republican                34.0          32.0   \n",
            "14           Maryland         columnist                 7.0           6.0   \n",
            "15            Vermont       independent                18.0          12.0   \n",
            "16           Illinois          democrat                70.0          71.0   \n",
            "18          Wisconsin          democrat                 3.0           4.0   \n",
            "19  Washington, D.C.           democrat                 0.0           1.0   \n",
            "20             Oregon        republican                 0.0           4.0   \n",
            "22           New York          democrat                40.0          29.0   \n",
            "24   Washington, D.C.              none                 1.0           0.0   \n",
            "25         California          democrat                 3.0           7.0   \n",
            "26           New York          democrat                40.0          29.0   \n",
            "27              Texas        republican                 0.0           0.0   \n",
            "\n",
            "    half_true_counts  mostly_true_counts  pants_on_fire_counts  \\\n",
            "0                0.0                 0.0                   0.0   \n",
            "1                1.0                 1.0                   0.0   \n",
            "2              160.0               163.0                   9.0   \n",
            "5                2.0                 5.0                   1.0   \n",
            "7              160.0               163.0                   9.0   \n",
            "9                0.0                 1.0                   0.0   \n",
            "10               1.0                 3.0                   0.0   \n",
            "11              22.0                41.0                   0.0   \n",
            "12              58.0                33.0                  19.0   \n",
            "14               3.0                 5.0                   1.0   \n",
            "15              22.0                41.0                   0.0   \n",
            "16             160.0               163.0                   9.0   \n",
            "18               4.0                 3.0                   1.0   \n",
            "19               0.0                 1.0                   0.0   \n",
            "20               1.0                 2.0                   0.0   \n",
            "22              69.0                76.0                   7.0   \n",
            "24               0.0                 0.0                   0.0   \n",
            "25              11.0                 2.0                   3.0   \n",
            "26              69.0                76.0                   7.0   \n",
            "27               2.0                 0.0                   2.0   \n",
            "\n",
            "                                         context  \n",
            "0                                       a mailer  \n",
            "1                                a floor speech.  \n",
            "2                                         Denver  \n",
            "5                      a an online opinion-piece  \n",
            "7       a Democratic debate in Philadelphia, Pa.  \n",
            "9                                an online video  \n",
            "10                                      a speech  \n",
            "11                                       a tweet  \n",
            "12                    an interview with CBN News  \n",
            "14                 comments on \"Fox News Sunday\"  \n",
            "15                  a town hall in Austin, Texas  \n",
            "16                                    a radio ad  \n",
            "18                       a congressional hearing  \n",
            "19              an interview with Bloomberg News  \n",
            "20                             a campaign debate  \n",
            "22                   remarks at a Kentucky rally  \n",
            "24                                    a radio ad  \n",
            "25                             a news conference  \n",
            "26  a speech after a terrorist attack in Orlando  \n",
            "27                               an oped column.  \n",
            "\n",
            "Tokenized sentences(6721 sentences, 89464 total tokens) peek:\n",
            "  ['medicare', 'proposal', 'rep.', 'paul', 'ryan', ',', 'r-wis.', ',', 'would', 'allow', 'insurance', 'companies', 'deny', 'coverage', 'drop', 'pre-existing', 'conditions']\n",
            "  ['declaration', 'independence', 'written', 'paper', 'made', 'hemp', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['one-half', 'undocumented', 'workers', 'pay', 'federal', 'income', 'taxes', ',', 'means', 'paying', 'federal', 'income', 'taxes', 'donald', 'trump', 'pays', '.']\n",
            "  ['marijuana', 'today', 'genetically', 'modified', ',', 'thc', 'levels', 'far', 'surpass', 'marijuana', '1970s', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['fewest', 'people', 'employed', 'country', 'since', '1979', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['republican', 'leaders', 'congress', 'pushing', 'make', 'privatizing', 'social', 'security', 'key', 'part', 'legislative', 'agenda', 'win', 'majority', 'congress', 'fall', '.']\n",
            "  ['jeff', 'weems', 'legal', 'record', 'one', 'defending', 'bp', ',', 'enron', 'every', 'big', 'oil', 'company', 'working', 'man', '.', '<PAD>']\n",
            "  ['trump', 'television', 'ad', 'shows', 'mexicans', 'swarming', 'southern', 'border', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', 'president', 'barack', 'obama', 'promised', 'pathway', 'citizenship', 'undocumented', 'immigrants', 'didnt', 'deliver', 'jack', 'squat', 'it', '.', '<PAD>', '<PAD>']\n",
            "  ['university', 'texas', 'starting', 'first', 'medical', 'school', 'major', 'tier', 'one', 'university', 'last', '50', 'years', '.', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['amazing', '5,400', 'jobs', 'lost', 'first', '12', 'months', '(', 'smoking', ')', 'bans', 'implementation', 'ohios', 'hospitality', 'industry', 'alone', '.']\n",
            "  ['dave', 'aronberg', 'first', 'demand', 'bp', 'create', 'billion-dollar', 'fund', 'pay', 'devastation', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['dont', 'understand', 'republicans', 'this', '.', 'idea', '.', 'john', 'mccain', 'introduced', 'cap', 'trade', 'legislation', 'three', 'times', '.', '<PAD>']\n",
            "  ['1.53', 'million', 'jobs', 'risk', '(', 'from', 'potential', 'defense', 'cuts', ')', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['youre', 'mexican', ',', 'get', 'sent', 'back', '.', '...', 'youre', 'noncontiguous', 'country', 'like', 'central', 'american', 'countries', 'stay', 'united']\n",
            "  ['says', 'president', 'obama', 'grown', 'federal', 'government', 'payroll', '141,000', 'workers', 'wants', 'add', '125,000', 'more', '.', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['starting', 'december', '2007', ',', '1.4', 'million', 'jobs', 'created', 'texas', '.', 'period', ',', 'rest', 'country', 'lost', '400,000', 'jobs']\n",
            "  ['billionaires', 'tax', 'rate', 'low', '1', 'percent', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['aids', 'transmitted', 'humans', 'one', 'guy', 'sex', 'monkey', 'started', 'sex', 'men', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['hollywood', 'celebrities', 'endorsing', 'hillary', 'clinton', 'many', 'cases', 'celebrities', 'arent', 'hot', 'anymore', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "\n",
            "10000 unique tokens\n",
            "Unique tokens peek:\n",
            "  wright\n",
            "  no-cost\n",
            "  billionaire\n",
            "  ward\n",
            "  boccieri\n",
            "  crackdown\n",
            "  comparative\n",
            "  screamed\n",
            "  flies\n",
            "  turn\n",
            "  indicators\n",
            "  610\n",
            "  questions\n",
            "  357,000\n",
            "  pull\n",
            "  orientation\n",
            "  42-inch\n",
            "  york.the\n",
            "  implemented\n",
            "  dukakis\n",
            "\n",
            "Final Index Sets(Set_Size = 17, 6721 index sets) peek:\n",
            "  [38, 17, 147, 5, 150, 31, 5014, 2, 8599, 2388, 644, 324, 2, 204, 147, 4314, 260]\n",
            "  [4039, 2611, 23, 5, 5857, 673, 2226, 25, 953, 5858, 5, 230, 22, 5859, 2, 43, 1]\n",
            "  [244, 1356, 3313, 993, 892, 2470, 177, 53, 982, 3, 61, 830, 2, 85, 2197, 333, 1618]\n",
            "  [4, 732, 657, 42, 283, 289, 67, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [3836, 2, 2837, 7119, 96, 49, 279, 2305, 199, 146, 7120, 521, 1219, 7121, 2, 1, 1]\n",
            "  [8155, 22, 18, 1184, 7, 1621, 2376, 420, 141, 8, 2, 1, 1, 1, 1, 1, 1]\n",
            "  [2275, 6, 15, 34, 161, 100, 68, 562, 979, 40, 2, 1, 1, 1, 1, 1, 1]\n",
            "  [150, 6, 744, 3616, 2719, 2362, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [992, 6755, 1118, 98, 1118, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [2977, 1003, 1154, 1839, 1502, 377, 2456, 18, 268, 228, 2, 1, 1, 1, 1, 1, 1]\n",
            "  [59, 4998, 721, 5, 822, 17, 277, 173, 688, 528, 1168, 30, 3616, 69, 2, 1, 1]\n",
            "  [4, 818, 1155, 61, 42, 424, 14, 996, 778, 1050, 262, 380, 2, 1, 1, 1, 1]\n",
            "  [4, 9, 67, 1333, 1222, 5, 6761, 121, 714, 6762, 6763, 216, 83, 12, 2, 1, 1]\n",
            "  [55, 1208, 3, 41, 3, 1067, 817, 118, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [4, 23, 539, 55, 1065, 270, 42, 186, 148, 279, 71, 146, 557, 245, 170, 246, 901]\n",
            "  [938, 3, 58, 14, 3096, 234, 185, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [15, 10, 4, 1994, 1675, 6677, 296, 278, 3, 3298, 4, 195, 927, 3884, 3, 4481, 3]\n",
            "  [94, 164, 175, 12, 226, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [24, 36, 127, 20, 2098, 527, 65, 33, 267, 86, 2, 1, 1, 1, 1, 1, 1]\n",
            "  [309, 3, 103, 4961, 4962, 111, 238, 30, 90, 73, 2, 1, 1, 1, 1, 1, 1]\n",
            "TRAIN SPLIT: 6721 overall samples: torch.Size([6721, 17])\n",
            "The training dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 853 entries, 0 to 1265\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   id                    853 non-null    object\n",
            " 1   label                 853 non-null    object\n",
            " 2   claim                 853 non-null    object\n",
            " 3   subject               853 non-null    object\n",
            " 4   speaker               853 non-null    object\n",
            " 5   speaker_job_title     853 non-null    object\n",
            " 6   state_info            853 non-null    object\n",
            " 7   party_affiliation     853 non-null    object\n",
            " 8   barely_true_counts    853 non-null    int64 \n",
            " 9   false_counts          853 non-null    int64 \n",
            " 10  half_true_counts      853 non-null    int64 \n",
            " 11  mostly_true_counts    853 non-null    int64 \n",
            " 12  pants_on_fire_counts  853 non-null    int64 \n",
            " 13  context               853 non-null    object\n",
            "dtypes: int64(5), object(9)\n",
            "memory usage: 100.0+ KB\n",
            "\n",
            "Data peek:\n",
            "            id        label  \\\n",
            "0   11972.json         true   \n",
            "1   11685.json        false   \n",
            "2   11096.json        false   \n",
            "3    5209.json    half-true   \n",
            "6    7070.json         true   \n",
            "7    1046.json  barely-true   \n",
            "8   12849.json         true   \n",
            "9   13270.json  barely-true   \n",
            "11   2508.json  barely-true   \n",
            "14   8047.json    half-true   \n",
            "16   3331.json   pants-fire   \n",
            "17   9198.json    half-true   \n",
            "18     73.json         true   \n",
            "20  12169.json  mostly-true   \n",
            "21   5244.json    half-true   \n",
            "22  12996.json   pants-fire   \n",
            "25   1084.json         true   \n",
            "26  13231.json  barely-true   \n",
            "28   1051.json         true   \n",
            "29   4161.json  mostly-true   \n",
            "\n",
            "                                                claim  \\\n",
            "0   Building a wall on the U.S.-Mexico border will...   \n",
            "1   Wisconsin is on pace to double the number of l...   \n",
            "2   Says John McCain has done nothing to help the ...   \n",
            "3   Suzanne Bonamici supports a plan that will cut...   \n",
            "6   Says that Tennessee law requires that schools ...   \n",
            "7   Says Vice President Joe Biden \"admits that the...   \n",
            "8   Donald Trump is against marriage equality. He ...   \n",
            "9   We know that more than half of Hillary Clinton...   \n",
            "11  PolitiFact Texas says Congressman Edwards atta...   \n",
            "14       On residency requirements for public workers   \n",
            "16  Unfortunately we have documented instances whe...   \n",
            "17  A recent Gallup poll found that 72 percent of ...   \n",
            "18  Each year, 18,000 people die in America becaus...   \n",
            "20  There have not been any public safety issues i...   \n",
            "21  Says Mitt Romney was one of the first national...   \n",
            "22  The number of illegal immigrants could be 3 mi...   \n",
            "25  Now, there was a time when someone like Scalia...   \n",
            "26  I was gone when there was a red line against S...   \n",
            "28  Contends that President Obama literally said (...   \n",
            "29  Active duty males in the military are twice as...   \n",
            "\n",
            "                                              subject  \\\n",
            "0                                         immigration   \n",
            "1                                                jobs   \n",
            "2                     military,veterans,voting-record   \n",
            "3   medicare,message-machine-2012,campaign-adverti...   \n",
            "6     county-budget,county-government,education,taxes   \n",
            "7                                    economy,stimulus   \n",
            "8                          gays-and-lesbians,marriage   \n",
            "9                                      foreign-policy   \n",
            "11                             ethics,message-machine   \n",
            "14           city-government,county-government,unions   \n",
            "16                                 labor,state-budget   \n",
            "17  government-efficiency,government-regulation,polls   \n",
            "18                                        health-care   \n",
            "20                  crime,gays-and-lesbians,sexuality   \n",
            "21                                          elections   \n",
            "22                                        immigration   \n",
            "25                 sotomayor-nomination,supreme-court   \n",
            "26                                     foreign-policy   \n",
            "28                              climate-change,energy   \n",
            "29                               health-care,military   \n",
            "\n",
            "                     speaker                           speaker_job_title  \\\n",
            "0                 rick-perry                                    Governor   \n",
            "1          katrina-shankland                        State representative   \n",
            "2               donald-trump                             President-Elect   \n",
            "3              rob-cornilles                                  consultant   \n",
            "6   stand-children-tennessee  Child and education advocacy organization.   \n",
            "7               john-boehner     Speaker of the House of Representatives   \n",
            "8       sean-patrick-maloney                       Congressman for NY-18   \n",
            "9                 mike-pence                                    Governor   \n",
            "11               bill-flores                                 Businessman   \n",
            "14               chris-abele                              Philanthropist   \n",
            "16               tom-niehaus                President of the Ohio Senate   \n",
            "17          marsha-blackburn                         U.S. Representative   \n",
            "18           hillary-clinton                      Presidential candidate   \n",
            "20                chris-sgro             Executive Director, Equality NC   \n",
            "21               marco-rubio                                U.S. Senator   \n",
            "22              donald-trump                             President-Elect   \n",
            "25            lindsey-graham                                U.S. senator   \n",
            "26           hillary-clinton                      Presidential candidate   \n",
            "28                mike-pence                                    Governor   \n",
            "29             cliff-stearns                         U.S. representative   \n",
            "\n",
            "        state_info party_affiliation  barely_true_counts  false_counts  \\\n",
            "0            Texas        republican                  30            30   \n",
            "1        Wisconsin          democrat                   2             1   \n",
            "2         New York        republican                  63           114   \n",
            "3           Oregon        republican                   1             1   \n",
            "6        Tennessee              none                   0             0   \n",
            "7             Ohio        republican                  13            22   \n",
            "8         New York          democrat                   0             0   \n",
            "9          Indiana        republican                   8            10   \n",
            "11           Texas        republican                   2             0   \n",
            "14       Wisconsin              none                   3             5   \n",
            "16            Ohio        republican                   0             0   \n",
            "17       Tennessee        republican                   2             2   \n",
            "18        New York          democrat                  40            29   \n",
            "20  North Carolina          activist                   0             1   \n",
            "21         Florida        republican                  33            24   \n",
            "22        New York        republican                  63           114   \n",
            "25  South Carolina        republican                   2             2   \n",
            "26        New York          democrat                  40            29   \n",
            "28         Indiana        republican                   8            10   \n",
            "29         Florida        republican                   0             0   \n",
            "\n",
            "    half_true_counts  mostly_true_counts  pants_on_fire_counts  \\\n",
            "0                 42                  23                    18   \n",
            "1                  0                   0                     0   \n",
            "2                 51                  37                    61   \n",
            "3                  3                   1                     1   \n",
            "6                  0                   0                     0   \n",
            "7                 11                   4                     2   \n",
            "8                  0                   0                     0   \n",
            "9                 12                   5                     0   \n",
            "11                 0                   0                     0   \n",
            "14                 4                   4                     2   \n",
            "16                 0                   0                     1   \n",
            "17                 1                   0                     0   \n",
            "18                69                  76                     7   \n",
            "20                 0                   1                     0   \n",
            "21                32                  35                     5   \n",
            "22                51                  37                    61   \n",
            "25                 4                   2                     0   \n",
            "26                69                  76                     7   \n",
            "28                12                   5                     0   \n",
            "29                 2                   1                     0   \n",
            "\n",
            "                                              context  \n",
            "0                                     Radio interview  \n",
            "1                                   a news conference  \n",
            "2                        comments on ABC's This Week.  \n",
            "3                                        a radio show  \n",
            "6                              in a post on Facebook.  \n",
            "7                                    a press release.  \n",
            "8      a speech at the Democratic National Convention  \n",
            "9                        comments on \"Meet the Press\"  \n",
            "11                                           a TV ad.  \n",
            "14                                           a letter  \n",
            "16                          interviews with reporters  \n",
            "17    a speech to the Freedom Summit in New Hampshire  \n",
            "18                      a speech in Des Moines, Iowa.  \n",
            "20  a speech urging Charlotte's anti-discriminatio...  \n",
            "21                               a prepared statement  \n",
            "22                         a speech in Phoenix, Ariz.  \n",
            "25                                   a Senate hearing  \n",
            "26                     the second presidential debate  \n",
            "28                                   MSNBC interview.  \n",
            "29                                    a press release  \n",
            "\n",
            "Tokenized sentences(853 sentences, 11396 total tokens) peek:\n",
            "  ['reason', 'even', 'colleges', 'point', 'politicians', 'said', ',', 'know', 'what', '?', 'start', 'colleges', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['economy', 'growing', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['obamacare', 'drive', '2.5', 'million', 'americans', 'workforce', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['eighty', 'percent', 'wall', 'street', 'executives', 'spouses', 'donations', 'go', 'democrats', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', '315,000', 'mostly', 'minority', 'texas', 'students', 'enrolled', 'failing', 'schools', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['small', 'business', 'receiving', 'federal', 'tax', 'credit', '``', 'to', 'provide', 'health', 'care', 'employees', '...', 'would', 'barred', 'stupak', 'amendment']\n",
            "  ['months', 'ago', 'asked', 'whats', 'biggest', 'geopolitical', 'threat', 'facing', 'america', ',', 'said', 'russia', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['reason', 'even', 'colleges', 'point', 'politicians', 'said', ',', 'know', 'what', '?', 'start', 'colleges', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', 'obama', 'administration', 'approved', 'major', 'disaster', 'declaration', 'oklahoma', '2009', ',', 'nine', 'states', '77', 'counties', 'burned', 'three', 'days']\n",
            "  ['98', 'percent', 'small', 'businesses', 'make', 'less', '$', '250,000', 'would', 'see', 'tax', 'increase', 'barack', 'obama', \"'s\", 'plan', '.']\n",
            "  ['im', 'running', 'office', 'much', 'experience', 'qualifications', 'barack', 'obama', 'ran', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['10', 'years', ',', '(', 'extending', 'tax', 'cuts', 'wealthy', ')', 'would', 'add', 'less', 'deficit', 'obama', 'added', 'stimulus', 'one']\n",
            "  ['mailer', 'obama', 'campaign', '``', 'accurately', 'indicates', \"''\", 'hillary', 'clinton', 'would', '``', 'force', 'uninsured', 'people', 'buy', 'insurance', ',']\n",
            "  ['high', 'school', 'dropout', 'makes', 'average', '$', '19,000', 'year', ',', 'high', 'school', 'graduate', 'makes', '$', '28,000', 'year', ',']\n",
            "  ['mayor', 'tom', 'barrett', ',', 'number', 'milwaukee', 'police', 'officers', 'increased', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['bill', 'earl', 'blumenauer', 'would', 'mandate', 'gps', 'tracking', 'devices', 'vehicles', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', '80', 'percent', 'health', 'care', 'dollars', 'spent', '20', 'percent', 'population', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['mortgage', 'buyback', 'plan', '``', 'my', 'proposal', ',', 'sen.', 'obama', \"'s\", 'proposal', ',', 'president', 'bush', \"'s\", 'proposal', '.']\n",
            "  ['says', 'bruce', 'starr', 'broke', 'law', 'letting', 'lobbyists', 'wine', 'dine', 'maui', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['president', 'barack', 'obamas', 'health', 'care', 'reform', 'slashed', '$', '500', 'billion', 'medicare', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "\n",
            "3231 unique tokens\n",
            "Unique tokens peek:\n",
            "  drugs\n",
            "  =\n",
            "  contends\n",
            "  delegates\n",
            "  fast\n",
            "  science\n",
            "  increase\n",
            "  sports\n",
            "  dominate\n",
            "  [\n",
            "  federally\n",
            "  raeses\n",
            "  ran\n",
            "  wear\n",
            "  idea\n",
            "  overturned\n",
            "  trailing\n",
            "  arent\n",
            "  four\n",
            "  opium\n",
            "\n",
            "Final Index Sets(Set_Size = 17, 853 index sets) peek:\n",
            "  [4, 669, 6, 195, 2513, 395, 120, 2514, 998, 13, 782, 783, 2, 1, 1, 1, 1]\n",
            "  [319, 184, 3, 61, 68, 3, 2039, 2040, 6, 52, 713, 2041, 44, 38, 34, 479, 17]\n",
            "  [1352, 173, 5, 574, 118, 201, 3, 274, 575, 3, 420, 575, 3, 171, 1353, 1354, 2]\n",
            "  [1164, 342, 311, 1161, 463, 5, 46, 15, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [212, 63, 918, 377, 94, 5, 3009, 12, 3, 212, 63, 1174, 377, 5, 3010, 12, 3]\n",
            "  [2487, 870, 7, 116, 150, 34, 76, 309, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [13, 2313, 2314, 190, 138, 2315, 1164, 139, 62, 188, 1165, 7, 2, 1, 1, 1, 1]\n",
            "  [8, 915, 9, 2599, 152, 75, 661, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [4, 234, 207, 176, 2887, 2888, 157, 7, 1188, 177, 2889, 2890, 2891, 492, 1202, 1253, 2892]\n",
            "  [31, 12, 322, 47, 3, 7, 159, 2444, 2445, 29, 3, 2446, 1204, 29, 657, 509, 2]\n",
            "  [4, 2062, 1093, 603, 1094, 741, 2063, 2064, 2065, 2, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [68, 225, 443, 362, 310, 6, 2, 271, 2912, 126, 188, 222, 1014, 2913, 2914, 2, 1]\n",
            "  [4, 61, 179, 262, 1679, 85, 1680, 1681, 978, 61, 1682, 263, 1683, 2, 1, 1, 1]\n",
            "  [4, 58, 1496, 45, 885, 6, 65, 26, 10, 2, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [2691, 292, 2692, 2693, 2694, 2695, 16, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [137, 2, 717, 2488, 270, 2489, 879, 1208, 2490, 11, 632, 760, 5, 1184, 15, 910, 7]\n",
            "  [1715, 180, 992, 680, 863, 345, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [144, 5, 976, 12, 224, 356, 63, 494, 144, 5, 977, 12, 224, 233, 1678, 2, 1]\n",
            "  [1195, 81, 22, 3155, 5, 381, 15, 12, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [59, 65, 129, 96, 374, 70, 3055, 480, 38, 107, 2, 1, 1, 1, 1, 1, 1]\n",
            "TEST SPLIT: 853 overall samples: torch.Size([853, 17])\n",
            "The training dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 861 entries, 0 to 1283\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Non-Null Count  Dtype \n",
            "---  ------                --------------  ----- \n",
            " 0   id                    861 non-null    object\n",
            " 1   label                 861 non-null    object\n",
            " 2   claim                 861 non-null    object\n",
            " 3   subject               861 non-null    object\n",
            " 4   speaker               861 non-null    object\n",
            " 5   speaker_job_title     861 non-null    object\n",
            " 6   state_info            861 non-null    object\n",
            " 7   party_affiliation     861 non-null    object\n",
            " 8   barely_true_counts    861 non-null    int64 \n",
            " 9   false_counts          861 non-null    int64 \n",
            " 10  half_true_counts      861 non-null    int64 \n",
            " 11  mostly_true_counts    861 non-null    int64 \n",
            " 12  pants_on_fire_counts  861 non-null    int64 \n",
            " 13  context               861 non-null    object\n",
            "dtypes: int64(5), object(9)\n",
            "memory usage: 100.9+ KB\n",
            "\n",
            "Data peek:\n",
            "            id        label  \\\n",
            "0   12134.json  barely-true   \n",
            "2    7891.json        false   \n",
            "3    8169.json    half-true   \n",
            "4     929.json    half-true   \n",
            "5    9416.json        false   \n",
            "6    6861.json         true   \n",
            "8   13138.json         true   \n",
            "10  12803.json    half-true   \n",
            "12   7313.json    half-true   \n",
            "13   4809.json         true   \n",
            "14   1671.json  barely-true   \n",
            "15   4348.json    half-true   \n",
            "16   6225.json    half-true   \n",
            "17   7675.json  mostly-true   \n",
            "18   2255.json  barely-true   \n",
            "21  10337.json   pants-fire   \n",
            "25  10385.json  barely-true   \n",
            "26   5373.json  mostly-true   \n",
            "28  10376.json  mostly-true   \n",
            "29   8031.json   pants-fire   \n",
            "\n",
            "                                                claim  \\\n",
            "0   We have less Americans working now than in the...   \n",
            "2   Says Having organizations parading as being so...   \n",
            "3      Says nearly half of Oregons children are poor.   \n",
            "4   On attacks by Republicans that various program...   \n",
            "5   Says when armed civilians stop mass shootings ...   \n",
            "6   Says Tennessee is providing millions of dollar...   \n",
            "8   Says Donald Trump started his career back in 1...   \n",
            "10  John McCains chief economic adviser during the...   \n",
            "12  State revenue projections have missed the mark...   \n",
            "13  The median income of a middle class family wen...   \n",
            "14  Every citizen is entitled to the freedom of sp...   \n",
            "15  Rick Perry has advocated abandoning Social Sec...   \n",
            "16  Two thirds to three quarters of people without...   \n",
            "17  Congress has spent 66 of the first 100 days of...   \n",
            "18  Mark Sharpe has lowered property taxes by 17 p...   \n",
            "21  Says President Barack Obama has said that ever...   \n",
            "25  If people work and make more money, they lose ...   \n",
            "26  We are poised to get rid of over 1,000 more re...   \n",
            "28  Administrative employees at colleges and unive...   \n",
            "29  Private prison systems are calculating how man...   \n",
            "\n",
            "                                      subject            speaker  \\\n",
            "0                                economy,jobs     vicky-hartzler   \n",
            "2             campaign-finance,congress,taxes    earl-blumenauer   \n",
            "3                                     poverty    jim-francesconi   \n",
            "4                            economy,stimulus       barack-obama   \n",
            "5                                        guns         jim-rubens   \n",
            "6                      education,state-budget         andy-berke   \n",
            "8      candidates-biography,diversity,housing    hillary-clinton   \n",
            "10                                    economy          tim-kaine   \n",
            "12                               state-budget       steve-henson   \n",
            "13                  income,new-hampshire-2012          joe-biden   \n",
            "14                          gays-and-lesbians     david-dewhurst   \n",
            "15             medicaid,social-security,taxes   margaret-carlson   \n",
            "16  health-care,poverty,public-health,welfare  elizabeth-roberts   \n",
            "17                                   congress        john-barrow   \n",
            "18                 candidates-biography,taxes        mark-sharpe   \n",
            "21        civil-rights,crime,criminal-justice      rudy-giuliani   \n",
            "25                            poverty,welfare        marco-rubio   \n",
            "26    government-regulation,market-regulation         rick-scott   \n",
            "28                                  education        marco-rubio   \n",
            "29                            crime,education      kathleen-ford   \n",
            "\n",
            "                                speaker_job_title            state_info  \\\n",
            "0                             U.S. Representative              Missouri   \n",
            "2                             U.S. representative                Oregon   \n",
            "3   Member of the State Board of Higher Education                Oregon   \n",
            "4                                       President              Illinois   \n",
            "5                            Small business owner         New Hampshire   \n",
            "6                        Lawyer and state senator             Tennessee   \n",
            "8                          Presidential candidate              New York   \n",
            "10                                   U.S. Senator              Virginia   \n",
            "12                                  State Senator               Georgia   \n",
            "13                                   U.S. senator              Delaware   \n",
            "14                            Lieutenant governor                 Texas   \n",
            "15                                      Columnist  District of Columbia   \n",
            "16                            Lieutenant Governor          Rhode Island   \n",
            "17                                    Congressman               Georgia   \n",
            "18               Hillsborough County commissioner               Florida   \n",
            "21                                       Attorney              New York   \n",
            "25                                   U.S. Senator               Florida   \n",
            "26                                       Governor               Florida   \n",
            "28                                   U.S. Senator               Florida   \n",
            "29                                       Attorney               Florida   \n",
            "\n",
            "   party_affiliation  barely_true_counts  false_counts  half_true_counts  \\\n",
            "0         republican                   1             0                 1   \n",
            "2           democrat                   0             1                 1   \n",
            "3               none                   0             1                 1   \n",
            "4           democrat                  70            71               160   \n",
            "5         republican                   1             1                 0   \n",
            "6           democrat                   0             0                 0   \n",
            "8           democrat                  40            29                69   \n",
            "10          democrat                   8             3                15   \n",
            "12          democrat                   0             0                 1   \n",
            "13          democrat                  11            10                21   \n",
            "14        republican                   8             8                10   \n",
            "15              none                   0             0                 1   \n",
            "16          democrat                   1             0                 2   \n",
            "17          democrat                   0             0                 1   \n",
            "18        republican                   1             0                 0   \n",
            "21        republican                   9            11                10   \n",
            "25        republican                  33            24                32   \n",
            "26        republican                  28            23                38   \n",
            "28        republican                  33            24                32   \n",
            "29          democrat                   0             1                 0   \n",
            "\n",
            "    mostly_true_counts  pants_on_fire_counts  \\\n",
            "0                    0                     0   \n",
            "2                    1                     0   \n",
            "3                    1                     0   \n",
            "4                  163                     9   \n",
            "5                    1                     0   \n",
            "6                    0                     0   \n",
            "8                   76                     7   \n",
            "10                  15                     0   \n",
            "12                   0                     0   \n",
            "13                  16                     4   \n",
            "14                   5                     5   \n",
            "15                   0                     0   \n",
            "16                   0                     0   \n",
            "17                   1                     0   \n",
            "18                   0                     0   \n",
            "21                   7                     3   \n",
            "25                  35                     5   \n",
            "26                  34                     7   \n",
            "28                  35                     5   \n",
            "29                   1                     1   \n",
            "\n",
            "                                              context  \n",
            "0                        an interview with ABC17 News  \n",
            "2                       a U.S. Ways and Means hearing  \n",
            "3                                  an opinion article  \n",
            "4                             interview with CBS News  \n",
            "5         in an interview at gun shop in Hudson, N.H.  \n",
            "6   a letter to state Senate education committee c...  \n",
            "8                       the first presidential debate  \n",
            "10  a speech at the Democratic National Convention...  \n",
            "12                                    a press release  \n",
            "13  speaking at New Hampshires Plymouth State Uni...  \n",
            "14                                    a press release  \n",
            "15                                 a politics column.  \n",
            "16        a panel discussion on \"A Lively Experiment\"  \n",
            "17                                           a letter  \n",
            "18                                  a campaign mailer  \n",
            "21                           an interview on Fox News  \n",
            "25                        his book, \"American Dreams\"  \n",
            "26                                     speech at CPAC  \n",
            "28                                             a book  \n",
            "29                                    a mayoral forum  \n",
            "\n",
            "Tokenized sentences(861 sentences, 11524 total tokens) peek:\n",
            "  ['says', 'pennsylvania', 'charges', 'top', 'income', 'tax', 'rate', '3', 'percent', 'delaware', 'state', 'income', 'tax', 'all', '.', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['david', 'dewhurst', 'currently', 'owes', '$', '1', 'million', 'vendors', 'services', 'provided', '2012', 'u.s.', 'senate', 'campaign', '.', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['sen.', 'obama', \"'s\", 'chair', 'new', 'hampshire', 'lobbyist', '.', 'lobbies', 'drug', 'companies', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['manufactured', '5-million', '2.5-meg', 'windmills', 'across', 'country', ',', 'could', 'electrify', 'entire', 'nation', 'entire', 'nation', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['mitt', 'romney', 'proposed', 'cutting', 'taxes', 'raising', '18', 'million', 'working', 'families', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', 'hillary', 'clinton', 'abided', 'ethics', 'agreement', 'clinton', 'foundation', 'obama', 'administration', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', 'apple', 'ceo', 'steve', 'jobs', 'told', 'president', 'obama', 'company', 'moved', 'factories', 'china', 'needed', '30,000', 'engineers', '.', '<PAD>', '<PAD>']\n",
            "  ['hillary', '(', 'clinton', ')', ',', 'one', 'time', 'late', 'night', 'exhausted', ',', 'misstated', 'immediately', 'apologized', 'it', ',', 'happened', 'bosnia']\n",
            "  ['closing', 'states', 'wage', 'gap', 'would', 'make', '$', '9', 'billion', 'difference', 'missouri', 'women', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['average', 'federal', 'employee', 'makes', '$', '120,000', 'year', '.', 'average', 'private', 'employee', 'makes', '$', '60,000', 'year', '.', '<PAD>', '<PAD>']\n",
            "  ['john', 'boehner', 'said', 'jobs', 'teachers', 'nurses', 'police', 'officers', 'firefighters', 'government', 'jobs', 'werent', 'worth', 'saving', '.', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['bill', 'mccollum', 'voted', 'higher', 'taxes', 'fees', '42', 'times', 'congress', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['even', 'advocate', 'wiping', '401', '(', 'k', ')', 's', 'entirely', 'replacing', 'government-run', 'accounts', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['past', 'eight', 'years', ',', 'united', 'states', 'reduced', 'total', 'carbon', 'pollution', 'nation', 'earth', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['obamacare', ',', 'turn', 'medicaid', 'states', '...', 'money', 'available', 'front', 'expansion', 'years', '.', 'then', ',', 'money', 'go', 'away', '.']\n",
            "  ['obama', \"'s\", 'one', 'education', 'accomplishment', '``', 'legislation', 'teach', \"'comprehensive\", 'sex', 'education', \"'\", 'kindergartners', '.', \"''\", '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['national', 'poll', 'found', 'among', 'people', 'oppose', 'democratic', 'health', 'care', 'reform', 'bill', ',', '``', 'almost', '40', 'percent', \"''\", 'opposed']\n",
            "  ['social', 'security', 'never', 'contributed', 'one', 'cent', 'deficit', '.', 'one', 'cent', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', 'republican', 'hasnt', '[', 'an', 'election', ']', 'presidency', 'new', 'jersey', 'since', '1988', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', 'new', 'years', 'day', 'fiscal', 'cliff', 'deal', 'reduces', 'deficit', '.', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "\n",
            "3390 unique tokens\n",
            "Unique tokens peek:\n",
            "  prisons\n",
            "  youngsters\n",
            "  instead\n",
            "  maternal\n",
            "  kaine\n",
            "  prohibited\n",
            "  have\n",
            "  maps\n",
            "  el\n",
            "  florida\n",
            "  back\n",
            "  rhode\n",
            "  tearing\n",
            "  complaints\n",
            "  18,000\n",
            "  2013\n",
            "  powers\n",
            "  consulates\n",
            "  bernie\n",
            "  accounts\n",
            "\n",
            "Final Index Sets(Set_Size = 18, 861 index sets) peek:\n",
            "  [7, 2598, 8, 30, 3, 62, 7, 58, 8, 326, 367, 238, 5, 1189, 2, 1, 1, 1]\n",
            "  [4, 2819, 1272, 2820, 652, 102, 80, 37, 475, 207, 710, 48, 1274, 2821, 2, 1, 1, 1]\n",
            "  [2173, 262, 1134, 164, 22, 396, 2174, 2175, 178, 2176, 219, 2, 1, 1, 1, 1, 1, 1]\n",
            "  [946, 12, 145, 29, 69, 366, 426, 1722, 1723, 2, 174, 3, 77, 6, 82, 1724, 1725, 409]\n",
            "  [2248, 50, 538, 2249, 117, 6, 15, 152, 169, 142, 112, 37, 2, 1, 1, 1, 1, 1]\n",
            "  [2038, 14, 1080, 237, 716, 641, 640, 717, 3, 127, 183, 661, 2039, 161, 245, 195, 718, 2040]\n",
            "  [780, 2639, 219, 14, 2640, 1029, 398, 1248, 192, 2641, 217, 2, 1, 1, 1, 1, 1, 1]\n",
            "  [2282, 40, 143, 688, 252, 2283, 2284, 25, 17, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [181, 690, 130, 131, 1145, 255, 126, 2247, 1145, 2, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [312, 2194, 2195, 2196, 533, 527, 20, 2197, 387, 2198, 2, 33, 1, 1, 1, 1, 1, 1]\n",
            "  [4, 42, 280, 1856, 1857, 1858, 41, 1859, 642, 22, 142, 74, 569, 37, 79, 3, 1860, 6]\n",
            "  [39, 72, 75, 685, 1198, 147, 81, 5, 274, 19, 24, 70, 121, 16, 38, 10, 54, 2]\n",
            "  [169, 656, 657, 1297, 3, 58, 92, 200, 255, 652, 302, 512, 6, 13, 88, 743, 3, 9]\n",
            "  [36, 1352, 1353, 805, 806, 3, 18, 128, 268, 53, 558, 1354, 1355, 1356, 1357, 1358, 559, 223]\n",
            "  [2179, 7, 76, 1135, 2180, 209, 1136, 8, 101, 2181, 2182, 953, 2183, 93, 2, 1, 1, 1]\n",
            "  [2950, 45, 224, 191, 588, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [313, 2835, 9, 39, 2836, 188, 2837, 2838, 488, 400, 2839, 2840, 783, 408, 2841, 1279, 188, 313]\n",
            "  [538, 712, 762, 2388, 138, 2389, 79, 128, 282, 3, 37, 228, 545, 32, 2390, 2, 1, 1]\n",
            "  [32, 94, 70, 197, 749, 25, 17, 3, 175, 143, 3335, 11, 2, 1, 1, 1, 1, 1]\n",
            "  [2248, 50, 538, 2249, 117, 6, 15, 152, 169, 142, 112, 37, 2, 1, 1, 1, 1, 1]\n",
            "VALID SPLIT: 861 overall samples: torch.Size([861, 18])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_labels = np.unique(train_input_labels)\n",
        "print(len(unique_labels), \"labels\")\n",
        "train_label_counts = pd.DataFrame({\"label\": unique_labels})[\"label\"].value_counts(normalize=True)\n",
        "print(train_label_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QamRMQfAeu9R",
        "outputId": "babe6354-6f02-4bfe-9abb-dfac106a5d1f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6 labels\n",
            "label\n",
            "barely-true    0.166667\n",
            "false          0.166667\n",
            "half-true      0.166667\n",
            "mostly-true    0.166667\n",
            "pants-fire     0.166667\n",
            "true           0.166667\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MisInformationDetectionLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
        "\n",
        "    super(MisInformationDetectionLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size+1, embed_size, padding_idx=0)\n",
        "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first= True, dropout = dropout)\n",
        "    self.fc = nn.Linear(hidden_size, output_size)\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    _, (hidden, _) = self.lstm(embedded)\n",
        "    output = self.fc(hidden[-1])\n",
        "    return self.softmax(output)\n",
        "\n",
        "vocab_size = len(token_to_idx)\n",
        "embed_size = 256\n",
        "input_size = embed_size\n",
        "print(vocab_size)\n",
        "\n",
        "hidden_size = 256\n",
        "output_size = len(unique_labels)\n",
        "num_layers = 3\n",
        "dropout = 0.2\n",
        "print(output_size)\n",
        "\n",
        "model = MisInformationDetectionLSTM(input_size, hidden_size, output_size, num_layers, dropout)\n",
        "\n",
        "'''\n",
        "counter_data = Counter(input_label)\n",
        "class_counts = list(counter_data.values())\n",
        "print(class_counts)\n",
        "\n",
        "\n",
        "class_counts = train_label_counts\n",
        "class_weights = 1./ torch.tensor(class_counts, dtype = torch.float)\n",
        "print(class_weights)\n",
        "#criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight = torch.tensor([1.5, 1.2,1.0]))'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs = 10):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss+=loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Training loss : {total_loss/ len(train_loader):.4f}')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    correct, total = 0,0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for inputs, labels in valid_loader:\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct+= (predicted == labels).sum().item()\n",
        "    print(f\"Validation Accuracy: {correct / total* 100:.2f}%\")\n",
        "\n",
        "train_model(model, train_loader, valid_loader, criterion, optimizer, epochs = 30)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names = unique_labels))\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "evaluate_model(model, valid_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfJXHEHMZtHd",
        "outputId": "aade10e7-9501-4260-d210-6bbc318af28f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "6\n",
            "Epoch 1, Training loss : 1.7755\n",
            "Validation Accuracy: 21.72%\n",
            "Epoch 2, Training loss : 1.7669\n",
            "Validation Accuracy: 22.88%\n",
            "Epoch 3, Training loss : 1.7638\n",
            "Validation Accuracy: 21.84%\n",
            "Epoch 4, Training loss : 1.7560\n",
            "Validation Accuracy: 21.25%\n",
            "Epoch 5, Training loss : 1.7372\n",
            "Validation Accuracy: 21.02%\n",
            "Epoch 6, Training loss : 1.7149\n",
            "Validation Accuracy: 22.07%\n",
            "Epoch 7, Training loss : 1.6840\n",
            "Validation Accuracy: 20.09%\n",
            "Epoch 8, Training loss : 1.6563\n",
            "Validation Accuracy: 21.95%\n",
            "Epoch 9, Training loss : 1.6289\n",
            "Validation Accuracy: 19.74%\n",
            "Epoch 10, Training loss : 1.6055\n",
            "Validation Accuracy: 21.95%\n",
            "Epoch 11, Training loss : 1.5855\n",
            "Validation Accuracy: 20.79%\n",
            "Epoch 12, Training loss : 1.5606\n",
            "Validation Accuracy: 20.79%\n",
            "Epoch 13, Training loss : 1.5493\n",
            "Validation Accuracy: 19.74%\n",
            "Epoch 14, Training loss : 1.5455\n",
            "Validation Accuracy: 19.74%\n",
            "Epoch 15, Training loss : 1.5293\n",
            "Validation Accuracy: 19.86%\n",
            "Epoch 16, Training loss : 1.5150\n",
            "Validation Accuracy: 18.82%\n",
            "Epoch 17, Training loss : 1.5297\n",
            "Validation Accuracy: 18.93%\n",
            "Epoch 18, Training loss : 1.4985\n",
            "Validation Accuracy: 19.28%\n",
            "Epoch 19, Training loss : 1.4787\n",
            "Validation Accuracy: 19.16%\n",
            "Epoch 20, Training loss : 1.4799\n",
            "Validation Accuracy: 20.67%\n",
            "Epoch 21, Training loss : 1.4691\n",
            "Validation Accuracy: 19.74%\n",
            "Epoch 22, Training loss : 1.4579\n",
            "Validation Accuracy: 19.05%\n",
            "Epoch 23, Training loss : 1.4556\n",
            "Validation Accuracy: 18.93%\n",
            "Epoch 24, Training loss : 1.4602\n",
            "Validation Accuracy: 19.05%\n",
            "Epoch 25, Training loss : 1.4490\n",
            "Validation Accuracy: 18.70%\n",
            "Epoch 26, Training loss : 1.4625\n",
            "Validation Accuracy: 19.40%\n",
            "Epoch 27, Training loss : 1.4527\n",
            "Validation Accuracy: 20.21%\n",
            "Epoch 28, Training loss : 1.4460\n",
            "Validation Accuracy: 18.82%\n",
            "Epoch 29, Training loss : 1.4434\n",
            "Validation Accuracy: 19.16%\n",
            "Epoch 30, Training loss : 1.4449\n",
            "Validation Accuracy: 20.33%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            " barely-true       0.19      0.06      0.09       154\n",
            "       false       0.23      0.22      0.22       175\n",
            "   half-true       0.27      0.30      0.28       175\n",
            " mostly-true       0.20      0.19      0.19       173\n",
            "  pants-fire       0.00      0.00      0.00        60\n",
            "        true       0.15      0.35      0.21       124\n",
            "\n",
            "    accuracy                           0.20       861\n",
            "   macro avg       0.17      0.19      0.17       861\n",
            "weighted avg       0.20      0.20      0.19       861\n",
            "\n",
            "[[ 9 27 34 30  0 54]\n",
            " [ 6 38 30 38  0 63]\n",
            " [13 38 52 25  0 47]\n",
            " [10 32 35 33  0 63]\n",
            " [ 1 10 18 13  0 18]\n",
            " [ 8 19 26 28  0 43]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MisInformationDetectionBiLSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, output_size, num_layers = 1, dropout = 0.2):\n",
        "\n",
        "    super(MisInformationDetectionBiLSTM, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size+1, embed_size, padding_idx=0)\n",
        "    self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first= True, dropout = dropout, bidirectional = True)\n",
        "    self.fc = nn.Linear(hidden_size *2, output_size)\n",
        "    self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    embedded = self.embedding(x)\n",
        "    _, (hidden, _) = self.bilstm(embedded)\n",
        "    hidden = torch.cat((hidden[-2], hidden[-1]), dim = 1)\n",
        "    output = self.fc(hidden)\n",
        "    return self.softmax(output)\n",
        "\n",
        "model_bilstm = MisInformationDetectionBiLSTM(input_size, hidden_size, output_size, num_layers, dropout)\n",
        "\n",
        "train_model(model_bilstm, train_loader, valid_loader, criterion, optimizer, epochs = 30)\n",
        "\n",
        "evaluate_model(model_bilstm, valid_loader)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SWuuTnup1WL",
        "outputId": "205cf16c-4b4c-46ea-8959-323bea046e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss : 1.7920\n",
            "Validation Accuracy: 20.56%\n",
            "Epoch 2, Training loss : 1.7920\n",
            "Validation Accuracy: 20.56%\n",
            "Epoch 3, Training loss : 1.7920\n",
            "Validation Accuracy: 20.56%\n",
            "Epoch 4, Training loss : 1.7920\n",
            "Validation Accuracy: 20.56%\n",
            "Epoch 5, Training loss : 1.7920\n",
            "Validation Accuracy: 20.56%\n",
            "Epoch 6, Training loss : 1.7920\n",
            "Validation Accuracy: 20.56%\n",
            "Epoch 7, Training loss : 1.7920\n",
            "Validation Accuracy: 20.56%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "ZSiUSiQyyrv7"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}