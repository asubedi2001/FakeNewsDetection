{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0tD5KQCTwbw3CWe5taByW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asubedi2001/FakeNewsDetection/blob/BiLSTM/BiLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk --upgrade --force-reinstall"
      ],
      "metadata": {
        "id": "3g-a6gmS2FXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# Constants\n",
        "PEEK = 10\n",
        "EMBED_SIZE = 128\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_LAYERS = 3\n",
        "DROPOUT = 0.2\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 30\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Helper function to print a peek of a list-like object\n",
        "def present_list_like(name, list_like, peek=PEEK):\n",
        "    print(f\"{name} peek:\")\n",
        "    print('  ' + '\\n  '.join([str(seq) for seq in list_like[0:peek]]) )\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "  df = pd.read_csv('covid_lies.csv')  # Make sure 'covid_lies.csv' is in the correct path\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'covid_lies.csv' not found. Please ensure it is in the same directory or provide the correct path.\")\n",
        "    exit()\n",
        "print(\"The dataset:\")\n",
        "df.info()\n",
        "print(\"\\nData peek:\")\n",
        "print(df.head(PEEK))\n",
        "print()\n",
        "\n",
        "# Separate out text data and labels\n",
        "input_text = df['misconception'].to_numpy()\n",
        "input_labels = df['label'].to_numpy()\n",
        "print(\"Unique labels:\", np.unique(input_labels))\n",
        "\n",
        "\n",
        "# Text preprocessing function (precompiled regex)\n",
        "RE_SPACE = re.compile(r'\\s+')\n",
        "RE_URL = re.compile(r'http\\S+')\n",
        "\n",
        "def preprocess_text(text)->str:\n",
        "    text = text.lower()\n",
        "    valid_asciis = {9, *range(32, 127)}\n",
        "    text = ''.join(filter(lambda x: ord(x) in valid_asciis, text))\n",
        "    text = RE_SPACE.sub(' ', text)\n",
        "    text = RE_URL.sub('', text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Preprocess text\n",
        "for i in range(len(input_text)):\n",
        "    input_text[i] = preprocess_text(input_text[i])\n",
        "\n",
        "# Tokenize text\n",
        "input_tokens = [nltk.word_tokenize(text) for text in input_text]\n",
        "\n",
        "# Further split tokens based on non-alphanumeric characters\n",
        "final_tokens = []\n",
        "total_tokens = 0\n",
        "for token_set in input_tokens:\n",
        "    final_tkn_set = []\n",
        "    for tk in token_set:\n",
        "        sub_tkns = [c for c in re.split(r\"(\\W+)\", tk) if c]\n",
        "        total_tokens += len(sub_tkns)\n",
        "        final_tkn_set += sub_tkns\n",
        "    final_tokens.append(final_tkn_set)\n",
        "present_list_like(f\"Tokenized sentences({len(final_tokens)} sentences, {total_tokens} tokens)\", final_tokens)\n",
        "\n",
        "# Create vocabulary\n",
        "flat_tokens = [word for token_set in final_tokens for word in token_set]\n",
        "frequencies = Counter(flat_tokens)\n",
        "token_to_idx = {word: idx+1 for idx, (word, _) in enumerate(frequencies.most_common())}\n",
        "vocab_size = len(token_to_idx)\n",
        "print(vocab_size, \"unique tokens\")\n",
        "present_list_like(\"Unique tokens\", list(token_to_idx.keys()))\n",
        "\n",
        "# Convert tokens to indices\n",
        "input_indices = [[token_to_idx[token] for token in token_set] for token_set in final_tokens]\n",
        "\n",
        "# Pad sequences\n",
        "input_indices_padded = [torch.tensor(seq, dtype=torch.long) for seq in input_indices]\n",
        "input_indices_padded = pad_sequence(input_indices_padded, batch_first=True, padding_value=0).to(device)\n",
        "\n",
        "present_list_like(f\"\\nFinal Index Sets({len(input_indices_padded)} index sets)\", input_indices_padded)\n",
        "\n",
        "# Print label distribution\n",
        "print(Counter(input_labels))\n",
        "\n",
        "# Convert labels to numerical representations before splitting\n",
        "labels_numeric, unique_labels = pd.factorize(input_labels)\n",
        "\n",
        "# Split data into training and validation sets BEFORE SMOTE\n",
        "x_train, x_val, y_train, y_val = train_test_split(input_indices_padded, labels_numeric, test_size=0.2, random_state=RANDOM_STATE)\n",
        "\n",
        "# Handle class imbalance ONLY on the training data using SMOTE\n",
        "# IMPORTANT FIX: Move to CPU and numpy after the train/val split\n",
        "x_train_cpu = x_train.cpu().numpy()\n",
        "y_train_cpu = y_train\n",
        "smote = SMOTE(random_state=RANDOM_STATE)\n",
        "x_train_resampled, y_train_resampled = smote.fit_resample(x_train_cpu, y_train_cpu)\n",
        "x_train_resampled = torch.tensor(x_train_resampled, dtype=torch.long).to(device)\n",
        "y_train_resampled = torch.tensor(y_train_resampled, dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "# Convert y_val to a PyTorch tensor before creating the TensorDataset\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(x_train_resampled, y_train_resampled)\n",
        "val_dataset = TensorDataset(x_val, torch.tensor(y_val, dtype=torch.long).to(device))  # Convert y_val to tensor\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Define the BiLSTM model with dropout\n",
        "class MisInformationDetectionBiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2, embed_size=128):  # Add embed_size here\n",
        "        super(MisInformationDetectionBiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size + 1, embed_size, padding_idx=0)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.dropout1(embedded)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        hidden = self.dropout2(hidden)\n",
        "        output = self.fc(hidden)\n",
        "        return self.softmax(output)\n",
        "\n",
        "# Instantiate the BiLSTM model\n",
        "model = MisInformationDetectionBiLSTM(EMBED_SIZE, HIDDEN_SIZE, len(unique_labels), NUM_LAYERS, DROPOUT).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "counter_data = Counter(input_labels)\n",
        "class_counts = list(counter_data.values())\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.5, 1.2, 1.0]).to(device))\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n",
        "\n",
        "# Model training function with early stopping\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=10, patience=5):\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        train_loss_avg = total_loss/len(train_loader)\n",
        "        train_losses.append(train_loss_avg)\n",
        "        print(f'Epoch {epoch+1}, Training loss: {train_loss_avg:.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        val_loss_avg = val_loss/len(val_loader)\n",
        "        val_losses.append(val_loss_avg)\n",
        "        val_accuracy = correct / total\n",
        "        val_accuracies.append(val_accuracy)\n",
        "        print(f\"Validation Accuracy: {val_accuracy*100:.2f}%, Validation Loss {val_loss_avg:.4f}\")\n",
        "        scheduler.step(val_loss_avg) #Step scheduler based on validation loss\n",
        "\n",
        "        #Early Stopping\n",
        "        if val_loss_avg < best_val_loss:\n",
        "            best_val_loss = val_loss_avg\n",
        "            epochs_no_improve = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth') #save best performing model\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve == patience:\n",
        "                print(\"Early stopping triggered\")\n",
        "                break\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies #Return for logging\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, epochs=EPOCHS)\n",
        "\n",
        "# Model evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.load_state_dict(torch.load('best_model.pth')) #load best weights\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(predicted.tolist())\n",
        "    print(classification_report(y_true, y_pred, target_names=unique_labels))\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, val_loader)\n",
        "\n",
        "#plot graphs\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DbCG0mD33hdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DO NOT USE THIS VERSION BALANCES THE WHOLE DATASET\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "# Determine if CUDA (GPU) is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "# Helper function to print a peek of a list-like object\n",
        "peek = 10\n",
        "def present_list_like(name, list_like, peek=peek):\n",
        "    print(f\"{name} peek:\")\n",
        "    print('  ' + '\\n  '.join([str(seq) for seq in list_like[0:peek]]))\n",
        "\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('covid_lies.csv')  # Make sure 'covid_lies.csv' is in the correct path\n",
        "print(\"The dataset:\")\n",
        "df.info()\n",
        "print(\"\\nData peek:\")\n",
        "print(df.head(peek))\n",
        "print()\n",
        "\n",
        "# Separate out text data and labels\n",
        "input_text = df['misconception'].to_numpy()\n",
        "input_label = df['label'].to_numpy()\n",
        "print(\"Unique labels:\", np.unique(input_label))\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text)->str:\n",
        "    # Letter-level cleaning\n",
        "    text = text.lower()\n",
        "    valid_asciis = {9, *range(32, 127)}\n",
        "    text = ''.join(filter(lambda x: ord(x) in valid_asciis, text))\n",
        "\n",
        "    # Word/sequence-level cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Preprocess text\n",
        "for i in range(len(input_text)):\n",
        "    input_text[i] = preprocess_text(input_text[i])\n",
        "\n",
        "# Tokenize text using nltk.word_tokenize\n",
        "input_tokens = [nltk.word_tokenize(text) for text in input_text]\n",
        "\n",
        "\n",
        "# Further split tokens based on non-alphanumeric characters\n",
        "final_tokens = []\n",
        "total_tokens = 0\n",
        "for token_set in input_tokens:\n",
        "    final_tkn_set = []\n",
        "    for tk in token_set:\n",
        "        sub_tkns = [c for c in re.split(r\"(\\W+)\", tk) if c]\n",
        "        total_tokens += len(sub_tkns)\n",
        "        final_tkn_set += sub_tkns\n",
        "    final_tokens.append(final_tkn_set)\n",
        "present_list_like(f\"Tokenized sentences({len(final_tokens)} sentences, {total_tokens} tokens)\", final_tokens)\n",
        "\n",
        "# Embed the tokens\n",
        "# Map each token to its frequency in the dataset\n",
        "flat_tokens = [word for token_set in final_tokens for word in token_set]\n",
        "frequencies = Counter(flat_tokens)\n",
        "token_to_idx = {word: idx+1 for idx, (word, _) in enumerate(frequencies.most_common())}\n",
        "vocab_size = len(token_to_idx)\n",
        "print(vocab_size, \"unique tokens\")\n",
        "present_list_like(\"Unique tokens\", list(token_to_idx.keys()))\n",
        "\n",
        "# Convert tokens to numerical indices\n",
        "freq_indexed = [[token_to_idx[token] for token in token_set] for token_set in final_tokens]\n",
        "\n",
        "# Pad sequences to make them of the same length\n",
        "forced_idx_set_size = max(len(idxs) for idxs in freq_indexed)\n",
        "freq_indexed = [\n",
        "    idxs[:forced_idx_set_size] + [0]*(forced_idx_set_size - len(idxs))\n",
        "    for idxs in freq_indexed\n",
        "]\n",
        "present_list_like(f\"\\nFinal Index Sets(Set_Size = {forced_idx_set_size}, {len(freq_indexed)} index sets)\", freq_indexed)\n",
        "\n",
        "# Print label counts\n",
        "print(Counter(input_label))\n",
        "\n",
        "# Handle class imbalance using SMOTE\n",
        "labels_numeric, unique_labels = pd.factorize(input_label)\n",
        "x = np.array(freq_indexed)\n",
        "y = np.array(labels_numeric)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "x_resampled, y_resampled = smote.fit_resample(x, y)\n",
        "\n",
        "# Split data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors and move to the appropriate device\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.long).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "x_val_tensor = torch.tensor(x_val, dtype=torch.long).to(device)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.long).to(device)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Define the BiLSTM model\n",
        "class MisInformationDetectionBiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, dropout=0.2):\n",
        "        super(MisInformationDetectionBiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size + 1, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        output = self.fc(hidden)\n",
        "        return self.softmax(output)\n",
        "\n",
        "# Model parameters\n",
        "vocab_size = len(token_to_idx)\n",
        "embed_size = 128\n",
        "input_size = embed_size\n",
        "hidden_size = 256\n",
        "output_size = len(unique_labels)\n",
        "num_layers = 3\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the BiLSTM model and move to the device\n",
        "model = MisInformationDetectionBiLSTM(input_size, hidden_size, output_size, num_layers, dropout).to(device)\n",
        "\n",
        "\n",
        "# Loss function and optimizer\n",
        "counter_data = Counter(input_label)\n",
        "class_counts = list(counter_data.values())\n",
        "class_weights = 1. / torch.tensor(class_counts, dtype=torch.float).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.5, 1.2, 1.0]).to(device))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "\n",
        "# Model training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}, Training loss: {total_loss/len(train_loader):.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        print(f\"Validation Accuracy: {correct/total*100:.2f}%\")\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=30)\n",
        "\n",
        "# Model evaluation function\n",
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            y_true.extend(labels.tolist())\n",
        "            y_pred.extend(predicted.tolist())\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=unique_labels))\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, val_loader)"
      ],
      "metadata": {
        "id": "FFgnZlUz8_lj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}