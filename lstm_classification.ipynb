{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Ozyb-vXSAMtT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch.optim as optim\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import random\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cF0LGpIbKgBP",
    "outputId": "07dc46d3-92d5-4977-f959-c7a8d7a571a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vGZJybiAMtV"
   },
   "source": [
    "## Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "BCo7k9jXdxOe"
   },
   "outputs": [],
   "source": [
    "peek = 10\n",
    "def present_list_like(name, list_like, peek=peek):\n",
    "    print(f\"{name} peek:\")\n",
    "    print('  ' + '\\n  '.join([str(seq) for seq in list_like[0:peek]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lNu54LNAMtV",
    "outputId": "d3117b0d-a30c-4ab6-8ba6-ef21efe2f573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6591 entries, 0 to 6590\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   misconception_id  6591 non-null   int64 \n",
      " 1   misconception     6591 non-null   object\n",
      " 2   tweet_id          6591 non-null   int64 \n",
      " 3   label             6591 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 206.1+ KB\n",
      "\n",
      "Data peek:\n",
      "   misconception_id                                      misconception  \\\n",
      "0                 3             Coronavirus is genetically engineered.   \n",
      "1                30  Blowing conch shells destroys coronavirus pote...   \n",
      "2                57  Swans and dolphins swimming in Venice canals f...   \n",
      "3                22                         Cocaine cures coronavirus.   \n",
      "4                32  Observing janata curfew will result in the red...   \n",
      "5                25  Holy communion cannot be the cause of the spre...   \n",
      "6                61  Lions were freed to keep people off the street...   \n",
      "7                 3             Coronavirus is genetically engineered.   \n",
      "8                40                Cannabis protects against COVID-19.   \n",
      "9                50  It is safe for individuals infected with COVID...   \n",
      "\n",
      "              tweet_id label  \n",
      "0  1233965490948591616    na  \n",
      "1  1233907923765559296    na  \n",
      "2  1233911842910720000    na  \n",
      "3  1233947734094290944    na  \n",
      "4  1233937085297332224    na  \n",
      "5  1233955845756653568    na  \n",
      "6  1233917889557692416    na  \n",
      "7  1233952635725778944    na  \n",
      "8  1233940738439815168    na  \n",
      "9  1233920123183828992    na  \n",
      "\n",
      "Unique labels: ['na' 'neg' 'pos']\n",
      "Counts of 'labels': label\n",
      "na     0.932939\n",
      "pos    0.043696\n",
      "neg    0.023365\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Read data from covid_lies.csv dataset into dataframe\n",
    "df = pd.read_csv('./data/covid_lies.csv')\n",
    "print(\"The dataset:\")\n",
    "df.info()\n",
    "print(\"\\nData peek:\")\n",
    "print(df.head(peek))\n",
    "print()\n",
    "\n",
    "#Seperate out text data and labels\n",
    "input_text = df['misconception'].to_numpy()\n",
    "input_label = df['label'].to_numpy()\n",
    "print(\"Unique labels:\", np.unique(input_label))\n",
    "orig_label_counts = df['label'].value_counts(normalize=True)\n",
    "print(\"Counts of 'labels':\", orig_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNsUDJdhAMtW"
   },
   "source": [
    "## Preprocess input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "z8PtD24vAMtW"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text)->str:\n",
    "    #Letter-level cleaning\n",
    "    text = text.lower()\n",
    "    valid_asciis = {9, *range(32, 127)}\n",
    "    text = ''.join(filter(lambda x: ord(x) in valid_asciis, text))\n",
    "\n",
    "    #Word/sequence-level cleaning\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "z-l4_oDDUSLj"
   },
   "outputs": [],
   "source": [
    "#Preprocess the text\n",
    "for i in range(len(input_text)):\n",
    "    input_text[i] = preprocess_text(input_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwmzm7pYAMtW"
   },
   "source": [
    "## Tokenize input text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvKgthj0WYNm",
    "outputId": "e0af97f4-bd6d-410a-e0e4-068668cd3f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentences(18447 sentences, 133252 tokens) peek:\n",
      "  ['coronavirus', 'genetically', 'engineered', '.']\n",
      "  ['blowing', 'conch', 'shells', 'destroys', 'coronavirus', 'potency', '.']\n",
      "  ['swans', 'dolphins', 'swimming', 'venice', 'canals', 'following', 'covid', '-', '19', 'lockdown', '.']\n",
      "  ['cocaine', 'cures', 'coronavirus', '.']\n",
      "  ['observing', 'janata', 'curfew', 'result', 'reduction', 'covid', '-', '19', 'cases', '40', '%', '.']\n",
      "  ['holy', 'communion', 'can', 'not', 'cause', 'spread', 'coronavirus']\n",
      "  ['lions', 'freed', 'keep', 'people', 'streets', 'moscow', '.']\n",
      "  ['coronavirus', 'genetically', 'engineered', '.']\n",
      "  ['cannabis', 'protects', 'covid', '-', '19', '.']\n",
      "  ['safe', 'individuals', 'infected', 'covid', '-', '19', 'go', 'work', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the text\n",
    "\n",
    "input_tokens = [nltk.word_tokenize(text) for text in input_text]\n",
    "\n",
    "# Split tokens based on non-alphanumeric characters\n",
    "final_tokens = []\n",
    "total_tokens = 0\n",
    "for token_set in input_tokens:\n",
    "    final_tkn_set = []\n",
    "    for tk in token_set:\n",
    "        sub_tkns = [c for c in re.split(r\"(\\W+)\", tk) if c]\n",
    "        total_tokens += len(sub_tkns)\n",
    "        final_tkn_set += sub_tkns\n",
    "    final_tokens.append(final_tkn_set)\n",
    "present_list_like(f\"Tokenized sentences({len(final_tokens)} sentences, {total_tokens} tokens)\", final_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fg2l2lJsAMtX"
   },
   "source": [
    "## Form embeddings for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xR1GIMwXbRzb",
    "outputId": "c8b17f97-e299-4ca5-a83e-9d2a3c465808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 unique tokens\n",
      "Unique tokens peek:\n",
      "  .\n",
      "  -\n",
      "  coronavirus\n",
      "  covid\n",
      "  19\n",
      "  deadly\n",
      "  seasonal\n",
      "  flu\n",
      "  survive\n",
      "  cure\n",
      "\n",
      "Final Index Sets(Set_Size = 19, 18447 index sets) peek:\n",
      "  [3, 15, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [134, 135, 136, 137, 3, 138, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [165, 166, 167, 99, 100, 62, 4, 2, 5, 63, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [168, 30, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [241, 242, 243, 244, 245, 4, 2, 5, 246, 247, 248, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [22, 23, 17, 18, 24, 19, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [169, 170, 171, 172, 96, 173, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [3, 15, 16, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [174, 66, 4, 2, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  [108, 109, 74, 4, 2, 5, 110, 111, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#Embed the tokens\n",
    "\n",
    "# Map each token to its frequency in the dataset\n",
    "flat_tokens = [word for token_set in final_tokens for word in token_set]\n",
    "frequencies = Counter(flat_tokens)\n",
    "token_to_idx = {word: idx+1 for idx, (word, _) in enumerate(frequencies.most_common())}\n",
    "vocab_size = len(token_to_idx)\n",
    "print(vocab_size, \"unique tokens\")\n",
    "present_list_like(\"Unique tokens\", list(token_to_idx.keys()))\n",
    "\n",
    "# Embed the tokens\n",
    "freq_indexed = [[token_to_idx[token] for token in token_set] for token_set in final_tokens]\n",
    "\n",
    "# Make embeddings the same size\n",
    "forced_idx_set_size = max(len(idxs) for idxs in freq_indexed)\n",
    "freq_indexed = [\n",
    "    idxs[:forced_idx_set_size] + [0]*(forced_idx_set_size - len(idxs))\n",
    "    for idxs in freq_indexed\n",
    "]\n",
    "present_list_like(f\"\\nFinal Index Sets(Set_Size = {forced_idx_set_size}, {len(freq_indexed)} index sets)\", freq_indexed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YKnA7ShAMtX"
   },
   "source": [
    "## Define and train a GRU model using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWNwH4d3AMtX"
   },
   "source": [
    "## Save Model Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5c__mmaAMtX"
   },
   "source": [
    "## Evaluate Model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
