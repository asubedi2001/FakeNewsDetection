{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Ozyb-vXSAMtT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torch.optim as optim\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import random\n",
        "import pprint\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "random.seed(184)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF0LGpIbKgBP",
        "outputId": "c5cd7c12-f033-4ee8-c7fc-80a50a0de82d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vGZJybiAMtV"
      },
      "source": [
        "## Read in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "BCo7k9jXdxOe"
      },
      "outputs": [],
      "source": [
        "peek = 20\n",
        "def present_list_like(name, list_like, peek=peek):\n",
        "    print(f\"{name} peek:\")\n",
        "    print('  ' + '\\n  '.join( str(v) for v in list_like[:peek]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lNu54LNAMtV",
        "outputId": "854cf426-aed9-419d-8930-a480571ef3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset columns(14 in total) peek:\n",
            "  id\n",
            "  label\n",
            "  claim\n",
            "  subject\n",
            "  speaker\n",
            "  speaker_job_title\n",
            "  state_info\n",
            "  party_affiliation\n",
            "  barely_true_counts\n",
            "  false_counts\n",
            "  half_true_counts\n",
            "  mostly_true_counts\n",
            "  pants_on_fire_counts\n",
            "  context\n"
          ]
        }
      ],
      "source": [
        "columns = [\n",
        "    'id', 'label', 'claim', 'subject', 'speaker', 'speaker_job_title', 'state_info',\n",
        "    'party_affiliation', 'barely_true_counts', 'false_counts',\n",
        "    'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
        "]\n",
        "present_list_like(f\"Dataset columns({len(columns)} in total)\", columns, len(columns))\n",
        "def load_data(split):\n",
        "    df = pd.read_csv(f\"./data/{split}.tsv\", sep='\\t', names=columns)\n",
        "    df = df.drop(index=[\n",
        "        idx for idx in df.index if type(df[\"claim\"][idx]) == type(None) or not len(df[\"claim\"][idx])\n",
        "    ])\n",
        "    print(\"The training dataset:\")\n",
        "    df.info()\n",
        "    print(\"\\nData peek:\")\n",
        "    print(df.head(peek))\n",
        "    print()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pmP3E0AW-Wj"
      },
      "source": [
        "##Tokenize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "8KEuwtbf8v0e"
      },
      "outputs": [],
      "source": [
        "pad_tkn = \"<PAD>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "z8PtD24vAMtW"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(input_text, known_vector_size=None, token_to_idx={}):\n",
        "    def preprocess_text(text)->str:\n",
        "        #Letter-level cleaning\n",
        "        text = text.lower()\n",
        "        valid_asciis = {9, *range(32, 127)}\n",
        "        text = ''.join(filter(lambda x: ord(x) in valid_asciis, text))\n",
        "\n",
        "        #Word/sequence-level cleaning\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "        return text\n",
        "\n",
        "    #Preprocess the text\n",
        "    for i in range(len(input_text)):\n",
        "        input_text[i] = preprocess_text(input_text[i])\n",
        "\n",
        "\n",
        "    #Tokenize\n",
        "    final_tokens = input_tokens = [nltk.word_tokenize(text) for text in input_text]\n",
        "    total_tokens = sum(len(tkns) for tkns in final_tokens)\n",
        "\n",
        "    # Make all token sets the same length\n",
        "    forced_tkn_set_size = (\n",
        "        known_vector_size if known_vector_size\n",
        "        else int(np.percentile([len(tkns) for tkns in final_tokens], 80))\n",
        "    )\n",
        "    final_tokens = [\n",
        "        tkns[:forced_tkn_set_size] + [pad_tkn]*(forced_tkn_set_size - len(tkns))\n",
        "        for tkns in final_tokens\n",
        "    ]\n",
        "\n",
        "    # Present results\n",
        "    present_list_like(f\"Tokenized sentences({len(final_tokens)} sentences, {total_tokens} total tokens)\", final_tokens)\n",
        "\n",
        "\n",
        "    #Index the tokens\n",
        "    # Map each token to its frequency in the dataset\n",
        "    if not len(token_to_idx):\n",
        "        flat_tokens = [word for token_set in final_tokens for word in token_set]\n",
        "        frequencies = Counter(flat_tokens)\n",
        "        token_to_idx = {}\n",
        "        for idx, (word, _) in enumerate(frequencies.most_common()):\n",
        "            if idx >= 10000:\n",
        "                break\n",
        "            token_to_idx[word] = idx + 1\n",
        "        if pad_tkn not in token_to_idx:\n",
        "            token_to_idx[pad_tkn] = len(token_to_idx) + 1\n",
        "    vocab_size = len(token_to_idx)\n",
        "    print()\n",
        "    print(vocab_size, \"unique tokens\")\n",
        "    present_list_like(\"Unique tokens\", list(token_to_idx.keys()))\n",
        "\n",
        "    # Index the tokens\n",
        "    freq_indexed = [\n",
        "        [(token_to_idx[token] if token in token_to_idx else 0) for token in token_set]\n",
        "        for token_set in final_tokens\n",
        "    ]\n",
        "\n",
        "    # Present results\n",
        "    present_list_like(f\"\\nFinal Index Sets(Set_Size = {forced_tkn_set_size}, {len(freq_indexed)} index sets)\", freq_indexed)\n",
        "\n",
        "    return freq_indexed, token_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "L4ep_xTTR8ZA"
      },
      "outputs": [],
      "source": [
        "def get_freq_indexed_and_labels(split, known_vector_size=None, token_to_idx={}):\n",
        "    df = load_data(split)\n",
        "    input_text = df[\"claim\"].to_numpy()\n",
        "    #Augment input text with the other columns\n",
        "    other_cols = {\n",
        "        \"context\",\n",
        "        \"subject\",\n",
        "        \"speaker\",\n",
        "        \"speaker_job_title\",\n",
        "        \"state_info\",\n",
        "        \"party_affiliation\",\n",
        "    }\n",
        "    for i in range(len(input_text)):\n",
        "        extra_data = [f\"{col}: {df[col].values[i]}\" for col in other_cols if df[col].values[i]]\n",
        "        input_text[i] += \" | \\n\"*(len(extra_data) > 0) + \" | \\n\".join(extra_data)\n",
        "    input_labels = df[\"label\"].to_numpy()\n",
        "    code_switch = \"\"\"\"\"\"\n",
        "    #Fuse some labels\n",
        "    input_labels = np.array([\n",
        "        \"false\" if x in (\"false\", \"half-true\", \"barely-true\", \"pants-fire\")\n",
        "        else \"true\" if x in (\"true\", \"mostly-true\")\n",
        "        else x\n",
        "        for x in input_labels\n",
        "    ])\n",
        "    #\"\"\"\n",
        "    freq_indexed, token_to_idx = tokenize_text(input_text, known_vector_size, token_to_idx)\n",
        "\n",
        "    return freq_indexed, token_to_idx, input_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSG8ChPrXEac"
      },
      "source": [
        "##Turn the data into tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "DU41k5wmRzHk"
      },
      "outputs": [],
      "source": [
        "def as_tensors(split, label_encoder=None, known_vector_size=None, token_to_idx={}):\n",
        "    freq_indexed, token_to_idx, input_labels = get_freq_indexed_and_labels(split, known_vector_size, token_to_idx)\n",
        "    X = torch.tensor(freq_indexed, dtype=torch.long)\n",
        "    label_encoder_existed = (type(label_encoder) != type(None))\n",
        "    label_encoder = (LabelEncoder() if not label_encoder_existed else label_encoder)\n",
        "    y = (\n",
        "        label_encoder.fit_transform(input_labels) if not label_encoder_existed\n",
        "        else label_encoder.transform(input_labels)\n",
        "    )\n",
        "    y = torch.tensor(y, dtype=torch.long)\n",
        "    print(f\"{split.upper()} SPLIT:\", X.size(0), \"overall samples:\", X.shape)\n",
        "\n",
        "    return X, token_to_idx, label_encoder, input_labels, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPYRtrwdXJVh"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "8y8fHZqpbL88"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kV1XEzpgVvtQ",
        "outputId": "2403dfd2-91ce-4931-f85c-96410a1d8d4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The training dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10240 entries, 0 to 10239\n",
            "Data columns (total 14 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   id                    10240 non-null  object \n",
            " 1   label                 10240 non-null  object \n",
            " 2   claim                 10240 non-null  object \n",
            " 3   subject               10238 non-null  object \n",
            " 4   speaker               10238 non-null  object \n",
            " 5   speaker_job_title     7342 non-null   object \n",
            " 6   state_info            8030 non-null   object \n",
            " 7   party_affiliation     10238 non-null  object \n",
            " 8   barely_true_counts    10238 non-null  float64\n",
            " 9   false_counts          10238 non-null  float64\n",
            " 10  half_true_counts      10238 non-null  float64\n",
            " 11  mostly_true_counts    10238 non-null  float64\n",
            " 12  pants_on_fire_counts  10238 non-null  float64\n",
            " 13  context               10138 non-null  object \n",
            "dtypes: float64(5), object(9)\n",
            "memory usage: 1.1+ MB\n",
            "\n",
            "Data peek:\n",
            "            id        label  \\\n",
            "0    2635.json        false   \n",
            "1   10540.json    half-true   \n",
            "2     324.json  mostly-true   \n",
            "3    1123.json        false   \n",
            "4    9028.json    half-true   \n",
            "5   12465.json         true   \n",
            "6    2342.json  barely-true   \n",
            "7     153.json    half-true   \n",
            "8    5602.json    half-true   \n",
            "9    9741.json  mostly-true   \n",
            "10   7115.json  mostly-true   \n",
            "11   4148.json    half-true   \n",
            "12   5947.json        false   \n",
            "13   8616.json  mostly-true   \n",
            "14   8705.json  barely-true   \n",
            "15  10683.json    half-true   \n",
            "16    620.json         true   \n",
            "17   3863.json  barely-true   \n",
            "18  12372.json    half-true   \n",
            "19  12385.json  mostly-true   \n",
            "\n",
            "                                                claim  \\\n",
            "0   Says the Annies List political group supports ...   \n",
            "1   When did the decline of coal start? It started...   \n",
            "2   Hillary Clinton agrees with John McCain \"by vo...   \n",
            "3   Health care reform legislation is likely to ma...   \n",
            "4   The economic turnaround started at the end of ...   \n",
            "5   The Chicago Bears have had more starting quart...   \n",
            "6   Jim Dunnam has not lived in the district he re...   \n",
            "7   I'm the only person on this stage who has work...   \n",
            "8   However, it took $19.5 million in Oregon Lotte...   \n",
            "9   Says GOP primary opponents Glenn Grothman and ...   \n",
            "10  For the first time in history, the share of th...   \n",
            "11  Since 2000, nearly 12 million Americans have s...   \n",
            "12  When Mitt Romney was governor of Massachusetts...   \n",
            "13  The economy bled $24 billion due to the govern...   \n",
            "14  Most of the (Affordable Care Act) has already ...   \n",
            "15  In this last election in November, ... 63 perc...   \n",
            "16  McCain opposed a requirement that the governme...   \n",
            "17  U.S. Rep. Ron Kind, D-Wis., and his fellow Dem...   \n",
            "18  Water rates in Manila, Philippines, were raise...   \n",
            "19  Almost 100,000 people left Puerto Rico last year.   \n",
            "\n",
            "                                      subject  \\\n",
            "0                                    abortion   \n",
            "1          energy,history,job-accomplishments   \n",
            "2                              foreign-policy   \n",
            "3                                 health-care   \n",
            "4                                economy,jobs   \n",
            "5                                   education   \n",
            "6                        candidates-biography   \n",
            "7                                      ethics   \n",
            "8                                        jobs   \n",
            "9   energy,message-machine-2014,voting-record   \n",
            "10                                  elections   \n",
            "11    economy,jobs,new-hampshire-2012,poverty   \n",
            "12                       history,state-budget   \n",
            "13         economy,federal-budget,health-care   \n",
            "14                                health-care   \n",
            "15                                  elections   \n",
            "16                             federal-budget   \n",
            "17                             federal-budget   \n",
            "18  financial-regulation,foreign-policy,water   \n",
            "19              bankruptcy,economy,population   \n",
            "\n",
            "                                        speaker  \\\n",
            "0                                  dwayne-bohac   \n",
            "1                                scott-surovell   \n",
            "2                                  barack-obama   \n",
            "3                                  blog-posting   \n",
            "4                                 charlie-crist   \n",
            "5                                     robin-vos   \n",
            "6                        republican-party-texas   \n",
            "7                                  barack-obama   \n",
            "8                                oregon-lottery   \n",
            "9                                 duey-stroebel   \n",
            "10                              robert-menendez   \n",
            "11                                     bernie-s   \n",
            "12                                  mitt-romney   \n",
            "13                                   doonesbury   \n",
            "14                                  george-will   \n",
            "15                                     bernie-s   \n",
            "16                                 barack-obama   \n",
            "17  national-republican-congressional-committee   \n",
            "18                                   gwen-moore   \n",
            "19                                     jack-lew   \n",
            "\n",
            "                    speaker_job_title         state_info party_affiliation  \\\n",
            "0                State representative              Texas        republican   \n",
            "1                      State delegate           Virginia          democrat   \n",
            "2                           President           Illinois          democrat   \n",
            "3                                 NaN                NaN              none   \n",
            "4                                 NaN            Florida          democrat   \n",
            "5          Wisconsin Assembly speaker          Wisconsin        republican   \n",
            "6                                 NaN              Texas        republican   \n",
            "7                           President           Illinois          democrat   \n",
            "8                                 NaN                NaN      organization   \n",
            "9                State representative          Wisconsin        republican   \n",
            "10                       U.S. Senator         New Jersey          democrat   \n",
            "11                       U.S. Senator            Vermont       independent   \n",
            "12                    Former governor      Massachusetts        republican   \n",
            "13                                NaN                NaN              none   \n",
            "14                          Columnist           Maryland         columnist   \n",
            "15                       U.S. Senator            Vermont       independent   \n",
            "16                          President           Illinois          democrat   \n",
            "17                                NaN                NaN        republican   \n",
            "18  U.S. House member -- 4th District          Wisconsin          democrat   \n",
            "19                Treasury secretary   Washington, D.C.           democrat   \n",
            "\n",
            "    barely_true_counts  false_counts  half_true_counts  mostly_true_counts  \\\n",
            "0                  0.0           1.0               0.0                 0.0   \n",
            "1                  0.0           0.0               1.0                 1.0   \n",
            "2                 70.0          71.0             160.0               163.0   \n",
            "3                  7.0          19.0               3.0                 5.0   \n",
            "4                 15.0           9.0              20.0                19.0   \n",
            "5                  0.0           3.0               2.0                 5.0   \n",
            "6                  3.0           1.0               1.0                 3.0   \n",
            "7                 70.0          71.0             160.0               163.0   \n",
            "8                  0.0           0.0               1.0                 0.0   \n",
            "9                  0.0           0.0               0.0                 1.0   \n",
            "10                 1.0           3.0               1.0                 3.0   \n",
            "11                18.0          12.0              22.0                41.0   \n",
            "12                34.0          32.0              58.0                33.0   \n",
            "13                 0.0           0.0               2.0                 4.0   \n",
            "14                 7.0           6.0               3.0                 5.0   \n",
            "15                18.0          12.0              22.0                41.0   \n",
            "16                70.0          71.0             160.0               163.0   \n",
            "17                18.0           9.0               8.0                 5.0   \n",
            "18                 3.0           4.0               4.0                 3.0   \n",
            "19                 0.0           1.0               0.0                 1.0   \n",
            "\n",
            "    pants_on_fire_counts                                   context  \n",
            "0                    0.0                                  a mailer  \n",
            "1                    0.0                           a floor speech.  \n",
            "2                    9.0                                    Denver  \n",
            "3                   44.0                            a news release  \n",
            "4                    2.0                       an interview on CNN  \n",
            "5                    1.0                 a an online opinion-piece  \n",
            "6                    1.0                          a press release.  \n",
            "7                    9.0  a Democratic debate in Philadelphia, Pa.  \n",
            "8                    1.0                                a website   \n",
            "9                    0.0                           an online video  \n",
            "10                   0.0                                  a speech  \n",
            "11                   0.0                                   a tweet  \n",
            "12                  19.0                an interview with CBN News  \n",
            "13                   0.0   a Doonesbury strip in the Sunday comics  \n",
            "14                   1.0             comments on \"Fox News Sunday\"  \n",
            "15                   0.0              a town hall in Austin, Texas  \n",
            "16                   9.0                                a radio ad  \n",
            "17                   8.0                            a news release  \n",
            "18                   1.0                   a congressional hearing  \n",
            "19                   0.0          an interview with Bloomberg News  \n",
            "\n",
            "Tokenized sentences(10240 sentences, 442929 total tokens) peek:\n",
            "  ['says', 'annies', 'list', 'political', 'group', 'supports', 'third-trimester', 'abortions', 'demand', '.', '|', 'party_affiliation', ':', 'republican', '|', 'subject', ':', 'abortion', '|', 'state_info', ':', 'texas', '|', 'speaker_job_title', ':', 'state', 'representative', '|', 'context', ':', 'mailer', '|', 'speaker', ':', 'dwayne-bohac', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['decline', 'coal', 'start', '?', 'started', 'natural', 'gas', 'took', 'started', 'begin', '(', 'president', 'george', 'w.', ')', 'bushs', 'administration', '.', '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'energy', ',', 'history', ',', 'job-accomplishments', '|', 'state_info', ':', 'virginia', '|', 'speaker_job_title', ':', 'state', 'delegate', '|', 'context', ':', 'floor', 'speech', '.', '|', 'speaker', ':', 'scott-surovell']\n",
            "  ['hillary', 'clinton', 'agrees', 'john', 'mccain', '``', 'by', 'voting', 'give', 'george', 'bush', 'benefit', 'doubt', 'iran', '.', \"''\", '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'foreign-policy', '|', 'state_info', ':', 'illinois', '|', 'speaker_job_title', ':', 'president', '|', 'context', ':', 'denver', '|', 'speaker', ':', 'barack-obama', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['health', 'care', 'reform', 'legislation', 'likely', 'mandate', 'free', 'sex', 'change', 'surgeries', '.', '|', 'party_affiliation', ':', 'none', '|', 'subject', ':', 'health-care', '|', 'state_info', ':', 'nan', '|', 'speaker_job_title', ':', 'nan', '|', 'context', ':', 'news', 'release', '|', 'speaker', ':', 'blog-posting', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['economic', 'turnaround', 'started', 'end', 'term', '.', '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'economy', ',', 'jobs', '|', 'state_info', ':', 'florida', '|', 'speaker_job_title', ':', 'nan', '|', 'context', ':', 'interview', 'cnn', '|', 'speaker', ':', 'charlie-crist', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['chicago', 'bears', 'starting', 'quarterbacks', 'last', '10', 'years', 'total', 'number', 'tenured', '(', 'uw', ')', 'faculty', 'fired', 'last', 'two', 'decades', '.', '|', 'party_affiliation', ':', 'republican', '|', 'subject', ':', 'education', '|', 'state_info', ':', 'wisconsin', '|', 'speaker_job_title', ':', 'wisconsin', 'assembly', 'speaker', '|', 'context', ':', 'online', 'opinion-piece', '|', 'speaker', ':', 'robin-vos', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['jim', 'dunnam', 'lived', 'district', 'represents', 'years', 'now', '.', '|', 'party_affiliation', ':', 'republican', '|', 'subject', ':', 'candidates-biography', '|', 'state_info', ':', 'texas', '|', 'speaker_job_title', ':', 'nan', '|', 'context', ':', 'press', 'release', '.', '|', 'speaker', ':', 'republican-party-texas', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['i', \"'m\", 'person', 'stage', 'worked', 'actively', 'last', 'year', 'passing', ',', 'along', 'russ', 'feingold', ',', 'toughest', 'ethics', 'reform', 'since', 'watergate', '.', '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'ethics', '|', 'state_info', ':', 'illinois', '|', 'speaker_job_title', ':', 'president', '|', 'context', ':', 'democratic', 'debate', 'philadelphia', ',', 'pa.', '|', 'speaker', ':', 'barack-obama', '<PAD>']\n",
            "  ['however', ',', 'took', '$', '19.5', 'million', 'oregon', 'lottery', 'funds', 'port', 'newport', 'eventually', 'land', 'new', 'noaa', 'marine', 'operations', 'center-pacific', '.', '|', 'party_affiliation', ':', 'organization', '|', 'subject', ':', 'jobs', '|', 'state_info', ':', 'nan', '|', 'speaker_job_title', ':', 'nan', '|', 'context', ':', 'website', '|', 'speaker', ':', 'oregon-lottery', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['says', 'gop', 'primary', 'opponents', 'glenn', 'grothman', 'joe', 'leibham', 'cast', 'compromise', 'vote', 'cost', '$', '788', 'million', 'higher', 'electricity', 'costs', '.', '|', 'party_affiliation', ':', 'republican', '|', 'subject', ':', 'energy', ',', 'message-machine-2014', ',', 'voting-record', '|', 'state_info', ':', 'wisconsin', '|', 'speaker_job_title', ':', 'state', 'representative', '|', 'context', ':', 'online', 'video', '|', 'speaker', ':', 'duey-stroebel']\n",
            "  ['first', 'time', 'history', ',', 'share', 'national', 'popular', 'vote', 'margin', 'smaller', 'latino', 'vote', 'margin', '.', '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'elections', '|', 'state_info', ':', 'new', 'jersey', '|', 'speaker_job_title', ':', 'u.s.', 'senator', '|', 'context', ':', 'speech', '|', 'speaker', ':', 'robert-menendez', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['since', '2000', ',', 'nearly', '12', 'million', 'americans', 'slipped', 'middle', 'class', 'poverty', '.', '|', 'party_affiliation', ':', 'independent', '|', 'subject', ':', 'economy', ',', 'jobs', ',', 'new-hampshire-2012', ',', 'poverty', '|', 'state_info', ':', 'vermont', '|', 'speaker_job_title', ':', 'u.s.', 'senator', '|', 'context', ':', 'tweet', '|', 'speaker', ':', 'bernie-s', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['mitt', 'romney', 'governor', 'massachusetts', ',', 'didnt', 'slow', 'rate', 'growth', 'government', ',', 'actually', 'cut', 'it', '.', '|', 'party_affiliation', ':', 'republican', '|', 'subject', ':', 'history', ',', 'state-budget', '|', 'state_info', ':', 'massachusetts', '|', 'speaker_job_title', ':', 'former', 'governor', '|', 'context', ':', 'interview', 'cbn', 'news', '|', 'speaker', ':', 'mitt-romney', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['economy', 'bled', '$', '24', 'billion', 'due', 'government', 'shutdown', '.', '|', 'party_affiliation', ':', 'none', '|', 'subject', ':', 'economy', ',', 'federal-budget', ',', 'health-care', '|', 'state_info', ':', 'nan', '|', 'speaker_job_title', ':', 'nan', '|', 'context', ':', 'doonesbury', 'strip', 'sunday', 'comics', '|', 'speaker', ':', 'doonesbury', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['(', 'affordable', 'care', 'act', ')', 'already', 'sense', 'waived', 'otherwise', 'suspended', '.', '|', 'party_affiliation', ':', 'columnist', '|', 'subject', ':', 'health-care', '|', 'state_info', ':', 'maryland', '|', 'speaker_job_title', ':', 'columnist', '|', 'context', ':', 'comments', '``', 'fox', 'news', 'sunday', \"''\", '|', 'speaker', ':', 'george-will', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['last', 'election', 'november', ',', '...', '63', 'percent', 'american', 'people', 'chose', 'vote', ',', '...', '80', 'percent', 'young', 'people', ',', '(', 'and', ')', '75', 'percent', 'low-income', 'workers', 'chose', 'vote', '.', '|', 'party_affiliation', ':', 'independent', '|', 'subject', ':', 'elections', '|', 'state_info', ':', 'vermont', '|', 'speaker_job_title', ':', 'u.s.', 'senator', '|', 'context', ':', 'town']\n",
            "  ['mccain', 'opposed', 'requirement', 'government', 'buy', 'american-made', 'motorcycles', '.', 'said', 'buy-american', 'provisions', 'quote', \"'disgraceful\", '.', \"'\", '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'federal-budget', '|', 'state_info', ':', 'illinois', '|', 'speaker_job_title', ':', 'president', '|', 'context', ':', 'radio', 'ad', '|', 'speaker', ':', 'barack-obama', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['u.s.', 'rep.', 'ron', 'kind', ',', 'd-wis.', ',', 'fellow', 'democrats', 'went', 'spending', 'spree', 'credit', 'card', 'maxed', '|', 'party_affiliation', ':', 'republican', '|', 'subject', ':', 'federal-budget', '|', 'state_info', ':', 'nan', '|', 'speaker_job_title', ':', 'nan', '|', 'context', ':', 'news', 'release', '|', 'speaker', ':', 'national-republican-congressional-committee', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "  ['water', 'rates', 'manila', ',', 'philippines', ',', 'raised', '845', 'percent', 'subsidiary', 'world', 'bank', 'became', 'partial', 'owner', '.', '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'financial-regulation', ',', 'foreign-policy', ',', 'water', '|', 'state_info', ':', 'wisconsin', '|', 'speaker_job_title', ':', 'u.s.', 'house', 'member', '--', '4th', 'district', '|', 'context', ':', 'congressional', 'hearing', '|', 'speaker', ':']\n",
            "  ['almost', '100,000', 'people', 'left', 'puerto', 'rico', 'last', 'year', '.', '|', 'party_affiliation', ':', 'democrat', '|', 'subject', ':', 'bankruptcy', ',', 'economy', ',', 'population', '|', 'state_info', ':', 'washington', ',', 'd.c.', '|', 'speaker_job_title', ':', 'treasury', 'secretary', '|', 'context', ':', 'interview', 'bloomberg', 'news', '|', 'speaker', ':', 'jack-lew', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "\n",
            "10000 unique tokens\n",
            "Unique tokens peek:\n",
            "  <PAD>\n",
            "  |\n",
            "  :\n",
            "  ,\n",
            "  .\n",
            "  subject\n",
            "  party_affiliation\n",
            "  state_info\n",
            "  speaker_job_title\n",
            "  context\n",
            "  speaker\n",
            "  nan\n",
            "  republican\n",
            "  democrat\n",
            "  says\n",
            "  u.s.\n",
            "  none\n",
            "  state\n",
            "  new\n",
            "  interview\n",
            "\n",
            "Final Index Sets(Set_Size = 49, 10240 index sets) peek:\n",
            "  [15, 8789, 1138, 189, 282, 516, 6333, 767, 1976, 5, 2, 7, 3, 13, 2, 6, 3, 101, 2, 8, 3, 21, 2, 9, 3, 18, 53, 2, 10, 3, 357, 2, 11, 3, 8790, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [2568, 1325, 1180, 484, 784, 1433, 408, 245, 784, 2377, 39, 25, 340, 785, 38, 1699, 231, 5, 2, 7, 3, 14, 2, 6, 3, 77, 4, 63, 4, 124, 2, 8, 3, 71, 2, 9, 3, 18, 1434, 2, 10, 3, 195, 35, 5, 2, 11, 3, 8791]\n",
            "  [174, 133, 4344, 238, 314, 24, 4345, 443, 460, 340, 226, 1375, 4346, 544, 5, 28, 2, 7, 3, 14, 2, 6, 3, 75, 2, 8, 3, 62, 2, 9, 3, 25, 2, 10, 3, 1048, 2, 11, 3, 79, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [58, 70, 328, 388, 869, 870, 578, 871, 537, 5090, 5, 2, 7, 3, 17, 2, 6, 3, 32, 2, 8, 3, 12, 2, 9, 3, 12, 2, 10, 3, 41, 57, 2, 11, 3, 619, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [355, 5091, 784, 554, 818, 5, 2, 7, 3, 14, 2, 6, 3, 26, 4, 30, 2, 8, 3, 22, 2, 9, 3, 12, 2, 10, 3, 20, 119, 2, 11, 3, 562, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [1279, 4347, 1977, 6334, 109, 149, 60, 710, 306, 6335, 39, 5092, 38, 2569, 1700, 109, 212, 1229, 5, 2, 7, 3, 13, 2, 6, 3, 40, 2, 8, 3, 34, 2, 9, 3, 34, 485, 11, 2, 10, 3, 409, 8792, 2, 11, 3, 2378, 1, 1, 1]\n",
            "  [872, 8793, 2770, 104, 1876, 60, 517, 5, 2, 7, 3, 13, 2, 6, 3, 54, 2, 8, 3, 21, 2, 9, 3, 12, 2, 10, 3, 50, 57, 5, 2, 11, 3, 2771, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [610, 1623, 658, 2097, 892, 3771, 109, 65, 2226, 4, 1624, 1280, 1281, 4, 3772, 202, 328, 100, 8794, 5, 2, 7, 3, 14, 2, 6, 3, 202, 2, 8, 3, 62, 2, 9, 3, 25, 2, 10, 3, 116, 47, 834, 4, 1492, 2, 11, 3, 79, 1]\n",
            "  [6336, 4, 245, 27, 8795, 72, 107, 1795, 626, 3034, 3773, 4348, 819, 19, 6337, 3774, 1796, 8796, 5, 2, 7, 3, 150, 2, 6, 3, 30, 2, 8, 3, 12, 2, 9, 3, 12, 2, 10, 3, 153, 2, 11, 3, 8797, 1, 1, 1, 1, 1, 1]\n",
            "  [15, 444, 499, 3327, 1701, 6338, 470, 8798, 1702, 3328, 232, 233, 27, 8799, 72, 335, 2379, 358, 5, 2, 7, 3, 13, 2, 6, 3, 77, 4, 820, 4, 187, 2, 8, 3, 34, 2, 9, 3, 18, 53, 2, 10, 3, 409, 192, 2, 11, 3, 8800]\n",
            "  [131, 140, 63, 4, 1376, 102, 1139, 232, 2227, 1703, 2570, 232, 2227, 5, 2, 7, 3, 14, 2, 6, 3, 56, 2, 8, 3, 19, 84, 2, 9, 3, 16, 29, 2, 10, 3, 35, 2, 11, 3, 3035, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [100, 973, 4, 211, 545, 72, 154, 8801, 535, 611, 157, 5, 2, 7, 3, 205, 2, 6, 3, 26, 4, 30, 4, 555, 4, 157, 2, 8, 3, 374, 2, 9, 3, 16, 29, 2, 10, 3, 180, 2, 11, 3, 486, 1, 1, 1, 1, 1, 1]\n",
            "  [296, 246, 33, 138, 4, 461, 5093, 142, 454, 112, 4, 301, 146, 292, 5, 2, 7, 3, 13, 2, 6, 3, 63, 4, 51, 2, 8, 3, 138, 2, 9, 3, 94, 33, 2, 10, 3, 20, 8802, 41, 2, 11, 3, 247, 1, 1, 1, 1, 1]\n",
            "  [26, 8803, 27, 1140, 93, 768, 112, 1978, 5, 2, 7, 3, 17, 2, 6, 3, 26, 4, 49, 4, 32, 2, 8, 3, 12, 2, 9, 3, 12, 2, 10, 3, 3036, 1979, 389, 5094, 2, 11, 3, 3036, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [39, 645, 70, 303, 38, 471, 3037, 4349, 4350, 4351, 5, 2, 7, 3, 373, 2, 6, 3, 32, 2, 8, 3, 434, 2, 9, 3, 373, 2, 10, 3, 121, 24, 88, 41, 389, 28, 2, 11, 3, 1493, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [109, 375, 1559, 4, 98, 3329, 31, 156, 61, 2571, 232, 4, 98, 646, 31, 659, 61, 4, 39, 758, 38, 1282, 31, 2098, 86, 2571, 232, 5, 2, 7, 3, 205, 2, 6, 3, 56, 2, 8, 3, 374, 2, 9, 3, 16, 29, 2, 10, 3, 368]\n",
            "  [314, 676, 2380, 112, 546, 8804, 6339, 5, 91, 8805, 2228, 2572, 8806, 5, 145, 2, 7, 3, 14, 2, 6, 3, 49, 2, 8, 3, 62, 2, 9, 3, 25, 2, 10, 3, 92, 42, 2, 11, 3, 79, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [16, 329, 1181, 1230, 4, 8807, 4, 1704, 239, 392, 169, 4352, 759, 1705, 3330, 2, 7, 3, 13, 2, 6, 3, 49, 2, 8, 3, 12, 2, 9, 3, 12, 2, 10, 3, 41, 57, 2, 11, 3, 786, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "  [324, 393, 8808, 4, 6340, 4, 445, 6341, 31, 8809, 206, 1000, 760, 5095, 745, 5, 2, 7, 3, 14, 2, 6, 3, 396, 4, 75, 4, 324, 2, 8, 3, 34, 2, 9, 3, 16, 46, 165, 155, 1283, 104, 2, 10, 3, 274, 376, 2, 11, 3]\n",
            "  [307, 711, 61, 579, 3331, 4353, 109, 65, 5, 2, 7, 3, 14, 2, 6, 3, 787, 4, 26, 4, 287, 2, 8, 3, 122, 4, 203, 2, 9, 3, 2099, 270, 2, 10, 3, 20, 3038, 41, 2, 11, 3, 6342, 1, 1, 1, 1, 1, 1, 1]\n",
            "TRAIN SPLIT: 10240 overall samples: torch.Size([10240, 49])\n"
          ]
        }
      ],
      "source": [
        "X_train, token_to_idx, label_encoder, train_input_labels, y_train = as_tensors(\"train\")\n",
        "label_to_idx = {l: i for i, l in enumerate(label_encoder.classes_)}\n",
        "train_vocab_size = len(token_to_idx)\n",
        "input_vector_size = X_train.shape[1]\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QamRMQfAeu9R",
        "outputId": "b2ff0a9b-c387-4b84-bc35-3c37add6e81c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 labels\n",
            "\n",
            "label\n",
            "false    0.644727\n",
            "true     0.355273\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "train_label_counts = pd.DataFrame({\"label\": train_input_labels})[\"label\"].value_counts(normalize=True)\n",
        "print(train_label_counts.shape[0], \"labels\\n\")\n",
        "print(train_label_counts)\n",
        "\n",
        "code_switch = \"\\\"\"\"\"\"\n",
        "#Balance if necessary\n",
        "print(f\"TRAIN SPLIT(pre-balancing):\", X_train.size(0), \"overall samples:\", X_train.shape)\n",
        "X_train, y_train = SMOTE(random_state=42).fit_resample(X_train, y_train)\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "print()\n",
        "print(f\"TRAIN SPLIT(post-balancing):\", X_train.size(0), \"overall samples:\", X_train.shape)\n",
        "print(pd.DataFrame({\"label\": [label_encoder.classes_[y] for y in y_train]})[\"label\"].value_counts())\n",
        "#\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
