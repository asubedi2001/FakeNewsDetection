{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ozyb-vXSAMtT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF0LGpIbKgBP",
        "outputId": "5fcfcca5-6f60-40f4-da3c-b784638aba98"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vGZJybiAMtV"
      },
      "source": [
        "## Read in data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "peek = 10\n",
        "def present_list_like(name, list_like, peek=peek):\n",
        "    print(f\"{name} peek:\")\n",
        "    print('  ' + '\\n  '.join([str(seq) for seq in list_like[0:peek]]))"
      ],
      "metadata": {
        "id": "BCo7k9jXdxOe"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lNu54LNAMtV",
        "outputId": "c2d7de4e-0184-4187-905d-b56c239d2e25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6591 entries, 0 to 6590\n",
            "Data columns (total 4 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   misconception_id  6591 non-null   int64 \n",
            " 1   misconception     6591 non-null   object\n",
            " 2   tweet_id          6591 non-null   int64 \n",
            " 3   label             6591 non-null   object\n",
            "dtypes: int64(2), object(2)\n",
            "memory usage: 206.1+ KB\n",
            "\n",
            "Data peek:\n",
            "   misconception_id                                      misconception  \\\n",
            "0                 3             Coronavirus is genetically engineered.   \n",
            "1                30  Blowing conch shells destroys coronavirus pote...   \n",
            "2                57  Swans and dolphins swimming in Venice canals f...   \n",
            "3                22                         Cocaine cures coronavirus.   \n",
            "4                32  Observing janata curfew will result in the red...   \n",
            "5                25  Holy communion cannot be the cause of the spre...   \n",
            "6                61  Lions were freed to keep people off the street...   \n",
            "7                 3             Coronavirus is genetically engineered.   \n",
            "8                40                Cannabis protects against COVID-19.   \n",
            "9                50  It is safe for individuals infected with COVID...   \n",
            "\n",
            "              tweet_id label  \n",
            "0  1233965490948591616    na  \n",
            "1  1233907923765559296    na  \n",
            "2  1233911842910720000    na  \n",
            "3  1233947734094290944    na  \n",
            "4  1233937085297332224    na  \n",
            "5  1233955845756653568    na  \n",
            "6  1233917889557692416    na  \n",
            "7  1233952635725778944    na  \n",
            "8  1233940738439815168    na  \n",
            "9  1233920123183828992    na  \n",
            "\n",
            "Unique labels: ['na' 'neg' 'pos']\n"
          ]
        }
      ],
      "source": [
        "# read data from covid_lies.csv dataset into dataframe\n",
        "df = pd.read_csv('./data/covid_lies.csv')\n",
        "print(\"The dataset:\")\n",
        "df.info()\n",
        "print(\"\\nData peek:\")\n",
        "print(df.head(peek))\n",
        "print()\n",
        "\n",
        "# seperate out text data and labels\n",
        "input_text = df['misconception'].to_numpy()\n",
        "input_label = df['label'].to_numpy()\n",
        "print(\"Unique labels:\", np.unique(input_label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNsUDJdhAMtW"
      },
      "source": [
        "## Preprocess input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "z8PtD24vAMtW"
      },
      "outputs": [],
      "source": [
        "# handle setting all values to our preferred casing style\n",
        "# removing erroneous punctuation\n",
        "# handle stop words\n",
        "# think about standardizing number formats, not fully necessary (careful with 'covid-19')\n",
        "\n",
        "def preprocess_text(text)->str:\n",
        "    #Letter-level cleaning\n",
        "    text = text.lower() #Lowering significantly reduces the number of possible tokens to deal with\n",
        "    valid_asciis = {9, *range(32, 127)}\n",
        "    text = ''.join(filter(lambda x: ord(x) in valid_asciis, text)) #Remove irrelevant text(emojis, special characters/symbols, etc)\n",
        "\n",
        "    #Word/sequence-level cleaning\n",
        "    text = re.sub(r'\\s+', ' ', text) #Remove extra spaces\n",
        "    text = re.sub(r'http\\S+', '', text) #Remove URLs\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Preprocess the text\n",
        "for i in range(len(input_text)):\n",
        "    input_text[i] = preprocess_text(input_text[i])"
      ],
      "metadata": {
        "id": "z-l4_oDDUSLj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwmzm7pYAMtW"
      },
      "source": [
        "## Tokenize input text data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the text\n",
        "input_tokens = [nltk.word_tokenize(text) for text in input_text]\n",
        "present_list_like(\"Tokenized sentences\", input_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvKgthj0WYNm",
        "outputId": "a32ed859-34a8-4390-abde-91ec929555e7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized sentences peek:\n",
            "  ['coronavirus', 'genetically', 'engineered', '.']\n",
            "  ['blowing', 'conch', 'shells', 'destroys', 'coronavirus', 'potency', '.']\n",
            "  ['swans', 'dolphins', 'swimming', 'venice', 'canals', 'following', 'covid-19', 'lockdown', '.']\n",
            "  ['cocaine', 'cures', 'coronavirus', '.']\n",
            "  ['observing', 'janata', 'curfew', 'result', 'reduction', 'covid-19', 'cases', '40', '%', '.']\n",
            "  ['holy', 'communion', 'can', 'not', 'cause', 'spread', 'coronavirus']\n",
            "  ['lions', 'freed', 'keep', 'people', 'streets', 'moscow', '.']\n",
            "  ['coronavirus', 'genetically', 'engineered', '.']\n",
            "  ['cannabis', 'protects', 'covid-19', '.']\n",
            "  ['safe', 'individuals', 'infected', 'covid-19', 'go', 'work', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fg2l2lJsAMtX"
      },
      "source": [
        "## Form embeddings for input data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define a vocab based on the unique tokens in the training set\n",
        "vocab = set()\n",
        "for token_set in input_tokens:\n",
        "    vocab.update(token_set)\n",
        "vocab = list(vocab)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "present_list_like(\"Vocabulary\", vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xR1GIMwXbRzb",
        "outputId": "af47b767-735a-4788-c337-6f422794b756"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 260\n",
            "Vocabulary peek:\n",
            "  silver-infused\n",
            "  amounts\n",
            "  vaccine\n",
            "  wastes\n",
            "  protects\n",
            "  infected\n",
            "  published\n",
            "  predicted\n",
            "  cbd\n",
            "  solution\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89dsk5DrAMtX",
        "outputId": "cbf513c6-11ee-4eec-c715-1f7d10299463"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Avg Density(mean of the % of non-zero values per embedding): 1.99%\n",
            "Bag-of-Words Matrix peek:\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n",
            "  [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0]\n"
          ]
        }
      ],
      "source": [
        "vectorizer = CountVectorizer(vocabulary=vocab)\n",
        "bow_matrix = vectorizer.fit_transform([' '.join(token_set) for token_set in input_tokens])\n",
        "bow_array = bow_matrix.toarray()\n",
        "print(\n",
        "    \"Avg Density(mean of the % of non-zero values per embedding):\",\n",
        "    f\"{round(np.mean([np.sum(arr > 0)/len(vocab)*100 for arr in bow_array]), 2)}%\"\n",
        ")\n",
        "present_list_like(\"Bag-of-Words Matrix\", bow_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YKnA7ShAMtX"
      },
      "source": [
        "## Define model using PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMnIo7dVAMtX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWNwH4d3AMtX"
      },
      "source": [
        "## Save Model Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGBxr7V6AMtX"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5c__mmaAMtX"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsaIBH_uAMtX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cmsc673",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}