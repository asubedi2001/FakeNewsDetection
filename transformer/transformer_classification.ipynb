{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if PyTorch recognizes GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import os\n",
    "\n",
    "columns = [\n",
    "    'id', 'label', 'claim', 'subject', 'speaker', 'speaker_job_title', 'state_info',\n",
    "    'party_affiliation', 'barely_true_counts', 'false_counts',\n",
    "    'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
    "]\n",
    "\n",
    "# read in original LIAR dataset\n",
    "df_train = pd.read_csv('../data/LIAR/train.tsv', sep='\\t', names=columns).dropna()\n",
    "df_valid = pd.read_csv('../data/LIAR/valid.tsv', sep='\\t', names=columns).dropna()\n",
    "df_test = pd.read_csv('../data/LIAR/test.tsv', sep='\\t', names=columns).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_liar(samples, labels, tokenizer):\n",
    "  tokenized = []\n",
    "  for idx in range(len(samples)):\n",
    "    tokenized_claim = tokenizer(samples[idx], return_tensors='pt')\n",
    "    \n",
    "    n_inst = {\n",
    "      'claim_token': tokenized_claim,\n",
    "      'claim_origin': samples[idx],\n",
    "      'label': labels[idx], \n",
    "      'idx': idx\n",
    "    }\n",
    "    tokenized.append(n_inst)\n",
    "\n",
    "  return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim_origin': 'Says the Annies List political group supports '\n",
      "                 'third-trimester abortions on demand.',\n",
      " 'claim_token': {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      "                 'input_ids': tensor([[  101,  8652,  1116,  1103,  7765,  1116,  5619,  1741,  1372,  6253,\n",
      "          1503,   118, 13373, 12831, 12030,  1116,  1113,  4555,   119,   102]]),\n",
      "                 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])},\n",
      " 'idx': 0,\n",
      " 'label': 'false'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "#LIAR Code:\n",
    "# obtain training samples/label pairs from dataset\n",
    "liar_train_samples = np.array(df_train['claim'])\n",
    "liar_train_labels = np.array(df_train['label'])\n",
    "liar_test_samples = np.array(df_test['claim'])\n",
    "liar_test_labels = np.array(df_test['label'])\n",
    "\n",
    "tokenized_train_dataset = tokenize_liar(liar_train_samples, liar_train_labels, tokenizer)\n",
    "tokenized_test_dataset = tokenize_liar(liar_test_samples, liar_test_labels, tokenizer)\n",
    "pprint.pprint(tokenized_train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTorch Datasets (Augmented using SMOTE & Unaugmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class LiarDataset(Dataset):\n",
    "    def __init__(self, liar_data):\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "    \n",
    "        self.label_map = {\n",
    "            'pants-fire': 0,\n",
    "            'false': 0,\n",
    "            'barely-true': 0,\n",
    "            'half-true': 0,\n",
    "            'mostly-true': 1,\n",
    "            'true': 1\n",
    "        }\n",
    "\n",
    "        for idx in range(len(liar_data)):\n",
    "            self.data.append(liar_data[idx]['claim_token'])\n",
    "            self.labels.append(self.label_map[liar_data[idx]['label']])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        claim = {key: torch.tensor(value, dtype=torch.long) for key, value in self.data[idx].items()}\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "        return claim, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Dataset into training and test portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = [inst['claim_token'] for inst in tokenized_train_dataset]\n",
    "y_train = [inst['label'] for inst in tokenized_train_dataset]\n",
    "X_test = [inst['claim_token'] for inst in tokenized_test_dataset]\n",
    "y_test = [inst['label'] for inst in tokenized_test_dataset]\n",
    "\n",
    "# reconstruct dictionaries using training/test sets\n",
    "train_set = []\n",
    "for claim_token, label in zip(X_train, y_train):\n",
    "    train_inst = {\n",
    "        'claim_token': claim_token,  \n",
    "        'label': label\n",
    "    }\n",
    "    train_set.append(train_inst)\n",
    "\n",
    "test_set = []\n",
    "for claim_token, label in zip(X_test, y_test):\n",
    "    test_inst = {\n",
    "        'claim_token': claim_token,\n",
    "        'label': label\n",
    "    }\n",
    "    test_set.append(test_inst)\n",
    "\n",
    "train_dataset = LiarDataset(train_set)\n",
    "test_dataset = LiarDataset(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, dataloaders, loss function, collate function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to create collate function to pad variable length sequences for input\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # as per cell 6 output, item[0] will look like this:\n",
    "    # 'tweet_token': {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]),\n",
    "    #                   'input_ids': tensor([[  101, 21887, 23350,  2003, 19345, 13685,  1012,   102]]),\n",
    "    #                   'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]])}}\n",
    "    # item[1] will be a numeric label according to MisinformationDataset's label_map\n",
    "    input_ids = [item[0]['input_ids'].squeeze(0) for item in batch]\n",
    "    attention_masks = [item[0]['attention_mask'].squeeze(0) for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    # pad sequences for input_ids and attention_masks with 0 values\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: \n",
      "False: 1.5983353151010702\n",
      "True: 2.6713036565977744\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from collections import Counter\n",
    "\n",
    "# change num_labels in accordance with current problem design (binary or multi-class classification)\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "# freeze base model layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze last two layers of base model for fine tuning\n",
    "#for param in model.base_model.encoder.layer[-2:]:\n",
    "#    param.requires_grad = True\n",
    "\n",
    "# attempt to use class weights to offset imbalance of dataset\n",
    "label_counts = Counter(liar_train_labels)\n",
    "label_map = {\n",
    "            'pants-fire': 0,\n",
    "            'false': 0,\n",
    "            'barely-true': 0,\n",
    "            'half-true': 0,\n",
    "            'mostly-true': 1,\n",
    "            'true': 1\n",
    "        }\n",
    "numeric_labels = np.array([label_map[inst['label']] for inst in tokenized_train_dataset])\n",
    "true_count = np.count_nonzero(numeric_labels == 1)\n",
    "false_count = np.count_nonzero(numeric_labels == 0)\n",
    "total_count = len(train_dataset)\n",
    "true_weight = total_count / true_count\n",
    "false_weight = total_count / false_count\n",
    "print(f'Weights: \\nFalse: {false_weight}\\nTrue: {true_weight}')\n",
    "\n",
    "class_weights = torch.tensor([false_weight, true_weight]).to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "#dev_dataloader = DataLoader(dev_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_covid(model, optim, loss_fn, dataloader, epochs):\n",
    "#def train_covid(model, optim, dataloader, epochs):\n",
    "  for epoch in range(epochs):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "\n",
    "      for batch_idx, batch in enumerate(dataloader):\n",
    "          optim.zero_grad()\n",
    "          \n",
    "          # unpack batch of form (tweets, labels)\n",
    "          claims, labels = batch\n",
    "          # send tweets dict's values to device\n",
    "          claims = {key: value.to(device) for key, value in claims.items()}\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          # forward pass on BERT\n",
    "          outputs = model(**claims, labels=labels)\n",
    "          logits = outputs.logits\n",
    "          \n",
    "          # class weighted CrossEntropyLoss\n",
    "          loss = loss_fn(logits, labels)\n",
    "          \n",
    "          # loss provided by model\n",
    "          #loss = outputs.loss \n",
    "\n",
    "          # backwards pass on BERT\n",
    "          loss.backward()\n",
    "          optim.step()\n",
    "\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "      print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(model.config.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1/211, Loss: 0.7164663076400757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asubedi\\AppData\\Local\\Temp\\ipykernel_280080\\751014086.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  claim = {key: torch.tensor(value, dtype=torch.long) for key, value in self.data[idx].items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 2/211, Loss: 0.748382031917572\n",
      "Epoch 1, Batch 3/211, Loss: 0.6427400708198547\n",
      "Epoch 1, Batch 4/211, Loss: 0.7271836996078491\n",
      "Epoch 1, Batch 5/211, Loss: 0.7478856444358826\n",
      "Epoch 1, Batch 6/211, Loss: 0.6884716749191284\n",
      "Epoch 1, Batch 7/211, Loss: 0.7412406802177429\n",
      "Epoch 1, Batch 8/211, Loss: 0.6987291574478149\n",
      "Epoch 1, Batch 9/211, Loss: 0.6998040080070496\n",
      "Epoch 1, Batch 10/211, Loss: 0.7079986929893494\n",
      "Epoch 1, Batch 11/211, Loss: 0.6642714142799377\n",
      "Epoch 1, Batch 12/211, Loss: 0.7646110653877258\n",
      "Epoch 1, Batch 13/211, Loss: 0.6738997101783752\n",
      "Epoch 1, Batch 14/211, Loss: 0.7221581339836121\n",
      "Epoch 1, Batch 15/211, Loss: 0.7242905497550964\n",
      "Epoch 1, Batch 16/211, Loss: 0.6721864342689514\n",
      "Epoch 1, Batch 17/211, Loss: 0.7252640128135681\n",
      "Epoch 1, Batch 18/211, Loss: 0.6813012957572937\n",
      "Epoch 1, Batch 19/211, Loss: 0.7155381441116333\n",
      "Epoch 1, Batch 20/211, Loss: 0.69950270652771\n",
      "Epoch 1, Batch 21/211, Loss: 0.6960915923118591\n",
      "Epoch 1, Batch 22/211, Loss: 0.7300071716308594\n",
      "Epoch 1, Batch 23/211, Loss: 0.7156476378440857\n",
      "Epoch 1, Batch 24/211, Loss: 0.6814598441123962\n",
      "Epoch 1, Batch 25/211, Loss: 0.6834345459938049\n",
      "Epoch 1, Batch 26/211, Loss: 0.7058224678039551\n",
      "Epoch 1, Batch 27/211, Loss: 0.7025312781333923\n",
      "Epoch 1, Batch 28/211, Loss: 0.6953522562980652\n",
      "Epoch 1, Batch 29/211, Loss: 0.703943133354187\n",
      "Epoch 1, Batch 30/211, Loss: 0.6880922317504883\n",
      "Epoch 1, Batch 31/211, Loss: 0.6875811815261841\n",
      "Epoch 1, Batch 32/211, Loss: 0.689727246761322\n",
      "Epoch 1, Batch 33/211, Loss: 0.709315299987793\n",
      "Epoch 1, Batch 34/211, Loss: 0.698483407497406\n",
      "Epoch 1, Batch 35/211, Loss: 0.7026551365852356\n",
      "Epoch 1, Batch 36/211, Loss: 0.6735501885414124\n",
      "Epoch 1, Batch 37/211, Loss: 0.7279521226882935\n",
      "Epoch 1, Batch 38/211, Loss: 0.6787523031234741\n",
      "Epoch 1, Batch 39/211, Loss: 0.7025257349014282\n",
      "Epoch 1, Batch 40/211, Loss: 0.7043389678001404\n",
      "Epoch 1, Batch 41/211, Loss: 0.6807456612586975\n",
      "Epoch 1, Batch 42/211, Loss: 0.693946361541748\n",
      "Epoch 1, Batch 43/211, Loss: 0.7009565830230713\n",
      "Epoch 1, Batch 44/211, Loss: 0.7096526622772217\n",
      "Epoch 1, Batch 45/211, Loss: 0.7116255164146423\n",
      "Epoch 1, Batch 46/211, Loss: 0.6979079842567444\n",
      "Epoch 1, Batch 47/211, Loss: 0.6944831609725952\n",
      "Epoch 1, Batch 48/211, Loss: 0.7186393737792969\n",
      "Epoch 1, Batch 49/211, Loss: 0.6897295117378235\n",
      "Epoch 1, Batch 50/211, Loss: 0.6978186964988708\n",
      "Epoch 1, Batch 51/211, Loss: 0.6949367523193359\n",
      "Epoch 1, Batch 52/211, Loss: 0.686707615852356\n",
      "Epoch 1, Batch 53/211, Loss: 0.6960332989692688\n",
      "Epoch 1, Batch 54/211, Loss: 0.6931469440460205\n",
      "Epoch 1, Batch 55/211, Loss: 0.7302711606025696\n",
      "Epoch 1, Batch 56/211, Loss: 0.6879842281341553\n",
      "Epoch 1, Batch 57/211, Loss: 0.7134514451026917\n",
      "Epoch 1, Batch 58/211, Loss: 0.6999327540397644\n",
      "Epoch 1, Batch 59/211, Loss: 0.6990451812744141\n",
      "Epoch 1, Batch 60/211, Loss: 0.7165654897689819\n",
      "Epoch 1, Batch 61/211, Loss: 0.6806734800338745\n",
      "Epoch 1, Batch 62/211, Loss: 0.7083641290664673\n",
      "Epoch 1, Batch 63/211, Loss: 0.6814228892326355\n",
      "Epoch 1, Batch 64/211, Loss: 0.6965856552124023\n",
      "Epoch 1, Batch 65/211, Loss: 0.6989946365356445\n",
      "Epoch 1, Batch 66/211, Loss: 0.7200817465782166\n",
      "Epoch 1, Batch 67/211, Loss: 0.6968430280685425\n",
      "Epoch 1, Batch 68/211, Loss: 0.6848928332328796\n",
      "Epoch 1, Batch 69/211, Loss: 0.6850740313529968\n",
      "Epoch 1, Batch 70/211, Loss: 0.7036576867103577\n",
      "Epoch 1, Batch 71/211, Loss: 0.6910374164581299\n",
      "Epoch 1, Batch 72/211, Loss: 0.691874086856842\n",
      "Epoch 1, Batch 73/211, Loss: 0.690255880355835\n",
      "Epoch 1, Batch 74/211, Loss: 0.6905357241630554\n",
      "Epoch 1, Batch 75/211, Loss: 0.6869504451751709\n",
      "Epoch 1, Batch 76/211, Loss: 0.6965992450714111\n",
      "Epoch 1, Batch 77/211, Loss: 0.7127654552459717\n",
      "Epoch 1, Batch 78/211, Loss: 0.6846805214881897\n",
      "Epoch 1, Batch 79/211, Loss: 0.6965305209159851\n",
      "Epoch 1, Batch 80/211, Loss: 0.7234406471252441\n",
      "Epoch 1, Batch 81/211, Loss: 0.6941775679588318\n",
      "Epoch 1, Batch 82/211, Loss: 0.6911259889602661\n",
      "Epoch 1, Batch 83/211, Loss: 0.7094281315803528\n",
      "Epoch 1, Batch 84/211, Loss: 0.7141075134277344\n",
      "Epoch 1, Batch 85/211, Loss: 0.6899498105049133\n",
      "Epoch 1, Batch 86/211, Loss: 0.6885648369789124\n",
      "Epoch 1, Batch 87/211, Loss: 0.6944282054901123\n",
      "Epoch 1, Batch 88/211, Loss: 0.6933803558349609\n",
      "Epoch 1, Batch 89/211, Loss: 0.7207423448562622\n",
      "Epoch 1, Batch 90/211, Loss: 0.6847088932991028\n",
      "Epoch 1, Batch 91/211, Loss: 0.7002171874046326\n",
      "Epoch 1, Batch 92/211, Loss: 0.6713388562202454\n",
      "Epoch 1, Batch 93/211, Loss: 0.6909898519515991\n",
      "Epoch 1, Batch 94/211, Loss: 0.720412015914917\n",
      "Epoch 1, Batch 95/211, Loss: 0.7110589146614075\n",
      "Epoch 1, Batch 96/211, Loss: 0.6766482591629028\n",
      "Epoch 1, Batch 97/211, Loss: 0.689379096031189\n",
      "Epoch 1, Batch 98/211, Loss: 0.7082462310791016\n",
      "Epoch 1, Batch 99/211, Loss: 0.7054306268692017\n",
      "Epoch 1, Batch 100/211, Loss: 0.7026623487472534\n",
      "Epoch 1, Batch 101/211, Loss: 0.6838610768318176\n",
      "Epoch 1, Batch 102/211, Loss: 0.6642559766769409\n",
      "Epoch 1, Batch 103/211, Loss: 0.6903740167617798\n",
      "Epoch 1, Batch 104/211, Loss: 0.6962444186210632\n",
      "Epoch 1, Batch 105/211, Loss: 0.6939066648483276\n",
      "Epoch 1, Batch 106/211, Loss: 0.6973745822906494\n",
      "Epoch 1, Batch 107/211, Loss: 0.693697452545166\n",
      "Epoch 1, Batch 108/211, Loss: 0.7033271789550781\n",
      "Epoch 1, Batch 109/211, Loss: 0.6996264457702637\n",
      "Epoch 1, Batch 110/211, Loss: 0.6883487105369568\n",
      "Epoch 1, Batch 111/211, Loss: 0.6815007328987122\n",
      "Epoch 1, Batch 112/211, Loss: 0.6871867179870605\n",
      "Epoch 1, Batch 113/211, Loss: 0.7002867460250854\n",
      "Epoch 1, Batch 114/211, Loss: 0.6918830275535583\n",
      "Epoch 1, Batch 115/211, Loss: 0.6819332242012024\n",
      "Epoch 1, Batch 116/211, Loss: 0.706150233745575\n",
      "Epoch 1, Batch 117/211, Loss: 0.6878027319908142\n",
      "Epoch 1, Batch 118/211, Loss: 0.6999524831771851\n",
      "Epoch 1, Batch 119/211, Loss: 0.6711382269859314\n",
      "Epoch 1, Batch 120/211, Loss: 0.700371503829956\n",
      "Epoch 1, Batch 121/211, Loss: 0.7129706740379333\n",
      "Epoch 1, Batch 122/211, Loss: 0.6795496940612793\n",
      "Epoch 1, Batch 123/211, Loss: 0.7295021414756775\n",
      "Epoch 1, Batch 124/211, Loss: 0.689918577671051\n",
      "Epoch 1, Batch 125/211, Loss: 0.700628399848938\n",
      "Epoch 1, Batch 126/211, Loss: 0.7045418620109558\n",
      "Epoch 1, Batch 127/211, Loss: 0.6931942701339722\n",
      "Epoch 1, Batch 128/211, Loss: 0.6869792342185974\n",
      "Epoch 1, Batch 129/211, Loss: 0.6945274472236633\n",
      "Epoch 1, Batch 130/211, Loss: 0.7169342041015625\n",
      "Epoch 1, Batch 131/211, Loss: 0.6811580061912537\n",
      "Epoch 1, Batch 132/211, Loss: 0.6943411231040955\n",
      "Epoch 1, Batch 133/211, Loss: 0.6907037496566772\n",
      "Epoch 1, Batch 134/211, Loss: 0.697473406791687\n",
      "Epoch 1, Batch 135/211, Loss: 0.696220874786377\n",
      "Epoch 1, Batch 136/211, Loss: 0.6826699376106262\n",
      "Epoch 1, Batch 137/211, Loss: 0.6919392943382263\n",
      "Epoch 1, Batch 138/211, Loss: 0.6948182582855225\n",
      "Epoch 1, Batch 139/211, Loss: 0.7200868725776672\n",
      "Epoch 1, Batch 140/211, Loss: 0.6795579195022583\n",
      "Epoch 1, Batch 141/211, Loss: 0.6923383474349976\n",
      "Epoch 1, Batch 142/211, Loss: 0.70168536901474\n",
      "Epoch 1, Batch 143/211, Loss: 0.6883427500724792\n",
      "Epoch 1, Batch 144/211, Loss: 0.6788667440414429\n",
      "Epoch 1, Batch 145/211, Loss: 0.7010905742645264\n",
      "Epoch 1, Batch 146/211, Loss: 0.680865466594696\n",
      "Epoch 1, Batch 147/211, Loss: 0.6886865496635437\n",
      "Epoch 1, Batch 148/211, Loss: 0.7125058174133301\n",
      "Epoch 1, Batch 149/211, Loss: 0.6938223242759705\n",
      "Epoch 1, Batch 150/211, Loss: 0.7313275337219238\n",
      "Epoch 1, Batch 151/211, Loss: 0.6961806416511536\n",
      "Epoch 1, Batch 152/211, Loss: 0.7052834033966064\n",
      "Epoch 1, Batch 153/211, Loss: 0.7132744193077087\n",
      "Epoch 1, Batch 154/211, Loss: 0.7039124965667725\n",
      "Epoch 1, Batch 155/211, Loss: 0.6856041550636292\n",
      "Epoch 1, Batch 156/211, Loss: 0.7122513055801392\n",
      "Epoch 1, Batch 157/211, Loss: 0.6834716796875\n",
      "Epoch 1, Batch 158/211, Loss: 0.7123582363128662\n",
      "Epoch 1, Batch 159/211, Loss: 0.6717401742935181\n",
      "Epoch 1, Batch 160/211, Loss: 0.7142549157142639\n",
      "Epoch 1, Batch 161/211, Loss: 0.6724600195884705\n",
      "Epoch 1, Batch 162/211, Loss: 0.7002891302108765\n",
      "Epoch 1, Batch 163/211, Loss: 0.7018360495567322\n",
      "Epoch 1, Batch 164/211, Loss: 0.7043085098266602\n",
      "Epoch 1, Batch 165/211, Loss: 0.7101923227310181\n",
      "Epoch 1, Batch 166/211, Loss: 0.6773645877838135\n",
      "Epoch 1, Batch 167/211, Loss: 0.6826578378677368\n",
      "Epoch 1, Batch 168/211, Loss: 0.7008199691772461\n",
      "Epoch 1, Batch 169/211, Loss: 0.6970129609107971\n",
      "Epoch 1, Batch 170/211, Loss: 0.7031224370002747\n",
      "Epoch 1, Batch 171/211, Loss: 0.6896559596061707\n",
      "Epoch 1, Batch 172/211, Loss: 0.683705747127533\n",
      "Epoch 1, Batch 173/211, Loss: 0.7063825130462646\n",
      "Epoch 1, Batch 174/211, Loss: 0.6772152185440063\n",
      "Epoch 1, Batch 175/211, Loss: 0.7071269750595093\n",
      "Epoch 1, Batch 176/211, Loss: 0.7139866948127747\n",
      "Epoch 1, Batch 177/211, Loss: 0.6981999278068542\n",
      "Epoch 1, Batch 178/211, Loss: 0.6929026246070862\n",
      "Epoch 1, Batch 179/211, Loss: 0.6886666417121887\n",
      "Epoch 1, Batch 180/211, Loss: 0.7125439643859863\n",
      "Epoch 1, Batch 181/211, Loss: 0.6810120940208435\n",
      "Epoch 1, Batch 182/211, Loss: 0.7072075605392456\n",
      "Epoch 1, Batch 183/211, Loss: 0.7201636433601379\n",
      "Epoch 1, Batch 184/211, Loss: 0.7144042253494263\n",
      "Epoch 1, Batch 185/211, Loss: 0.710436999797821\n",
      "Epoch 1, Batch 186/211, Loss: 0.6823845505714417\n",
      "Epoch 1, Batch 187/211, Loss: 0.6900672316551208\n",
      "Epoch 1, Batch 188/211, Loss: 0.700949490070343\n",
      "Epoch 1, Batch 189/211, Loss: 0.6701917052268982\n",
      "Epoch 1, Batch 190/211, Loss: 0.7040717601776123\n",
      "Epoch 1, Batch 191/211, Loss: 0.7086431980133057\n",
      "Epoch 1, Batch 192/211, Loss: 0.6919739246368408\n",
      "Epoch 1, Batch 193/211, Loss: 0.6880148649215698\n",
      "Epoch 1, Batch 194/211, Loss: 0.6791443824768066\n",
      "Epoch 1, Batch 195/211, Loss: 0.685945451259613\n",
      "Epoch 1, Batch 196/211, Loss: 0.6829141974449158\n",
      "Epoch 1, Batch 197/211, Loss: 0.6618263125419617\n",
      "Epoch 1, Batch 198/211, Loss: 0.6790683269500732\n",
      "Epoch 1, Batch 199/211, Loss: 0.7170379757881165\n",
      "Epoch 1, Batch 200/211, Loss: 0.6933221817016602\n",
      "Epoch 1, Batch 201/211, Loss: 0.7087774276733398\n",
      "Epoch 1, Batch 202/211, Loss: 0.6776719093322754\n",
      "Epoch 1, Batch 203/211, Loss: 0.6845986247062683\n",
      "Epoch 1, Batch 204/211, Loss: 0.6760780811309814\n",
      "Epoch 1, Batch 205/211, Loss: 0.693229615688324\n",
      "Epoch 1, Batch 206/211, Loss: 0.6835296154022217\n",
      "Epoch 1, Batch 207/211, Loss: 0.6770980954170227\n",
      "Epoch 1, Batch 208/211, Loss: 0.7078438401222229\n",
      "Epoch 1, Batch 209/211, Loss: 0.688198447227478\n",
      "Epoch 1, Batch 210/211, Loss: 0.6808169484138489\n",
      "Epoch 1, Batch 211/211, Loss: 0.7375789284706116\n",
      "Epoch 1, Loss: 147.16047936677933\n",
      "Epoch 2, Batch 1/211, Loss: 0.6766804456710815\n",
      "Epoch 2, Batch 2/211, Loss: 0.6632675528526306\n",
      "Epoch 2, Batch 3/211, Loss: 0.7106578350067139\n",
      "Epoch 2, Batch 4/211, Loss: 0.7218387126922607\n",
      "Epoch 2, Batch 5/211, Loss: 0.6767046451568604\n",
      "Epoch 2, Batch 6/211, Loss: 0.6932331323623657\n",
      "Epoch 2, Batch 7/211, Loss: 0.7259387969970703\n",
      "Epoch 2, Batch 8/211, Loss: 0.6958390474319458\n",
      "Epoch 2, Batch 9/211, Loss: 0.6970903277397156\n",
      "Epoch 2, Batch 10/211, Loss: 0.7100586295127869\n",
      "Epoch 2, Batch 11/211, Loss: 0.7073438167572021\n",
      "Epoch 2, Batch 12/211, Loss: 0.6889230012893677\n",
      "Epoch 2, Batch 13/211, Loss: 0.6730063557624817\n",
      "Epoch 2, Batch 14/211, Loss: 0.6715382933616638\n",
      "Epoch 2, Batch 15/211, Loss: 0.7209344506263733\n",
      "Epoch 2, Batch 16/211, Loss: 0.6932804584503174\n",
      "Epoch 2, Batch 17/211, Loss: 0.6916428208351135\n",
      "Epoch 2, Batch 18/211, Loss: 0.6830454468727112\n",
      "Epoch 2, Batch 19/211, Loss: 0.6771951913833618\n",
      "Epoch 2, Batch 20/211, Loss: 0.7001349329948425\n",
      "Epoch 2, Batch 21/211, Loss: 0.6880632042884827\n",
      "Epoch 2, Batch 22/211, Loss: 0.7191280126571655\n",
      "Epoch 2, Batch 23/211, Loss: 0.6906087398529053\n",
      "Epoch 2, Batch 24/211, Loss: 0.6934719085693359\n",
      "Epoch 2, Batch 25/211, Loss: 0.6961217522621155\n",
      "Epoch 2, Batch 26/211, Loss: 0.7129507064819336\n",
      "Epoch 2, Batch 27/211, Loss: 0.6917998790740967\n",
      "Epoch 2, Batch 28/211, Loss: 0.7088131904602051\n",
      "Epoch 2, Batch 29/211, Loss: 0.6940432190895081\n",
      "Epoch 2, Batch 30/211, Loss: 0.7032007575035095\n",
      "Epoch 2, Batch 31/211, Loss: 0.6873680353164673\n",
      "Epoch 2, Batch 32/211, Loss: 0.7050716280937195\n",
      "Epoch 2, Batch 33/211, Loss: 0.7082214951515198\n",
      "Epoch 2, Batch 34/211, Loss: 0.6962499022483826\n",
      "Epoch 2, Batch 35/211, Loss: 0.6852232813835144\n",
      "Epoch 2, Batch 36/211, Loss: 0.7052605748176575\n",
      "Epoch 2, Batch 37/211, Loss: 0.7147497534751892\n",
      "Epoch 2, Batch 38/211, Loss: 0.6897324323654175\n",
      "Epoch 2, Batch 39/211, Loss: 0.6937363147735596\n",
      "Epoch 2, Batch 40/211, Loss: 0.6954435110092163\n",
      "Epoch 2, Batch 41/211, Loss: 0.6871432065963745\n",
      "Epoch 2, Batch 42/211, Loss: 0.7093738913536072\n",
      "Epoch 2, Batch 43/211, Loss: 0.7117112278938293\n",
      "Epoch 2, Batch 44/211, Loss: 0.6839777231216431\n",
      "Epoch 2, Batch 45/211, Loss: 0.6981306076049805\n",
      "Epoch 2, Batch 46/211, Loss: 0.7050333619117737\n",
      "Epoch 2, Batch 47/211, Loss: 0.7182542681694031\n",
      "Epoch 2, Batch 48/211, Loss: 0.6980741024017334\n",
      "Epoch 2, Batch 49/211, Loss: 0.689995527267456\n",
      "Epoch 2, Batch 50/211, Loss: 0.7019062638282776\n",
      "Epoch 2, Batch 51/211, Loss: 0.6926600933074951\n",
      "Epoch 2, Batch 52/211, Loss: 0.6783899068832397\n",
      "Epoch 2, Batch 53/211, Loss: 0.6857211589813232\n",
      "Epoch 2, Batch 54/211, Loss: 0.6964903473854065\n",
      "Epoch 2, Batch 55/211, Loss: 0.6701955795288086\n",
      "Epoch 2, Batch 56/211, Loss: 0.6860215663909912\n",
      "Epoch 2, Batch 57/211, Loss: 0.6911638975143433\n",
      "Epoch 2, Batch 58/211, Loss: 0.6854551434516907\n",
      "Epoch 2, Batch 59/211, Loss: 0.6816302537918091\n",
      "Epoch 2, Batch 60/211, Loss: 0.6868334412574768\n",
      "Epoch 2, Batch 61/211, Loss: 0.7170679569244385\n",
      "Epoch 2, Batch 62/211, Loss: 0.6912534832954407\n",
      "Epoch 2, Batch 63/211, Loss: 0.6771405935287476\n",
      "Epoch 2, Batch 64/211, Loss: 0.6902388334274292\n",
      "Epoch 2, Batch 65/211, Loss: 0.706486165523529\n",
      "Epoch 2, Batch 66/211, Loss: 0.6900622248649597\n",
      "Epoch 2, Batch 67/211, Loss: 0.6777330636978149\n",
      "Epoch 2, Batch 68/211, Loss: 0.6677082777023315\n",
      "Epoch 2, Batch 69/211, Loss: 0.6877778768539429\n",
      "Epoch 2, Batch 70/211, Loss: 0.6945046782493591\n",
      "Epoch 2, Batch 71/211, Loss: 0.6835113763809204\n",
      "Epoch 2, Batch 72/211, Loss: 0.7041404247283936\n",
      "Epoch 2, Batch 73/211, Loss: 0.6740603446960449\n",
      "Epoch 2, Batch 74/211, Loss: 0.6858673095703125\n",
      "Epoch 2, Batch 75/211, Loss: 0.6865595579147339\n",
      "Epoch 2, Batch 76/211, Loss: 0.70490962266922\n",
      "Epoch 2, Batch 77/211, Loss: 0.6933656334877014\n",
      "Epoch 2, Batch 78/211, Loss: 0.7042565941810608\n",
      "Epoch 2, Batch 79/211, Loss: 0.6721972823143005\n",
      "Epoch 2, Batch 80/211, Loss: 0.7103855609893799\n",
      "Epoch 2, Batch 81/211, Loss: 0.6992049813270569\n",
      "Epoch 2, Batch 82/211, Loss: 0.7130101919174194\n",
      "Epoch 2, Batch 83/211, Loss: 0.7212845683097839\n",
      "Epoch 2, Batch 84/211, Loss: 0.6895717978477478\n",
      "Epoch 2, Batch 85/211, Loss: 0.695356011390686\n",
      "Epoch 2, Batch 86/211, Loss: 0.7023268938064575\n",
      "Epoch 2, Batch 87/211, Loss: 0.6572821736335754\n",
      "Epoch 2, Batch 88/211, Loss: 0.709164559841156\n",
      "Epoch 2, Batch 89/211, Loss: 0.7142243385314941\n",
      "Epoch 2, Batch 90/211, Loss: 0.6854631900787354\n",
      "Epoch 2, Batch 91/211, Loss: 0.6798980236053467\n",
      "Epoch 2, Batch 92/211, Loss: 0.6800888776779175\n",
      "Epoch 2, Batch 93/211, Loss: 0.6932811141014099\n",
      "Epoch 2, Batch 94/211, Loss: 0.6979209780693054\n",
      "Epoch 2, Batch 95/211, Loss: 0.70072340965271\n",
      "Epoch 2, Batch 96/211, Loss: 0.6840391159057617\n",
      "Epoch 2, Batch 97/211, Loss: 0.705099880695343\n",
      "Epoch 2, Batch 98/211, Loss: 0.6900220513343811\n",
      "Epoch 2, Batch 99/211, Loss: 0.6805477738380432\n",
      "Epoch 2, Batch 100/211, Loss: 0.6924293041229248\n",
      "Epoch 2, Batch 101/211, Loss: 0.690150797367096\n",
      "Epoch 2, Batch 102/211, Loss: 0.6840973496437073\n",
      "Epoch 2, Batch 103/211, Loss: 0.6984716653823853\n",
      "Epoch 2, Batch 104/211, Loss: 0.6905422806739807\n",
      "Epoch 2, Batch 105/211, Loss: 0.696146547794342\n",
      "Epoch 2, Batch 106/211, Loss: 0.6966382265090942\n",
      "Epoch 2, Batch 107/211, Loss: 0.6902399659156799\n",
      "Epoch 2, Batch 108/211, Loss: 0.6911627650260925\n",
      "Epoch 2, Batch 109/211, Loss: 0.6871649622917175\n",
      "Epoch 2, Batch 110/211, Loss: 0.7058717012405396\n",
      "Epoch 2, Batch 111/211, Loss: 0.6949721574783325\n",
      "Epoch 2, Batch 112/211, Loss: 0.6933595538139343\n",
      "Epoch 2, Batch 113/211, Loss: 0.703624427318573\n",
      "Epoch 2, Batch 114/211, Loss: 0.6805441379547119\n",
      "Epoch 2, Batch 115/211, Loss: 0.6851813197135925\n",
      "Epoch 2, Batch 116/211, Loss: 0.6678202748298645\n",
      "Epoch 2, Batch 117/211, Loss: 0.7025566101074219\n",
      "Epoch 2, Batch 118/211, Loss: 0.7152052521705627\n",
      "Epoch 2, Batch 119/211, Loss: 0.7014073133468628\n",
      "Epoch 2, Batch 120/211, Loss: 0.6863284707069397\n",
      "Epoch 2, Batch 121/211, Loss: 0.6972368359565735\n",
      "Epoch 2, Batch 122/211, Loss: 0.6848945021629333\n",
      "Epoch 2, Batch 123/211, Loss: 0.7113191485404968\n",
      "Epoch 2, Batch 124/211, Loss: 0.6930978298187256\n",
      "Epoch 2, Batch 125/211, Loss: 0.6876127123832703\n",
      "Epoch 2, Batch 126/211, Loss: 0.6770352125167847\n",
      "Epoch 2, Batch 127/211, Loss: 0.7003179788589478\n",
      "Epoch 2, Batch 128/211, Loss: 0.6960871815681458\n",
      "Epoch 2, Batch 129/211, Loss: 0.6778076887130737\n",
      "Epoch 2, Batch 130/211, Loss: 0.7062047719955444\n",
      "Epoch 2, Batch 131/211, Loss: 0.7106380462646484\n",
      "Epoch 2, Batch 132/211, Loss: 0.6932494044303894\n",
      "Epoch 2, Batch 133/211, Loss: 0.6922184228897095\n",
      "Epoch 2, Batch 134/211, Loss: 0.6715455055236816\n",
      "Epoch 2, Batch 135/211, Loss: 0.6920052766799927\n",
      "Epoch 2, Batch 136/211, Loss: 0.6736590266227722\n",
      "Epoch 2, Batch 137/211, Loss: 0.6861103773117065\n",
      "Epoch 2, Batch 138/211, Loss: 0.696243941783905\n",
      "Epoch 2, Batch 139/211, Loss: 0.6879920363426208\n",
      "Epoch 2, Batch 140/211, Loss: 0.6805290579795837\n",
      "Epoch 2, Batch 141/211, Loss: 0.694057285785675\n",
      "Epoch 2, Batch 142/211, Loss: 0.6801163554191589\n",
      "Epoch 2, Batch 143/211, Loss: 0.6795721054077148\n",
      "Epoch 2, Batch 144/211, Loss: 0.670189619064331\n",
      "Epoch 2, Batch 145/211, Loss: 0.6867011189460754\n",
      "Epoch 2, Batch 146/211, Loss: 0.6955252885818481\n",
      "Epoch 2, Batch 147/211, Loss: 0.710721492767334\n",
      "Epoch 2, Batch 148/211, Loss: 0.689078152179718\n",
      "Epoch 2, Batch 149/211, Loss: 0.707484245300293\n",
      "Epoch 2, Batch 150/211, Loss: 0.6784350275993347\n",
      "Epoch 2, Batch 151/211, Loss: 0.71147620677948\n",
      "Epoch 2, Batch 152/211, Loss: 0.6894409656524658\n",
      "Epoch 2, Batch 153/211, Loss: 0.6833391189575195\n",
      "Epoch 2, Batch 154/211, Loss: 0.7008358836174011\n",
      "Epoch 2, Batch 155/211, Loss: 0.698245108127594\n",
      "Epoch 2, Batch 156/211, Loss: 0.6950012445449829\n",
      "Epoch 2, Batch 157/211, Loss: 0.7065220475196838\n",
      "Epoch 2, Batch 158/211, Loss: 0.7180600166320801\n",
      "Epoch 2, Batch 159/211, Loss: 0.680482804775238\n",
      "Epoch 2, Batch 160/211, Loss: 0.6877840757369995\n",
      "Epoch 2, Batch 161/211, Loss: 0.7344843745231628\n",
      "Epoch 2, Batch 162/211, Loss: 0.7157672047615051\n",
      "Epoch 2, Batch 163/211, Loss: 0.6993706226348877\n",
      "Epoch 2, Batch 164/211, Loss: 0.6897835731506348\n",
      "Epoch 2, Batch 165/211, Loss: 0.6903323531150818\n",
      "Epoch 2, Batch 166/211, Loss: 0.6853542923927307\n",
      "Epoch 2, Batch 167/211, Loss: 0.6788211464881897\n",
      "Epoch 2, Batch 168/211, Loss: 0.6740342378616333\n",
      "Epoch 2, Batch 169/211, Loss: 0.6650421023368835\n",
      "Epoch 2, Batch 170/211, Loss: 0.6905295252799988\n",
      "Epoch 2, Batch 171/211, Loss: 0.7018694281578064\n",
      "Epoch 2, Batch 172/211, Loss: 0.6832422018051147\n",
      "Epoch 2, Batch 173/211, Loss: 0.6783714890480042\n",
      "Epoch 2, Batch 174/211, Loss: 0.6858057975769043\n",
      "Epoch 2, Batch 175/211, Loss: 0.6941210627555847\n",
      "Epoch 2, Batch 176/211, Loss: 0.6906969547271729\n",
      "Epoch 2, Batch 177/211, Loss: 0.7071831822395325\n",
      "Epoch 2, Batch 178/211, Loss: 0.6830990314483643\n",
      "Epoch 2, Batch 179/211, Loss: 0.6982128620147705\n",
      "Epoch 2, Batch 180/211, Loss: 0.6867551803588867\n",
      "Epoch 2, Batch 181/211, Loss: 0.7008712887763977\n",
      "Epoch 2, Batch 182/211, Loss: 0.6736287474632263\n",
      "Epoch 2, Batch 183/211, Loss: 0.6989128589630127\n",
      "Epoch 2, Batch 184/211, Loss: 0.690302848815918\n",
      "Epoch 2, Batch 185/211, Loss: 0.6901711821556091\n",
      "Epoch 2, Batch 186/211, Loss: 0.6831260919570923\n",
      "Epoch 2, Batch 187/211, Loss: 0.6879702806472778\n",
      "Epoch 2, Batch 188/211, Loss: 0.6914401650428772\n",
      "Epoch 2, Batch 189/211, Loss: 0.680595874786377\n",
      "Epoch 2, Batch 190/211, Loss: 0.6687313318252563\n",
      "Epoch 2, Batch 191/211, Loss: 0.7085620164871216\n",
      "Epoch 2, Batch 192/211, Loss: 0.7116754651069641\n",
      "Epoch 2, Batch 193/211, Loss: 0.7199046611785889\n",
      "Epoch 2, Batch 194/211, Loss: 0.6827300190925598\n",
      "Epoch 2, Batch 195/211, Loss: 0.698667049407959\n",
      "Epoch 2, Batch 196/211, Loss: 0.712548017501831\n",
      "Epoch 2, Batch 197/211, Loss: 0.705748438835144\n",
      "Epoch 2, Batch 198/211, Loss: 0.7091653347015381\n",
      "Epoch 2, Batch 199/211, Loss: 0.6856877207756042\n",
      "Epoch 2, Batch 200/211, Loss: 0.7219240665435791\n",
      "Epoch 2, Batch 201/211, Loss: 0.6928926706314087\n",
      "Epoch 2, Batch 202/211, Loss: 0.7166902422904968\n",
      "Epoch 2, Batch 203/211, Loss: 0.6909180879592896\n",
      "Epoch 2, Batch 204/211, Loss: 0.7125988006591797\n",
      "Epoch 2, Batch 205/211, Loss: 0.6947303414344788\n",
      "Epoch 2, Batch 206/211, Loss: 0.6882639527320862\n",
      "Epoch 2, Batch 207/211, Loss: 0.6984692215919495\n",
      "Epoch 2, Batch 208/211, Loss: 0.7056440114974976\n",
      "Epoch 2, Batch 209/211, Loss: 0.7076480388641357\n",
      "Epoch 2, Batch 210/211, Loss: 0.6991983652114868\n",
      "Epoch 2, Batch 211/211, Loss: 0.8018772602081299\n",
      "Epoch 2, Loss: 146.53046983480453\n",
      "Epoch 3, Batch 1/211, Loss: 0.7216601371765137\n",
      "Epoch 3, Batch 2/211, Loss: 0.6881832480430603\n",
      "Epoch 3, Batch 3/211, Loss: 0.7140491604804993\n",
      "Epoch 3, Batch 4/211, Loss: 0.6822717189788818\n",
      "Epoch 3, Batch 5/211, Loss: 0.6907919645309448\n",
      "Epoch 3, Batch 6/211, Loss: 0.6934576034545898\n",
      "Epoch 3, Batch 7/211, Loss: 0.6974924802780151\n",
      "Epoch 3, Batch 8/211, Loss: 0.6875156760215759\n",
      "Epoch 3, Batch 9/211, Loss: 0.7079717516899109\n",
      "Epoch 3, Batch 10/211, Loss: 0.6875033378601074\n",
      "Epoch 3, Batch 11/211, Loss: 0.685345470905304\n",
      "Epoch 3, Batch 12/211, Loss: 0.6828742027282715\n",
      "Epoch 3, Batch 13/211, Loss: 0.6919046640396118\n",
      "Epoch 3, Batch 14/211, Loss: 0.6786508560180664\n",
      "Epoch 3, Batch 15/211, Loss: 0.7116115093231201\n",
      "Epoch 3, Batch 16/211, Loss: 0.7180343866348267\n",
      "Epoch 3, Batch 17/211, Loss: 0.6972447037696838\n",
      "Epoch 3, Batch 18/211, Loss: 0.6838670969009399\n",
      "Epoch 3, Batch 19/211, Loss: 0.7160061001777649\n",
      "Epoch 3, Batch 20/211, Loss: 0.7229021787643433\n",
      "Epoch 3, Batch 21/211, Loss: 0.6883394718170166\n",
      "Epoch 3, Batch 22/211, Loss: 0.6918238997459412\n",
      "Epoch 3, Batch 23/211, Loss: 0.6809862852096558\n",
      "Epoch 3, Batch 24/211, Loss: 0.6930978894233704\n",
      "Epoch 3, Batch 25/211, Loss: 0.7035552263259888\n",
      "Epoch 3, Batch 26/211, Loss: 0.7053537368774414\n",
      "Epoch 3, Batch 27/211, Loss: 0.7074539661407471\n",
      "Epoch 3, Batch 28/211, Loss: 0.698476254940033\n",
      "Epoch 3, Batch 29/211, Loss: 0.7090300917625427\n",
      "Epoch 3, Batch 30/211, Loss: 0.70207279920578\n",
      "Epoch 3, Batch 31/211, Loss: 0.6914757490158081\n",
      "Epoch 3, Batch 32/211, Loss: 0.6932708024978638\n",
      "Epoch 3, Batch 33/211, Loss: 0.6975628137588501\n",
      "Epoch 3, Batch 34/211, Loss: 0.6923003792762756\n",
      "Epoch 3, Batch 35/211, Loss: 0.7069427371025085\n",
      "Epoch 3, Batch 36/211, Loss: 0.6997132897377014\n",
      "Epoch 3, Batch 37/211, Loss: 0.6664314866065979\n",
      "Epoch 3, Batch 38/211, Loss: 0.6918670535087585\n",
      "Epoch 3, Batch 39/211, Loss: 0.6947733759880066\n",
      "Epoch 3, Batch 40/211, Loss: 0.6967265009880066\n",
      "Epoch 3, Batch 41/211, Loss: 0.7045229077339172\n",
      "Epoch 3, Batch 42/211, Loss: 0.6923226714134216\n",
      "Epoch 3, Batch 43/211, Loss: 0.6969566345214844\n",
      "Epoch 3, Batch 44/211, Loss: 0.6858387589454651\n",
      "Epoch 3, Batch 45/211, Loss: 0.7019624710083008\n",
      "Epoch 3, Batch 46/211, Loss: 0.6841610074043274\n",
      "Epoch 3, Batch 47/211, Loss: 0.6675757765769958\n",
      "Epoch 3, Batch 48/211, Loss: 0.6935789585113525\n",
      "Epoch 3, Batch 49/211, Loss: 0.6766476631164551\n",
      "Epoch 3, Batch 50/211, Loss: 0.6812170743942261\n",
      "Epoch 3, Batch 51/211, Loss: 0.6903826594352722\n",
      "Epoch 3, Batch 52/211, Loss: 0.6728976368904114\n",
      "Epoch 3, Batch 53/211, Loss: 0.6922580003738403\n",
      "Epoch 3, Batch 54/211, Loss: 0.6893829107284546\n",
      "Epoch 3, Batch 55/211, Loss: 0.6920047402381897\n",
      "Epoch 3, Batch 56/211, Loss: 0.7023026943206787\n",
      "Epoch 3, Batch 57/211, Loss: 0.709881603717804\n",
      "Epoch 3, Batch 58/211, Loss: 0.7037013173103333\n",
      "Epoch 3, Batch 59/211, Loss: 0.6872817873954773\n",
      "Epoch 3, Batch 60/211, Loss: 0.698117196559906\n",
      "Epoch 3, Batch 61/211, Loss: 0.7057068943977356\n",
      "Epoch 3, Batch 62/211, Loss: 0.6823553442955017\n",
      "Epoch 3, Batch 63/211, Loss: 0.6976855993270874\n",
      "Epoch 3, Batch 64/211, Loss: 0.6830788850784302\n",
      "Epoch 3, Batch 65/211, Loss: 0.6852017045021057\n",
      "Epoch 3, Batch 66/211, Loss: 0.6754596829414368\n",
      "Epoch 3, Batch 67/211, Loss: 0.6890255808830261\n",
      "Epoch 3, Batch 68/211, Loss: 0.6995863914489746\n",
      "Epoch 3, Batch 69/211, Loss: 0.6938018798828125\n",
      "Epoch 3, Batch 70/211, Loss: 0.6808681488037109\n",
      "Epoch 3, Batch 71/211, Loss: 0.6909539103507996\n",
      "Epoch 3, Batch 72/211, Loss: 0.6778889298439026\n",
      "Epoch 3, Batch 73/211, Loss: 0.688352108001709\n",
      "Epoch 3, Batch 74/211, Loss: 0.6959749460220337\n",
      "Epoch 3, Batch 75/211, Loss: 0.6964484453201294\n",
      "Epoch 3, Batch 76/211, Loss: 0.7115151882171631\n",
      "Epoch 3, Batch 77/211, Loss: 0.6826730966567993\n",
      "Epoch 3, Batch 78/211, Loss: 0.7073517441749573\n",
      "Epoch 3, Batch 79/211, Loss: 0.6965531706809998\n",
      "Epoch 3, Batch 80/211, Loss: 0.6784409880638123\n",
      "Epoch 3, Batch 81/211, Loss: 0.7186892032623291\n",
      "Epoch 3, Batch 82/211, Loss: 0.7017591595649719\n",
      "Epoch 3, Batch 83/211, Loss: 0.6727375388145447\n",
      "Epoch 3, Batch 84/211, Loss: 0.683525025844574\n",
      "Epoch 3, Batch 85/211, Loss: 0.6966283321380615\n",
      "Epoch 3, Batch 86/211, Loss: 0.6788626313209534\n",
      "Epoch 3, Batch 87/211, Loss: 0.6920057535171509\n",
      "Epoch 3, Batch 88/211, Loss: 0.7059299945831299\n",
      "Epoch 3, Batch 89/211, Loss: 0.6823636889457703\n",
      "Epoch 3, Batch 90/211, Loss: 0.6998552680015564\n",
      "Epoch 3, Batch 91/211, Loss: 0.7112810611724854\n",
      "Epoch 3, Batch 92/211, Loss: 0.698119044303894\n",
      "Epoch 3, Batch 93/211, Loss: 0.6898939609527588\n",
      "Epoch 3, Batch 94/211, Loss: 0.6665207743644714\n",
      "Epoch 3, Batch 95/211, Loss: 0.7022753953933716\n",
      "Epoch 3, Batch 96/211, Loss: 0.6745153665542603\n",
      "Epoch 3, Batch 97/211, Loss: 0.6868237853050232\n",
      "Epoch 3, Batch 98/211, Loss: 0.7169036269187927\n",
      "Epoch 3, Batch 99/211, Loss: 0.6992146372795105\n",
      "Epoch 3, Batch 100/211, Loss: 0.6956321597099304\n",
      "Epoch 3, Batch 101/211, Loss: 0.6954587697982788\n",
      "Epoch 3, Batch 102/211, Loss: 0.7025707364082336\n",
      "Epoch 3, Batch 103/211, Loss: 0.6711249351501465\n",
      "Epoch 3, Batch 104/211, Loss: 0.7037357687950134\n",
      "Epoch 3, Batch 105/211, Loss: 0.6966300010681152\n",
      "Epoch 3, Batch 106/211, Loss: 0.6897476315498352\n",
      "Epoch 3, Batch 107/211, Loss: 0.7053162455558777\n",
      "Epoch 3, Batch 108/211, Loss: 0.6793600916862488\n",
      "Epoch 3, Batch 109/211, Loss: 0.6967825889587402\n",
      "Epoch 3, Batch 110/211, Loss: 0.6797131299972534\n",
      "Epoch 3, Batch 111/211, Loss: 0.6883528232574463\n",
      "Epoch 3, Batch 112/211, Loss: 0.7130131125450134\n",
      "Epoch 3, Batch 113/211, Loss: 0.6785374879837036\n",
      "Epoch 3, Batch 114/211, Loss: 0.6888376474380493\n",
      "Epoch 3, Batch 115/211, Loss: 0.6834515929222107\n",
      "Epoch 3, Batch 116/211, Loss: 0.7041574120521545\n",
      "Epoch 3, Batch 117/211, Loss: 0.7026138305664062\n",
      "Epoch 3, Batch 118/211, Loss: 0.6926214098930359\n",
      "Epoch 3, Batch 119/211, Loss: 0.6611924767494202\n",
      "Epoch 3, Batch 120/211, Loss: 0.6681461334228516\n",
      "Epoch 3, Batch 121/211, Loss: 0.6723809242248535\n",
      "Epoch 3, Batch 122/211, Loss: 0.7032399773597717\n",
      "Epoch 3, Batch 123/211, Loss: 0.6774389743804932\n",
      "Epoch 3, Batch 124/211, Loss: 0.6770034432411194\n",
      "Epoch 3, Batch 125/211, Loss: 0.6859046220779419\n",
      "Epoch 3, Batch 126/211, Loss: 0.689914882183075\n",
      "Epoch 3, Batch 127/211, Loss: 0.6767067909240723\n",
      "Epoch 3, Batch 128/211, Loss: 0.7179507613182068\n",
      "Epoch 3, Batch 129/211, Loss: 0.721513032913208\n",
      "Epoch 3, Batch 130/211, Loss: 0.6946954131126404\n",
      "Epoch 3, Batch 131/211, Loss: 0.7041213512420654\n",
      "Epoch 3, Batch 132/211, Loss: 0.6822680830955505\n",
      "Epoch 3, Batch 133/211, Loss: 0.6936788558959961\n",
      "Epoch 3, Batch 134/211, Loss: 0.7010400891304016\n",
      "Epoch 3, Batch 135/211, Loss: 0.699945330619812\n",
      "Epoch 3, Batch 136/211, Loss: 0.6958721876144409\n",
      "Epoch 3, Batch 137/211, Loss: 0.6896525025367737\n",
      "Epoch 3, Batch 138/211, Loss: 0.6996622085571289\n",
      "Epoch 3, Batch 139/211, Loss: 0.6947638988494873\n",
      "Epoch 3, Batch 140/211, Loss: 0.6997331976890564\n",
      "Epoch 3, Batch 141/211, Loss: 0.6920319199562073\n",
      "Epoch 3, Batch 142/211, Loss: 0.6880939602851868\n",
      "Epoch 3, Batch 143/211, Loss: 0.6790683269500732\n",
      "Epoch 3, Batch 144/211, Loss: 0.7001031637191772\n",
      "Epoch 3, Batch 145/211, Loss: 0.6930524706840515\n",
      "Epoch 3, Batch 146/211, Loss: 0.6842416524887085\n",
      "Epoch 3, Batch 147/211, Loss: 0.6972919702529907\n",
      "Epoch 3, Batch 148/211, Loss: 0.6988767981529236\n",
      "Epoch 3, Batch 149/211, Loss: 0.6986768841743469\n",
      "Epoch 3, Batch 150/211, Loss: 0.7032379508018494\n",
      "Epoch 3, Batch 151/211, Loss: 0.7241209745407104\n",
      "Epoch 3, Batch 152/211, Loss: 0.7032199501991272\n",
      "Epoch 3, Batch 153/211, Loss: 0.6815734505653381\n",
      "Epoch 3, Batch 154/211, Loss: 0.6952468752861023\n",
      "Epoch 3, Batch 155/211, Loss: 0.7168061137199402\n",
      "Epoch 3, Batch 156/211, Loss: 0.7014850974082947\n",
      "Epoch 3, Batch 157/211, Loss: 0.6955312490463257\n",
      "Epoch 3, Batch 158/211, Loss: 0.6826232075691223\n",
      "Epoch 3, Batch 159/211, Loss: 0.6617209911346436\n",
      "Epoch 3, Batch 160/211, Loss: 0.7102941274642944\n",
      "Epoch 3, Batch 161/211, Loss: 0.6832841038703918\n",
      "Epoch 3, Batch 162/211, Loss: 0.6665618419647217\n",
      "Epoch 3, Batch 163/211, Loss: 0.6815253496170044\n",
      "Epoch 3, Batch 164/211, Loss: 0.6942126154899597\n",
      "Epoch 3, Batch 165/211, Loss: 0.6930903196334839\n",
      "Epoch 3, Batch 166/211, Loss: 0.7080584764480591\n",
      "Epoch 3, Batch 167/211, Loss: 0.6904948949813843\n",
      "Epoch 3, Batch 168/211, Loss: 0.6858820915222168\n",
      "Epoch 3, Batch 169/211, Loss: 0.6857023239135742\n",
      "Epoch 3, Batch 170/211, Loss: 0.7099835276603699\n",
      "Epoch 3, Batch 171/211, Loss: 0.7018195986747742\n",
      "Epoch 3, Batch 172/211, Loss: 0.6714162230491638\n",
      "Epoch 3, Batch 173/211, Loss: 0.6868119239807129\n",
      "Epoch 3, Batch 174/211, Loss: 0.6973374485969543\n",
      "Epoch 3, Batch 175/211, Loss: 0.6883859038352966\n",
      "Epoch 3, Batch 176/211, Loss: 0.7060227394104004\n",
      "Epoch 3, Batch 177/211, Loss: 0.7124969363212585\n",
      "Epoch 3, Batch 178/211, Loss: 0.7128089666366577\n",
      "Epoch 3, Batch 179/211, Loss: 0.7100796699523926\n",
      "Epoch 3, Batch 180/211, Loss: 0.6677647233009338\n",
      "Epoch 3, Batch 181/211, Loss: 0.700038492679596\n",
      "Epoch 3, Batch 182/211, Loss: 0.7259711623191833\n",
      "Epoch 3, Batch 183/211, Loss: 0.682689905166626\n",
      "Epoch 3, Batch 184/211, Loss: 0.6957038640975952\n",
      "Epoch 3, Batch 185/211, Loss: 0.7066212892532349\n",
      "Epoch 3, Batch 186/211, Loss: 0.6827918291091919\n",
      "Epoch 3, Batch 187/211, Loss: 0.6586267352104187\n",
      "Epoch 3, Batch 188/211, Loss: 0.6984919905662537\n",
      "Epoch 3, Batch 189/211, Loss: 0.6730037927627563\n",
      "Epoch 3, Batch 190/211, Loss: 0.7075535655021667\n",
      "Epoch 3, Batch 191/211, Loss: 0.6780721545219421\n",
      "Epoch 3, Batch 192/211, Loss: 0.674616813659668\n",
      "Epoch 3, Batch 193/211, Loss: 0.7067375183105469\n",
      "Epoch 3, Batch 194/211, Loss: 0.7039376497268677\n",
      "Epoch 3, Batch 195/211, Loss: 0.7237083911895752\n",
      "Epoch 3, Batch 196/211, Loss: 0.6857296228408813\n",
      "Epoch 3, Batch 197/211, Loss: 0.6747732758522034\n",
      "Epoch 3, Batch 198/211, Loss: 0.6823976039886475\n",
      "Epoch 3, Batch 199/211, Loss: 0.681418240070343\n",
      "Epoch 3, Batch 200/211, Loss: 0.6752913594245911\n",
      "Epoch 3, Batch 201/211, Loss: 0.6959336996078491\n",
      "Epoch 3, Batch 202/211, Loss: 0.6779070496559143\n",
      "Epoch 3, Batch 203/211, Loss: 0.7227609157562256\n",
      "Epoch 3, Batch 204/211, Loss: 0.6838923096656799\n",
      "Epoch 3, Batch 205/211, Loss: 0.6742440462112427\n",
      "Epoch 3, Batch 206/211, Loss: 0.6764926910400391\n",
      "Epoch 3, Batch 207/211, Loss: 0.693376898765564\n",
      "Epoch 3, Batch 208/211, Loss: 0.6802735924720764\n",
      "Epoch 3, Batch 209/211, Loss: 0.7039238214492798\n",
      "Epoch 3, Batch 210/211, Loss: 0.6818888187408447\n",
      "Epoch 3, Batch 211/211, Loss: 0.5104756951332092\n",
      "Epoch 3, Loss: 146.04527753591537\n",
      "Epoch 4, Batch 1/211, Loss: 0.723983883857727\n",
      "Epoch 4, Batch 2/211, Loss: 0.6569302082061768\n",
      "Epoch 4, Batch 3/211, Loss: 0.6892849206924438\n",
      "Epoch 4, Batch 4/211, Loss: 0.6879375576972961\n",
      "Epoch 4, Batch 5/211, Loss: 0.7016400694847107\n",
      "Epoch 4, Batch 6/211, Loss: 0.7037452459335327\n",
      "Epoch 4, Batch 7/211, Loss: 0.7070798277854919\n",
      "Epoch 4, Batch 8/211, Loss: 0.7118295431137085\n",
      "Epoch 4, Batch 9/211, Loss: 0.7043129205703735\n",
      "Epoch 4, Batch 10/211, Loss: 0.6743220686912537\n",
      "Epoch 4, Batch 11/211, Loss: 0.6675190925598145\n",
      "Epoch 4, Batch 12/211, Loss: 0.6828605532646179\n",
      "Epoch 4, Batch 13/211, Loss: 0.7022837400436401\n",
      "Epoch 4, Batch 14/211, Loss: 0.6676428914070129\n",
      "Epoch 4, Batch 15/211, Loss: 0.717329204082489\n",
      "Epoch 4, Batch 16/211, Loss: 0.7019518613815308\n",
      "Epoch 4, Batch 17/211, Loss: 0.7008583545684814\n",
      "Epoch 4, Batch 18/211, Loss: 0.6883497834205627\n",
      "Epoch 4, Batch 19/211, Loss: 0.7156876921653748\n",
      "Epoch 4, Batch 20/211, Loss: 0.688424825668335\n",
      "Epoch 4, Batch 21/211, Loss: 0.6944084167480469\n",
      "Epoch 4, Batch 22/211, Loss: 0.6889582872390747\n",
      "Epoch 4, Batch 23/211, Loss: 0.6773019433021545\n",
      "Epoch 4, Batch 24/211, Loss: 0.6785707473754883\n",
      "Epoch 4, Batch 25/211, Loss: 0.7231135964393616\n",
      "Epoch 4, Batch 26/211, Loss: 0.6874436736106873\n",
      "Epoch 4, Batch 27/211, Loss: 0.6652914881706238\n",
      "Epoch 4, Batch 28/211, Loss: 0.6902072429656982\n",
      "Epoch 4, Batch 29/211, Loss: 0.6777360439300537\n",
      "Epoch 4, Batch 30/211, Loss: 0.6822357773780823\n",
      "Epoch 4, Batch 31/211, Loss: 0.6750159859657288\n",
      "Epoch 4, Batch 32/211, Loss: 0.6965799331665039\n",
      "Epoch 4, Batch 33/211, Loss: 0.6941143274307251\n",
      "Epoch 4, Batch 34/211, Loss: 0.7009916305541992\n",
      "Epoch 4, Batch 35/211, Loss: 0.6818047761917114\n",
      "Epoch 4, Batch 36/211, Loss: 0.6628961563110352\n",
      "Epoch 4, Batch 37/211, Loss: 0.6937248110771179\n",
      "Epoch 4, Batch 38/211, Loss: 0.7200101017951965\n",
      "Epoch 4, Batch 39/211, Loss: 0.6930189728736877\n",
      "Epoch 4, Batch 40/211, Loss: 0.6871465444564819\n",
      "Epoch 4, Batch 41/211, Loss: 0.6790301203727722\n",
      "Epoch 4, Batch 42/211, Loss: 0.7126680016517639\n",
      "Epoch 4, Batch 43/211, Loss: 0.6567863821983337\n",
      "Epoch 4, Batch 44/211, Loss: 0.6945524215698242\n",
      "Epoch 4, Batch 45/211, Loss: 0.7042332291603088\n",
      "Epoch 4, Batch 46/211, Loss: 0.6911907196044922\n",
      "Epoch 4, Batch 47/211, Loss: 0.6920549869537354\n",
      "Epoch 4, Batch 48/211, Loss: 0.7165980339050293\n",
      "Epoch 4, Batch 49/211, Loss: 0.6823644638061523\n",
      "Epoch 4, Batch 50/211, Loss: 0.6742076873779297\n",
      "Epoch 4, Batch 51/211, Loss: 0.6761118173599243\n",
      "Epoch 4, Batch 52/211, Loss: 0.6855549812316895\n",
      "Epoch 4, Batch 53/211, Loss: 0.677941083908081\n",
      "Epoch 4, Batch 54/211, Loss: 0.687292218208313\n",
      "Epoch 4, Batch 55/211, Loss: 0.7049022316932678\n",
      "Epoch 4, Batch 56/211, Loss: 0.6939029693603516\n",
      "Epoch 4, Batch 57/211, Loss: 0.716861367225647\n",
      "Epoch 4, Batch 58/211, Loss: 0.6987096667289734\n",
      "Epoch 4, Batch 59/211, Loss: 0.6885760426521301\n",
      "Epoch 4, Batch 60/211, Loss: 0.7058443427085876\n",
      "Epoch 4, Batch 61/211, Loss: 0.6880046725273132\n",
      "Epoch 4, Batch 62/211, Loss: 0.7151185274124146\n",
      "Epoch 4, Batch 63/211, Loss: 0.6802269220352173\n",
      "Epoch 4, Batch 64/211, Loss: 0.6755203604698181\n",
      "Epoch 4, Batch 65/211, Loss: 0.7278845310211182\n",
      "Epoch 4, Batch 66/211, Loss: 0.6922526359558105\n",
      "Epoch 4, Batch 67/211, Loss: 0.718244731426239\n",
      "Epoch 4, Batch 68/211, Loss: 0.6905148029327393\n",
      "Epoch 4, Batch 69/211, Loss: 0.6992904543876648\n",
      "Epoch 4, Batch 70/211, Loss: 0.699910044670105\n",
      "Epoch 4, Batch 71/211, Loss: 0.6805505752563477\n",
      "Epoch 4, Batch 72/211, Loss: 0.6797448396682739\n",
      "Epoch 4, Batch 73/211, Loss: 0.7085253000259399\n",
      "Epoch 4, Batch 74/211, Loss: 0.7066329121589661\n",
      "Epoch 4, Batch 75/211, Loss: 0.6926537156105042\n",
      "Epoch 4, Batch 76/211, Loss: 0.6991174221038818\n",
      "Epoch 4, Batch 77/211, Loss: 0.7181087136268616\n",
      "Epoch 4, Batch 78/211, Loss: 0.7110399603843689\n",
      "Epoch 4, Batch 79/211, Loss: 0.6824524998664856\n",
      "Epoch 4, Batch 80/211, Loss: 0.7045035362243652\n",
      "Epoch 4, Batch 81/211, Loss: 0.6937224268913269\n",
      "Epoch 4, Batch 82/211, Loss: 0.6650915145874023\n",
      "Epoch 4, Batch 83/211, Loss: 0.7063933610916138\n",
      "Epoch 4, Batch 84/211, Loss: 0.6739398837089539\n",
      "Epoch 4, Batch 85/211, Loss: 0.6947303414344788\n",
      "Epoch 4, Batch 86/211, Loss: 0.70942223072052\n",
      "Epoch 4, Batch 87/211, Loss: 0.6725444197654724\n",
      "Epoch 4, Batch 88/211, Loss: 0.7113237977027893\n",
      "Epoch 4, Batch 89/211, Loss: 0.6979307532310486\n",
      "Epoch 4, Batch 90/211, Loss: 0.6790322065353394\n",
      "Epoch 4, Batch 91/211, Loss: 0.6819651126861572\n",
      "Epoch 4, Batch 92/211, Loss: 0.6961522698402405\n",
      "Epoch 4, Batch 93/211, Loss: 0.7061465382575989\n",
      "Epoch 4, Batch 94/211, Loss: 0.7079273462295532\n",
      "Epoch 4, Batch 95/211, Loss: 0.6893160939216614\n",
      "Epoch 4, Batch 96/211, Loss: 0.6892526745796204\n",
      "Epoch 4, Batch 97/211, Loss: 0.6878701448440552\n",
      "Epoch 4, Batch 98/211, Loss: 0.6788761019706726\n",
      "Epoch 4, Batch 99/211, Loss: 0.698983371257782\n",
      "Epoch 4, Batch 100/211, Loss: 0.7053853273391724\n",
      "Epoch 4, Batch 101/211, Loss: 0.725375235080719\n",
      "Epoch 4, Batch 102/211, Loss: 0.7078851461410522\n",
      "Epoch 4, Batch 103/211, Loss: 0.6737381815910339\n",
      "Epoch 4, Batch 104/211, Loss: 0.6965963244438171\n",
      "Epoch 4, Batch 105/211, Loss: 0.6662287712097168\n",
      "Epoch 4, Batch 106/211, Loss: 0.7127394676208496\n",
      "Epoch 4, Batch 107/211, Loss: 0.6910681128501892\n",
      "Epoch 4, Batch 108/211, Loss: 0.6910534501075745\n",
      "Epoch 4, Batch 109/211, Loss: 0.708500325679779\n",
      "Epoch 4, Batch 110/211, Loss: 0.6861947774887085\n",
      "Epoch 4, Batch 111/211, Loss: 0.6937683820724487\n",
      "Epoch 4, Batch 112/211, Loss: 0.6877248883247375\n",
      "Epoch 4, Batch 113/211, Loss: 0.6959466934204102\n",
      "Epoch 4, Batch 114/211, Loss: 0.6871659159660339\n",
      "Epoch 4, Batch 115/211, Loss: 0.6811593770980835\n",
      "Epoch 4, Batch 116/211, Loss: 0.6536604166030884\n",
      "Epoch 4, Batch 117/211, Loss: 0.7041217088699341\n",
      "Epoch 4, Batch 118/211, Loss: 0.6616912484169006\n",
      "Epoch 4, Batch 119/211, Loss: 0.6653265357017517\n",
      "Epoch 4, Batch 120/211, Loss: 0.6965355277061462\n",
      "Epoch 4, Batch 121/211, Loss: 0.69046950340271\n",
      "Epoch 4, Batch 122/211, Loss: 0.6986646056175232\n",
      "Epoch 4, Batch 123/211, Loss: 0.6561394333839417\n",
      "Epoch 4, Batch 124/211, Loss: 0.6690573692321777\n",
      "Epoch 4, Batch 125/211, Loss: 0.6665461659431458\n",
      "Epoch 4, Batch 126/211, Loss: 0.6965884566307068\n",
      "Epoch 4, Batch 127/211, Loss: 0.7005987763404846\n",
      "Epoch 4, Batch 128/211, Loss: 0.739743709564209\n",
      "Epoch 4, Batch 129/211, Loss: 0.6934523582458496\n",
      "Epoch 4, Batch 130/211, Loss: 0.7010268568992615\n",
      "Epoch 4, Batch 131/211, Loss: 0.7187780141830444\n",
      "Epoch 4, Batch 132/211, Loss: 0.721613883972168\n",
      "Epoch 4, Batch 133/211, Loss: 0.6868210434913635\n",
      "Epoch 4, Batch 134/211, Loss: 0.668005645275116\n",
      "Epoch 4, Batch 135/211, Loss: 0.7267138361930847\n",
      "Epoch 4, Batch 136/211, Loss: 0.6867235898971558\n",
      "Epoch 4, Batch 137/211, Loss: 0.6704850792884827\n",
      "Epoch 4, Batch 138/211, Loss: 0.7100434303283691\n",
      "Epoch 4, Batch 139/211, Loss: 0.7068856358528137\n",
      "Epoch 4, Batch 140/211, Loss: 0.7047979831695557\n",
      "Epoch 4, Batch 141/211, Loss: 0.6809073090553284\n",
      "Epoch 4, Batch 142/211, Loss: 0.6938841342926025\n",
      "Epoch 4, Batch 143/211, Loss: 0.7092328667640686\n",
      "Epoch 4, Batch 144/211, Loss: 0.7115006446838379\n",
      "Epoch 4, Batch 145/211, Loss: 0.6912112832069397\n",
      "Epoch 4, Batch 146/211, Loss: 0.7154139876365662\n",
      "Epoch 4, Batch 147/211, Loss: 0.6612947583198547\n",
      "Epoch 4, Batch 148/211, Loss: 0.6790949702262878\n",
      "Epoch 4, Batch 149/211, Loss: 0.6954300403594971\n",
      "Epoch 4, Batch 150/211, Loss: 0.7093974351882935\n",
      "Epoch 4, Batch 151/211, Loss: 0.7095786333084106\n",
      "Epoch 4, Batch 152/211, Loss: 0.7040072083473206\n",
      "Epoch 4, Batch 153/211, Loss: 0.7133861184120178\n",
      "Epoch 4, Batch 154/211, Loss: 0.7018486857414246\n",
      "Epoch 4, Batch 155/211, Loss: 0.690010130405426\n",
      "Epoch 4, Batch 156/211, Loss: 0.7099552750587463\n",
      "Epoch 4, Batch 157/211, Loss: 0.7022092342376709\n",
      "Epoch 4, Batch 158/211, Loss: 0.6912978291511536\n",
      "Epoch 4, Batch 159/211, Loss: 0.6815562844276428\n",
      "Epoch 4, Batch 160/211, Loss: 0.6911188364028931\n",
      "Epoch 4, Batch 161/211, Loss: 0.6818172931671143\n",
      "Epoch 4, Batch 162/211, Loss: 0.6839109063148499\n",
      "Epoch 4, Batch 163/211, Loss: 0.7226057052612305\n",
      "Epoch 4, Batch 164/211, Loss: 0.6786460876464844\n",
      "Epoch 4, Batch 165/211, Loss: 0.7180991768836975\n",
      "Epoch 4, Batch 166/211, Loss: 0.681806743144989\n",
      "Epoch 4, Batch 167/211, Loss: 0.6810264587402344\n",
      "Epoch 4, Batch 168/211, Loss: 0.7057165503501892\n",
      "Epoch 4, Batch 169/211, Loss: 0.6758103370666504\n",
      "Epoch 4, Batch 170/211, Loss: 0.660908579826355\n",
      "Epoch 4, Batch 171/211, Loss: 0.6708064675331116\n",
      "Epoch 4, Batch 172/211, Loss: 0.6744366884231567\n",
      "Epoch 4, Batch 173/211, Loss: 0.6938999891281128\n",
      "Epoch 4, Batch 174/211, Loss: 0.6822965741157532\n",
      "Epoch 4, Batch 175/211, Loss: 0.6977117657661438\n",
      "Epoch 4, Batch 176/211, Loss: 0.6612171530723572\n",
      "Epoch 4, Batch 177/211, Loss: 0.7102199196815491\n",
      "Epoch 4, Batch 178/211, Loss: 0.6774132251739502\n",
      "Epoch 4, Batch 179/211, Loss: 0.6964653730392456\n",
      "Epoch 4, Batch 180/211, Loss: 0.686190128326416\n",
      "Epoch 4, Batch 181/211, Loss: 0.6624371409416199\n",
      "Epoch 4, Batch 182/211, Loss: 0.6862838268280029\n",
      "Epoch 4, Batch 183/211, Loss: 0.7056116461753845\n",
      "Epoch 4, Batch 184/211, Loss: 0.6678032875061035\n",
      "Epoch 4, Batch 185/211, Loss: 0.7169817090034485\n",
      "Epoch 4, Batch 186/211, Loss: 0.6743252277374268\n",
      "Epoch 4, Batch 187/211, Loss: 0.6972433924674988\n",
      "Epoch 4, Batch 188/211, Loss: 0.6898337602615356\n",
      "Epoch 4, Batch 189/211, Loss: 0.6912952661514282\n",
      "Epoch 4, Batch 190/211, Loss: 0.6752762198448181\n",
      "Epoch 4, Batch 191/211, Loss: 0.7267307639122009\n",
      "Epoch 4, Batch 192/211, Loss: 0.7234975099563599\n",
      "Epoch 4, Batch 193/211, Loss: 0.6943812966346741\n",
      "Epoch 4, Batch 194/211, Loss: 0.7216137647628784\n",
      "Epoch 4, Batch 195/211, Loss: 0.6981722712516785\n",
      "Epoch 4, Batch 196/211, Loss: 0.6609896421432495\n",
      "Epoch 4, Batch 197/211, Loss: 0.6869530081748962\n",
      "Epoch 4, Batch 198/211, Loss: 0.6948108077049255\n",
      "Epoch 4, Batch 199/211, Loss: 0.7027137279510498\n",
      "Epoch 4, Batch 200/211, Loss: 0.7012826800346375\n",
      "Epoch 4, Batch 201/211, Loss: 0.695490837097168\n",
      "Epoch 4, Batch 202/211, Loss: 0.6971387267112732\n",
      "Epoch 4, Batch 203/211, Loss: 0.668394148349762\n",
      "Epoch 4, Batch 204/211, Loss: 0.684258759021759\n",
      "Epoch 4, Batch 205/211, Loss: 0.6927480101585388\n",
      "Epoch 4, Batch 206/211, Loss: 0.6830459237098694\n",
      "Epoch 4, Batch 207/211, Loss: 0.7152493000030518\n",
      "Epoch 4, Batch 208/211, Loss: 0.7042719721794128\n",
      "Epoch 4, Batch 209/211, Loss: 0.6998302340507507\n",
      "Epoch 4, Batch 210/211, Loss: 0.6909425854682922\n",
      "Epoch 4, Batch 211/211, Loss: 0.7665749192237854\n",
      "Epoch 4, Loss: 146.27938956022263\n",
      "Epoch 5, Batch 1/211, Loss: 0.6887096166610718\n",
      "Epoch 5, Batch 2/211, Loss: 0.6981191039085388\n",
      "Epoch 5, Batch 3/211, Loss: 0.6905254125595093\n",
      "Epoch 5, Batch 4/211, Loss: 0.6858925819396973\n",
      "Epoch 5, Batch 5/211, Loss: 0.6780665516853333\n",
      "Epoch 5, Batch 6/211, Loss: 0.7319448590278625\n",
      "Epoch 5, Batch 7/211, Loss: 0.6984086632728577\n",
      "Epoch 5, Batch 8/211, Loss: 0.7016670107841492\n",
      "Epoch 5, Batch 9/211, Loss: 0.6820989847183228\n",
      "Epoch 5, Batch 10/211, Loss: 0.6941738724708557\n",
      "Epoch 5, Batch 11/211, Loss: 0.7212501764297485\n",
      "Epoch 5, Batch 12/211, Loss: 0.7057449817657471\n",
      "Epoch 5, Batch 13/211, Loss: 0.698884904384613\n",
      "Epoch 5, Batch 14/211, Loss: 0.6900658011436462\n",
      "Epoch 5, Batch 15/211, Loss: 0.6943002343177795\n",
      "Epoch 5, Batch 16/211, Loss: 0.691635012626648\n",
      "Epoch 5, Batch 17/211, Loss: 0.6736516356468201\n",
      "Epoch 5, Batch 18/211, Loss: 0.6951387524604797\n",
      "Epoch 5, Batch 19/211, Loss: 0.6840808987617493\n",
      "Epoch 5, Batch 20/211, Loss: 0.6982691287994385\n",
      "Epoch 5, Batch 21/211, Loss: 0.677053689956665\n",
      "Epoch 5, Batch 22/211, Loss: 0.6997488737106323\n",
      "Epoch 5, Batch 23/211, Loss: 0.6555752158164978\n",
      "Epoch 5, Batch 24/211, Loss: 0.6953635215759277\n",
      "Epoch 5, Batch 25/211, Loss: 0.7052088379859924\n",
      "Epoch 5, Batch 26/211, Loss: 0.6704362034797668\n",
      "Epoch 5, Batch 27/211, Loss: 0.7005450129508972\n",
      "Epoch 5, Batch 28/211, Loss: 0.6747456192970276\n",
      "Epoch 5, Batch 29/211, Loss: 0.6882767081260681\n",
      "Epoch 5, Batch 30/211, Loss: 0.6904041171073914\n",
      "Epoch 5, Batch 31/211, Loss: 0.7073152661323547\n",
      "Epoch 5, Batch 32/211, Loss: 0.7095290422439575\n",
      "Epoch 5, Batch 33/211, Loss: 0.6875324845314026\n",
      "Epoch 5, Batch 34/211, Loss: 0.6931285858154297\n",
      "Epoch 5, Batch 35/211, Loss: 0.6743649840354919\n",
      "Epoch 5, Batch 36/211, Loss: 0.703409731388092\n",
      "Epoch 5, Batch 37/211, Loss: 0.6789212226867676\n",
      "Epoch 5, Batch 38/211, Loss: 0.6803756952285767\n",
      "Epoch 5, Batch 39/211, Loss: 0.6859040856361389\n",
      "Epoch 5, Batch 40/211, Loss: 0.6980540752410889\n",
      "Epoch 5, Batch 41/211, Loss: 0.6856751441955566\n",
      "Epoch 5, Batch 42/211, Loss: 0.6768694519996643\n",
      "Epoch 5, Batch 43/211, Loss: 0.7343975305557251\n",
      "Epoch 5, Batch 44/211, Loss: 0.7193124890327454\n",
      "Epoch 5, Batch 45/211, Loss: 0.6934168934822083\n",
      "Epoch 5, Batch 46/211, Loss: 0.6965929865837097\n",
      "Epoch 5, Batch 47/211, Loss: 0.7013784050941467\n",
      "Epoch 5, Batch 48/211, Loss: 0.6456120014190674\n",
      "Epoch 5, Batch 49/211, Loss: 0.713895320892334\n",
      "Epoch 5, Batch 50/211, Loss: 0.7741789817810059\n",
      "Epoch 5, Batch 51/211, Loss: 0.703682005405426\n",
      "Epoch 5, Batch 52/211, Loss: 0.6606351733207703\n",
      "Epoch 5, Batch 53/211, Loss: 0.7120041251182556\n",
      "Epoch 5, Batch 54/211, Loss: 0.6910788416862488\n",
      "Epoch 5, Batch 55/211, Loss: 0.7360674738883972\n",
      "Epoch 5, Batch 56/211, Loss: 0.6965445280075073\n",
      "Epoch 5, Batch 57/211, Loss: 0.6943615078926086\n",
      "Epoch 5, Batch 58/211, Loss: 0.7167933583259583\n",
      "Epoch 5, Batch 59/211, Loss: 0.6953727006912231\n",
      "Epoch 5, Batch 60/211, Loss: 0.7165653705596924\n",
      "Epoch 5, Batch 61/211, Loss: 0.6793834567070007\n",
      "Epoch 5, Batch 62/211, Loss: 0.7031523585319519\n",
      "Epoch 5, Batch 63/211, Loss: 0.6815446615219116\n",
      "Epoch 5, Batch 64/211, Loss: 0.6655755043029785\n",
      "Epoch 5, Batch 65/211, Loss: 0.6606348752975464\n",
      "Epoch 5, Batch 66/211, Loss: 0.7104209661483765\n",
      "Epoch 5, Batch 67/211, Loss: 0.6905508041381836\n",
      "Epoch 5, Batch 68/211, Loss: 0.7016692757606506\n",
      "Epoch 5, Batch 69/211, Loss: 0.694962739944458\n",
      "Epoch 5, Batch 70/211, Loss: 0.6555472612380981\n",
      "Epoch 5, Batch 71/211, Loss: 0.6826815605163574\n",
      "Epoch 5, Batch 72/211, Loss: 0.6647651195526123\n",
      "Epoch 5, Batch 73/211, Loss: 0.6697793006896973\n",
      "Epoch 5, Batch 74/211, Loss: 0.676753044128418\n",
      "Epoch 5, Batch 75/211, Loss: 0.7138378620147705\n",
      "Epoch 5, Batch 76/211, Loss: 0.6717082262039185\n",
      "Epoch 5, Batch 77/211, Loss: 0.7092136740684509\n",
      "Epoch 5, Batch 78/211, Loss: 0.6810784935951233\n",
      "Epoch 5, Batch 79/211, Loss: 0.6875277161598206\n",
      "Epoch 5, Batch 80/211, Loss: 0.6777477860450745\n",
      "Epoch 5, Batch 81/211, Loss: 0.6672698259353638\n",
      "Epoch 5, Batch 82/211, Loss: 0.6841184496879578\n",
      "Epoch 5, Batch 83/211, Loss: 0.6940656900405884\n",
      "Epoch 5, Batch 84/211, Loss: 0.6984908580780029\n",
      "Epoch 5, Batch 85/211, Loss: 0.7181312441825867\n",
      "Epoch 5, Batch 86/211, Loss: 0.7046287059783936\n",
      "Epoch 5, Batch 87/211, Loss: 0.6977402567863464\n",
      "Epoch 5, Batch 88/211, Loss: 0.6782169342041016\n",
      "Epoch 5, Batch 89/211, Loss: 0.6931849718093872\n",
      "Epoch 5, Batch 90/211, Loss: 0.6859989166259766\n",
      "Epoch 5, Batch 91/211, Loss: 0.6875966191291809\n",
      "Epoch 5, Batch 92/211, Loss: 0.7027431726455688\n",
      "Epoch 5, Batch 93/211, Loss: 0.6896055340766907\n",
      "Epoch 5, Batch 94/211, Loss: 0.6984003782272339\n",
      "Epoch 5, Batch 95/211, Loss: 0.6782981157302856\n",
      "Epoch 5, Batch 96/211, Loss: 0.7201253175735474\n",
      "Epoch 5, Batch 97/211, Loss: 0.7032442688941956\n",
      "Epoch 5, Batch 98/211, Loss: 0.6969522833824158\n",
      "Epoch 5, Batch 99/211, Loss: 0.670089602470398\n",
      "Epoch 5, Batch 100/211, Loss: 0.7067076563835144\n",
      "Epoch 5, Batch 101/211, Loss: 0.6822224855422974\n",
      "Epoch 5, Batch 102/211, Loss: 0.6874505281448364\n",
      "Epoch 5, Batch 103/211, Loss: 0.705336332321167\n",
      "Epoch 5, Batch 104/211, Loss: 0.7101428508758545\n",
      "Epoch 5, Batch 105/211, Loss: 0.6815573573112488\n",
      "Epoch 5, Batch 106/211, Loss: 0.6867709159851074\n",
      "Epoch 5, Batch 107/211, Loss: 0.6948958039283752\n",
      "Epoch 5, Batch 108/211, Loss: 0.6766194105148315\n",
      "Epoch 5, Batch 109/211, Loss: 0.691289484500885\n",
      "Epoch 5, Batch 110/211, Loss: 0.7137671709060669\n",
      "Epoch 5, Batch 111/211, Loss: 0.6819252967834473\n",
      "Epoch 5, Batch 112/211, Loss: 0.702258825302124\n",
      "Epoch 5, Batch 113/211, Loss: 0.6978384852409363\n",
      "Epoch 5, Batch 114/211, Loss: 0.681491494178772\n",
      "Epoch 5, Batch 115/211, Loss: 0.715691328048706\n",
      "Epoch 5, Batch 116/211, Loss: 0.6942681074142456\n",
      "Epoch 5, Batch 117/211, Loss: 0.6699857711791992\n",
      "Epoch 5, Batch 118/211, Loss: 0.699377715587616\n",
      "Epoch 5, Batch 119/211, Loss: 0.6977769136428833\n",
      "Epoch 5, Batch 120/211, Loss: 0.6944153308868408\n",
      "Epoch 5, Batch 121/211, Loss: 0.6952690482139587\n",
      "Epoch 5, Batch 122/211, Loss: 0.6924940347671509\n",
      "Epoch 5, Batch 123/211, Loss: 0.7038936614990234\n",
      "Epoch 5, Batch 124/211, Loss: 0.6610340476036072\n",
      "Epoch 5, Batch 125/211, Loss: 0.6939136981964111\n",
      "Epoch 5, Batch 126/211, Loss: 0.7025222182273865\n",
      "Epoch 5, Batch 127/211, Loss: 0.7188311219215393\n",
      "Epoch 5, Batch 128/211, Loss: 0.6932568550109863\n",
      "Epoch 5, Batch 129/211, Loss: 0.6865069270133972\n",
      "Epoch 5, Batch 130/211, Loss: 0.6884540915489197\n",
      "Epoch 5, Batch 131/211, Loss: 0.682968020439148\n",
      "Epoch 5, Batch 132/211, Loss: 0.6910902261734009\n",
      "Epoch 5, Batch 133/211, Loss: 0.6939063668251038\n",
      "Epoch 5, Batch 134/211, Loss: 0.6990941762924194\n",
      "Epoch 5, Batch 135/211, Loss: 0.707909882068634\n",
      "Epoch 5, Batch 136/211, Loss: 0.6939937472343445\n",
      "Epoch 5, Batch 137/211, Loss: 0.7071725726127625\n",
      "Epoch 5, Batch 138/211, Loss: 0.6938303112983704\n",
      "Epoch 5, Batch 139/211, Loss: 0.6747037172317505\n",
      "Epoch 5, Batch 140/211, Loss: 0.6958585381507874\n",
      "Epoch 5, Batch 141/211, Loss: 0.6960501074790955\n",
      "Epoch 5, Batch 142/211, Loss: 0.6822836399078369\n",
      "Epoch 5, Batch 143/211, Loss: 0.6772207617759705\n",
      "Epoch 5, Batch 144/211, Loss: 0.6979357004165649\n",
      "Epoch 5, Batch 145/211, Loss: 0.7223666310310364\n",
      "Epoch 5, Batch 146/211, Loss: 0.6876664161682129\n",
      "Epoch 5, Batch 147/211, Loss: 0.6709643602371216\n",
      "Epoch 5, Batch 148/211, Loss: 0.6991407871246338\n",
      "Epoch 5, Batch 149/211, Loss: 0.6746247410774231\n",
      "Epoch 5, Batch 150/211, Loss: 0.6674688458442688\n",
      "Epoch 5, Batch 151/211, Loss: 0.6920378804206848\n",
      "Epoch 5, Batch 152/211, Loss: 0.7117850184440613\n",
      "Epoch 5, Batch 153/211, Loss: 0.6531789898872375\n",
      "Epoch 5, Batch 154/211, Loss: 0.6934104561805725\n",
      "Epoch 5, Batch 155/211, Loss: 0.6855978965759277\n",
      "Epoch 5, Batch 156/211, Loss: 0.6931398510932922\n",
      "Epoch 5, Batch 157/211, Loss: 0.6937938332557678\n",
      "Epoch 5, Batch 158/211, Loss: 0.712254524230957\n",
      "Epoch 5, Batch 159/211, Loss: 0.6772947907447815\n",
      "Epoch 5, Batch 160/211, Loss: 0.696107029914856\n",
      "Epoch 5, Batch 161/211, Loss: 0.705183207988739\n",
      "Epoch 5, Batch 162/211, Loss: 0.7086026072502136\n",
      "Epoch 5, Batch 163/211, Loss: 0.6965028643608093\n",
      "Epoch 5, Batch 164/211, Loss: 0.6988118886947632\n",
      "Epoch 5, Batch 165/211, Loss: 0.7000798583030701\n",
      "Epoch 5, Batch 166/211, Loss: 0.7000952959060669\n",
      "Epoch 5, Batch 167/211, Loss: 0.7138597369194031\n",
      "Epoch 5, Batch 168/211, Loss: 0.6931072473526001\n",
      "Epoch 5, Batch 169/211, Loss: 0.6572059988975525\n",
      "Epoch 5, Batch 170/211, Loss: 0.7358116507530212\n",
      "Epoch 5, Batch 171/211, Loss: 0.6843161582946777\n",
      "Epoch 5, Batch 172/211, Loss: 0.6940577626228333\n",
      "Epoch 5, Batch 173/211, Loss: 0.7172585725784302\n",
      "Epoch 5, Batch 174/211, Loss: 0.7157866358757019\n",
      "Epoch 5, Batch 175/211, Loss: 0.692510724067688\n",
      "Epoch 5, Batch 176/211, Loss: 0.6842730641365051\n",
      "Epoch 5, Batch 177/211, Loss: 0.7065897583961487\n",
      "Epoch 5, Batch 178/211, Loss: 0.7086906433105469\n",
      "Epoch 5, Batch 179/211, Loss: 0.7020728588104248\n",
      "Epoch 5, Batch 180/211, Loss: 0.7051145434379578\n",
      "Epoch 5, Batch 181/211, Loss: 0.719841480255127\n",
      "Epoch 5, Batch 182/211, Loss: 0.661231279373169\n",
      "Epoch 5, Batch 183/211, Loss: 0.666746199131012\n",
      "Epoch 5, Batch 184/211, Loss: 0.7223507165908813\n",
      "Epoch 5, Batch 185/211, Loss: 0.6987813115119934\n",
      "Epoch 5, Batch 186/211, Loss: 0.6978166103363037\n",
      "Epoch 5, Batch 187/211, Loss: 0.7192272543907166\n",
      "Epoch 5, Batch 188/211, Loss: 0.7071793079376221\n",
      "Epoch 5, Batch 189/211, Loss: 0.7035590410232544\n",
      "Epoch 5, Batch 190/211, Loss: 0.7012121677398682\n",
      "Epoch 5, Batch 191/211, Loss: 0.6818803548812866\n",
      "Epoch 5, Batch 192/211, Loss: 0.6980913877487183\n",
      "Epoch 5, Batch 193/211, Loss: 0.6975117325782776\n",
      "Epoch 5, Batch 194/211, Loss: 0.6774476766586304\n",
      "Epoch 5, Batch 195/211, Loss: 0.7021130323410034\n",
      "Epoch 5, Batch 196/211, Loss: 0.7149623036384583\n",
      "Epoch 5, Batch 197/211, Loss: 0.6834061741828918\n",
      "Epoch 5, Batch 198/211, Loss: 0.6728328466415405\n",
      "Epoch 5, Batch 199/211, Loss: 0.7072058916091919\n",
      "Epoch 5, Batch 200/211, Loss: 0.6644576191902161\n",
      "Epoch 5, Batch 201/211, Loss: 0.6784193515777588\n",
      "Epoch 5, Batch 202/211, Loss: 0.7029857635498047\n",
      "Epoch 5, Batch 203/211, Loss: 0.6898933053016663\n",
      "Epoch 5, Batch 204/211, Loss: 0.6995862722396851\n",
      "Epoch 5, Batch 205/211, Loss: 0.6600132584571838\n",
      "Epoch 5, Batch 206/211, Loss: 0.6714410781860352\n",
      "Epoch 5, Batch 207/211, Loss: 0.6818609833717346\n",
      "Epoch 5, Batch 208/211, Loss: 0.7043721675872803\n",
      "Epoch 5, Batch 209/211, Loss: 0.7025896310806274\n",
      "Epoch 5, Batch 210/211, Loss: 0.6859041452407837\n",
      "Epoch 5, Batch 211/211, Loss: 0.7687661051750183\n",
      "Epoch 5, Loss: 146.38682359457016\n",
      "Epoch 6, Batch 1/211, Loss: 0.6919282078742981\n",
      "Epoch 6, Batch 2/211, Loss: 0.6953592300415039\n",
      "Epoch 6, Batch 3/211, Loss: 0.7210776805877686\n",
      "Epoch 6, Batch 4/211, Loss: 0.6911767721176147\n",
      "Epoch 6, Batch 5/211, Loss: 0.6971563696861267\n",
      "Epoch 6, Batch 6/211, Loss: 0.6932173371315002\n",
      "Epoch 6, Batch 7/211, Loss: 0.6786189079284668\n",
      "Epoch 6, Batch 8/211, Loss: 0.6750092506408691\n",
      "Epoch 6, Batch 9/211, Loss: 0.6793323159217834\n",
      "Epoch 6, Batch 10/211, Loss: 0.6902204155921936\n",
      "Epoch 6, Batch 11/211, Loss: 0.6878005862236023\n",
      "Epoch 6, Batch 12/211, Loss: 0.6901994347572327\n",
      "Epoch 6, Batch 13/211, Loss: 0.6913615465164185\n",
      "Epoch 6, Batch 14/211, Loss: 0.7144086956977844\n",
      "Epoch 6, Batch 15/211, Loss: 0.6925733089447021\n",
      "Epoch 6, Batch 16/211, Loss: 0.6835950016975403\n",
      "Epoch 6, Batch 17/211, Loss: 0.7014826536178589\n",
      "Epoch 6, Batch 18/211, Loss: 0.6842450499534607\n",
      "Epoch 6, Batch 19/211, Loss: 0.7004497051239014\n",
      "Epoch 6, Batch 20/211, Loss: 0.7208943367004395\n",
      "Epoch 6, Batch 21/211, Loss: 0.7057049870491028\n",
      "Epoch 6, Batch 22/211, Loss: 0.6893163323402405\n",
      "Epoch 6, Batch 23/211, Loss: 0.6880519986152649\n",
      "Epoch 6, Batch 24/211, Loss: 0.6967615485191345\n",
      "Epoch 6, Batch 25/211, Loss: 0.7428054213523865\n",
      "Epoch 6, Batch 26/211, Loss: 0.6780259609222412\n",
      "Epoch 6, Batch 27/211, Loss: 0.6986861824989319\n",
      "Epoch 6, Batch 28/211, Loss: 0.6907679438591003\n",
      "Epoch 6, Batch 29/211, Loss: 0.6894363164901733\n",
      "Epoch 6, Batch 30/211, Loss: 0.6961937546730042\n",
      "Epoch 6, Batch 31/211, Loss: 0.6876755952835083\n",
      "Epoch 6, Batch 32/211, Loss: 0.7019382119178772\n",
      "Epoch 6, Batch 33/211, Loss: 0.6966822743415833\n",
      "Epoch 6, Batch 34/211, Loss: 0.7014580368995667\n",
      "Epoch 6, Batch 35/211, Loss: 0.6985870003700256\n",
      "Epoch 6, Batch 36/211, Loss: 0.7034576535224915\n",
      "Epoch 6, Batch 37/211, Loss: 0.708597719669342\n",
      "Epoch 6, Batch 38/211, Loss: 0.689240038394928\n",
      "Epoch 6, Batch 39/211, Loss: 0.698813259601593\n",
      "Epoch 6, Batch 40/211, Loss: 0.7433648109436035\n",
      "Epoch 6, Batch 41/211, Loss: 0.6915795803070068\n",
      "Epoch 6, Batch 42/211, Loss: 0.6635345816612244\n",
      "Epoch 6, Batch 43/211, Loss: 0.688030481338501\n",
      "Epoch 6, Batch 44/211, Loss: 0.7006852626800537\n",
      "Epoch 6, Batch 45/211, Loss: 0.702307939529419\n",
      "Epoch 6, Batch 46/211, Loss: 0.6993063688278198\n",
      "Epoch 6, Batch 47/211, Loss: 0.684821605682373\n",
      "Epoch 6, Batch 48/211, Loss: 0.6913632750511169\n",
      "Epoch 6, Batch 49/211, Loss: 0.70184326171875\n",
      "Epoch 6, Batch 50/211, Loss: 0.6924623250961304\n",
      "Epoch 6, Batch 51/211, Loss: 0.6968404054641724\n",
      "Epoch 6, Batch 52/211, Loss: 0.6838647127151489\n",
      "Epoch 6, Batch 53/211, Loss: 0.6973628401756287\n",
      "Epoch 6, Batch 54/211, Loss: 0.7157837748527527\n",
      "Epoch 6, Batch 55/211, Loss: 0.7145619988441467\n",
      "Epoch 6, Batch 56/211, Loss: 0.684428334236145\n",
      "Epoch 6, Batch 57/211, Loss: 0.6948505640029907\n",
      "Epoch 6, Batch 58/211, Loss: 0.6896925568580627\n",
      "Epoch 6, Batch 59/211, Loss: 0.689077615737915\n",
      "Epoch 6, Batch 60/211, Loss: 0.6998481750488281\n",
      "Epoch 6, Batch 61/211, Loss: 0.7092608213424683\n",
      "Epoch 6, Batch 62/211, Loss: 0.7124480605125427\n",
      "Epoch 6, Batch 63/211, Loss: 0.6894716024398804\n",
      "Epoch 6, Batch 64/211, Loss: 0.7138676047325134\n",
      "Epoch 6, Batch 65/211, Loss: 0.6969645619392395\n",
      "Epoch 6, Batch 66/211, Loss: 0.7015600800514221\n",
      "Epoch 6, Batch 67/211, Loss: 0.6782989501953125\n",
      "Epoch 6, Batch 68/211, Loss: 0.7009444832801819\n",
      "Epoch 6, Batch 69/211, Loss: 0.680655300617218\n",
      "Epoch 6, Batch 70/211, Loss: 0.6882567405700684\n",
      "Epoch 6, Batch 71/211, Loss: 0.6938357353210449\n",
      "Epoch 6, Batch 72/211, Loss: 0.6751168966293335\n",
      "Epoch 6, Batch 73/211, Loss: 0.6753352284431458\n",
      "Epoch 6, Batch 74/211, Loss: 0.676165759563446\n",
      "Epoch 6, Batch 75/211, Loss: 0.6920425295829773\n",
      "Epoch 6, Batch 76/211, Loss: 0.6860429048538208\n",
      "Epoch 6, Batch 77/211, Loss: 0.6657167673110962\n",
      "Epoch 6, Batch 78/211, Loss: 0.696378231048584\n",
      "Epoch 6, Batch 79/211, Loss: 0.6879197955131531\n",
      "Epoch 6, Batch 80/211, Loss: 0.6925944089889526\n",
      "Epoch 6, Batch 81/211, Loss: 0.6580612063407898\n",
      "Epoch 6, Batch 82/211, Loss: 0.6798518300056458\n",
      "Epoch 6, Batch 83/211, Loss: 0.685181736946106\n",
      "Epoch 6, Batch 84/211, Loss: 0.6968773007392883\n",
      "Epoch 6, Batch 85/211, Loss: 0.6773072481155396\n",
      "Epoch 6, Batch 86/211, Loss: 0.703763484954834\n",
      "Epoch 6, Batch 87/211, Loss: 0.6851710081100464\n",
      "Epoch 6, Batch 88/211, Loss: 0.6602378487586975\n",
      "Epoch 6, Batch 89/211, Loss: 0.6931790113449097\n",
      "Epoch 6, Batch 90/211, Loss: 0.675605058670044\n",
      "Epoch 6, Batch 91/211, Loss: 0.6914331316947937\n",
      "Epoch 6, Batch 92/211, Loss: 0.7148430943489075\n",
      "Epoch 6, Batch 93/211, Loss: 0.6869425177574158\n",
      "Epoch 6, Batch 94/211, Loss: 0.6993553638458252\n",
      "Epoch 6, Batch 95/211, Loss: 0.7082382440567017\n",
      "Epoch 6, Batch 96/211, Loss: 0.6899842619895935\n",
      "Epoch 6, Batch 97/211, Loss: 0.7053206562995911\n",
      "Epoch 6, Batch 98/211, Loss: 0.7083480358123779\n",
      "Epoch 6, Batch 99/211, Loss: 0.6701164245605469\n",
      "Epoch 6, Batch 100/211, Loss: 0.6629006266593933\n",
      "Epoch 6, Batch 101/211, Loss: 0.6693755388259888\n",
      "Epoch 6, Batch 102/211, Loss: 0.6988028287887573\n",
      "Epoch 6, Batch 103/211, Loss: 0.682263195514679\n",
      "Epoch 6, Batch 104/211, Loss: 0.66996169090271\n",
      "Epoch 6, Batch 105/211, Loss: 0.674121081829071\n",
      "Epoch 6, Batch 106/211, Loss: 0.6878713965415955\n",
      "Epoch 6, Batch 107/211, Loss: 0.6734797358512878\n",
      "Epoch 6, Batch 108/211, Loss: 0.6868239641189575\n",
      "Epoch 6, Batch 109/211, Loss: 0.6815539598464966\n",
      "Epoch 6, Batch 110/211, Loss: 0.6985369920730591\n",
      "Epoch 6, Batch 111/211, Loss: 0.6931387186050415\n",
      "Epoch 6, Batch 112/211, Loss: 0.6914510726928711\n",
      "Epoch 6, Batch 113/211, Loss: 0.6766448020935059\n",
      "Epoch 6, Batch 114/211, Loss: 0.6847742795944214\n",
      "Epoch 6, Batch 115/211, Loss: 0.70418381690979\n",
      "Epoch 6, Batch 116/211, Loss: 0.7127886414527893\n",
      "Epoch 6, Batch 117/211, Loss: 0.6842830777168274\n",
      "Epoch 6, Batch 118/211, Loss: 0.6830998659133911\n",
      "Epoch 6, Batch 119/211, Loss: 0.6863707304000854\n",
      "Epoch 6, Batch 120/211, Loss: 0.7070762515068054\n",
      "Epoch 6, Batch 121/211, Loss: 0.6982204914093018\n",
      "Epoch 6, Batch 122/211, Loss: 0.6780628561973572\n",
      "Epoch 6, Batch 123/211, Loss: 0.6967520713806152\n",
      "Epoch 6, Batch 124/211, Loss: 0.7010133862495422\n",
      "Epoch 6, Batch 125/211, Loss: 0.6884471774101257\n",
      "Epoch 6, Batch 126/211, Loss: 0.6865623593330383\n",
      "Epoch 6, Batch 127/211, Loss: 0.705686628818512\n",
      "Epoch 6, Batch 128/211, Loss: 0.6886622905731201\n",
      "Epoch 6, Batch 129/211, Loss: 0.6940085887908936\n",
      "Epoch 6, Batch 130/211, Loss: 0.68510901927948\n",
      "Epoch 6, Batch 131/211, Loss: 0.6879481673240662\n",
      "Epoch 6, Batch 132/211, Loss: 0.6881479620933533\n",
      "Epoch 6, Batch 133/211, Loss: 0.6872050762176514\n",
      "Epoch 6, Batch 134/211, Loss: 0.684867262840271\n",
      "Epoch 6, Batch 135/211, Loss: 0.6891513466835022\n",
      "Epoch 6, Batch 136/211, Loss: 0.6837655901908875\n",
      "Epoch 6, Batch 137/211, Loss: 0.7035149335861206\n",
      "Epoch 6, Batch 138/211, Loss: 0.653839647769928\n",
      "Epoch 6, Batch 139/211, Loss: 0.7536192536354065\n",
      "Epoch 6, Batch 140/211, Loss: 0.6630475521087646\n",
      "Epoch 6, Batch 141/211, Loss: 0.6785783171653748\n",
      "Epoch 6, Batch 142/211, Loss: 0.6699909567832947\n",
      "Epoch 6, Batch 143/211, Loss: 0.6715046763420105\n",
      "Epoch 6, Batch 144/211, Loss: 0.665520191192627\n",
      "Epoch 6, Batch 145/211, Loss: 0.7143545150756836\n",
      "Epoch 6, Batch 146/211, Loss: 0.7233178615570068\n",
      "Epoch 6, Batch 147/211, Loss: 0.7037764191627502\n",
      "Epoch 6, Batch 148/211, Loss: 0.7014615535736084\n",
      "Epoch 6, Batch 149/211, Loss: 0.6881794333457947\n",
      "Epoch 6, Batch 150/211, Loss: 0.6942495107650757\n",
      "Epoch 6, Batch 151/211, Loss: 0.7225111126899719\n",
      "Epoch 6, Batch 152/211, Loss: 0.7025824785232544\n",
      "Epoch 6, Batch 153/211, Loss: 0.6878258585929871\n",
      "Epoch 6, Batch 154/211, Loss: 0.67937833070755\n",
      "Epoch 6, Batch 155/211, Loss: 0.6990474462509155\n",
      "Epoch 6, Batch 156/211, Loss: 0.6968168616294861\n",
      "Epoch 6, Batch 157/211, Loss: 0.673913836479187\n",
      "Epoch 6, Batch 158/211, Loss: 0.6888338923454285\n",
      "Epoch 6, Batch 159/211, Loss: 0.7077550292015076\n",
      "Epoch 6, Batch 160/211, Loss: 0.7030450701713562\n",
      "Epoch 6, Batch 161/211, Loss: 0.6744232773780823\n",
      "Epoch 6, Batch 162/211, Loss: 0.6989117860794067\n",
      "Epoch 6, Batch 163/211, Loss: 0.6843937039375305\n",
      "Epoch 6, Batch 164/211, Loss: 0.6924746036529541\n",
      "Epoch 6, Batch 165/211, Loss: 0.7012925744056702\n",
      "Epoch 6, Batch 166/211, Loss: 0.7035443186759949\n",
      "Epoch 6, Batch 167/211, Loss: 0.7054120302200317\n",
      "Epoch 6, Batch 168/211, Loss: 0.6863340139389038\n",
      "Epoch 6, Batch 169/211, Loss: 0.7005288600921631\n",
      "Epoch 6, Batch 170/211, Loss: 0.6961854100227356\n",
      "Epoch 6, Batch 171/211, Loss: 0.7085731625556946\n",
      "Epoch 6, Batch 172/211, Loss: 0.7122830152511597\n",
      "Epoch 6, Batch 173/211, Loss: 0.6983859539031982\n",
      "Epoch 6, Batch 174/211, Loss: 0.704085648059845\n",
      "Epoch 6, Batch 175/211, Loss: 0.7050254344940186\n",
      "Epoch 6, Batch 176/211, Loss: 0.7135907411575317\n",
      "Epoch 6, Batch 177/211, Loss: 0.7092404961585999\n",
      "Epoch 6, Batch 178/211, Loss: 0.6904392242431641\n",
      "Epoch 6, Batch 179/211, Loss: 0.6849090456962585\n",
      "Epoch 6, Batch 180/211, Loss: 0.7066085338592529\n",
      "Epoch 6, Batch 181/211, Loss: 0.69614177942276\n",
      "Epoch 6, Batch 182/211, Loss: 0.7005501389503479\n",
      "Epoch 6, Batch 183/211, Loss: 0.7156461477279663\n",
      "Epoch 6, Batch 184/211, Loss: 0.6902433633804321\n",
      "Epoch 6, Batch 185/211, Loss: 0.685344934463501\n",
      "Epoch 6, Batch 186/211, Loss: 0.695939838886261\n",
      "Epoch 6, Batch 187/211, Loss: 0.6916643977165222\n",
      "Epoch 6, Batch 188/211, Loss: 0.7051107287406921\n",
      "Epoch 6, Batch 189/211, Loss: 0.7036645412445068\n",
      "Epoch 6, Batch 190/211, Loss: 0.708092987537384\n",
      "Epoch 6, Batch 191/211, Loss: 0.6933174133300781\n",
      "Epoch 6, Batch 192/211, Loss: 0.6913865208625793\n",
      "Epoch 6, Batch 193/211, Loss: 0.6967774033546448\n",
      "Epoch 6, Batch 194/211, Loss: 0.7120820879936218\n",
      "Epoch 6, Batch 195/211, Loss: 0.6782272458076477\n",
      "Epoch 6, Batch 196/211, Loss: 0.6922826170921326\n",
      "Epoch 6, Batch 197/211, Loss: 0.6911239624023438\n",
      "Epoch 6, Batch 198/211, Loss: 0.6944599151611328\n",
      "Epoch 6, Batch 199/211, Loss: 0.695050060749054\n",
      "Epoch 6, Batch 200/211, Loss: 0.69731605052948\n",
      "Epoch 6, Batch 201/211, Loss: 0.7170677185058594\n",
      "Epoch 6, Batch 202/211, Loss: 0.6984047889709473\n",
      "Epoch 6, Batch 203/211, Loss: 0.6791063547134399\n",
      "Epoch 6, Batch 204/211, Loss: 0.7131379842758179\n",
      "Epoch 6, Batch 205/211, Loss: 0.7291088700294495\n",
      "Epoch 6, Batch 206/211, Loss: 0.6891520619392395\n",
      "Epoch 6, Batch 207/211, Loss: 0.7053236961364746\n",
      "Epoch 6, Batch 208/211, Loss: 0.7123281955718994\n",
      "Epoch 6, Batch 209/211, Loss: 0.6877837777137756\n",
      "Epoch 6, Batch 210/211, Loss: 0.6812182068824768\n",
      "Epoch 6, Batch 211/211, Loss: 0.6897213459014893\n",
      "Epoch 6, Loss: 146.3838056921959\n",
      "Epoch 7, Batch 1/211, Loss: 0.6650100350379944\n",
      "Epoch 7, Batch 2/211, Loss: 0.7187067866325378\n",
      "Epoch 7, Batch 3/211, Loss: 0.6920323371887207\n",
      "Epoch 7, Batch 4/211, Loss: 0.7231578230857849\n",
      "Epoch 7, Batch 5/211, Loss: 0.7169460654258728\n",
      "Epoch 7, Batch 6/211, Loss: 0.6789247393608093\n",
      "Epoch 7, Batch 7/211, Loss: 0.6923910975456238\n",
      "Epoch 7, Batch 8/211, Loss: 0.7030901312828064\n",
      "Epoch 7, Batch 9/211, Loss: 0.7028553485870361\n",
      "Epoch 7, Batch 10/211, Loss: 0.7043601870536804\n",
      "Epoch 7, Batch 11/211, Loss: 0.6867551803588867\n",
      "Epoch 7, Batch 12/211, Loss: 0.6907839775085449\n",
      "Epoch 7, Batch 13/211, Loss: 0.6984778642654419\n",
      "Epoch 7, Batch 14/211, Loss: 0.7142269015312195\n",
      "Epoch 7, Batch 15/211, Loss: 0.6752848029136658\n",
      "Epoch 7, Batch 16/211, Loss: 0.6768831014633179\n",
      "Epoch 7, Batch 17/211, Loss: 0.699607253074646\n",
      "Epoch 7, Batch 18/211, Loss: 0.703411340713501\n",
      "Epoch 7, Batch 19/211, Loss: 0.7087501287460327\n",
      "Epoch 7, Batch 20/211, Loss: 0.6640576720237732\n",
      "Epoch 7, Batch 21/211, Loss: 0.7144319415092468\n",
      "Epoch 7, Batch 22/211, Loss: 0.685897707939148\n",
      "Epoch 7, Batch 23/211, Loss: 0.6805514693260193\n",
      "Epoch 7, Batch 24/211, Loss: 0.6963522434234619\n",
      "Epoch 7, Batch 25/211, Loss: 0.7102020978927612\n",
      "Epoch 7, Batch 26/211, Loss: 0.6835381984710693\n",
      "Epoch 7, Batch 27/211, Loss: 0.6731202602386475\n",
      "Epoch 7, Batch 28/211, Loss: 0.69905686378479\n",
      "Epoch 7, Batch 29/211, Loss: 0.7171496152877808\n",
      "Epoch 7, Batch 30/211, Loss: 0.7061042189598083\n",
      "Epoch 7, Batch 31/211, Loss: 0.6847999691963196\n",
      "Epoch 7, Batch 32/211, Loss: 0.6915692687034607\n",
      "Epoch 7, Batch 33/211, Loss: 0.6931387782096863\n",
      "Epoch 7, Batch 34/211, Loss: 0.6717267632484436\n",
      "Epoch 7, Batch 35/211, Loss: 0.6968991160392761\n",
      "Epoch 7, Batch 36/211, Loss: 0.7136555314064026\n",
      "Epoch 7, Batch 37/211, Loss: 0.6746773719787598\n",
      "Epoch 7, Batch 38/211, Loss: 0.6995684504508972\n",
      "Epoch 7, Batch 39/211, Loss: 0.727433443069458\n",
      "Epoch 7, Batch 40/211, Loss: 0.6913782358169556\n",
      "Epoch 7, Batch 41/211, Loss: 0.678604781627655\n",
      "Epoch 7, Batch 42/211, Loss: 0.7006872892379761\n",
      "Epoch 7, Batch 43/211, Loss: 0.6985191106796265\n",
      "Epoch 7, Batch 44/211, Loss: 0.6908169984817505\n",
      "Epoch 7, Batch 45/211, Loss: 0.7014127373695374\n",
      "Epoch 7, Batch 46/211, Loss: 0.6793743371963501\n",
      "Epoch 7, Batch 47/211, Loss: 0.6787475347518921\n",
      "Epoch 7, Batch 48/211, Loss: 0.7130860686302185\n",
      "Epoch 7, Batch 49/211, Loss: 0.7091678380966187\n",
      "Epoch 7, Batch 50/211, Loss: 0.7066275477409363\n",
      "Epoch 7, Batch 51/211, Loss: 0.7089036107063293\n",
      "Epoch 7, Batch 52/211, Loss: 0.7086305618286133\n",
      "Epoch 7, Batch 53/211, Loss: 0.7027056217193604\n",
      "Epoch 7, Batch 54/211, Loss: 0.6943359971046448\n",
      "Epoch 7, Batch 55/211, Loss: 0.6927396655082703\n",
      "Epoch 7, Batch 56/211, Loss: 0.703408420085907\n",
      "Epoch 7, Batch 57/211, Loss: 0.6824319958686829\n",
      "Epoch 7, Batch 58/211, Loss: 0.7138743996620178\n",
      "Epoch 7, Batch 59/211, Loss: 0.7014907002449036\n",
      "Epoch 7, Batch 60/211, Loss: 0.7159574031829834\n",
      "Epoch 7, Batch 61/211, Loss: 0.6836919188499451\n",
      "Epoch 7, Batch 62/211, Loss: 0.6663437485694885\n",
      "Epoch 7, Batch 63/211, Loss: 0.6592743992805481\n",
      "Epoch 7, Batch 64/211, Loss: 0.6940687894821167\n",
      "Epoch 7, Batch 65/211, Loss: 0.702481746673584\n",
      "Epoch 7, Batch 66/211, Loss: 0.700838565826416\n",
      "Epoch 7, Batch 67/211, Loss: 0.6897976994514465\n",
      "Epoch 7, Batch 68/211, Loss: 0.6783301830291748\n",
      "Epoch 7, Batch 69/211, Loss: 0.7125886678695679\n",
      "Epoch 7, Batch 70/211, Loss: 0.6951340436935425\n",
      "Epoch 7, Batch 71/211, Loss: 0.7107841968536377\n",
      "Epoch 7, Batch 72/211, Loss: 0.6885817646980286\n",
      "Epoch 7, Batch 73/211, Loss: 0.6914133429527283\n",
      "Epoch 7, Batch 74/211, Loss: 0.7029502391815186\n",
      "Epoch 7, Batch 75/211, Loss: 0.6662012934684753\n",
      "Epoch 7, Batch 76/211, Loss: 0.6924270987510681\n",
      "Epoch 7, Batch 77/211, Loss: 0.7496224641799927\n",
      "Epoch 7, Batch 78/211, Loss: 0.6769715547561646\n",
      "Epoch 7, Batch 79/211, Loss: 0.7224035859107971\n",
      "Epoch 7, Batch 80/211, Loss: 0.6817254424095154\n",
      "Epoch 7, Batch 81/211, Loss: 0.6852980256080627\n",
      "Epoch 7, Batch 82/211, Loss: 0.6927844285964966\n",
      "Epoch 7, Batch 83/211, Loss: 0.6724985241889954\n",
      "Epoch 7, Batch 84/211, Loss: 0.6925992965698242\n",
      "Epoch 7, Batch 85/211, Loss: 0.7028464078903198\n",
      "Epoch 7, Batch 86/211, Loss: 0.7103304266929626\n",
      "Epoch 7, Batch 87/211, Loss: 0.6757621765136719\n",
      "Epoch 7, Batch 88/211, Loss: 0.6900972127914429\n",
      "Epoch 7, Batch 89/211, Loss: 0.7116239666938782\n",
      "Epoch 7, Batch 90/211, Loss: 0.706500232219696\n",
      "Epoch 7, Batch 91/211, Loss: 0.6986729502677917\n",
      "Epoch 7, Batch 92/211, Loss: 0.7156960368156433\n",
      "Epoch 7, Batch 93/211, Loss: 0.6915990710258484\n",
      "Epoch 7, Batch 94/211, Loss: 0.6819244027137756\n",
      "Epoch 7, Batch 95/211, Loss: 0.7033323049545288\n",
      "Epoch 7, Batch 96/211, Loss: 0.706093966960907\n",
      "Epoch 7, Batch 97/211, Loss: 0.685737669467926\n",
      "Epoch 7, Batch 98/211, Loss: 0.6704949736595154\n",
      "Epoch 7, Batch 99/211, Loss: 0.6689904928207397\n",
      "Epoch 7, Batch 100/211, Loss: 0.711066484451294\n",
      "Epoch 7, Batch 101/211, Loss: 0.7043813467025757\n",
      "Epoch 7, Batch 102/211, Loss: 0.6982983946800232\n",
      "Epoch 7, Batch 103/211, Loss: 0.7191352844238281\n",
      "Epoch 7, Batch 104/211, Loss: 0.7039406895637512\n",
      "Epoch 7, Batch 105/211, Loss: 0.7059564590454102\n",
      "Epoch 7, Batch 106/211, Loss: 0.7156440019607544\n",
      "Epoch 7, Batch 107/211, Loss: 0.6869872212409973\n",
      "Epoch 7, Batch 108/211, Loss: 0.684966504573822\n",
      "Epoch 7, Batch 109/211, Loss: 0.6934763193130493\n",
      "Epoch 7, Batch 110/211, Loss: 0.6906059384346008\n",
      "Epoch 7, Batch 111/211, Loss: 0.6789035797119141\n",
      "Epoch 7, Batch 112/211, Loss: 0.719639241695404\n",
      "Epoch 7, Batch 113/211, Loss: 0.6910948157310486\n",
      "Epoch 7, Batch 114/211, Loss: 0.7253856658935547\n",
      "Epoch 7, Batch 115/211, Loss: 0.6950588822364807\n",
      "Epoch 7, Batch 116/211, Loss: 0.7096658945083618\n",
      "Epoch 7, Batch 117/211, Loss: 0.7091904878616333\n",
      "Epoch 7, Batch 118/211, Loss: 0.6903303861618042\n",
      "Epoch 7, Batch 119/211, Loss: 0.6858440041542053\n",
      "Epoch 7, Batch 120/211, Loss: 0.6669045686721802\n",
      "Epoch 7, Batch 121/211, Loss: 0.690620481967926\n",
      "Epoch 7, Batch 122/211, Loss: 0.7097527384757996\n",
      "Epoch 7, Batch 123/211, Loss: 0.6990051865577698\n",
      "Epoch 7, Batch 124/211, Loss: 0.6820982098579407\n",
      "Epoch 7, Batch 125/211, Loss: 0.6937897801399231\n",
      "Epoch 7, Batch 126/211, Loss: 0.7143756151199341\n",
      "Epoch 7, Batch 127/211, Loss: 0.6945328712463379\n",
      "Epoch 7, Batch 128/211, Loss: 0.6681756973266602\n",
      "Epoch 7, Batch 129/211, Loss: 0.6656280755996704\n",
      "Epoch 7, Batch 130/211, Loss: 0.7144063115119934\n",
      "Epoch 7, Batch 131/211, Loss: 0.7120063900947571\n",
      "Epoch 7, Batch 132/211, Loss: 0.7177815437316895\n",
      "Epoch 7, Batch 133/211, Loss: 0.6981766223907471\n",
      "Epoch 7, Batch 134/211, Loss: 0.6942051649093628\n",
      "Epoch 7, Batch 135/211, Loss: 0.6798110008239746\n",
      "Epoch 7, Batch 136/211, Loss: 0.6999360918998718\n",
      "Epoch 7, Batch 137/211, Loss: 0.6616429686546326\n",
      "Epoch 7, Batch 138/211, Loss: 0.6906936764717102\n",
      "Epoch 7, Batch 139/211, Loss: 0.6898366808891296\n",
      "Epoch 7, Batch 140/211, Loss: 0.7068026661872864\n",
      "Epoch 7, Batch 141/211, Loss: 0.6833493113517761\n",
      "Epoch 7, Batch 142/211, Loss: 0.6696770787239075\n",
      "Epoch 7, Batch 143/211, Loss: 0.6908201575279236\n",
      "Epoch 7, Batch 144/211, Loss: 0.6753042936325073\n",
      "Epoch 7, Batch 145/211, Loss: 0.6634390950202942\n",
      "Epoch 7, Batch 146/211, Loss: 0.6869503855705261\n",
      "Epoch 7, Batch 147/211, Loss: 0.6888269186019897\n",
      "Epoch 7, Batch 148/211, Loss: 0.6997824907302856\n",
      "Epoch 7, Batch 149/211, Loss: 0.6930258274078369\n",
      "Epoch 7, Batch 150/211, Loss: 0.6995634436607361\n",
      "Epoch 7, Batch 151/211, Loss: 0.6863003373146057\n",
      "Epoch 7, Batch 152/211, Loss: 0.6845352649688721\n",
      "Epoch 7, Batch 153/211, Loss: 0.6881553530693054\n",
      "Epoch 7, Batch 154/211, Loss: 0.7101808786392212\n",
      "Epoch 7, Batch 155/211, Loss: 0.6989805102348328\n",
      "Epoch 7, Batch 156/211, Loss: 0.6986867189407349\n",
      "Epoch 7, Batch 157/211, Loss: 0.6818984150886536\n",
      "Epoch 7, Batch 158/211, Loss: 0.700721263885498\n",
      "Epoch 7, Batch 159/211, Loss: 0.6843557953834534\n",
      "Epoch 7, Batch 160/211, Loss: 0.6852608323097229\n",
      "Epoch 7, Batch 161/211, Loss: 0.663494348526001\n",
      "Epoch 7, Batch 162/211, Loss: 0.6864578127861023\n",
      "Epoch 7, Batch 163/211, Loss: 0.695462703704834\n",
      "Epoch 7, Batch 164/211, Loss: 0.6835921406745911\n",
      "Epoch 7, Batch 165/211, Loss: 0.6848371028900146\n",
      "Epoch 7, Batch 166/211, Loss: 0.7121289372444153\n",
      "Epoch 7, Batch 167/211, Loss: 0.6777061820030212\n",
      "Epoch 7, Batch 168/211, Loss: 0.7012766599655151\n",
      "Epoch 7, Batch 169/211, Loss: 0.7058488726615906\n",
      "Epoch 7, Batch 170/211, Loss: 0.6892641186714172\n",
      "Epoch 7, Batch 171/211, Loss: 0.6921659708023071\n",
      "Epoch 7, Batch 172/211, Loss: 0.6912097334861755\n",
      "Epoch 7, Batch 173/211, Loss: 0.7010210752487183\n",
      "Epoch 7, Batch 174/211, Loss: 0.693443775177002\n",
      "Epoch 7, Batch 175/211, Loss: 0.6559416055679321\n",
      "Epoch 7, Batch 176/211, Loss: 0.7121947407722473\n",
      "Epoch 7, Batch 177/211, Loss: 0.6926622986793518\n",
      "Epoch 7, Batch 178/211, Loss: 0.6713302135467529\n",
      "Epoch 7, Batch 179/211, Loss: 0.6956454515457153\n",
      "Epoch 7, Batch 180/211, Loss: 0.6924900412559509\n",
      "Epoch 7, Batch 181/211, Loss: 0.6933016180992126\n",
      "Epoch 7, Batch 182/211, Loss: 0.6926902532577515\n",
      "Epoch 7, Batch 183/211, Loss: 0.6927539706230164\n",
      "Epoch 7, Batch 184/211, Loss: 0.6854944825172424\n",
      "Epoch 7, Batch 185/211, Loss: 0.6977241635322571\n",
      "Epoch 7, Batch 186/211, Loss: 0.7064844369888306\n",
      "Epoch 7, Batch 187/211, Loss: 0.7061111927032471\n",
      "Epoch 7, Batch 188/211, Loss: 0.7029017806053162\n",
      "Epoch 7, Batch 189/211, Loss: 0.7028588652610779\n",
      "Epoch 7, Batch 190/211, Loss: 0.6858826279640198\n",
      "Epoch 7, Batch 191/211, Loss: 0.7136092185974121\n",
      "Epoch 7, Batch 192/211, Loss: 0.6924134492874146\n",
      "Epoch 7, Batch 193/211, Loss: 0.6775705218315125\n",
      "Epoch 7, Batch 194/211, Loss: 0.6953273415565491\n",
      "Epoch 7, Batch 195/211, Loss: 0.7125316858291626\n",
      "Epoch 7, Batch 196/211, Loss: 0.6962151527404785\n",
      "Epoch 7, Batch 197/211, Loss: 0.7256830334663391\n",
      "Epoch 7, Batch 198/211, Loss: 0.696874737739563\n",
      "Epoch 7, Batch 199/211, Loss: 0.7022069692611694\n",
      "Epoch 7, Batch 200/211, Loss: 0.7003573179244995\n",
      "Epoch 7, Batch 201/211, Loss: 0.7188823819160461\n",
      "Epoch 7, Batch 202/211, Loss: 0.693241536617279\n",
      "Epoch 7, Batch 203/211, Loss: 0.6972051858901978\n",
      "Epoch 7, Batch 204/211, Loss: 0.6949620246887207\n",
      "Epoch 7, Batch 205/211, Loss: 0.6753384470939636\n",
      "Epoch 7, Batch 206/211, Loss: 0.7051185369491577\n",
      "Epoch 7, Batch 207/211, Loss: 0.680938184261322\n",
      "Epoch 7, Batch 208/211, Loss: 0.6835454702377319\n",
      "Epoch 7, Batch 209/211, Loss: 0.6828317642211914\n",
      "Epoch 7, Batch 210/211, Loss: 0.7029913663864136\n",
      "Epoch 7, Batch 211/211, Loss: 0.5946371555328369\n",
      "Epoch 7, Loss: 146.48342722654343\n",
      "Epoch 8, Batch 1/211, Loss: 0.7161681652069092\n",
      "Epoch 8, Batch 2/211, Loss: 0.6768770813941956\n",
      "Epoch 8, Batch 3/211, Loss: 0.6945477724075317\n",
      "Epoch 8, Batch 4/211, Loss: 0.7142974138259888\n",
      "Epoch 8, Batch 5/211, Loss: 0.6539782881736755\n",
      "Epoch 8, Batch 6/211, Loss: 0.6638913154602051\n",
      "Epoch 8, Batch 7/211, Loss: 0.7237340807914734\n",
      "Epoch 8, Batch 8/211, Loss: 0.6925328373908997\n",
      "Epoch 8, Batch 9/211, Loss: 0.6752418279647827\n",
      "Epoch 8, Batch 10/211, Loss: 0.7018197774887085\n",
      "Epoch 8, Batch 11/211, Loss: 0.7055333852767944\n",
      "Epoch 8, Batch 12/211, Loss: 0.6984022855758667\n",
      "Epoch 8, Batch 13/211, Loss: 0.7013974785804749\n",
      "Epoch 8, Batch 14/211, Loss: 0.682141900062561\n",
      "Epoch 8, Batch 15/211, Loss: 0.6995322108268738\n",
      "Epoch 8, Batch 16/211, Loss: 0.6961852312088013\n",
      "Epoch 8, Batch 17/211, Loss: 0.6673775911331177\n",
      "Epoch 8, Batch 18/211, Loss: 0.6902971863746643\n",
      "Epoch 8, Batch 19/211, Loss: 0.6826974153518677\n",
      "Epoch 8, Batch 20/211, Loss: 0.6866620182991028\n",
      "Epoch 8, Batch 21/211, Loss: 0.6870850324630737\n",
      "Epoch 8, Batch 22/211, Loss: 0.7072569727897644\n",
      "Epoch 8, Batch 23/211, Loss: 0.701056718826294\n",
      "Epoch 8, Batch 24/211, Loss: 0.6799603700637817\n",
      "Epoch 8, Batch 25/211, Loss: 0.6957308053970337\n",
      "Epoch 8, Batch 26/211, Loss: 0.7107638120651245\n",
      "Epoch 8, Batch 27/211, Loss: 0.7082440257072449\n",
      "Epoch 8, Batch 28/211, Loss: 0.708823561668396\n",
      "Epoch 8, Batch 29/211, Loss: 0.6977927684783936\n",
      "Epoch 8, Batch 30/211, Loss: 0.6977301836013794\n",
      "Epoch 8, Batch 31/211, Loss: 0.6759528517723083\n",
      "Epoch 8, Batch 32/211, Loss: 0.6892194747924805\n",
      "Epoch 8, Batch 33/211, Loss: 0.7088691592216492\n",
      "Epoch 8, Batch 34/211, Loss: 0.7087568640708923\n",
      "Epoch 8, Batch 35/211, Loss: 0.7007256746292114\n",
      "Epoch 8, Batch 36/211, Loss: 0.7020326852798462\n",
      "Epoch 8, Batch 37/211, Loss: 0.6944339871406555\n",
      "Epoch 8, Batch 38/211, Loss: 0.6863824129104614\n",
      "Epoch 8, Batch 39/211, Loss: 0.6891271471977234\n",
      "Epoch 8, Batch 40/211, Loss: 0.6881352663040161\n",
      "Epoch 8, Batch 41/211, Loss: 0.6846027374267578\n",
      "Epoch 8, Batch 42/211, Loss: 0.679925799369812\n",
      "Epoch 8, Batch 43/211, Loss: 0.6735414266586304\n",
      "Epoch 8, Batch 44/211, Loss: 0.7035929560661316\n",
      "Epoch 8, Batch 45/211, Loss: 0.6775587797164917\n",
      "Epoch 8, Batch 46/211, Loss: 0.6943390965461731\n",
      "Epoch 8, Batch 47/211, Loss: 0.7017139792442322\n",
      "Epoch 8, Batch 48/211, Loss: 0.7067084312438965\n",
      "Epoch 8, Batch 49/211, Loss: 0.6795819997787476\n",
      "Epoch 8, Batch 50/211, Loss: 0.6740815043449402\n",
      "Epoch 8, Batch 51/211, Loss: 0.6790697574615479\n",
      "Epoch 8, Batch 52/211, Loss: 0.7105546593666077\n",
      "Epoch 8, Batch 53/211, Loss: 0.681306779384613\n",
      "Epoch 8, Batch 54/211, Loss: 0.6919824481010437\n",
      "Epoch 8, Batch 55/211, Loss: 0.6806320548057556\n",
      "Epoch 8, Batch 56/211, Loss: 0.6957895755767822\n",
      "Epoch 8, Batch 57/211, Loss: 0.7060866355895996\n",
      "Epoch 8, Batch 58/211, Loss: 0.6891701817512512\n",
      "Epoch 8, Batch 59/211, Loss: 0.6796355843544006\n",
      "Epoch 8, Batch 60/211, Loss: 0.6817066669464111\n",
      "Epoch 8, Batch 61/211, Loss: 0.6976799964904785\n",
      "Epoch 8, Batch 62/211, Loss: 0.7126660943031311\n",
      "Epoch 8, Batch 63/211, Loss: 0.6661308407783508\n",
      "Epoch 8, Batch 64/211, Loss: 0.6864560842514038\n",
      "Epoch 8, Batch 65/211, Loss: 0.6674470901489258\n",
      "Epoch 8, Batch 66/211, Loss: 0.69576096534729\n",
      "Epoch 8, Batch 67/211, Loss: 0.6648945212364197\n",
      "Epoch 8, Batch 68/211, Loss: 0.6907569169998169\n",
      "Epoch 8, Batch 69/211, Loss: 0.6865159869194031\n",
      "Epoch 8, Batch 70/211, Loss: 0.7071301937103271\n",
      "Epoch 8, Batch 71/211, Loss: 0.7134737968444824\n",
      "Epoch 8, Batch 72/211, Loss: 0.7024303078651428\n",
      "Epoch 8, Batch 73/211, Loss: 0.7250012755393982\n",
      "Epoch 8, Batch 74/211, Loss: 0.701579749584198\n",
      "Epoch 8, Batch 75/211, Loss: 0.6916163563728333\n",
      "Epoch 8, Batch 76/211, Loss: 0.6847176551818848\n",
      "Epoch 8, Batch 77/211, Loss: 0.6752221584320068\n",
      "Epoch 8, Batch 78/211, Loss: 0.6974491477012634\n",
      "Epoch 8, Batch 79/211, Loss: 0.6999953985214233\n",
      "Epoch 8, Batch 80/211, Loss: 0.7171223759651184\n",
      "Epoch 8, Batch 81/211, Loss: 0.6802754402160645\n",
      "Epoch 8, Batch 82/211, Loss: 0.6935160160064697\n",
      "Epoch 8, Batch 83/211, Loss: 0.7160894274711609\n",
      "Epoch 8, Batch 84/211, Loss: 0.689577579498291\n",
      "Epoch 8, Batch 85/211, Loss: 0.6905794143676758\n",
      "Epoch 8, Batch 86/211, Loss: 0.698826789855957\n",
      "Epoch 8, Batch 87/211, Loss: 0.6790217161178589\n",
      "Epoch 8, Batch 88/211, Loss: 0.68304443359375\n",
      "Epoch 8, Batch 89/211, Loss: 0.6825920343399048\n",
      "Epoch 8, Batch 90/211, Loss: 0.6971845030784607\n",
      "Epoch 8, Batch 91/211, Loss: 0.6757753491401672\n",
      "Epoch 8, Batch 92/211, Loss: 0.7108147144317627\n",
      "Epoch 8, Batch 93/211, Loss: 0.7003557085990906\n",
      "Epoch 8, Batch 94/211, Loss: 0.6872169375419617\n",
      "Epoch 8, Batch 95/211, Loss: 0.7069951295852661\n",
      "Epoch 8, Batch 96/211, Loss: 0.6797179579734802\n",
      "Epoch 8, Batch 97/211, Loss: 0.6780304908752441\n",
      "Epoch 8, Batch 98/211, Loss: 0.7237153649330139\n",
      "Epoch 8, Batch 99/211, Loss: 0.6984338760375977\n",
      "Epoch 8, Batch 100/211, Loss: 0.6927717328071594\n",
      "Epoch 8, Batch 101/211, Loss: 0.6768456697463989\n",
      "Epoch 8, Batch 102/211, Loss: 0.721000611782074\n",
      "Epoch 8, Batch 103/211, Loss: 0.6828839182853699\n",
      "Epoch 8, Batch 104/211, Loss: 0.6776860356330872\n",
      "Epoch 8, Batch 105/211, Loss: 0.7011228799819946\n",
      "Epoch 8, Batch 106/211, Loss: 0.7165419459342957\n",
      "Epoch 8, Batch 107/211, Loss: 0.6912713646888733\n",
      "Epoch 8, Batch 108/211, Loss: 0.7037060260772705\n",
      "Epoch 8, Batch 109/211, Loss: 0.691093385219574\n",
      "Epoch 8, Batch 110/211, Loss: 0.7053332924842834\n",
      "Epoch 8, Batch 111/211, Loss: 0.6687362790107727\n",
      "Epoch 8, Batch 112/211, Loss: 0.708411455154419\n",
      "Epoch 8, Batch 113/211, Loss: 0.6911638975143433\n",
      "Epoch 8, Batch 114/211, Loss: 0.6742322444915771\n",
      "Epoch 8, Batch 115/211, Loss: 0.687828540802002\n",
      "Epoch 8, Batch 116/211, Loss: 0.6899812817573547\n",
      "Epoch 8, Batch 117/211, Loss: 0.6935555934906006\n",
      "Epoch 8, Batch 118/211, Loss: 0.6934913992881775\n",
      "Epoch 8, Batch 119/211, Loss: 0.7092028856277466\n",
      "Epoch 8, Batch 120/211, Loss: 0.6895001530647278\n",
      "Epoch 8, Batch 121/211, Loss: 0.7098997831344604\n",
      "Epoch 8, Batch 122/211, Loss: 0.6785085797309875\n",
      "Epoch 8, Batch 123/211, Loss: 0.7078883051872253\n",
      "Epoch 8, Batch 124/211, Loss: 0.6942417621612549\n",
      "Epoch 8, Batch 125/211, Loss: 0.6885203123092651\n",
      "Epoch 8, Batch 126/211, Loss: 0.6887810230255127\n",
      "Epoch 8, Batch 127/211, Loss: 0.690926194190979\n",
      "Epoch 8, Batch 128/211, Loss: 0.6730834245681763\n",
      "Epoch 8, Batch 129/211, Loss: 0.6786402463912964\n",
      "Epoch 8, Batch 130/211, Loss: 0.7122067213058472\n",
      "Epoch 8, Batch 131/211, Loss: 0.7074392437934875\n",
      "Epoch 8, Batch 132/211, Loss: 0.6885868906974792\n",
      "Epoch 8, Batch 133/211, Loss: 0.6978266835212708\n",
      "Epoch 8, Batch 134/211, Loss: 0.6854270696640015\n",
      "Epoch 8, Batch 135/211, Loss: 0.6981142163276672\n",
      "Epoch 8, Batch 136/211, Loss: 0.6762508153915405\n",
      "Epoch 8, Batch 137/211, Loss: 0.6871643662452698\n",
      "Epoch 8, Batch 138/211, Loss: 0.7005655765533447\n",
      "Epoch 8, Batch 139/211, Loss: 0.6982919573783875\n",
      "Epoch 8, Batch 140/211, Loss: 0.6783899664878845\n",
      "Epoch 8, Batch 141/211, Loss: 0.7168150544166565\n",
      "Epoch 8, Batch 142/211, Loss: 0.6976650357246399\n",
      "Epoch 8, Batch 143/211, Loss: 0.713455319404602\n",
      "Epoch 8, Batch 144/211, Loss: 0.697170078754425\n",
      "Epoch 8, Batch 145/211, Loss: 0.6840914487838745\n",
      "Epoch 8, Batch 146/211, Loss: 0.7017973065376282\n",
      "Epoch 8, Batch 147/211, Loss: 0.6684417128562927\n",
      "Epoch 8, Batch 148/211, Loss: 0.6741697788238525\n",
      "Epoch 8, Batch 149/211, Loss: 0.6737185120582581\n",
      "Epoch 8, Batch 150/211, Loss: 0.6949062943458557\n",
      "Epoch 8, Batch 151/211, Loss: 0.7051732540130615\n",
      "Epoch 8, Batch 152/211, Loss: 0.6948835253715515\n",
      "Epoch 8, Batch 153/211, Loss: 0.6999226808547974\n",
      "Epoch 8, Batch 154/211, Loss: 0.6680861115455627\n",
      "Epoch 8, Batch 155/211, Loss: 0.6989404559135437\n",
      "Epoch 8, Batch 156/211, Loss: 0.69805908203125\n",
      "Epoch 8, Batch 157/211, Loss: 0.7120764255523682\n",
      "Epoch 8, Batch 158/211, Loss: 0.6911579966545105\n",
      "Epoch 8, Batch 159/211, Loss: 0.6890220046043396\n",
      "Epoch 8, Batch 160/211, Loss: 0.6797631978988647\n",
      "Epoch 8, Batch 161/211, Loss: 0.6752194762229919\n",
      "Epoch 8, Batch 162/211, Loss: 0.697150468826294\n",
      "Epoch 8, Batch 163/211, Loss: 0.6877917051315308\n",
      "Epoch 8, Batch 164/211, Loss: 0.6738903522491455\n",
      "Epoch 8, Batch 165/211, Loss: 0.7027177214622498\n",
      "Epoch 8, Batch 166/211, Loss: 0.7178903222084045\n",
      "Epoch 8, Batch 167/211, Loss: 0.6808645129203796\n",
      "Epoch 8, Batch 168/211, Loss: 0.6955329775810242\n",
      "Epoch 8, Batch 169/211, Loss: 0.6785009503364563\n",
      "Epoch 8, Batch 170/211, Loss: 0.6964974403381348\n",
      "Epoch 8, Batch 171/211, Loss: 0.7053658366203308\n",
      "Epoch 8, Batch 172/211, Loss: 0.683571994304657\n",
      "Epoch 8, Batch 173/211, Loss: 0.6675342321395874\n",
      "Epoch 8, Batch 174/211, Loss: 0.6724429130554199\n",
      "Epoch 8, Batch 175/211, Loss: 0.6657193303108215\n",
      "Epoch 8, Batch 176/211, Loss: 0.6825921535491943\n",
      "Epoch 8, Batch 177/211, Loss: 0.7009710669517517\n",
      "Epoch 8, Batch 178/211, Loss: 0.7040100693702698\n",
      "Epoch 8, Batch 179/211, Loss: 0.6963590383529663\n",
      "Epoch 8, Batch 180/211, Loss: 0.6924037337303162\n",
      "Epoch 8, Batch 181/211, Loss: 0.6739475131034851\n",
      "Epoch 8, Batch 182/211, Loss: 0.7047627568244934\n",
      "Epoch 8, Batch 183/211, Loss: 0.7156891226768494\n",
      "Epoch 8, Batch 184/211, Loss: 0.6689810752868652\n",
      "Epoch 8, Batch 185/211, Loss: 0.6772553324699402\n",
      "Epoch 8, Batch 186/211, Loss: 0.6992917656898499\n",
      "Epoch 8, Batch 187/211, Loss: 0.688225269317627\n",
      "Epoch 8, Batch 188/211, Loss: 0.7071918249130249\n",
      "Epoch 8, Batch 189/211, Loss: 0.6931270360946655\n",
      "Epoch 8, Batch 190/211, Loss: 0.6666638851165771\n",
      "Epoch 8, Batch 191/211, Loss: 0.7227046489715576\n",
      "Epoch 8, Batch 192/211, Loss: 0.7261700630187988\n",
      "Epoch 8, Batch 193/211, Loss: 0.6882603168487549\n",
      "Epoch 8, Batch 194/211, Loss: 0.6834282875061035\n",
      "Epoch 8, Batch 195/211, Loss: 0.7070562839508057\n",
      "Epoch 8, Batch 196/211, Loss: 0.7076181173324585\n",
      "Epoch 8, Batch 197/211, Loss: 0.7105507254600525\n",
      "Epoch 8, Batch 198/211, Loss: 0.6943327784538269\n",
      "Epoch 8, Batch 199/211, Loss: 0.689395546913147\n",
      "Epoch 8, Batch 200/211, Loss: 0.6937806010246277\n",
      "Epoch 8, Batch 201/211, Loss: 0.6895974278450012\n",
      "Epoch 8, Batch 202/211, Loss: 0.7235333919525146\n",
      "Epoch 8, Batch 203/211, Loss: 0.6782590746879578\n",
      "Epoch 8, Batch 204/211, Loss: 0.7143503427505493\n",
      "Epoch 8, Batch 205/211, Loss: 0.7228109240531921\n",
      "Epoch 8, Batch 206/211, Loss: 0.6762667298316956\n",
      "Epoch 8, Batch 207/211, Loss: 0.7146920561790466\n",
      "Epoch 8, Batch 208/211, Loss: 0.6863822340965271\n",
      "Epoch 8, Batch 209/211, Loss: 0.6691616177558899\n",
      "Epoch 8, Batch 210/211, Loss: 0.6969771981239319\n",
      "Epoch 8, Batch 211/211, Loss: 0.6083428263664246\n",
      "Epoch 8, Loss: 146.13104796409607\n",
      "Epoch 9, Batch 1/211, Loss: 0.6759262084960938\n",
      "Epoch 9, Batch 2/211, Loss: 0.6995416283607483\n",
      "Epoch 9, Batch 3/211, Loss: 0.7148424386978149\n",
      "Epoch 9, Batch 4/211, Loss: 0.7012101411819458\n",
      "Epoch 9, Batch 5/211, Loss: 0.7203919887542725\n",
      "Epoch 9, Batch 6/211, Loss: 0.7230992317199707\n",
      "Epoch 9, Batch 7/211, Loss: 0.6803023219108582\n",
      "Epoch 9, Batch 8/211, Loss: 0.6980485916137695\n",
      "Epoch 9, Batch 9/211, Loss: 0.671843945980072\n",
      "Epoch 9, Batch 10/211, Loss: 0.6863877177238464\n",
      "Epoch 9, Batch 11/211, Loss: 0.7200044989585876\n",
      "Epoch 9, Batch 12/211, Loss: 0.7077609300613403\n",
      "Epoch 9, Batch 13/211, Loss: 0.6911234259605408\n",
      "Epoch 9, Batch 14/211, Loss: 0.7148488163948059\n",
      "Epoch 9, Batch 15/211, Loss: 0.690730094909668\n",
      "Epoch 9, Batch 16/211, Loss: 0.7149682641029358\n",
      "Epoch 9, Batch 17/211, Loss: 0.6778033375740051\n",
      "Epoch 9, Batch 18/211, Loss: 0.6918042898178101\n",
      "Epoch 9, Batch 19/211, Loss: 0.723798930644989\n",
      "Epoch 9, Batch 20/211, Loss: 0.7094866037368774\n",
      "Epoch 9, Batch 21/211, Loss: 0.6851562857627869\n",
      "Epoch 9, Batch 22/211, Loss: 0.7055509090423584\n",
      "Epoch 9, Batch 23/211, Loss: 0.7013446092605591\n",
      "Epoch 9, Batch 24/211, Loss: 0.707043468952179\n",
      "Epoch 9, Batch 25/211, Loss: 0.6978353261947632\n",
      "Epoch 9, Batch 26/211, Loss: 0.6667351126670837\n",
      "Epoch 9, Batch 27/211, Loss: 0.6946914196014404\n",
      "Epoch 9, Batch 28/211, Loss: 0.7080477476119995\n",
      "Epoch 9, Batch 29/211, Loss: 0.6909633874893188\n",
      "Epoch 9, Batch 30/211, Loss: 0.6868441104888916\n",
      "Epoch 9, Batch 31/211, Loss: 0.6918613314628601\n",
      "Epoch 9, Batch 32/211, Loss: 0.6996546983718872\n",
      "Epoch 9, Batch 33/211, Loss: 0.6808229088783264\n",
      "Epoch 9, Batch 34/211, Loss: 0.6885663270950317\n",
      "Epoch 9, Batch 35/211, Loss: 0.7100158333778381\n",
      "Epoch 9, Batch 36/211, Loss: 0.7164130806922913\n",
      "Epoch 9, Batch 37/211, Loss: 0.71062171459198\n",
      "Epoch 9, Batch 38/211, Loss: 0.7079443335533142\n",
      "Epoch 9, Batch 39/211, Loss: 0.706627368927002\n",
      "Epoch 9, Batch 40/211, Loss: 0.6782996654510498\n",
      "Epoch 9, Batch 41/211, Loss: 0.709166407585144\n",
      "Epoch 9, Batch 42/211, Loss: 0.7090091705322266\n",
      "Epoch 9, Batch 43/211, Loss: 0.6766538619995117\n",
      "Epoch 9, Batch 44/211, Loss: 0.6979405879974365\n",
      "Epoch 9, Batch 45/211, Loss: 0.6817949414253235\n",
      "Epoch 9, Batch 46/211, Loss: 0.7085081934928894\n",
      "Epoch 9, Batch 47/211, Loss: 0.7151578664779663\n",
      "Epoch 9, Batch 48/211, Loss: 0.6974810361862183\n",
      "Epoch 9, Batch 49/211, Loss: 0.6814638376235962\n",
      "Epoch 9, Batch 50/211, Loss: 0.6899847984313965\n",
      "Epoch 9, Batch 51/211, Loss: 0.6802013516426086\n",
      "Epoch 9, Batch 52/211, Loss: 0.7125434875488281\n",
      "Epoch 9, Batch 53/211, Loss: 0.688575267791748\n",
      "Epoch 9, Batch 54/211, Loss: 0.7028460502624512\n",
      "Epoch 9, Batch 55/211, Loss: 0.7057102918624878\n",
      "Epoch 9, Batch 56/211, Loss: 0.6885921359062195\n",
      "Epoch 9, Batch 57/211, Loss: 0.6675793528556824\n",
      "Epoch 9, Batch 58/211, Loss: 0.6967017650604248\n",
      "Epoch 9, Batch 59/211, Loss: 0.6707603931427002\n",
      "Epoch 9, Batch 60/211, Loss: 0.7067018151283264\n",
      "Epoch 9, Batch 61/211, Loss: 0.6845813393592834\n",
      "Epoch 9, Batch 62/211, Loss: 0.6839962601661682\n",
      "Epoch 9, Batch 63/211, Loss: 0.6820250153541565\n",
      "Epoch 9, Batch 64/211, Loss: 0.6910786628723145\n",
      "Epoch 9, Batch 65/211, Loss: 0.6884760856628418\n",
      "Epoch 9, Batch 66/211, Loss: 0.7238717675209045\n",
      "Epoch 9, Batch 67/211, Loss: 0.699415385723114\n",
      "Epoch 9, Batch 68/211, Loss: 0.6748300194740295\n",
      "Epoch 9, Batch 69/211, Loss: 0.7016357183456421\n",
      "Epoch 9, Batch 70/211, Loss: 0.6941372156143188\n",
      "Epoch 9, Batch 71/211, Loss: 0.7040063738822937\n",
      "Epoch 9, Batch 72/211, Loss: 0.7215005159378052\n",
      "Epoch 9, Batch 73/211, Loss: 0.6966027617454529\n",
      "Epoch 9, Batch 74/211, Loss: 0.6916866898536682\n",
      "Epoch 9, Batch 75/211, Loss: 0.6989375352859497\n",
      "Epoch 9, Batch 76/211, Loss: 0.6939803957939148\n",
      "Epoch 9, Batch 77/211, Loss: 0.7191054821014404\n",
      "Epoch 9, Batch 78/211, Loss: 0.6882246136665344\n",
      "Epoch 9, Batch 79/211, Loss: 0.6973068118095398\n",
      "Epoch 9, Batch 80/211, Loss: 0.7002825736999512\n",
      "Epoch 9, Batch 81/211, Loss: 0.6947388052940369\n",
      "Epoch 9, Batch 82/211, Loss: 0.6804589629173279\n",
      "Epoch 9, Batch 83/211, Loss: 0.7088678479194641\n",
      "Epoch 9, Batch 84/211, Loss: 0.6921260356903076\n",
      "Epoch 9, Batch 85/211, Loss: 0.6945027709007263\n",
      "Epoch 9, Batch 86/211, Loss: 0.6950518488883972\n",
      "Epoch 9, Batch 87/211, Loss: 0.6751073598861694\n",
      "Epoch 9, Batch 88/211, Loss: 0.6761780381202698\n",
      "Epoch 9, Batch 89/211, Loss: 0.6809589862823486\n",
      "Epoch 9, Batch 90/211, Loss: 0.7095902562141418\n",
      "Epoch 9, Batch 91/211, Loss: 0.6915591955184937\n",
      "Epoch 9, Batch 92/211, Loss: 0.6916802525520325\n",
      "Epoch 9, Batch 93/211, Loss: 0.6927229762077332\n",
      "Epoch 9, Batch 94/211, Loss: 0.6963120102882385\n",
      "Epoch 9, Batch 95/211, Loss: 0.7100485563278198\n",
      "Epoch 9, Batch 96/211, Loss: 0.6883895993232727\n",
      "Epoch 9, Batch 97/211, Loss: 0.7098148465156555\n",
      "Epoch 9, Batch 98/211, Loss: 0.6987123489379883\n",
      "Epoch 9, Batch 99/211, Loss: 0.6890901923179626\n",
      "Epoch 9, Batch 100/211, Loss: 0.7190352082252502\n",
      "Epoch 9, Batch 101/211, Loss: 0.7031019330024719\n",
      "Epoch 9, Batch 102/211, Loss: 0.7006454467773438\n",
      "Epoch 9, Batch 103/211, Loss: 0.666939377784729\n",
      "Epoch 9, Batch 104/211, Loss: 0.6929851770401001\n",
      "Epoch 9, Batch 105/211, Loss: 0.6836510300636292\n",
      "Epoch 9, Batch 106/211, Loss: 0.6881126165390015\n",
      "Epoch 9, Batch 107/211, Loss: 0.6973844170570374\n",
      "Epoch 9, Batch 108/211, Loss: 0.6995310187339783\n",
      "Epoch 9, Batch 109/211, Loss: 0.7060149312019348\n",
      "Epoch 9, Batch 110/211, Loss: 0.7085013389587402\n",
      "Epoch 9, Batch 111/211, Loss: 0.6760219931602478\n",
      "Epoch 9, Batch 112/211, Loss: 0.6968225836753845\n",
      "Epoch 9, Batch 113/211, Loss: 0.6660000085830688\n",
      "Epoch 9, Batch 114/211, Loss: 0.6883510947227478\n",
      "Epoch 9, Batch 115/211, Loss: 0.705272912979126\n",
      "Epoch 9, Batch 116/211, Loss: 0.7209547162055969\n",
      "Epoch 9, Batch 117/211, Loss: 0.6678792834281921\n",
      "Epoch 9, Batch 118/211, Loss: 0.6845730543136597\n",
      "Epoch 9, Batch 119/211, Loss: 0.6882611513137817\n",
      "Epoch 9, Batch 120/211, Loss: 0.6930736899375916\n",
      "Epoch 9, Batch 121/211, Loss: 0.6773101091384888\n",
      "Epoch 9, Batch 122/211, Loss: 0.7084287405014038\n",
      "Epoch 9, Batch 123/211, Loss: 0.7041962742805481\n",
      "Epoch 9, Batch 124/211, Loss: 0.7059112787246704\n",
      "Epoch 9, Batch 125/211, Loss: 0.7129648923873901\n",
      "Epoch 9, Batch 126/211, Loss: 0.6849377751350403\n",
      "Epoch 9, Batch 127/211, Loss: 0.6703492999076843\n",
      "Epoch 9, Batch 128/211, Loss: 0.6925934553146362\n",
      "Epoch 9, Batch 129/211, Loss: 0.6943856477737427\n",
      "Epoch 9, Batch 130/211, Loss: 0.6867913603782654\n",
      "Epoch 9, Batch 131/211, Loss: 0.6980145573616028\n",
      "Epoch 9, Batch 132/211, Loss: 0.676054060459137\n",
      "Epoch 9, Batch 133/211, Loss: 0.7015388607978821\n",
      "Epoch 9, Batch 134/211, Loss: 0.69991135597229\n",
      "Epoch 9, Batch 135/211, Loss: 0.7148351073265076\n",
      "Epoch 9, Batch 136/211, Loss: 0.6975030303001404\n",
      "Epoch 9, Batch 137/211, Loss: 0.7157741785049438\n",
      "Epoch 9, Batch 138/211, Loss: 0.7122853994369507\n",
      "Epoch 9, Batch 139/211, Loss: 0.6803915500640869\n",
      "Epoch 9, Batch 140/211, Loss: 0.6910921931266785\n",
      "Epoch 9, Batch 141/211, Loss: 0.6918274164199829\n",
      "Epoch 9, Batch 142/211, Loss: 0.6713991165161133\n",
      "Epoch 9, Batch 143/211, Loss: 0.6894136071205139\n",
      "Epoch 9, Batch 144/211, Loss: 0.7063311338424683\n",
      "Epoch 9, Batch 145/211, Loss: 0.6844539642333984\n",
      "Epoch 9, Batch 146/211, Loss: 0.7032283544540405\n",
      "Epoch 9, Batch 147/211, Loss: 0.6800991296768188\n",
      "Epoch 9, Batch 148/211, Loss: 0.6711357831954956\n",
      "Epoch 9, Batch 149/211, Loss: 0.6912196278572083\n",
      "Epoch 9, Batch 150/211, Loss: 0.6933071613311768\n",
      "Epoch 9, Batch 151/211, Loss: 0.715775728225708\n",
      "Epoch 9, Batch 152/211, Loss: 0.6858680248260498\n",
      "Epoch 9, Batch 153/211, Loss: 0.6969751715660095\n",
      "Epoch 9, Batch 154/211, Loss: 0.6543159484863281\n",
      "Epoch 9, Batch 155/211, Loss: 0.6803305745124817\n",
      "Epoch 9, Batch 156/211, Loss: 0.6899538040161133\n",
      "Epoch 9, Batch 157/211, Loss: 0.6939491033554077\n",
      "Epoch 9, Batch 158/211, Loss: 0.7224658727645874\n",
      "Epoch 9, Batch 159/211, Loss: 0.6916149854660034\n",
      "Epoch 9, Batch 160/211, Loss: 0.7124152779579163\n",
      "Epoch 9, Batch 161/211, Loss: 0.6914759874343872\n",
      "Epoch 9, Batch 162/211, Loss: 0.6795163750648499\n",
      "Epoch 9, Batch 163/211, Loss: 0.6763010621070862\n",
      "Epoch 9, Batch 164/211, Loss: 0.7090490460395813\n",
      "Epoch 9, Batch 165/211, Loss: 0.6700102090835571\n",
      "Epoch 9, Batch 166/211, Loss: 0.6800366640090942\n",
      "Epoch 9, Batch 167/211, Loss: 0.719214141368866\n",
      "Epoch 9, Batch 168/211, Loss: 0.6550078988075256\n",
      "Epoch 9, Batch 169/211, Loss: 0.6894032955169678\n",
      "Epoch 9, Batch 170/211, Loss: 0.7227545380592346\n",
      "Epoch 9, Batch 171/211, Loss: 0.6954115629196167\n",
      "Epoch 9, Batch 172/211, Loss: 0.6871110796928406\n",
      "Epoch 9, Batch 173/211, Loss: 0.7368324398994446\n",
      "Epoch 9, Batch 174/211, Loss: 0.6884172558784485\n",
      "Epoch 9, Batch 175/211, Loss: 0.6872650384902954\n",
      "Epoch 9, Batch 176/211, Loss: 0.6752168536186218\n",
      "Epoch 9, Batch 177/211, Loss: 0.6930195093154907\n",
      "Epoch 9, Batch 178/211, Loss: 0.6908729672431946\n",
      "Epoch 9, Batch 179/211, Loss: 0.7014586329460144\n",
      "Epoch 9, Batch 180/211, Loss: 0.709615170955658\n",
      "Epoch 9, Batch 181/211, Loss: 0.6838791966438293\n",
      "Epoch 9, Batch 182/211, Loss: 0.7008792161941528\n",
      "Epoch 9, Batch 183/211, Loss: 0.6871052384376526\n",
      "Epoch 9, Batch 184/211, Loss: 0.7264052033424377\n",
      "Epoch 9, Batch 185/211, Loss: 0.6664318442344666\n",
      "Epoch 9, Batch 186/211, Loss: 0.6880670189857483\n",
      "Epoch 9, Batch 187/211, Loss: 0.6895788311958313\n",
      "Epoch 9, Batch 188/211, Loss: 0.6798152923583984\n",
      "Epoch 9, Batch 189/211, Loss: 0.7064894437789917\n",
      "Epoch 9, Batch 190/211, Loss: 0.7027637362480164\n",
      "Epoch 9, Batch 191/211, Loss: 0.7106389999389648\n",
      "Epoch 9, Batch 192/211, Loss: 0.6872187852859497\n",
      "Epoch 9, Batch 193/211, Loss: 0.7006212472915649\n",
      "Epoch 9, Batch 194/211, Loss: 0.7062299251556396\n",
      "Epoch 9, Batch 195/211, Loss: 0.6984888911247253\n",
      "Epoch 9, Batch 196/211, Loss: 0.6589062809944153\n",
      "Epoch 9, Batch 197/211, Loss: 0.6823933720588684\n",
      "Epoch 9, Batch 198/211, Loss: 0.6862489581108093\n",
      "Epoch 9, Batch 199/211, Loss: 0.6868687868118286\n",
      "Epoch 9, Batch 200/211, Loss: 0.6943604350090027\n",
      "Epoch 9, Batch 201/211, Loss: 0.7042251229286194\n",
      "Epoch 9, Batch 202/211, Loss: 0.7071831822395325\n",
      "Epoch 9, Batch 203/211, Loss: 0.6867595314979553\n",
      "Epoch 9, Batch 204/211, Loss: 0.665596604347229\n",
      "Epoch 9, Batch 205/211, Loss: 0.6765624284744263\n",
      "Epoch 9, Batch 206/211, Loss: 0.7016847133636475\n",
      "Epoch 9, Batch 207/211, Loss: 0.7171432375907898\n",
      "Epoch 9, Batch 208/211, Loss: 0.6853424310684204\n",
      "Epoch 9, Batch 209/211, Loss: 0.6961645483970642\n",
      "Epoch 9, Batch 210/211, Loss: 0.676798403263092\n",
      "Epoch 9, Batch 211/211, Loss: 0.724801242351532\n",
      "Epoch 9, Loss: 146.5960315465927\n"
     ]
    }
   ],
   "source": [
    "epochs = 9\n",
    "train_covid(model, optimizer, loss_fn, train_dataloader, epochs)\n",
    "#train_covid(model, optimizer, train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asubedi\\AppData\\Local\\Temp\\ipykernel_280080\\751014086.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  claim = {key: torch.tensor(value, dtype=torch.long) for key, value in self.data[idx].items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 48.30%\n",
      "Per Class Accuracy: {0: 32.95668549905838, 1: 73.6024844720497}\n",
      "Precision: [67.30769231 39.96627319]\n",
      "Recall: [32.9566855  73.60248447]\n",
      "F1: [0.44247788 0.51803279]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            tweets, labels = batch\n",
    "            tweets = {key: value.to(device) for key, value in tweets.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # run sequences through BERT\n",
    "            outputs = model(**tweets, labels=labels)\n",
    "            \n",
    "            # highest energy class is our prediction\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    class_labels = [0, 1]\n",
    "    #class_labels = [0, 1, 2, 3, 4, 5]\n",
    "    per_class_accuracy = {}\n",
    "    for class_label in class_labels:\n",
    "        # get indices which match current class_label\n",
    "        class_indices = np.where(np.array(all_labels) == class_label)[0]\n",
    "        \n",
    "        # get predictions of current class label\n",
    "        class_preds = np.array(all_preds)[class_indices]\n",
    "        \n",
    "        # calculate accuracy for current class_label\n",
    "        correct_class_preds = np.sum(class_preds == class_label)\n",
    "        total_class_samples = len(class_indices)\n",
    "        \n",
    "        per_class_accuracy[class_label] = (correct_class_preds / total_class_samples) * 100\n",
    "        \n",
    "    accuracy = 100*accuracy_score(all_labels, all_preds)\n",
    "    precision = 100*precision_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "    recall = 100*recall_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, f1, per_class_accuracy\n",
    "\n",
    "\n",
    "# evaluate the model on the test set (unaugmented)\n",
    "accuracy, precision, recall, f1, per_class_accuracy = evaluate_model(model, test_dataloader)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Per Class Accuracy: {per_class_accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./models/model_weights15-{accuracy:.1f}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
