{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asubedi\\anaconda3\\envs\\cmsc673\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if PyTorch recognizes GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6591 entries, 0 to 6590\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   misconception_id  6591 non-null   int64 \n",
      " 1   misconception     6591 non-null   object\n",
      " 2   tweet_id          6591 non-null   int64 \n",
      " 3   label             6591 non-null   object\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 206.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/covid_lies.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_covid(samples, labels, tokenizer):\n",
    "  tokenized = []\n",
    "  for idx in range(len(samples)):\n",
    "    tokenized_tweet = tokenizer(samples[idx], return_tensors='pt')\n",
    "    \n",
    "    n_inst = {\n",
    "      'tweet_token': tokenized_tweet,\n",
    "      'tweet_origin': samples[idx],\n",
    "      'label': labels[idx], \n",
    "      'idx': idx\n",
    "    }\n",
    "    tokenized.append(n_inst)\n",
    "\n",
    "  return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 0,\n",
      " 'label': 'na',\n",
      " 'tweet_origin': 'Coronavirus is genetically engineered.',\n",
      " 'tweet_token': {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]),\n",
      "                 'input_ids': tensor([[  101, 21887, 23350,  2003, 19345, 13685,  1012,   102]]),\n",
      "                 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]])}}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")\n",
    "\n",
    "# obtain samples/label pairs from dataset\n",
    "covid_samples = np.array(df['misconception'])\n",
    "covid_labels = np.array(df['label'])\n",
    "\n",
    "tokenized_dataset = tokenize_covid(covid_samples, covid_labels, tokenizer)\n",
    "pprint.pprint(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTorch Datasets (Augmented using SMOTE & Unaugmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class MisinformationAugmentedDataset(Dataset):\n",
    "    def __init__(self, covid_data):\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "    \n",
    "        self.label_map = {\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 1,\n",
    "            \"na\": 2,\n",
    "        }\n",
    "\n",
    "        # perform upsampling of underrepresented data in our imbalanced dataset\n",
    "        input_ids = [inst['tweet_token']['input_ids'].squeeze(0) for inst in covid_data]\n",
    "        attention_masks = [inst['tweet_token']['attention_mask'].squeeze(0) for inst in covid_data]\n",
    "        token_type_ids = [inst['tweet_token']['token_type_ids'].squeeze(0) for inst in covid_data]\n",
    "        numeric_labels = [self.label_map[inst['label']] for inst in covid_data]\n",
    "        \n",
    "        padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0).numpy()\n",
    "        padded_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0).numpy()\n",
    "        padded_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0).numpy()\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_upsampled, y_upsampled = smote.fit_resample(padded_input_ids, numeric_labels)\n",
    "\n",
    "        # rebuild dictionaries\n",
    "        for idx in range(len(X_upsampled)):\n",
    "            input_ids_tensor = torch.tensor(X_upsampled[idx], dtype=torch.long)\n",
    "            attention_mask_tensor = torch.tensor(\n",
    "                padded_attention_masks[idx % len(padded_attention_masks)], dtype=torch.long\n",
    "            )\n",
    "            token_type_ids_tensor = torch.tensor(\n",
    "                padded_token_type_ids[idx % len(padded_token_type_ids)], dtype=torch.long\n",
    "            )\n",
    "            tweet_token = {\n",
    "                \"input_ids\": input_ids_tensor,\n",
    "                \"attention_mask\": attention_mask_tensor,\n",
    "                \"token_type_ids\": token_type_ids_tensor,\n",
    "            }\n",
    "\n",
    "            self.data.append(tweet_token)\n",
    "            self.labels.append(torch.tensor(y_upsampled[idx], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # adjust code according to received UserWarning:\n",
    "        # UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() \n",
    "        # or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
    "        tweet = {key: value.clone().detach() for key, value in tweet.items()}\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return tweet, label\n",
    "    \n",
    "class MisinformationDataset(Dataset):\n",
    "    def __init__(self, covid_data):\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "    \n",
    "        self.label_map = {\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 1,\n",
    "            \"na\": 2,\n",
    "        }\n",
    "\n",
    "        # perform upsampling of underrepresented data in our imbalanced dataset\n",
    "        input_ids = [inst['tweet_token']['input_ids'].squeeze(0) for inst in covid_data]\n",
    "        attention_masks = [inst['tweet_token']['attention_mask'].squeeze(0) for inst in covid_data]\n",
    "        token_type_ids = [inst['tweet_token']['token_type_ids'].squeeze(0) for inst in covid_data]\n",
    "        numeric_labels = [self.label_map[inst['label']] for inst in covid_data]\n",
    "        \n",
    "        padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0).numpy()\n",
    "        padded_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0).numpy()\n",
    "        padded_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0).numpy()\n",
    "\n",
    "        # rebuild dictionaries\n",
    "        for idx in range(len(covid_data)):\n",
    "            input_ids_tensor = torch.tensor(padded_input_ids[idx], dtype=torch.long)\n",
    "            attention_mask_tensor = torch.tensor(padded_attention_masks[idx], dtype=torch.long)\n",
    "            token_type_ids_tensor = torch.tensor(padded_token_type_ids[idx], dtype=torch.long)\n",
    "            tweet_token = {\n",
    "                \"input_ids\": input_ids_tensor,\n",
    "                \"attention_mask\": attention_mask_tensor,\n",
    "                \"token_type_ids\": token_type_ids_tensor,\n",
    "            }\n",
    "\n",
    "            self.data.append(tweet_token)\n",
    "            self.labels.append(torch.tensor(numeric_labels[idx], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the tokenized tweet and label\n",
    "        tweet = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return tweet, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Dataset into training and test portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_vals = [inst['tweet_token'] for inst in tokenized_dataset]\n",
    "y_vals = [inst['label'] for inst in tokenized_dataset]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vals, y_vals, test_size=0.2, random_state=42)\n",
    "\n",
    "# reconstruct dictionaries using training/test sets\n",
    "train_set = []\n",
    "for tweet_token, label in zip(X_train, y_train):\n",
    "    train_inst = {\n",
    "        'tweet_token': tweet_token,  \n",
    "        'label': label\n",
    "    }\n",
    "    train_set.append(train_inst)\n",
    "\n",
    "test_set = []\n",
    "for tweet_token, label in zip(X_test, y_test):\n",
    "    test_inst = {\n",
    "        'tweet_token': tweet_token,\n",
    "        'label': label\n",
    "    }\n",
    "    test_set.append(test_inst)\n",
    "\n",
    "train_dataset = MisinformationAugmentedDataset(train_set)\n",
    "test_dataset = MisinformationDataset(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, dataloaders, loss function, collate function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to create collate function to pad variable length sequences for input\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # as per cell 6 output, item[0] will look like this:\n",
    "    # 'tweet_token': {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]),\n",
    "    #                   'input_ids': tensor([[  101, 21887, 23350,  2003, 19345, 13685,  1012,   102]]),\n",
    "    #                   'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]])}}\n",
    "    # item[1] will be a numeric label according to MisinformationDataset's label_map\n",
    "    input_ids = [item[0]['input_ids'] for item in batch]\n",
    "    attention_masks = [item[0]['attention_mask'] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    # pad sequences for input_ids and attention_masks with 0 values\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at digitalepidemiologylab/covid-twitter-bert-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "''' \n",
    "https://huggingface.co/digitalepidemiologylab/covid-twitter-bert-v2\n",
    "\n",
    "CT-BERT: This model was trained on 97M unique tweets (1.2B training examples) \n",
    "collected between January 12 and July 5, 2020 containing at least one of the keywords \n",
    "\"wuhan\", \"ncov\", \"coronavirus\", \"covid\", or \"sars-cov-2\".  \n",
    "These tweets were filtered and preprocessed to reach a final sample of 22.5M tweets \n",
    "(containing 40.7M sentences and 633M tokens) which were used for training.\n",
    "'''\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"digitalepidemiologylab/covid-twitter-bert-v2\", \n",
    "    num_labels=3\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# attempt to use class weights to offset imbalance of dataset\n",
    "# pos_count = np.count_nonzero(covid_labels == 'pos')\n",
    "# neg_count = np.count_nonzero(covid_labels == 'neg')\n",
    "# na_count = np.count_nonzero(covid_labels == 'na')\n",
    "# total_count = len(covid_labels)\n",
    "# pos_weight = total_count / pos_count\n",
    "# neg_weight = total_count / neg_count\n",
    "# na_weight = total_count / na_count\n",
    "# print(f'Weights: \\nPos: {pos_weight}\\nNeg: {neg_weight}\\nNa: {na_weight}')\n",
    "\n",
    "# class_weights = torch.tensor([1.05*pos_weight, neg_weight, na_weight]).to(device)\n",
    "# loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# freeze base model layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze last two layers of base model for fine tuning\n",
    "#for param in model.base_model.encoder.layer[-2:]:\n",
    "#    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "#dev_dataloader = DataLoader(dev_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_covid(model, optim, loss_fn, dataloader, epochs):\n",
    "def train_covid(model, optim, dataloader, epochs):\n",
    "  for epoch in range(epochs):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "\n",
    "      for batch_idx, batch in enumerate(dataloader):\n",
    "          optim.zero_grad()\n",
    "          \n",
    "          # unpack batch of form (tweets, labels)\n",
    "          tweets, labels = batch\n",
    "          # send tweets dict's values to device\n",
    "          tweets = {key: value.to(device) for key, value in tweets.items()}\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          # forward pass on CT-BERT\n",
    "          outputs = model(**tweets, labels=labels)\n",
    "          #logits = outputs.logits\n",
    "          \n",
    "          # class weighted CrossEntropyLoss\n",
    "          #loss = loss_fn(logits, labels)\n",
    "          \n",
    "          # loss provided by model\n",
    "          loss = outputs.loss \n",
    "\n",
    "          # backwards pass on CT-BERT\n",
    "          loss.backward()\n",
    "          optim.step()\n",
    "\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "      print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asubedi\\AppData\\Local\\Temp\\ipykernel_246900\\3559363801.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1/462, Loss: 1.0853698253631592\n",
      "Epoch 1, Batch 2/462, Loss: 1.0650392770767212\n",
      "Epoch 1, Batch 3/462, Loss: 1.0514171123504639\n",
      "Epoch 1, Batch 4/462, Loss: 1.0701689720153809\n",
      "Epoch 1, Batch 5/462, Loss: 1.0448793172836304\n",
      "Epoch 1, Batch 6/462, Loss: 1.0889793634414673\n",
      "Epoch 1, Batch 7/462, Loss: 1.0558212995529175\n",
      "Epoch 1, Batch 8/462, Loss: 1.031874179840088\n",
      "Epoch 1, Batch 9/462, Loss: 1.0473425388336182\n",
      "Epoch 1, Batch 10/462, Loss: 1.0855754613876343\n",
      "Epoch 1, Batch 11/462, Loss: 1.0841615200042725\n",
      "Epoch 1, Batch 12/462, Loss: 1.0572271347045898\n",
      "Epoch 1, Batch 13/462, Loss: 0.9914488792419434\n",
      "Epoch 1, Batch 14/462, Loss: 1.0941407680511475\n",
      "Epoch 1, Batch 15/462, Loss: 1.0895123481750488\n",
      "Epoch 1, Batch 16/462, Loss: 1.0619837045669556\n",
      "Epoch 1, Batch 17/462, Loss: 1.085300326347351\n",
      "Epoch 1, Batch 18/462, Loss: 0.9517677426338196\n",
      "Epoch 1, Batch 19/462, Loss: 1.0503504276275635\n",
      "Epoch 1, Batch 20/462, Loss: 1.1387302875518799\n",
      "Epoch 1, Batch 21/462, Loss: 1.0425587892532349\n",
      "Epoch 1, Batch 22/462, Loss: 1.1027393341064453\n",
      "Epoch 1, Batch 23/462, Loss: 0.9880878329277039\n",
      "Epoch 1, Batch 24/462, Loss: 1.0473366975784302\n",
      "Epoch 1, Batch 25/462, Loss: 1.048061490058899\n",
      "Epoch 1, Batch 26/462, Loss: 0.9156699776649475\n",
      "Epoch 1, Batch 27/462, Loss: 1.0185078382492065\n",
      "Epoch 1, Batch 28/462, Loss: 0.9727072715759277\n",
      "Epoch 1, Batch 29/462, Loss: 1.0964488983154297\n",
      "Epoch 1, Batch 30/462, Loss: 1.004021406173706\n",
      "Epoch 1, Batch 31/462, Loss: 1.1418638229370117\n",
      "Epoch 1, Batch 32/462, Loss: 1.1128145456314087\n",
      "Epoch 1, Batch 33/462, Loss: 1.130313754081726\n",
      "Epoch 1, Batch 34/462, Loss: 0.9851726293563843\n",
      "Epoch 1, Batch 35/462, Loss: 1.0753477811813354\n",
      "Epoch 1, Batch 36/462, Loss: 0.9737439155578613\n",
      "Epoch 1, Batch 37/462, Loss: 0.9017570614814758\n",
      "Epoch 1, Batch 38/462, Loss: 1.0240615606307983\n",
      "Epoch 1, Batch 39/462, Loss: 1.1001155376434326\n",
      "Epoch 1, Batch 40/462, Loss: 0.9399524331092834\n",
      "Epoch 1, Batch 41/462, Loss: 0.9848682284355164\n",
      "Epoch 1, Batch 42/462, Loss: 1.0360857248306274\n",
      "Epoch 1, Batch 43/462, Loss: 1.2402633428573608\n",
      "Epoch 1, Batch 44/462, Loss: 0.9623484015464783\n",
      "Epoch 1, Batch 45/462, Loss: 1.1125574111938477\n",
      "Epoch 1, Batch 46/462, Loss: 1.0526163578033447\n",
      "Epoch 1, Batch 47/462, Loss: 1.0003886222839355\n",
      "Epoch 1, Batch 48/462, Loss: 1.0031901597976685\n",
      "Epoch 1, Batch 49/462, Loss: 0.9991006255149841\n",
      "Epoch 1, Batch 50/462, Loss: 1.0157362222671509\n",
      "Epoch 1, Batch 51/462, Loss: 0.9456479549407959\n",
      "Epoch 1, Batch 52/462, Loss: 0.9965834021568298\n",
      "Epoch 1, Batch 53/462, Loss: 1.0387042760849\n",
      "Epoch 1, Batch 54/462, Loss: 1.1473982334136963\n",
      "Epoch 1, Batch 55/462, Loss: 1.012477159500122\n",
      "Epoch 1, Batch 56/462, Loss: 1.1933826208114624\n",
      "Epoch 1, Batch 57/462, Loss: 0.9974044561386108\n",
      "Epoch 1, Batch 58/462, Loss: 1.0938868522644043\n",
      "Epoch 1, Batch 59/462, Loss: 0.9533776044845581\n",
      "Epoch 1, Batch 60/462, Loss: 1.0592204332351685\n",
      "Epoch 1, Batch 61/462, Loss: 0.947081983089447\n",
      "Epoch 1, Batch 62/462, Loss: 1.0018998384475708\n",
      "Epoch 1, Batch 63/462, Loss: 1.0014832019805908\n",
      "Epoch 1, Batch 64/462, Loss: 1.0564831495285034\n",
      "Epoch 1, Batch 65/462, Loss: 0.9990397095680237\n",
      "Epoch 1, Batch 66/462, Loss: 1.0341130495071411\n",
      "Epoch 1, Batch 67/462, Loss: 1.0637390613555908\n",
      "Epoch 1, Batch 68/462, Loss: 1.0106583833694458\n",
      "Epoch 1, Batch 69/462, Loss: 1.0178492069244385\n",
      "Epoch 1, Batch 70/462, Loss: 0.9640693664550781\n",
      "Epoch 1, Batch 71/462, Loss: 0.8833791613578796\n",
      "Epoch 1, Batch 72/462, Loss: 0.9996140599250793\n",
      "Epoch 1, Batch 73/462, Loss: 1.0608930587768555\n",
      "Epoch 1, Batch 74/462, Loss: 1.0300666093826294\n",
      "Epoch 1, Batch 75/462, Loss: 0.9617619514465332\n",
      "Epoch 1, Batch 76/462, Loss: 1.054004192352295\n",
      "Epoch 1, Batch 77/462, Loss: 1.0355716943740845\n",
      "Epoch 1, Batch 78/462, Loss: 1.0727351903915405\n",
      "Epoch 1, Batch 79/462, Loss: 1.167589783668518\n",
      "Epoch 1, Batch 80/462, Loss: 0.9795680642127991\n",
      "Epoch 1, Batch 81/462, Loss: 1.0903654098510742\n",
      "Epoch 1, Batch 82/462, Loss: 0.9304311275482178\n",
      "Epoch 1, Batch 83/462, Loss: 1.0937877893447876\n",
      "Epoch 1, Batch 84/462, Loss: 0.9340360164642334\n",
      "Epoch 1, Batch 85/462, Loss: 0.9494937658309937\n",
      "Epoch 1, Batch 86/462, Loss: 1.0035027265548706\n",
      "Epoch 1, Batch 87/462, Loss: 1.179320216178894\n",
      "Epoch 1, Batch 88/462, Loss: 1.002324104309082\n",
      "Epoch 1, Batch 89/462, Loss: 0.9457272887229919\n",
      "Epoch 1, Batch 90/462, Loss: 0.90301513671875\n",
      "Epoch 1, Batch 91/462, Loss: 0.9541244506835938\n",
      "Epoch 1, Batch 92/462, Loss: 0.9765726327896118\n",
      "Epoch 1, Batch 93/462, Loss: 1.0037178993225098\n",
      "Epoch 1, Batch 94/462, Loss: 1.053450345993042\n",
      "Epoch 1, Batch 95/462, Loss: 1.1259899139404297\n",
      "Epoch 1, Batch 96/462, Loss: 0.9564168453216553\n",
      "Epoch 1, Batch 97/462, Loss: 1.0513371229171753\n",
      "Epoch 1, Batch 98/462, Loss: 1.0310521125793457\n",
      "Epoch 1, Batch 99/462, Loss: 1.0164847373962402\n",
      "Epoch 1, Batch 100/462, Loss: 1.0858098268508911\n",
      "Epoch 1, Batch 101/462, Loss: 1.0376229286193848\n",
      "Epoch 1, Batch 102/462, Loss: 1.1020452976226807\n",
      "Epoch 1, Batch 103/462, Loss: 0.9773234128952026\n",
      "Epoch 1, Batch 104/462, Loss: 0.9475187063217163\n",
      "Epoch 1, Batch 105/462, Loss: 1.049004077911377\n",
      "Epoch 1, Batch 106/462, Loss: 0.9531291723251343\n",
      "Epoch 1, Batch 107/462, Loss: 1.049966812133789\n",
      "Epoch 1, Batch 108/462, Loss: 0.9975875616073608\n",
      "Epoch 1, Batch 109/462, Loss: 0.9903559684753418\n",
      "Epoch 1, Batch 110/462, Loss: 1.034550666809082\n",
      "Epoch 1, Batch 111/462, Loss: 0.973090410232544\n",
      "Epoch 1, Batch 112/462, Loss: 0.9734941124916077\n",
      "Epoch 1, Batch 113/462, Loss: 1.0991510152816772\n",
      "Epoch 1, Batch 114/462, Loss: 0.9721076488494873\n",
      "Epoch 1, Batch 115/462, Loss: 0.9256304502487183\n",
      "Epoch 1, Batch 116/462, Loss: 0.8812769651412964\n",
      "Epoch 1, Batch 117/462, Loss: 1.0895605087280273\n",
      "Epoch 1, Batch 118/462, Loss: 0.9704341292381287\n",
      "Epoch 1, Batch 119/462, Loss: 0.8956300020217896\n",
      "Epoch 1, Batch 120/462, Loss: 1.1457386016845703\n",
      "Epoch 1, Batch 121/462, Loss: 1.0406930446624756\n",
      "Epoch 1, Batch 122/462, Loss: 0.9949771165847778\n",
      "Epoch 1, Batch 123/462, Loss: 0.9202340841293335\n",
      "Epoch 1, Batch 124/462, Loss: 1.0620375871658325\n",
      "Epoch 1, Batch 125/462, Loss: 1.013742446899414\n",
      "Epoch 1, Batch 126/462, Loss: 1.010450839996338\n",
      "Epoch 1, Batch 127/462, Loss: 0.9714699983596802\n",
      "Epoch 1, Batch 128/462, Loss: 0.910656213760376\n",
      "Epoch 1, Batch 129/462, Loss: 1.0608909130096436\n",
      "Epoch 1, Batch 130/462, Loss: 0.8966638445854187\n",
      "Epoch 1, Batch 131/462, Loss: 0.9514076709747314\n",
      "Epoch 1, Batch 132/462, Loss: 1.0490833520889282\n",
      "Epoch 1, Batch 133/462, Loss: 0.9295020699501038\n",
      "Epoch 1, Batch 134/462, Loss: 1.017781138420105\n",
      "Epoch 1, Batch 135/462, Loss: 0.9437020421028137\n",
      "Epoch 1, Batch 136/462, Loss: 0.9559709429740906\n",
      "Epoch 1, Batch 137/462, Loss: 1.0450026988983154\n",
      "Epoch 1, Batch 138/462, Loss: 1.1082631349563599\n",
      "Epoch 1, Batch 139/462, Loss: 1.0084627866744995\n",
      "Epoch 1, Batch 140/462, Loss: 0.970185399055481\n",
      "Epoch 1, Batch 141/462, Loss: 0.9504792094230652\n",
      "Epoch 1, Batch 142/462, Loss: 0.9359932541847229\n",
      "Epoch 1, Batch 143/462, Loss: 0.8800684809684753\n",
      "Epoch 1, Batch 144/462, Loss: 1.0584120750427246\n",
      "Epoch 1, Batch 145/462, Loss: 0.9861823320388794\n",
      "Epoch 1, Batch 146/462, Loss: 0.8084184527397156\n",
      "Epoch 1, Batch 147/462, Loss: 0.991081953048706\n",
      "Epoch 1, Batch 148/462, Loss: 0.9379817247390747\n",
      "Epoch 1, Batch 149/462, Loss: 1.0260443687438965\n",
      "Epoch 1, Batch 150/462, Loss: 1.002529501914978\n",
      "Epoch 1, Batch 151/462, Loss: 0.9274471402168274\n",
      "Epoch 1, Batch 152/462, Loss: 0.9144951105117798\n",
      "Epoch 1, Batch 153/462, Loss: 1.0034269094467163\n",
      "Epoch 1, Batch 154/462, Loss: 1.1176544427871704\n",
      "Epoch 1, Batch 155/462, Loss: 1.0506622791290283\n",
      "Epoch 1, Batch 156/462, Loss: 0.816658079624176\n",
      "Epoch 1, Batch 157/462, Loss: 0.8447762727737427\n",
      "Epoch 1, Batch 158/462, Loss: 1.046555519104004\n",
      "Epoch 1, Batch 159/462, Loss: 1.1143780946731567\n",
      "Epoch 1, Batch 160/462, Loss: 0.9755295515060425\n",
      "Epoch 1, Batch 161/462, Loss: 0.9883849620819092\n",
      "Epoch 1, Batch 162/462, Loss: 0.9585326910018921\n",
      "Epoch 1, Batch 163/462, Loss: 1.0753698348999023\n",
      "Epoch 1, Batch 164/462, Loss: 1.0884759426116943\n",
      "Epoch 1, Batch 165/462, Loss: 0.97412109375\n",
      "Epoch 1, Batch 166/462, Loss: 0.9210838079452515\n",
      "Epoch 1, Batch 167/462, Loss: 0.9614107608795166\n",
      "Epoch 1, Batch 168/462, Loss: 0.9182770848274231\n",
      "Epoch 1, Batch 169/462, Loss: 0.9747439026832581\n",
      "Epoch 1, Batch 170/462, Loss: 0.9271370768547058\n",
      "Epoch 1, Batch 171/462, Loss: 1.0468019247055054\n",
      "Epoch 1, Batch 172/462, Loss: 0.9946705102920532\n",
      "Epoch 1, Batch 173/462, Loss: 0.9616899490356445\n",
      "Epoch 1, Batch 174/462, Loss: 1.0869803428649902\n",
      "Epoch 1, Batch 175/462, Loss: 1.0445406436920166\n",
      "Epoch 1, Batch 176/462, Loss: 0.8997747302055359\n",
      "Epoch 1, Batch 177/462, Loss: 0.9591324329376221\n",
      "Epoch 1, Batch 178/462, Loss: 0.9414643049240112\n",
      "Epoch 1, Batch 179/462, Loss: 0.9607224464416504\n",
      "Epoch 1, Batch 180/462, Loss: 1.007295846939087\n",
      "Epoch 1, Batch 181/462, Loss: 1.0007086992263794\n",
      "Epoch 1, Batch 182/462, Loss: 1.068960189819336\n",
      "Epoch 1, Batch 183/462, Loss: 0.8914046287536621\n",
      "Epoch 1, Batch 184/462, Loss: 0.965480625629425\n",
      "Epoch 1, Batch 185/462, Loss: 1.0898648500442505\n",
      "Epoch 1, Batch 186/462, Loss: 0.9441074132919312\n",
      "Epoch 1, Batch 187/462, Loss: 0.8924605846405029\n",
      "Epoch 1, Batch 188/462, Loss: 0.9847849011421204\n",
      "Epoch 1, Batch 189/462, Loss: 1.0283674001693726\n",
      "Epoch 1, Batch 190/462, Loss: 0.9281442761421204\n",
      "Epoch 1, Batch 191/462, Loss: 0.8815381526947021\n",
      "Epoch 1, Batch 192/462, Loss: 0.8993455767631531\n",
      "Epoch 1, Batch 193/462, Loss: 1.007747769355774\n",
      "Epoch 1, Batch 194/462, Loss: 1.000841736793518\n",
      "Epoch 1, Batch 195/462, Loss: 0.9293928742408752\n",
      "Epoch 1, Batch 196/462, Loss: 0.9414563775062561\n",
      "Epoch 1, Batch 197/462, Loss: 0.9786469340324402\n",
      "Epoch 1, Batch 198/462, Loss: 0.9760411381721497\n",
      "Epoch 1, Batch 199/462, Loss: 0.9645296335220337\n",
      "Epoch 1, Batch 200/462, Loss: 0.9125986695289612\n",
      "Epoch 1, Batch 201/462, Loss: 0.9822027683258057\n",
      "Epoch 1, Batch 202/462, Loss: 0.985919713973999\n",
      "Epoch 1, Batch 203/462, Loss: 0.9741370677947998\n",
      "Epoch 1, Batch 204/462, Loss: 0.9892432689666748\n",
      "Epoch 1, Batch 205/462, Loss: 0.9676967263221741\n",
      "Epoch 1, Batch 206/462, Loss: 0.8526007533073425\n",
      "Epoch 1, Batch 207/462, Loss: 1.0832585096359253\n",
      "Epoch 1, Batch 208/462, Loss: 0.9590814709663391\n",
      "Epoch 1, Batch 209/462, Loss: 1.0274275541305542\n",
      "Epoch 1, Batch 210/462, Loss: 0.992865800857544\n",
      "Epoch 1, Batch 211/462, Loss: 1.0473027229309082\n",
      "Epoch 1, Batch 212/462, Loss: 1.017737627029419\n",
      "Epoch 1, Batch 213/462, Loss: 0.9481000900268555\n",
      "Epoch 1, Batch 214/462, Loss: 0.957632839679718\n",
      "Epoch 1, Batch 215/462, Loss: 0.971271812915802\n",
      "Epoch 1, Batch 216/462, Loss: 1.1165087223052979\n",
      "Epoch 1, Batch 217/462, Loss: 0.8846219778060913\n",
      "Epoch 1, Batch 218/462, Loss: 1.0257911682128906\n",
      "Epoch 1, Batch 219/462, Loss: 0.9714430570602417\n",
      "Epoch 1, Batch 220/462, Loss: 0.9084455966949463\n",
      "Epoch 1, Batch 221/462, Loss: 0.9318264126777649\n",
      "Epoch 1, Batch 222/462, Loss: 0.9346227645874023\n",
      "Epoch 1, Batch 223/462, Loss: 0.8892173171043396\n",
      "Epoch 1, Batch 224/462, Loss: 0.9358005523681641\n",
      "Epoch 1, Batch 225/462, Loss: 1.0012917518615723\n",
      "Epoch 1, Batch 226/462, Loss: 0.9463005661964417\n",
      "Epoch 1, Batch 227/462, Loss: 1.1084243059158325\n",
      "Epoch 1, Batch 228/462, Loss: 1.085160493850708\n",
      "Epoch 1, Batch 229/462, Loss: 0.9579679369926453\n",
      "Epoch 1, Batch 230/462, Loss: 0.9515724182128906\n",
      "Epoch 1, Batch 231/462, Loss: 1.0531667470932007\n",
      "Epoch 1, Batch 232/462, Loss: 0.9094844460487366\n",
      "Epoch 1, Batch 233/462, Loss: 0.8899295926094055\n",
      "Epoch 1, Batch 234/462, Loss: 1.0368409156799316\n",
      "Epoch 1, Batch 235/462, Loss: 1.0716909170150757\n",
      "Epoch 1, Batch 236/462, Loss: 0.9479633569717407\n",
      "Epoch 1, Batch 237/462, Loss: 0.9284048676490784\n",
      "Epoch 1, Batch 238/462, Loss: 0.855635941028595\n",
      "Epoch 1, Batch 239/462, Loss: 1.0146061182022095\n",
      "Epoch 1, Batch 240/462, Loss: 1.0646082162857056\n",
      "Epoch 1, Batch 241/462, Loss: 0.965523898601532\n",
      "Epoch 1, Batch 242/462, Loss: 1.0405101776123047\n",
      "Epoch 1, Batch 243/462, Loss: 0.9895885586738586\n",
      "Epoch 1, Batch 244/462, Loss: 0.929576575756073\n",
      "Epoch 1, Batch 245/462, Loss: 0.9905229806900024\n",
      "Epoch 1, Batch 246/462, Loss: 0.9687672257423401\n",
      "Epoch 1, Batch 247/462, Loss: 1.059525489807129\n",
      "Epoch 1, Batch 248/462, Loss: 0.8642610311508179\n",
      "Epoch 1, Batch 249/462, Loss: 0.8449622392654419\n",
      "Epoch 1, Batch 250/462, Loss: 1.0775387287139893\n",
      "Epoch 1, Batch 251/462, Loss: 0.9252575635910034\n",
      "Epoch 1, Batch 252/462, Loss: 1.0761114358901978\n",
      "Epoch 1, Batch 253/462, Loss: 0.8958847522735596\n",
      "Epoch 1, Batch 254/462, Loss: 0.9770852327346802\n",
      "Epoch 1, Batch 255/462, Loss: 1.0395104885101318\n",
      "Epoch 1, Batch 256/462, Loss: 0.9831652045249939\n",
      "Epoch 1, Batch 257/462, Loss: 1.1159265041351318\n",
      "Epoch 1, Batch 258/462, Loss: 0.9884850978851318\n",
      "Epoch 1, Batch 259/462, Loss: 0.8770166039466858\n",
      "Epoch 1, Batch 260/462, Loss: 0.9120199084281921\n",
      "Epoch 1, Batch 261/462, Loss: 1.0059854984283447\n",
      "Epoch 1, Batch 262/462, Loss: 0.9928522706031799\n",
      "Epoch 1, Batch 263/462, Loss: 0.8632441759109497\n",
      "Epoch 1, Batch 264/462, Loss: 0.8580483794212341\n",
      "Epoch 1, Batch 265/462, Loss: 1.0167460441589355\n",
      "Epoch 1, Batch 266/462, Loss: 1.023638367652893\n",
      "Epoch 1, Batch 267/462, Loss: 1.0237362384796143\n",
      "Epoch 1, Batch 268/462, Loss: 0.9659061431884766\n",
      "Epoch 1, Batch 269/462, Loss: 1.136184573173523\n",
      "Epoch 1, Batch 270/462, Loss: 0.9225676655769348\n",
      "Epoch 1, Batch 271/462, Loss: 0.8865938186645508\n",
      "Epoch 1, Batch 272/462, Loss: 0.9487762451171875\n",
      "Epoch 1, Batch 273/462, Loss: 1.1548259258270264\n",
      "Epoch 1, Batch 274/462, Loss: 0.9433290958404541\n",
      "Epoch 1, Batch 275/462, Loss: 0.942755401134491\n",
      "Epoch 1, Batch 276/462, Loss: 1.032850742340088\n",
      "Epoch 1, Batch 277/462, Loss: 1.0319314002990723\n",
      "Epoch 1, Batch 278/462, Loss: 1.0041639804840088\n",
      "Epoch 1, Batch 279/462, Loss: 0.876759946346283\n",
      "Epoch 1, Batch 280/462, Loss: 0.9018874764442444\n",
      "Epoch 1, Batch 281/462, Loss: 0.875669002532959\n",
      "Epoch 1, Batch 282/462, Loss: 1.0060032606124878\n",
      "Epoch 1, Batch 283/462, Loss: 0.9329820275306702\n",
      "Epoch 1, Batch 284/462, Loss: 0.98898845911026\n",
      "Epoch 1, Batch 285/462, Loss: 0.9150503873825073\n",
      "Epoch 1, Batch 286/462, Loss: 1.0009279251098633\n",
      "Epoch 1, Batch 287/462, Loss: 0.9588173627853394\n",
      "Epoch 1, Batch 288/462, Loss: 0.9835848212242126\n",
      "Epoch 1, Batch 289/462, Loss: 0.9719025492668152\n",
      "Epoch 1, Batch 290/462, Loss: 0.9776691198348999\n",
      "Epoch 1, Batch 291/462, Loss: 0.9196252226829529\n",
      "Epoch 1, Batch 292/462, Loss: 0.9447081685066223\n",
      "Epoch 1, Batch 293/462, Loss: 0.8572644591331482\n",
      "Epoch 1, Batch 294/462, Loss: 0.9629088044166565\n",
      "Epoch 1, Batch 295/462, Loss: 0.8935271501541138\n",
      "Epoch 1, Batch 296/462, Loss: 0.9098321795463562\n",
      "Epoch 1, Batch 297/462, Loss: 0.8705812096595764\n",
      "Epoch 1, Batch 298/462, Loss: 0.9954829812049866\n",
      "Epoch 1, Batch 299/462, Loss: 0.9359841346740723\n",
      "Epoch 1, Batch 300/462, Loss: 1.0038483142852783\n",
      "Epoch 1, Batch 301/462, Loss: 0.9712554216384888\n",
      "Epoch 1, Batch 302/462, Loss: 1.0395889282226562\n",
      "Epoch 1, Batch 303/462, Loss: 1.0222771167755127\n",
      "Epoch 1, Batch 304/462, Loss: 1.0073282718658447\n",
      "Epoch 1, Batch 305/462, Loss: 0.960985004901886\n",
      "Epoch 1, Batch 306/462, Loss: 0.9144468307495117\n",
      "Epoch 1, Batch 307/462, Loss: 0.9203248023986816\n",
      "Epoch 1, Batch 308/462, Loss: 0.933715283870697\n",
      "Epoch 1, Batch 309/462, Loss: 0.8044953942298889\n",
      "Epoch 1, Batch 310/462, Loss: 1.0102025270462036\n",
      "Epoch 1, Batch 311/462, Loss: 0.8450711369514465\n",
      "Epoch 1, Batch 312/462, Loss: 0.9688159227371216\n",
      "Epoch 1, Batch 313/462, Loss: 0.9325012564659119\n",
      "Epoch 1, Batch 314/462, Loss: 1.0978071689605713\n",
      "Epoch 1, Batch 315/462, Loss: 0.9331673383712769\n",
      "Epoch 1, Batch 316/462, Loss: 0.9199158549308777\n",
      "Epoch 1, Batch 317/462, Loss: 0.9450986385345459\n",
      "Epoch 1, Batch 318/462, Loss: 1.017227053642273\n",
      "Epoch 1, Batch 319/462, Loss: 0.8832723498344421\n",
      "Epoch 1, Batch 320/462, Loss: 1.0465267896652222\n",
      "Epoch 1, Batch 321/462, Loss: 1.0711112022399902\n",
      "Epoch 1, Batch 322/462, Loss: 0.9311037659645081\n",
      "Epoch 1, Batch 323/462, Loss: 0.8776426911354065\n",
      "Epoch 1, Batch 324/462, Loss: 0.8861627578735352\n",
      "Epoch 1, Batch 325/462, Loss: 0.8525792360305786\n",
      "Epoch 1, Batch 326/462, Loss: 1.0141551494598389\n",
      "Epoch 1, Batch 327/462, Loss: 0.8290210366249084\n",
      "Epoch 1, Batch 328/462, Loss: 0.8418147563934326\n",
      "Epoch 1, Batch 329/462, Loss: 0.9587794542312622\n",
      "Epoch 1, Batch 330/462, Loss: 0.9169396162033081\n",
      "Epoch 1, Batch 331/462, Loss: 1.0146369934082031\n",
      "Epoch 1, Batch 332/462, Loss: 0.9289038777351379\n",
      "Epoch 1, Batch 333/462, Loss: 0.9035427570343018\n",
      "Epoch 1, Batch 334/462, Loss: 0.9978075623512268\n",
      "Epoch 1, Batch 335/462, Loss: 0.9314729571342468\n",
      "Epoch 1, Batch 336/462, Loss: 0.9688412547111511\n",
      "Epoch 1, Batch 337/462, Loss: 1.051514983177185\n",
      "Epoch 1, Batch 338/462, Loss: 0.9688858985900879\n",
      "Epoch 1, Batch 339/462, Loss: 1.0130689144134521\n",
      "Epoch 1, Batch 340/462, Loss: 1.0555297136306763\n",
      "Epoch 1, Batch 341/462, Loss: 0.9653759598731995\n",
      "Epoch 1, Batch 342/462, Loss: 1.0780434608459473\n",
      "Epoch 1, Batch 343/462, Loss: 0.8936548233032227\n",
      "Epoch 1, Batch 344/462, Loss: 1.0496506690979004\n",
      "Epoch 1, Batch 345/462, Loss: 0.9125983715057373\n",
      "Epoch 1, Batch 346/462, Loss: 0.852552592754364\n",
      "Epoch 1, Batch 347/462, Loss: 0.9466125965118408\n",
      "Epoch 1, Batch 348/462, Loss: 0.8609762191772461\n",
      "Epoch 1, Batch 349/462, Loss: 0.9798418879508972\n",
      "Epoch 1, Batch 350/462, Loss: 0.9435676336288452\n",
      "Epoch 1, Batch 351/462, Loss: 0.8103563785552979\n",
      "Epoch 1, Batch 352/462, Loss: 0.9239434003829956\n",
      "Epoch 1, Batch 353/462, Loss: 0.8140633702278137\n",
      "Epoch 1, Batch 354/462, Loss: 0.9124865531921387\n",
      "Epoch 1, Batch 355/462, Loss: 0.9111759662628174\n",
      "Epoch 1, Batch 356/462, Loss: 0.9019349217414856\n",
      "Epoch 1, Batch 357/462, Loss: 0.8530124425888062\n",
      "Epoch 1, Batch 358/462, Loss: 0.9507115483283997\n",
      "Epoch 1, Batch 359/462, Loss: 0.9583839774131775\n",
      "Epoch 1, Batch 360/462, Loss: 0.950320839881897\n",
      "Epoch 1, Batch 361/462, Loss: 0.8662164807319641\n",
      "Epoch 1, Batch 362/462, Loss: 1.0146231651306152\n",
      "Epoch 1, Batch 363/462, Loss: 0.91294926404953\n",
      "Epoch 1, Batch 364/462, Loss: 1.0172251462936401\n",
      "Epoch 1, Batch 365/462, Loss: 0.9337806105613708\n",
      "Epoch 1, Batch 366/462, Loss: 0.8928440809249878\n",
      "Epoch 1, Batch 367/462, Loss: 0.8705940246582031\n",
      "Epoch 1, Batch 368/462, Loss: 0.9250429272651672\n",
      "Epoch 1, Batch 369/462, Loss: 0.9519464373588562\n",
      "Epoch 1, Batch 370/462, Loss: 0.9234457612037659\n",
      "Epoch 1, Batch 371/462, Loss: 1.0815147161483765\n",
      "Epoch 1, Batch 372/462, Loss: 1.1180956363677979\n",
      "Epoch 1, Batch 373/462, Loss: 0.9051807522773743\n",
      "Epoch 1, Batch 374/462, Loss: 1.0775617361068726\n",
      "Epoch 1, Batch 375/462, Loss: 1.1123992204666138\n",
      "Epoch 1, Batch 376/462, Loss: 0.9292402267456055\n",
      "Epoch 1, Batch 377/462, Loss: 0.9647042155265808\n",
      "Epoch 1, Batch 378/462, Loss: 0.9007917642593384\n",
      "Epoch 1, Batch 379/462, Loss: 0.8171815872192383\n",
      "Epoch 1, Batch 380/462, Loss: 0.8384473919868469\n",
      "Epoch 1, Batch 381/462, Loss: 1.107426643371582\n",
      "Epoch 1, Batch 382/462, Loss: 1.0434637069702148\n",
      "Epoch 1, Batch 383/462, Loss: 0.964784562587738\n",
      "Epoch 1, Batch 384/462, Loss: 0.9942184090614319\n",
      "Epoch 1, Batch 385/462, Loss: 0.8436574339866638\n",
      "Epoch 1, Batch 386/462, Loss: 0.9202158451080322\n",
      "Epoch 1, Batch 387/462, Loss: 0.9938408136367798\n",
      "Epoch 1, Batch 388/462, Loss: 1.0490293502807617\n",
      "Epoch 1, Batch 389/462, Loss: 0.9888116121292114\n",
      "Epoch 1, Batch 390/462, Loss: 0.8061186075210571\n",
      "Epoch 1, Batch 391/462, Loss: 0.9155698418617249\n",
      "Epoch 1, Batch 392/462, Loss: 0.998651921749115\n",
      "Epoch 1, Batch 393/462, Loss: 0.9101375341415405\n",
      "Epoch 1, Batch 394/462, Loss: 0.9388350248336792\n",
      "Epoch 1, Batch 395/462, Loss: 0.9926376938819885\n",
      "Epoch 1, Batch 396/462, Loss: 0.9828242659568787\n",
      "Epoch 1, Batch 397/462, Loss: 0.9458106756210327\n",
      "Epoch 1, Batch 398/462, Loss: 0.9009923934936523\n",
      "Epoch 1, Batch 399/462, Loss: 1.0538525581359863\n",
      "Epoch 1, Batch 400/462, Loss: 0.9501509666442871\n",
      "Epoch 1, Batch 401/462, Loss: 0.9465759992599487\n",
      "Epoch 1, Batch 402/462, Loss: 0.9205893278121948\n",
      "Epoch 1, Batch 403/462, Loss: 0.8229839205741882\n",
      "Epoch 1, Batch 404/462, Loss: 0.9159004092216492\n",
      "Epoch 1, Batch 405/462, Loss: 0.8422143459320068\n",
      "Epoch 1, Batch 406/462, Loss: 0.9115159511566162\n",
      "Epoch 1, Batch 407/462, Loss: 1.1467931270599365\n",
      "Epoch 1, Batch 408/462, Loss: 0.9127318859100342\n",
      "Epoch 1, Batch 409/462, Loss: 1.0437787771224976\n",
      "Epoch 1, Batch 410/462, Loss: 0.8716855645179749\n",
      "Epoch 1, Batch 411/462, Loss: 1.041818618774414\n",
      "Epoch 1, Batch 412/462, Loss: 0.8368796706199646\n",
      "Epoch 1, Batch 413/462, Loss: 1.006884217262268\n",
      "Epoch 1, Batch 414/462, Loss: 0.8463286757469177\n",
      "Epoch 1, Batch 415/462, Loss: 0.8306838274002075\n",
      "Epoch 1, Batch 416/462, Loss: 0.8647207617759705\n",
      "Epoch 1, Batch 417/462, Loss: 1.119506597518921\n",
      "Epoch 1, Batch 418/462, Loss: 0.9315959215164185\n",
      "Epoch 1, Batch 419/462, Loss: 0.8942661881446838\n",
      "Epoch 1, Batch 420/462, Loss: 1.0295885801315308\n",
      "Epoch 1, Batch 421/462, Loss: 0.8859490156173706\n",
      "Epoch 1, Batch 422/462, Loss: 0.8985978960990906\n",
      "Epoch 1, Batch 423/462, Loss: 0.9771004319190979\n",
      "Epoch 1, Batch 424/462, Loss: 0.9795478582382202\n",
      "Epoch 1, Batch 425/462, Loss: 1.126002311706543\n",
      "Epoch 1, Batch 426/462, Loss: 0.987313985824585\n",
      "Epoch 1, Batch 427/462, Loss: 0.868421196937561\n",
      "Epoch 1, Batch 428/462, Loss: 0.9204726219177246\n",
      "Epoch 1, Batch 429/462, Loss: 0.8960257768630981\n",
      "Epoch 1, Batch 430/462, Loss: 1.0504906177520752\n",
      "Epoch 1, Batch 431/462, Loss: 0.9674829840660095\n",
      "Epoch 1, Batch 432/462, Loss: 0.8874962329864502\n",
      "Epoch 1, Batch 433/462, Loss: 0.9531109929084778\n",
      "Epoch 1, Batch 434/462, Loss: 0.9027001857757568\n",
      "Epoch 1, Batch 435/462, Loss: 0.9909619688987732\n",
      "Epoch 1, Batch 436/462, Loss: 0.8517095446586609\n",
      "Epoch 1, Batch 437/462, Loss: 0.978958249092102\n",
      "Epoch 1, Batch 438/462, Loss: 0.9829535484313965\n",
      "Epoch 1, Batch 439/462, Loss: 0.9535298347473145\n",
      "Epoch 1, Batch 440/462, Loss: 0.8487798571586609\n",
      "Epoch 1, Batch 441/462, Loss: 0.9567034840583801\n",
      "Epoch 1, Batch 442/462, Loss: 0.9939292669296265\n",
      "Epoch 1, Batch 443/462, Loss: 0.9302688837051392\n",
      "Epoch 1, Batch 444/462, Loss: 0.8874684572219849\n",
      "Epoch 1, Batch 445/462, Loss: 0.9151250123977661\n",
      "Epoch 1, Batch 446/462, Loss: 0.9663788676261902\n",
      "Epoch 1, Batch 447/462, Loss: 0.8181466460227966\n",
      "Epoch 1, Batch 448/462, Loss: 0.963767945766449\n",
      "Epoch 1, Batch 449/462, Loss: 0.9201834797859192\n",
      "Epoch 1, Batch 450/462, Loss: 1.060063362121582\n",
      "Epoch 1, Batch 451/462, Loss: 0.992721438407898\n",
      "Epoch 1, Batch 452/462, Loss: 0.972423255443573\n",
      "Epoch 1, Batch 453/462, Loss: 0.8525115847587585\n",
      "Epoch 1, Batch 454/462, Loss: 0.9045436382293701\n",
      "Epoch 1, Batch 455/462, Loss: 1.0064311027526855\n",
      "Epoch 1, Batch 456/462, Loss: 1.12432062625885\n",
      "Epoch 1, Batch 457/462, Loss: 0.9877674579620361\n",
      "Epoch 1, Batch 458/462, Loss: 0.7934263348579407\n",
      "Epoch 1, Batch 459/462, Loss: 0.9907612800598145\n",
      "Epoch 1, Batch 460/462, Loss: 0.8904588222503662\n",
      "Epoch 1, Batch 461/462, Loss: 0.9019448757171631\n",
      "Epoch 1, Batch 462/462, Loss: 0.7866367697715759\n",
      "Epoch 1, Loss: 452.0278064608574\n",
      "Epoch 2, Batch 1/462, Loss: 0.9973006248474121\n",
      "Epoch 2, Batch 2/462, Loss: 0.9145424365997314\n",
      "Epoch 2, Batch 3/462, Loss: 0.9129717946052551\n",
      "Epoch 2, Batch 4/462, Loss: 0.8887380957603455\n",
      "Epoch 2, Batch 5/462, Loss: 0.9418281316757202\n",
      "Epoch 2, Batch 6/462, Loss: 0.8300278186798096\n",
      "Epoch 2, Batch 7/462, Loss: 0.9445419907569885\n",
      "Epoch 2, Batch 8/462, Loss: 0.9562535881996155\n",
      "Epoch 2, Batch 9/462, Loss: 1.0053062438964844\n",
      "Epoch 2, Batch 10/462, Loss: 0.8803760409355164\n",
      "Epoch 2, Batch 11/462, Loss: 0.8752670288085938\n",
      "Epoch 2, Batch 12/462, Loss: 0.9583142399787903\n",
      "Epoch 2, Batch 13/462, Loss: 0.900675356388092\n",
      "Epoch 2, Batch 14/462, Loss: 0.9970256686210632\n",
      "Epoch 2, Batch 15/462, Loss: 0.8543393015861511\n",
      "Epoch 2, Batch 16/462, Loss: 0.8698735237121582\n",
      "Epoch 2, Batch 17/462, Loss: 0.841248095035553\n",
      "Epoch 2, Batch 18/462, Loss: 1.0338689088821411\n",
      "Epoch 2, Batch 19/462, Loss: 0.9299765229225159\n",
      "Epoch 2, Batch 20/462, Loss: 1.083004355430603\n",
      "Epoch 2, Batch 21/462, Loss: 0.8738849759101868\n",
      "Epoch 2, Batch 22/462, Loss: 0.886684238910675\n",
      "Epoch 2, Batch 23/462, Loss: 0.836767315864563\n",
      "Epoch 2, Batch 24/462, Loss: 0.8627771735191345\n",
      "Epoch 2, Batch 25/462, Loss: 0.8953694701194763\n",
      "Epoch 2, Batch 26/462, Loss: 0.9216126203536987\n",
      "Epoch 2, Batch 27/462, Loss: 0.9137285947799683\n",
      "Epoch 2, Batch 28/462, Loss: 0.9504037499427795\n",
      "Epoch 2, Batch 29/462, Loss: 0.9868651032447815\n",
      "Epoch 2, Batch 30/462, Loss: 1.0059804916381836\n",
      "Epoch 2, Batch 31/462, Loss: 0.9232778549194336\n",
      "Epoch 2, Batch 32/462, Loss: 0.8225475549697876\n",
      "Epoch 2, Batch 33/462, Loss: 1.0366252660751343\n",
      "Epoch 2, Batch 34/462, Loss: 0.8510338664054871\n",
      "Epoch 2, Batch 35/462, Loss: 0.9270006418228149\n",
      "Epoch 2, Batch 36/462, Loss: 1.058194875717163\n",
      "Epoch 2, Batch 37/462, Loss: 1.0388697385787964\n",
      "Epoch 2, Batch 38/462, Loss: 1.0237137079238892\n",
      "Epoch 2, Batch 39/462, Loss: 0.9556764364242554\n",
      "Epoch 2, Batch 40/462, Loss: 1.0207364559173584\n",
      "Epoch 2, Batch 41/462, Loss: 1.0141222476959229\n",
      "Epoch 2, Batch 42/462, Loss: 1.0200355052947998\n",
      "Epoch 2, Batch 43/462, Loss: 0.9958204030990601\n",
      "Epoch 2, Batch 44/462, Loss: 1.0146145820617676\n",
      "Epoch 2, Batch 45/462, Loss: 0.9448055028915405\n",
      "Epoch 2, Batch 46/462, Loss: 0.9393813610076904\n",
      "Epoch 2, Batch 47/462, Loss: 1.0084218978881836\n",
      "Epoch 2, Batch 48/462, Loss: 0.9604155421257019\n",
      "Epoch 2, Batch 49/462, Loss: 0.978198230266571\n",
      "Epoch 2, Batch 50/462, Loss: 0.8448215126991272\n",
      "Epoch 2, Batch 51/462, Loss: 0.9229776859283447\n",
      "Epoch 2, Batch 52/462, Loss: 0.9389336109161377\n",
      "Epoch 2, Batch 53/462, Loss: 1.0393275022506714\n",
      "Epoch 2, Batch 54/462, Loss: 0.9284521341323853\n",
      "Epoch 2, Batch 55/462, Loss: 1.0335192680358887\n",
      "Epoch 2, Batch 56/462, Loss: 0.8625615835189819\n",
      "Epoch 2, Batch 57/462, Loss: 0.9038508534431458\n",
      "Epoch 2, Batch 58/462, Loss: 1.037360668182373\n",
      "Epoch 2, Batch 59/462, Loss: 0.90293288230896\n",
      "Epoch 2, Batch 60/462, Loss: 0.9473736882209778\n",
      "Epoch 2, Batch 61/462, Loss: 0.9216373562812805\n",
      "Epoch 2, Batch 62/462, Loss: 0.9381349086761475\n",
      "Epoch 2, Batch 63/462, Loss: 1.024683952331543\n",
      "Epoch 2, Batch 64/462, Loss: 1.0081597566604614\n",
      "Epoch 2, Batch 65/462, Loss: 0.8988243937492371\n",
      "Epoch 2, Batch 66/462, Loss: 0.929970383644104\n",
      "Epoch 2, Batch 67/462, Loss: 0.9373265504837036\n",
      "Epoch 2, Batch 68/462, Loss: 0.932576060295105\n",
      "Epoch 2, Batch 69/462, Loss: 0.9833985567092896\n",
      "Epoch 2, Batch 70/462, Loss: 0.994094729423523\n",
      "Epoch 2, Batch 71/462, Loss: 0.9787178635597229\n",
      "Epoch 2, Batch 72/462, Loss: 0.9235877990722656\n",
      "Epoch 2, Batch 73/462, Loss: 0.8326951265335083\n",
      "Epoch 2, Batch 74/462, Loss: 0.9137307405471802\n",
      "Epoch 2, Batch 75/462, Loss: 0.7306094765663147\n",
      "Epoch 2, Batch 76/462, Loss: 0.9649670124053955\n",
      "Epoch 2, Batch 77/462, Loss: 0.9648025035858154\n",
      "Epoch 2, Batch 78/462, Loss: 0.95276939868927\n",
      "Epoch 2, Batch 79/462, Loss: 0.831575870513916\n",
      "Epoch 2, Batch 80/462, Loss: 0.9134523272514343\n",
      "Epoch 2, Batch 81/462, Loss: 0.9541739821434021\n",
      "Epoch 2, Batch 82/462, Loss: 1.054804801940918\n",
      "Epoch 2, Batch 83/462, Loss: 0.9802916049957275\n",
      "Epoch 2, Batch 84/462, Loss: 0.9777954816818237\n",
      "Epoch 2, Batch 85/462, Loss: 0.9881842136383057\n",
      "Epoch 2, Batch 86/462, Loss: 0.925703763961792\n",
      "Epoch 2, Batch 87/462, Loss: 1.1022740602493286\n",
      "Epoch 2, Batch 88/462, Loss: 0.8388213515281677\n",
      "Epoch 2, Batch 89/462, Loss: 0.9342664480209351\n",
      "Epoch 2, Batch 90/462, Loss: 0.8642671704292297\n",
      "Epoch 2, Batch 91/462, Loss: 0.7977927327156067\n",
      "Epoch 2, Batch 92/462, Loss: 0.9811962842941284\n",
      "Epoch 2, Batch 93/462, Loss: 0.9070838093757629\n",
      "Epoch 2, Batch 94/462, Loss: 0.9038002490997314\n",
      "Epoch 2, Batch 95/462, Loss: 0.9902487397193909\n",
      "Epoch 2, Batch 96/462, Loss: 0.815074622631073\n",
      "Epoch 2, Batch 97/462, Loss: 0.9358891844749451\n",
      "Epoch 2, Batch 98/462, Loss: 0.8979889154434204\n",
      "Epoch 2, Batch 99/462, Loss: 0.9161930680274963\n",
      "Epoch 2, Batch 100/462, Loss: 1.033414602279663\n",
      "Epoch 2, Batch 101/462, Loss: 0.8202367424964905\n",
      "Epoch 2, Batch 102/462, Loss: 0.9477614760398865\n",
      "Epoch 2, Batch 103/462, Loss: 0.9839686155319214\n",
      "Epoch 2, Batch 104/462, Loss: 0.9399693012237549\n",
      "Epoch 2, Batch 105/462, Loss: 1.0214236974716187\n",
      "Epoch 2, Batch 106/462, Loss: 0.954124927520752\n",
      "Epoch 2, Batch 107/462, Loss: 0.8909714221954346\n",
      "Epoch 2, Batch 108/462, Loss: 0.895957887172699\n",
      "Epoch 2, Batch 109/462, Loss: 0.9574777483940125\n",
      "Epoch 2, Batch 110/462, Loss: 0.9288129210472107\n",
      "Epoch 2, Batch 111/462, Loss: 0.8767420649528503\n",
      "Epoch 2, Batch 112/462, Loss: 0.958247184753418\n",
      "Epoch 2, Batch 113/462, Loss: 1.0044323205947876\n",
      "Epoch 2, Batch 114/462, Loss: 0.9390537738800049\n",
      "Epoch 2, Batch 115/462, Loss: 0.929473876953125\n",
      "Epoch 2, Batch 116/462, Loss: 0.9892715811729431\n",
      "Epoch 2, Batch 117/462, Loss: 0.9479643106460571\n",
      "Epoch 2, Batch 118/462, Loss: 0.8635976910591125\n",
      "Epoch 2, Batch 119/462, Loss: 1.0769753456115723\n",
      "Epoch 2, Batch 120/462, Loss: 0.9250958561897278\n",
      "Epoch 2, Batch 121/462, Loss: 0.9780951142311096\n",
      "Epoch 2, Batch 122/462, Loss: 0.9452956914901733\n",
      "Epoch 2, Batch 123/462, Loss: 1.0185303688049316\n",
      "Epoch 2, Batch 124/462, Loss: 0.9235551953315735\n",
      "Epoch 2, Batch 125/462, Loss: 1.0384817123413086\n",
      "Epoch 2, Batch 126/462, Loss: 0.8888687491416931\n",
      "Epoch 2, Batch 127/462, Loss: 0.9840390086174011\n",
      "Epoch 2, Batch 128/462, Loss: 0.9986082315444946\n",
      "Epoch 2, Batch 129/462, Loss: 0.9481173753738403\n",
      "Epoch 2, Batch 130/462, Loss: 1.0232796669006348\n",
      "Epoch 2, Batch 131/462, Loss: 0.9789366722106934\n",
      "Epoch 2, Batch 132/462, Loss: 0.9024645686149597\n",
      "Epoch 2, Batch 133/462, Loss: 0.8225419521331787\n",
      "Epoch 2, Batch 134/462, Loss: 0.9403061866760254\n",
      "Epoch 2, Batch 135/462, Loss: 0.8576563596725464\n",
      "Epoch 2, Batch 136/462, Loss: 0.8710077404975891\n",
      "Epoch 2, Batch 137/462, Loss: 0.7954577207565308\n",
      "Epoch 2, Batch 138/462, Loss: 0.9499616026878357\n",
      "Epoch 2, Batch 139/462, Loss: 0.9090929627418518\n",
      "Epoch 2, Batch 140/462, Loss: 0.9409202337265015\n",
      "Epoch 2, Batch 141/462, Loss: 0.9648550152778625\n",
      "Epoch 2, Batch 142/462, Loss: 1.0043504238128662\n",
      "Epoch 2, Batch 143/462, Loss: 0.9427887201309204\n",
      "Epoch 2, Batch 144/462, Loss: 1.0804353952407837\n",
      "Epoch 2, Batch 145/462, Loss: 0.9161781668663025\n",
      "Epoch 2, Batch 146/462, Loss: 0.8420314192771912\n",
      "Epoch 2, Batch 147/462, Loss: 0.8932123184204102\n",
      "Epoch 2, Batch 148/462, Loss: 0.9090177416801453\n",
      "Epoch 2, Batch 149/462, Loss: 1.0313310623168945\n",
      "Epoch 2, Batch 150/462, Loss: 0.8273764848709106\n",
      "Epoch 2, Batch 151/462, Loss: 0.9266315698623657\n",
      "Epoch 2, Batch 152/462, Loss: 0.9037332534790039\n",
      "Epoch 2, Batch 153/462, Loss: 0.8856549859046936\n",
      "Epoch 2, Batch 154/462, Loss: 0.9824436902999878\n",
      "Epoch 2, Batch 155/462, Loss: 1.0018854141235352\n",
      "Epoch 2, Batch 156/462, Loss: 0.9186068177223206\n",
      "Epoch 2, Batch 157/462, Loss: 0.896480143070221\n",
      "Epoch 2, Batch 158/462, Loss: 0.9396632313728333\n",
      "Epoch 2, Batch 159/462, Loss: 0.947407066822052\n",
      "Epoch 2, Batch 160/462, Loss: 0.8767139911651611\n",
      "Epoch 2, Batch 161/462, Loss: 0.8870612978935242\n",
      "Epoch 2, Batch 162/462, Loss: 0.9171640276908875\n",
      "Epoch 2, Batch 163/462, Loss: 0.8766396045684814\n",
      "Epoch 2, Batch 164/462, Loss: 0.8904638290405273\n",
      "Epoch 2, Batch 165/462, Loss: 0.9065937399864197\n",
      "Epoch 2, Batch 166/462, Loss: 0.9568594694137573\n",
      "Epoch 2, Batch 167/462, Loss: 0.9143399000167847\n",
      "Epoch 2, Batch 168/462, Loss: 0.8870565295219421\n",
      "Epoch 2, Batch 169/462, Loss: 0.927090585231781\n",
      "Epoch 2, Batch 170/462, Loss: 0.8598641157150269\n",
      "Epoch 2, Batch 171/462, Loss: 1.0198181867599487\n",
      "Epoch 2, Batch 172/462, Loss: 0.9497596621513367\n",
      "Epoch 2, Batch 173/462, Loss: 0.9374300241470337\n",
      "Epoch 2, Batch 174/462, Loss: 0.8545876741409302\n",
      "Epoch 2, Batch 175/462, Loss: 0.9107613563537598\n",
      "Epoch 2, Batch 176/462, Loss: 0.9214310646057129\n",
      "Epoch 2, Batch 177/462, Loss: 0.9656987190246582\n",
      "Epoch 2, Batch 178/462, Loss: 0.9627198576927185\n",
      "Epoch 2, Batch 179/462, Loss: 0.9333275556564331\n",
      "Epoch 2, Batch 180/462, Loss: 0.9783260822296143\n",
      "Epoch 2, Batch 181/462, Loss: 0.8741709589958191\n",
      "Epoch 2, Batch 182/462, Loss: 0.7944400310516357\n",
      "Epoch 2, Batch 183/462, Loss: 0.828498363494873\n",
      "Epoch 2, Batch 184/462, Loss: 0.9534759521484375\n",
      "Epoch 2, Batch 185/462, Loss: 0.9706739187240601\n",
      "Epoch 2, Batch 186/462, Loss: 0.9780858159065247\n",
      "Epoch 2, Batch 187/462, Loss: 1.0230965614318848\n",
      "Epoch 2, Batch 188/462, Loss: 0.9300262331962585\n",
      "Epoch 2, Batch 189/462, Loss: 0.8528417348861694\n",
      "Epoch 2, Batch 190/462, Loss: 0.907943606376648\n",
      "Epoch 2, Batch 191/462, Loss: 0.9211529493331909\n",
      "Epoch 2, Batch 192/462, Loss: 0.9612064957618713\n",
      "Epoch 2, Batch 193/462, Loss: 0.9671692848205566\n",
      "Epoch 2, Batch 194/462, Loss: 1.1507641077041626\n",
      "Epoch 2, Batch 195/462, Loss: 0.8916845321655273\n",
      "Epoch 2, Batch 196/462, Loss: 0.8623380064964294\n",
      "Epoch 2, Batch 197/462, Loss: 0.9225702285766602\n",
      "Epoch 2, Batch 198/462, Loss: 0.9829789400100708\n",
      "Epoch 2, Batch 199/462, Loss: 1.0253583192825317\n",
      "Epoch 2, Batch 200/462, Loss: 0.8420361876487732\n",
      "Epoch 2, Batch 201/462, Loss: 0.8344765901565552\n",
      "Epoch 2, Batch 202/462, Loss: 0.9592673182487488\n",
      "Epoch 2, Batch 203/462, Loss: 1.0051296949386597\n",
      "Epoch 2, Batch 204/462, Loss: 0.916271448135376\n",
      "Epoch 2, Batch 205/462, Loss: 0.9216002225875854\n",
      "Epoch 2, Batch 206/462, Loss: 0.8496442437171936\n",
      "Epoch 2, Batch 207/462, Loss: 0.8772448301315308\n",
      "Epoch 2, Batch 208/462, Loss: 0.9324868321418762\n",
      "Epoch 2, Batch 209/462, Loss: 0.9238337874412537\n",
      "Epoch 2, Batch 210/462, Loss: 1.0290924310684204\n",
      "Epoch 2, Batch 211/462, Loss: 0.9599102735519409\n",
      "Epoch 2, Batch 212/462, Loss: 0.9093536138534546\n",
      "Epoch 2, Batch 213/462, Loss: 0.9020362496376038\n",
      "Epoch 2, Batch 214/462, Loss: 0.7833628058433533\n",
      "Epoch 2, Batch 215/462, Loss: 0.9517360329627991\n",
      "Epoch 2, Batch 216/462, Loss: 0.94736248254776\n",
      "Epoch 2, Batch 217/462, Loss: 0.8144153952598572\n",
      "Epoch 2, Batch 218/462, Loss: 0.7986354827880859\n",
      "Epoch 2, Batch 219/462, Loss: 0.8626762628555298\n",
      "Epoch 2, Batch 220/462, Loss: 0.9273226261138916\n",
      "Epoch 2, Batch 221/462, Loss: 1.049500584602356\n",
      "Epoch 2, Batch 222/462, Loss: 0.8590432405471802\n",
      "Epoch 2, Batch 223/462, Loss: 0.8946776986122131\n",
      "Epoch 2, Batch 224/462, Loss: 0.7825506925582886\n",
      "Epoch 2, Batch 225/462, Loss: 0.9237218499183655\n",
      "Epoch 2, Batch 226/462, Loss: 0.8952783346176147\n",
      "Epoch 2, Batch 227/462, Loss: 0.9631991386413574\n",
      "Epoch 2, Batch 228/462, Loss: 0.9906805753707886\n",
      "Epoch 2, Batch 229/462, Loss: 0.9466823935508728\n",
      "Epoch 2, Batch 230/462, Loss: 0.9077496528625488\n",
      "Epoch 2, Batch 231/462, Loss: 0.9133954048156738\n",
      "Epoch 2, Batch 232/462, Loss: 0.9188356995582581\n",
      "Epoch 2, Batch 233/462, Loss: 0.8527914881706238\n",
      "Epoch 2, Batch 234/462, Loss: 0.884380042552948\n",
      "Epoch 2, Batch 235/462, Loss: 0.8732839822769165\n",
      "Epoch 2, Batch 236/462, Loss: 0.8715359568595886\n",
      "Epoch 2, Batch 237/462, Loss: 0.9885298609733582\n",
      "Epoch 2, Batch 238/462, Loss: 0.9371441602706909\n",
      "Epoch 2, Batch 239/462, Loss: 1.009820818901062\n",
      "Epoch 2, Batch 240/462, Loss: 1.0183196067810059\n",
      "Epoch 2, Batch 241/462, Loss: 1.01858651638031\n",
      "Epoch 2, Batch 242/462, Loss: 0.852574348449707\n",
      "Epoch 2, Batch 243/462, Loss: 0.8808716535568237\n",
      "Epoch 2, Batch 244/462, Loss: 0.8940193057060242\n",
      "Epoch 2, Batch 245/462, Loss: 0.9362295866012573\n",
      "Epoch 2, Batch 246/462, Loss: 0.8680466413497925\n",
      "Epoch 2, Batch 247/462, Loss: 0.7947055697441101\n",
      "Epoch 2, Batch 248/462, Loss: 1.0429928302764893\n",
      "Epoch 2, Batch 249/462, Loss: 0.9034508466720581\n",
      "Epoch 2, Batch 250/462, Loss: 0.8764604926109314\n",
      "Epoch 2, Batch 251/462, Loss: 0.8572308421134949\n",
      "Epoch 2, Batch 252/462, Loss: 0.8244453072547913\n",
      "Epoch 2, Batch 253/462, Loss: 0.9260956048965454\n",
      "Epoch 2, Batch 254/462, Loss: 1.002497911453247\n",
      "Epoch 2, Batch 255/462, Loss: 0.82660311460495\n",
      "Epoch 2, Batch 256/462, Loss: 0.9757078289985657\n",
      "Epoch 2, Batch 257/462, Loss: 0.8499464392662048\n",
      "Epoch 2, Batch 258/462, Loss: 1.0179073810577393\n",
      "Epoch 2, Batch 259/462, Loss: 1.029234528541565\n",
      "Epoch 2, Batch 260/462, Loss: 1.0016134977340698\n",
      "Epoch 2, Batch 261/462, Loss: 0.8248335719108582\n",
      "Epoch 2, Batch 262/462, Loss: 0.8428512811660767\n",
      "Epoch 2, Batch 263/462, Loss: 0.7659664750099182\n",
      "Epoch 2, Batch 264/462, Loss: 0.915436327457428\n",
      "Epoch 2, Batch 265/462, Loss: 0.9261912703514099\n",
      "Epoch 2, Batch 266/462, Loss: 0.8494015336036682\n",
      "Epoch 2, Batch 267/462, Loss: 0.927723228931427\n",
      "Epoch 2, Batch 268/462, Loss: 0.9426622986793518\n",
      "Epoch 2, Batch 269/462, Loss: 0.9077651500701904\n",
      "Epoch 2, Batch 270/462, Loss: 1.064622402191162\n",
      "Epoch 2, Batch 271/462, Loss: 0.779860258102417\n",
      "Epoch 2, Batch 272/462, Loss: 0.8860992193222046\n",
      "Epoch 2, Batch 273/462, Loss: 0.849831223487854\n",
      "Epoch 2, Batch 274/462, Loss: 0.9731873273849487\n",
      "Epoch 2, Batch 275/462, Loss: 0.851020097732544\n",
      "Epoch 2, Batch 276/462, Loss: 0.9653113484382629\n",
      "Epoch 2, Batch 277/462, Loss: 0.9002053737640381\n",
      "Epoch 2, Batch 278/462, Loss: 0.9551007747650146\n",
      "Epoch 2, Batch 279/462, Loss: 0.895099401473999\n",
      "Epoch 2, Batch 280/462, Loss: 0.8555632829666138\n",
      "Epoch 2, Batch 281/462, Loss: 0.8052558302879333\n",
      "Epoch 2, Batch 282/462, Loss: 1.0126694440841675\n",
      "Epoch 2, Batch 283/462, Loss: 1.042474627494812\n",
      "Epoch 2, Batch 284/462, Loss: 0.8745476007461548\n",
      "Epoch 2, Batch 285/462, Loss: 0.8841249942779541\n",
      "Epoch 2, Batch 286/462, Loss: 0.9875173568725586\n",
      "Epoch 2, Batch 287/462, Loss: 0.8896058797836304\n",
      "Epoch 2, Batch 288/462, Loss: 1.0071550607681274\n",
      "Epoch 2, Batch 289/462, Loss: 0.9543098211288452\n",
      "Epoch 2, Batch 290/462, Loss: 0.7915085554122925\n",
      "Epoch 2, Batch 291/462, Loss: 0.859290599822998\n",
      "Epoch 2, Batch 292/462, Loss: 0.8664617538452148\n",
      "Epoch 2, Batch 293/462, Loss: 0.9029639959335327\n",
      "Epoch 2, Batch 294/462, Loss: 1.0108088254928589\n",
      "Epoch 2, Batch 295/462, Loss: 0.9539009928703308\n",
      "Epoch 2, Batch 296/462, Loss: 0.8793723583221436\n",
      "Epoch 2, Batch 297/462, Loss: 0.8277289271354675\n",
      "Epoch 2, Batch 298/462, Loss: 0.9566795229911804\n",
      "Epoch 2, Batch 299/462, Loss: 0.921261727809906\n",
      "Epoch 2, Batch 300/462, Loss: 0.9146268367767334\n",
      "Epoch 2, Batch 301/462, Loss: 0.9478206038475037\n",
      "Epoch 2, Batch 302/462, Loss: 0.916845977306366\n",
      "Epoch 2, Batch 303/462, Loss: 0.9945513606071472\n",
      "Epoch 2, Batch 304/462, Loss: 0.9393378496170044\n",
      "Epoch 2, Batch 305/462, Loss: 0.7526402473449707\n",
      "Epoch 2, Batch 306/462, Loss: 0.9281901121139526\n",
      "Epoch 2, Batch 307/462, Loss: 0.8042882680892944\n",
      "Epoch 2, Batch 308/462, Loss: 0.7298101186752319\n",
      "Epoch 2, Batch 309/462, Loss: 0.8028579354286194\n",
      "Epoch 2, Batch 310/462, Loss: 0.9153327345848083\n",
      "Epoch 2, Batch 311/462, Loss: 0.9241539239883423\n",
      "Epoch 2, Batch 312/462, Loss: 0.9626789689064026\n",
      "Epoch 2, Batch 313/462, Loss: 0.914925217628479\n",
      "Epoch 2, Batch 314/462, Loss: 0.9445416927337646\n",
      "Epoch 2, Batch 315/462, Loss: 0.9140245914459229\n",
      "Epoch 2, Batch 316/462, Loss: 1.068191409111023\n",
      "Epoch 2, Batch 317/462, Loss: 0.8736415505409241\n",
      "Epoch 2, Batch 318/462, Loss: 0.9813703894615173\n",
      "Epoch 2, Batch 319/462, Loss: 0.9926712512969971\n",
      "Epoch 2, Batch 320/462, Loss: 0.9205968379974365\n",
      "Epoch 2, Batch 321/462, Loss: 0.8780554533004761\n",
      "Epoch 2, Batch 322/462, Loss: 0.9110846519470215\n",
      "Epoch 2, Batch 323/462, Loss: 0.9334122538566589\n",
      "Epoch 2, Batch 324/462, Loss: 0.9137251973152161\n",
      "Epoch 2, Batch 325/462, Loss: 0.9255463480949402\n",
      "Epoch 2, Batch 326/462, Loss: 0.8464279174804688\n",
      "Epoch 2, Batch 327/462, Loss: 0.9192032217979431\n",
      "Epoch 2, Batch 328/462, Loss: 1.0102397203445435\n",
      "Epoch 2, Batch 329/462, Loss: 0.94879150390625\n",
      "Epoch 2, Batch 330/462, Loss: 0.8765873312950134\n",
      "Epoch 2, Batch 331/462, Loss: 0.9979703426361084\n",
      "Epoch 2, Batch 332/462, Loss: 0.906743586063385\n",
      "Epoch 2, Batch 333/462, Loss: 0.8282831311225891\n",
      "Epoch 2, Batch 334/462, Loss: 0.7585319876670837\n",
      "Epoch 2, Batch 335/462, Loss: 0.9581802487373352\n",
      "Epoch 2, Batch 336/462, Loss: 0.9322985410690308\n",
      "Epoch 2, Batch 337/462, Loss: 0.8517699837684631\n",
      "Epoch 2, Batch 338/462, Loss: 0.8740031719207764\n",
      "Epoch 2, Batch 339/462, Loss: 0.8720518350601196\n",
      "Epoch 2, Batch 340/462, Loss: 0.9861700534820557\n",
      "Epoch 2, Batch 341/462, Loss: 0.8443517088890076\n",
      "Epoch 2, Batch 342/462, Loss: 0.9054111242294312\n",
      "Epoch 2, Batch 343/462, Loss: 0.952548086643219\n",
      "Epoch 2, Batch 344/462, Loss: 0.7256578207015991\n",
      "Epoch 2, Batch 345/462, Loss: 0.9478673934936523\n",
      "Epoch 2, Batch 346/462, Loss: 0.9493188858032227\n",
      "Epoch 2, Batch 347/462, Loss: 1.0662916898727417\n",
      "Epoch 2, Batch 348/462, Loss: 0.90683913230896\n",
      "Epoch 2, Batch 349/462, Loss: 0.9933027029037476\n",
      "Epoch 2, Batch 350/462, Loss: 1.0481054782867432\n",
      "Epoch 2, Batch 351/462, Loss: 0.9238466620445251\n",
      "Epoch 2, Batch 352/462, Loss: 0.8752635717391968\n",
      "Epoch 2, Batch 353/462, Loss: 0.8232349753379822\n",
      "Epoch 2, Batch 354/462, Loss: 1.0801199674606323\n",
      "Epoch 2, Batch 355/462, Loss: 0.871707558631897\n",
      "Epoch 2, Batch 356/462, Loss: 0.8665120005607605\n",
      "Epoch 2, Batch 357/462, Loss: 0.9296989440917969\n",
      "Epoch 2, Batch 358/462, Loss: 0.819930911064148\n",
      "Epoch 2, Batch 359/462, Loss: 0.7883929014205933\n",
      "Epoch 2, Batch 360/462, Loss: 0.930059015750885\n",
      "Epoch 2, Batch 361/462, Loss: 0.839189887046814\n",
      "Epoch 2, Batch 362/462, Loss: 0.8279354572296143\n",
      "Epoch 2, Batch 363/462, Loss: 0.9689372181892395\n",
      "Epoch 2, Batch 364/462, Loss: 0.8747677206993103\n",
      "Epoch 2, Batch 365/462, Loss: 0.9441900849342346\n",
      "Epoch 2, Batch 366/462, Loss: 0.9142435789108276\n",
      "Epoch 2, Batch 367/462, Loss: 1.1230802536010742\n",
      "Epoch 2, Batch 368/462, Loss: 0.8740254044532776\n",
      "Epoch 2, Batch 369/462, Loss: 0.9106222987174988\n",
      "Epoch 2, Batch 370/462, Loss: 0.9688414335250854\n",
      "Epoch 2, Batch 371/462, Loss: 0.9963256120681763\n",
      "Epoch 2, Batch 372/462, Loss: 0.9985186457633972\n",
      "Epoch 2, Batch 373/462, Loss: 0.9527239799499512\n",
      "Epoch 2, Batch 374/462, Loss: 0.7890116572380066\n",
      "Epoch 2, Batch 375/462, Loss: 0.9805846214294434\n",
      "Epoch 2, Batch 376/462, Loss: 0.9509414434432983\n",
      "Epoch 2, Batch 377/462, Loss: 0.8763350248336792\n",
      "Epoch 2, Batch 378/462, Loss: 0.8686461448669434\n",
      "Epoch 2, Batch 379/462, Loss: 0.8705721497535706\n",
      "Epoch 2, Batch 380/462, Loss: 0.8372805714607239\n",
      "Epoch 2, Batch 381/462, Loss: 1.082200050354004\n",
      "Epoch 2, Batch 382/462, Loss: 1.062540888786316\n",
      "Epoch 2, Batch 383/462, Loss: 0.944689154624939\n",
      "Epoch 2, Batch 384/462, Loss: 0.9545626044273376\n",
      "Epoch 2, Batch 385/462, Loss: 0.8861566781997681\n",
      "Epoch 2, Batch 386/462, Loss: 1.0302919149398804\n",
      "Epoch 2, Batch 387/462, Loss: 0.882915735244751\n",
      "Epoch 2, Batch 388/462, Loss: 0.9528290629386902\n",
      "Epoch 2, Batch 389/462, Loss: 0.858828604221344\n",
      "Epoch 2, Batch 390/462, Loss: 1.0005979537963867\n",
      "Epoch 2, Batch 391/462, Loss: 0.990684449672699\n",
      "Epoch 2, Batch 392/462, Loss: 0.8004408478736877\n",
      "Epoch 2, Batch 393/462, Loss: 0.886725664138794\n",
      "Epoch 2, Batch 394/462, Loss: 0.8047639727592468\n",
      "Epoch 2, Batch 395/462, Loss: 0.965298593044281\n",
      "Epoch 2, Batch 396/462, Loss: 0.859929084777832\n",
      "Epoch 2, Batch 397/462, Loss: 0.9255516529083252\n",
      "Epoch 2, Batch 398/462, Loss: 0.7767160534858704\n",
      "Epoch 2, Batch 399/462, Loss: 0.897826611995697\n",
      "Epoch 2, Batch 400/462, Loss: 1.0945826768875122\n",
      "Epoch 2, Batch 401/462, Loss: 0.8792070150375366\n",
      "Epoch 2, Batch 402/462, Loss: 0.8493906855583191\n",
      "Epoch 2, Batch 403/462, Loss: 0.8576750159263611\n",
      "Epoch 2, Batch 404/462, Loss: 0.9263626933097839\n",
      "Epoch 2, Batch 405/462, Loss: 0.8498690128326416\n",
      "Epoch 2, Batch 406/462, Loss: 0.9277770519256592\n",
      "Epoch 2, Batch 407/462, Loss: 0.9929977655410767\n",
      "Epoch 2, Batch 408/462, Loss: 0.8609859943389893\n",
      "Epoch 2, Batch 409/462, Loss: 1.0164169073104858\n",
      "Epoch 2, Batch 410/462, Loss: 0.9380097389221191\n",
      "Epoch 2, Batch 411/462, Loss: 0.8667174577713013\n",
      "Epoch 2, Batch 412/462, Loss: 0.9050517678260803\n",
      "Epoch 2, Batch 413/462, Loss: 1.0421510934829712\n",
      "Epoch 2, Batch 414/462, Loss: 0.8419278264045715\n",
      "Epoch 2, Batch 415/462, Loss: 0.9474399089813232\n",
      "Epoch 2, Batch 416/462, Loss: 0.971187949180603\n",
      "Epoch 2, Batch 417/462, Loss: 0.8297877311706543\n",
      "Epoch 2, Batch 418/462, Loss: 0.990117073059082\n",
      "Epoch 2, Batch 419/462, Loss: 1.0222620964050293\n",
      "Epoch 2, Batch 420/462, Loss: 0.9740719795227051\n",
      "Epoch 2, Batch 421/462, Loss: 1.0037617683410645\n",
      "Epoch 2, Batch 422/462, Loss: 0.9807750582695007\n",
      "Epoch 2, Batch 423/462, Loss: 0.9131903648376465\n",
      "Epoch 2, Batch 424/462, Loss: 0.997575581073761\n",
      "Epoch 2, Batch 425/462, Loss: 0.8989661931991577\n",
      "Epoch 2, Batch 426/462, Loss: 0.9062482118606567\n",
      "Epoch 2, Batch 427/462, Loss: 0.8085677027702332\n",
      "Epoch 2, Batch 428/462, Loss: 0.8556721806526184\n",
      "Epoch 2, Batch 429/462, Loss: 0.9635811448097229\n",
      "Epoch 2, Batch 430/462, Loss: 0.8633683919906616\n",
      "Epoch 2, Batch 431/462, Loss: 0.9386267066001892\n",
      "Epoch 2, Batch 432/462, Loss: 0.9059324264526367\n",
      "Epoch 2, Batch 433/462, Loss: 0.9402084350585938\n",
      "Epoch 2, Batch 434/462, Loss: 0.9617040157318115\n",
      "Epoch 2, Batch 435/462, Loss: 0.8045258522033691\n",
      "Epoch 2, Batch 436/462, Loss: 0.9388583898544312\n",
      "Epoch 2, Batch 437/462, Loss: 0.94383704662323\n",
      "Epoch 2, Batch 438/462, Loss: 0.9463568925857544\n",
      "Epoch 2, Batch 439/462, Loss: 0.8973436951637268\n",
      "Epoch 2, Batch 440/462, Loss: 1.015784740447998\n",
      "Epoch 2, Batch 441/462, Loss: 0.9164618849754333\n",
      "Epoch 2, Batch 442/462, Loss: 0.8748356103897095\n",
      "Epoch 2, Batch 443/462, Loss: 0.9392188191413879\n",
      "Epoch 2, Batch 444/462, Loss: 0.9090604782104492\n",
      "Epoch 2, Batch 445/462, Loss: 0.8697593808174133\n",
      "Epoch 2, Batch 446/462, Loss: 0.8814021348953247\n",
      "Epoch 2, Batch 447/462, Loss: 0.8745853304862976\n",
      "Epoch 2, Batch 448/462, Loss: 0.8737605214118958\n",
      "Epoch 2, Batch 449/462, Loss: 0.8312763571739197\n",
      "Epoch 2, Batch 450/462, Loss: 0.8399348855018616\n",
      "Epoch 2, Batch 451/462, Loss: 0.9469076991081238\n",
      "Epoch 2, Batch 452/462, Loss: 0.9459176659584045\n",
      "Epoch 2, Batch 453/462, Loss: 0.9456362724304199\n",
      "Epoch 2, Batch 454/462, Loss: 0.9794613718986511\n",
      "Epoch 2, Batch 455/462, Loss: 0.7631747126579285\n",
      "Epoch 2, Batch 456/462, Loss: 0.9137301445007324\n",
      "Epoch 2, Batch 457/462, Loss: 0.9884771108627319\n",
      "Epoch 2, Batch 458/462, Loss: 0.8383689522743225\n",
      "Epoch 2, Batch 459/462, Loss: 0.9627212882041931\n",
      "Epoch 2, Batch 460/462, Loss: 0.8628867268562317\n",
      "Epoch 2, Batch 461/462, Loss: 0.8689754009246826\n",
      "Epoch 2, Batch 462/462, Loss: 1.1869935989379883\n",
      "Epoch 2, Loss: 427.1811627149582\n",
      "Epoch 3, Batch 1/462, Loss: 0.8151336312294006\n",
      "Epoch 3, Batch 2/462, Loss: 1.036486029624939\n",
      "Epoch 3, Batch 3/462, Loss: 0.8086869120597839\n",
      "Epoch 3, Batch 4/462, Loss: 0.8621717691421509\n",
      "Epoch 3, Batch 5/462, Loss: 1.028249740600586\n",
      "Epoch 3, Batch 6/462, Loss: 1.047990322113037\n",
      "Epoch 3, Batch 7/462, Loss: 0.8815853595733643\n",
      "Epoch 3, Batch 8/462, Loss: 0.9304484724998474\n",
      "Epoch 3, Batch 9/462, Loss: 0.9429161548614502\n",
      "Epoch 3, Batch 10/462, Loss: 0.8967193961143494\n",
      "Epoch 3, Batch 11/462, Loss: 0.8443311452865601\n",
      "Epoch 3, Batch 12/462, Loss: 0.9831944704055786\n",
      "Epoch 3, Batch 13/462, Loss: 0.9796327352523804\n",
      "Epoch 3, Batch 14/462, Loss: 0.8928430676460266\n",
      "Epoch 3, Batch 15/462, Loss: 0.8072255849838257\n",
      "Epoch 3, Batch 16/462, Loss: 0.9884591698646545\n",
      "Epoch 3, Batch 17/462, Loss: 0.8155309557914734\n",
      "Epoch 3, Batch 18/462, Loss: 0.8977121114730835\n",
      "Epoch 3, Batch 19/462, Loss: 0.9532172679901123\n",
      "Epoch 3, Batch 20/462, Loss: 0.9633157253265381\n",
      "Epoch 3, Batch 21/462, Loss: 0.8960704207420349\n",
      "Epoch 3, Batch 22/462, Loss: 0.8984167575836182\n",
      "Epoch 3, Batch 23/462, Loss: 0.9113833904266357\n",
      "Epoch 3, Batch 24/462, Loss: 0.9416155219078064\n",
      "Epoch 3, Batch 25/462, Loss: 0.8536373972892761\n",
      "Epoch 3, Batch 26/462, Loss: 0.9365266561508179\n",
      "Epoch 3, Batch 27/462, Loss: 0.9350079894065857\n",
      "Epoch 3, Batch 28/462, Loss: 0.8832010626792908\n",
      "Epoch 3, Batch 29/462, Loss: 0.9549537897109985\n",
      "Epoch 3, Batch 30/462, Loss: 0.9153611063957214\n",
      "Epoch 3, Batch 31/462, Loss: 1.009292483329773\n",
      "Epoch 3, Batch 32/462, Loss: 1.0222111940383911\n",
      "Epoch 3, Batch 33/462, Loss: 0.9931827187538147\n",
      "Epoch 3, Batch 34/462, Loss: 0.9102568626403809\n",
      "Epoch 3, Batch 35/462, Loss: 0.9289788603782654\n",
      "Epoch 3, Batch 36/462, Loss: 0.9042103290557861\n",
      "Epoch 3, Batch 37/462, Loss: 0.8156620264053345\n",
      "Epoch 3, Batch 38/462, Loss: 0.9573820233345032\n",
      "Epoch 3, Batch 39/462, Loss: 0.9708319902420044\n",
      "Epoch 3, Batch 40/462, Loss: 0.9174848794937134\n",
      "Epoch 3, Batch 41/462, Loss: 0.8609315156936646\n",
      "Epoch 3, Batch 42/462, Loss: 0.8326711058616638\n",
      "Epoch 3, Batch 43/462, Loss: 0.8406649231910706\n",
      "Epoch 3, Batch 44/462, Loss: 0.8977581262588501\n",
      "Epoch 3, Batch 45/462, Loss: 0.8294547200202942\n",
      "Epoch 3, Batch 46/462, Loss: 0.965609073638916\n",
      "Epoch 3, Batch 47/462, Loss: 0.8447315692901611\n",
      "Epoch 3, Batch 48/462, Loss: 0.9780917763710022\n",
      "Epoch 3, Batch 49/462, Loss: 0.9065045118331909\n",
      "Epoch 3, Batch 50/462, Loss: 0.8924161195755005\n",
      "Epoch 3, Batch 51/462, Loss: 0.9222173690795898\n",
      "Epoch 3, Batch 52/462, Loss: 0.7947396039962769\n",
      "Epoch 3, Batch 53/462, Loss: 0.8792083859443665\n",
      "Epoch 3, Batch 54/462, Loss: 0.9201924800872803\n",
      "Epoch 3, Batch 55/462, Loss: 0.8902475833892822\n",
      "Epoch 3, Batch 56/462, Loss: 0.8773678541183472\n",
      "Epoch 3, Batch 57/462, Loss: 0.9053646922111511\n",
      "Epoch 3, Batch 58/462, Loss: 1.0959057807922363\n",
      "Epoch 3, Batch 59/462, Loss: 0.7771000862121582\n",
      "Epoch 3, Batch 60/462, Loss: 0.9544632434844971\n",
      "Epoch 3, Batch 61/462, Loss: 0.8593854904174805\n",
      "Epoch 3, Batch 62/462, Loss: 0.8863269090652466\n",
      "Epoch 3, Batch 63/462, Loss: 0.9209024310112\n",
      "Epoch 3, Batch 64/462, Loss: 0.8928602337837219\n",
      "Epoch 3, Batch 65/462, Loss: 0.8831138014793396\n",
      "Epoch 3, Batch 66/462, Loss: 0.8206359148025513\n",
      "Epoch 3, Batch 67/462, Loss: 0.8689092993736267\n",
      "Epoch 3, Batch 68/462, Loss: 1.042598843574524\n",
      "Epoch 3, Batch 69/462, Loss: 0.8345221877098083\n",
      "Epoch 3, Batch 70/462, Loss: 0.9110889434814453\n",
      "Epoch 3, Batch 71/462, Loss: 0.7494753003120422\n",
      "Epoch 3, Batch 72/462, Loss: 0.9278216361999512\n",
      "Epoch 3, Batch 73/462, Loss: 0.8362860083580017\n",
      "Epoch 3, Batch 74/462, Loss: 0.9775735139846802\n",
      "Epoch 3, Batch 75/462, Loss: 0.8066494464874268\n",
      "Epoch 3, Batch 76/462, Loss: 0.878718376159668\n",
      "Epoch 3, Batch 77/462, Loss: 0.7889080047607422\n",
      "Epoch 3, Batch 78/462, Loss: 0.9375095963478088\n",
      "Epoch 3, Batch 79/462, Loss: 0.9868819117546082\n",
      "Epoch 3, Batch 80/462, Loss: 0.8101648688316345\n",
      "Epoch 3, Batch 81/462, Loss: 0.7832661867141724\n",
      "Epoch 3, Batch 82/462, Loss: 1.0672789812088013\n",
      "Epoch 3, Batch 83/462, Loss: 0.8454711437225342\n",
      "Epoch 3, Batch 84/462, Loss: 0.8648343682289124\n",
      "Epoch 3, Batch 85/462, Loss: 0.8559104800224304\n",
      "Epoch 3, Batch 86/462, Loss: 0.8934299945831299\n",
      "Epoch 3, Batch 87/462, Loss: 0.908041775226593\n",
      "Epoch 3, Batch 88/462, Loss: 0.8702893257141113\n",
      "Epoch 3, Batch 89/462, Loss: 0.7398451566696167\n",
      "Epoch 3, Batch 90/462, Loss: 0.8783262372016907\n",
      "Epoch 3, Batch 91/462, Loss: 0.9177576303482056\n",
      "Epoch 3, Batch 92/462, Loss: 0.8362778425216675\n",
      "Epoch 3, Batch 93/462, Loss: 0.9126984477043152\n",
      "Epoch 3, Batch 94/462, Loss: 0.8030682802200317\n",
      "Epoch 3, Batch 95/462, Loss: 0.881216824054718\n",
      "Epoch 3, Batch 96/462, Loss: 0.8658022284507751\n",
      "Epoch 3, Batch 97/462, Loss: 0.9380215406417847\n",
      "Epoch 3, Batch 98/462, Loss: 0.8061494827270508\n",
      "Epoch 3, Batch 99/462, Loss: 0.7996296286582947\n",
      "Epoch 3, Batch 100/462, Loss: 1.0621742010116577\n",
      "Epoch 3, Batch 101/462, Loss: 0.7527949810028076\n",
      "Epoch 3, Batch 102/462, Loss: 0.917486846446991\n",
      "Epoch 3, Batch 103/462, Loss: 1.0328547954559326\n",
      "Epoch 3, Batch 104/462, Loss: 0.7875899076461792\n",
      "Epoch 3, Batch 105/462, Loss: 0.9083633422851562\n",
      "Epoch 3, Batch 106/462, Loss: 0.8745830655097961\n",
      "Epoch 3, Batch 107/462, Loss: 0.7604868412017822\n",
      "Epoch 3, Batch 108/462, Loss: 1.0702409744262695\n",
      "Epoch 3, Batch 109/462, Loss: 1.0362061262130737\n",
      "Epoch 3, Batch 110/462, Loss: 0.9638839960098267\n",
      "Epoch 3, Batch 111/462, Loss: 0.9612409472465515\n",
      "Epoch 3, Batch 112/462, Loss: 0.8297537565231323\n",
      "Epoch 3, Batch 113/462, Loss: 0.9209123849868774\n",
      "Epoch 3, Batch 114/462, Loss: 0.8216360211372375\n",
      "Epoch 3, Batch 115/462, Loss: 0.9265042543411255\n",
      "Epoch 3, Batch 116/462, Loss: 0.8540319800376892\n",
      "Epoch 3, Batch 117/462, Loss: 0.9551085233688354\n",
      "Epoch 3, Batch 118/462, Loss: 0.9843352437019348\n",
      "Epoch 3, Batch 119/462, Loss: 1.013327717781067\n",
      "Epoch 3, Batch 120/462, Loss: 0.9476012587547302\n",
      "Epoch 3, Batch 121/462, Loss: 0.8689221739768982\n",
      "Epoch 3, Batch 122/462, Loss: 0.8631505966186523\n",
      "Epoch 3, Batch 123/462, Loss: 0.8730074763298035\n",
      "Epoch 3, Batch 124/462, Loss: 0.9374629259109497\n",
      "Epoch 3, Batch 125/462, Loss: 0.8536757826805115\n",
      "Epoch 3, Batch 126/462, Loss: 0.8708831071853638\n",
      "Epoch 3, Batch 127/462, Loss: 1.0608783960342407\n",
      "Epoch 3, Batch 128/462, Loss: 0.8523104190826416\n",
      "Epoch 3, Batch 129/462, Loss: 0.7915367484092712\n",
      "Epoch 3, Batch 130/462, Loss: 1.0157488584518433\n",
      "Epoch 3, Batch 131/462, Loss: 0.9318270683288574\n",
      "Epoch 3, Batch 132/462, Loss: 0.8250843286514282\n",
      "Epoch 3, Batch 133/462, Loss: 1.0541542768478394\n",
      "Epoch 3, Batch 134/462, Loss: 0.7915720343589783\n",
      "Epoch 3, Batch 135/462, Loss: 0.8196012377738953\n",
      "Epoch 3, Batch 136/462, Loss: 0.8537158966064453\n",
      "Epoch 3, Batch 137/462, Loss: 0.9735637903213501\n",
      "Epoch 3, Batch 138/462, Loss: 0.7876389026641846\n",
      "Epoch 3, Batch 139/462, Loss: 0.8616353273391724\n",
      "Epoch 3, Batch 140/462, Loss: 1.064850926399231\n",
      "Epoch 3, Batch 141/462, Loss: 0.9123294949531555\n",
      "Epoch 3, Batch 142/462, Loss: 0.8574377298355103\n",
      "Epoch 3, Batch 143/462, Loss: 0.9459491968154907\n",
      "Epoch 3, Batch 144/462, Loss: 1.0343247652053833\n",
      "Epoch 3, Batch 145/462, Loss: 0.7787880897521973\n",
      "Epoch 3, Batch 146/462, Loss: 0.8189471364021301\n",
      "Epoch 3, Batch 147/462, Loss: 0.8824144005775452\n",
      "Epoch 3, Batch 148/462, Loss: 0.8294325470924377\n",
      "Epoch 3, Batch 149/462, Loss: 0.8950302004814148\n",
      "Epoch 3, Batch 150/462, Loss: 0.8372766971588135\n",
      "Epoch 3, Batch 151/462, Loss: 0.7919725179672241\n",
      "Epoch 3, Batch 152/462, Loss: 0.788318395614624\n",
      "Epoch 3, Batch 153/462, Loss: 0.8168268203735352\n",
      "Epoch 3, Batch 154/462, Loss: 0.8441146612167358\n",
      "Epoch 3, Batch 155/462, Loss: 0.8135438561439514\n",
      "Epoch 3, Batch 156/462, Loss: 1.0672061443328857\n",
      "Epoch 3, Batch 157/462, Loss: 0.7632145881652832\n",
      "Epoch 3, Batch 158/462, Loss: 0.8455968499183655\n",
      "Epoch 3, Batch 159/462, Loss: 0.8817705512046814\n",
      "Epoch 3, Batch 160/462, Loss: 0.8535062074661255\n",
      "Epoch 3, Batch 161/462, Loss: 0.943414568901062\n",
      "Epoch 3, Batch 162/462, Loss: 1.1257432699203491\n",
      "Epoch 3, Batch 163/462, Loss: 0.8644567131996155\n",
      "Epoch 3, Batch 164/462, Loss: 0.7823912501335144\n",
      "Epoch 3, Batch 165/462, Loss: 0.9495202302932739\n",
      "Epoch 3, Batch 166/462, Loss: 0.8856923580169678\n",
      "Epoch 3, Batch 167/462, Loss: 0.8646228909492493\n",
      "Epoch 3, Batch 168/462, Loss: 0.78254634141922\n",
      "Epoch 3, Batch 169/462, Loss: 0.8602295517921448\n",
      "Epoch 3, Batch 170/462, Loss: 0.8668297529220581\n",
      "Epoch 3, Batch 171/462, Loss: 0.8138993978500366\n",
      "Epoch 3, Batch 172/462, Loss: 0.8233302235603333\n",
      "Epoch 3, Batch 173/462, Loss: 0.9671599864959717\n",
      "Epoch 3, Batch 174/462, Loss: 0.9559931755065918\n",
      "Epoch 3, Batch 175/462, Loss: 0.8559088110923767\n",
      "Epoch 3, Batch 176/462, Loss: 1.0248897075653076\n",
      "Epoch 3, Batch 177/462, Loss: 0.9293548464775085\n",
      "Epoch 3, Batch 178/462, Loss: 0.8627709746360779\n",
      "Epoch 3, Batch 179/462, Loss: 0.84551602602005\n",
      "Epoch 3, Batch 180/462, Loss: 0.8496468663215637\n",
      "Epoch 3, Batch 181/462, Loss: 0.8988342881202698\n",
      "Epoch 3, Batch 182/462, Loss: 0.915073037147522\n",
      "Epoch 3, Batch 183/462, Loss: 0.8409938216209412\n",
      "Epoch 3, Batch 184/462, Loss: 0.8130250573158264\n",
      "Epoch 3, Batch 185/462, Loss: 0.9290987253189087\n",
      "Epoch 3, Batch 186/462, Loss: 0.8869974613189697\n",
      "Epoch 3, Batch 187/462, Loss: 0.701600968837738\n",
      "Epoch 3, Batch 188/462, Loss: 0.9430679082870483\n",
      "Epoch 3, Batch 189/462, Loss: 0.7247123122215271\n",
      "Epoch 3, Batch 190/462, Loss: 0.9217913150787354\n",
      "Epoch 3, Batch 191/462, Loss: 0.9032384753227234\n",
      "Epoch 3, Batch 192/462, Loss: 0.8385530710220337\n",
      "Epoch 3, Batch 193/462, Loss: 0.8311063051223755\n",
      "Epoch 3, Batch 194/462, Loss: 0.9738810658454895\n",
      "Epoch 3, Batch 195/462, Loss: 0.8011553287506104\n",
      "Epoch 3, Batch 196/462, Loss: 0.8327978849411011\n",
      "Epoch 3, Batch 197/462, Loss: 0.9670635461807251\n",
      "Epoch 3, Batch 198/462, Loss: 0.9749371409416199\n",
      "Epoch 3, Batch 199/462, Loss: 0.933385968208313\n",
      "Epoch 3, Batch 200/462, Loss: 0.8062112927436829\n",
      "Epoch 3, Batch 201/462, Loss: 0.8891712427139282\n",
      "Epoch 3, Batch 202/462, Loss: 0.8231472373008728\n",
      "Epoch 3, Batch 203/462, Loss: 0.8960145115852356\n",
      "Epoch 3, Batch 204/462, Loss: 0.842991828918457\n",
      "Epoch 3, Batch 205/462, Loss: 0.8810069561004639\n",
      "Epoch 3, Batch 206/462, Loss: 0.9234111309051514\n",
      "Epoch 3, Batch 207/462, Loss: 0.9464564919471741\n",
      "Epoch 3, Batch 208/462, Loss: 0.9707584381103516\n",
      "Epoch 3, Batch 209/462, Loss: 0.8534858822822571\n",
      "Epoch 3, Batch 210/462, Loss: 0.9658306241035461\n",
      "Epoch 3, Batch 211/462, Loss: 0.7375664114952087\n",
      "Epoch 3, Batch 212/462, Loss: 0.7902053594589233\n",
      "Epoch 3, Batch 213/462, Loss: 0.7760688662528992\n",
      "Epoch 3, Batch 214/462, Loss: 0.8763546347618103\n",
      "Epoch 3, Batch 215/462, Loss: 0.8663410544395447\n",
      "Epoch 3, Batch 216/462, Loss: 1.0720168352127075\n",
      "Epoch 3, Batch 217/462, Loss: 0.8172030448913574\n",
      "Epoch 3, Batch 218/462, Loss: 0.9409192800521851\n",
      "Epoch 3, Batch 219/462, Loss: 0.8069454431533813\n",
      "Epoch 3, Batch 220/462, Loss: 0.8806439638137817\n",
      "Epoch 3, Batch 221/462, Loss: 0.7694013118743896\n",
      "Epoch 3, Batch 222/462, Loss: 1.0590035915374756\n",
      "Epoch 3, Batch 223/462, Loss: 0.8297180533409119\n",
      "Epoch 3, Batch 224/462, Loss: 0.7571290135383606\n",
      "Epoch 3, Batch 225/462, Loss: 0.7008380889892578\n",
      "Epoch 3, Batch 226/462, Loss: 0.910327672958374\n",
      "Epoch 3, Batch 227/462, Loss: 1.0985356569290161\n",
      "Epoch 3, Batch 228/462, Loss: 1.0113744735717773\n",
      "Epoch 3, Batch 229/462, Loss: 0.742195188999176\n",
      "Epoch 3, Batch 230/462, Loss: 0.8028017282485962\n",
      "Epoch 3, Batch 231/462, Loss: 0.9532089233398438\n",
      "Epoch 3, Batch 232/462, Loss: 0.8156456351280212\n",
      "Epoch 3, Batch 233/462, Loss: 0.8784657716751099\n",
      "Epoch 3, Batch 234/462, Loss: 0.9664952158927917\n",
      "Epoch 3, Batch 235/462, Loss: 0.9112664461135864\n",
      "Epoch 3, Batch 236/462, Loss: 0.9769834280014038\n",
      "Epoch 3, Batch 237/462, Loss: 0.8599085211753845\n",
      "Epoch 3, Batch 238/462, Loss: 0.9273346662521362\n",
      "Epoch 3, Batch 239/462, Loss: 0.8730864524841309\n",
      "Epoch 3, Batch 240/462, Loss: 0.8109802007675171\n",
      "Epoch 3, Batch 241/462, Loss: 0.8187375068664551\n",
      "Epoch 3, Batch 242/462, Loss: 0.8916437029838562\n",
      "Epoch 3, Batch 243/462, Loss: 0.8409767746925354\n",
      "Epoch 3, Batch 244/462, Loss: 0.9241541624069214\n",
      "Epoch 3, Batch 245/462, Loss: 0.8750075101852417\n",
      "Epoch 3, Batch 246/462, Loss: 0.9744887351989746\n",
      "Epoch 3, Batch 247/462, Loss: 1.0150972604751587\n",
      "Epoch 3, Batch 248/462, Loss: 0.9846524000167847\n",
      "Epoch 3, Batch 249/462, Loss: 0.8178820610046387\n",
      "Epoch 3, Batch 250/462, Loss: 0.7994047999382019\n",
      "Epoch 3, Batch 251/462, Loss: 0.8992924690246582\n",
      "Epoch 3, Batch 252/462, Loss: 0.9393506050109863\n",
      "Epoch 3, Batch 253/462, Loss: 0.8107072710990906\n",
      "Epoch 3, Batch 254/462, Loss: 0.9506790637969971\n",
      "Epoch 3, Batch 255/462, Loss: 0.8178738951683044\n",
      "Epoch 3, Batch 256/462, Loss: 0.9941095113754272\n",
      "Epoch 3, Batch 257/462, Loss: 0.9117379188537598\n",
      "Epoch 3, Batch 258/462, Loss: 0.8918666243553162\n",
      "Epoch 3, Batch 259/462, Loss: 0.8260539770126343\n",
      "Epoch 3, Batch 260/462, Loss: 0.8918713331222534\n",
      "Epoch 3, Batch 261/462, Loss: 1.02681303024292\n",
      "Epoch 3, Batch 262/462, Loss: 0.8045908212661743\n",
      "Epoch 3, Batch 263/462, Loss: 0.9332910180091858\n",
      "Epoch 3, Batch 264/462, Loss: 0.8257373571395874\n",
      "Epoch 3, Batch 265/462, Loss: 0.8484407067298889\n",
      "Epoch 3, Batch 266/462, Loss: 0.8508177399635315\n",
      "Epoch 3, Batch 267/462, Loss: 0.8461988568305969\n",
      "Epoch 3, Batch 268/462, Loss: 1.0161707401275635\n",
      "Epoch 3, Batch 269/462, Loss: 0.9219154715538025\n",
      "Epoch 3, Batch 270/462, Loss: 0.9063866138458252\n",
      "Epoch 3, Batch 271/462, Loss: 0.7906447052955627\n",
      "Epoch 3, Batch 272/462, Loss: 0.8554949760437012\n",
      "Epoch 3, Batch 273/462, Loss: 0.9276331663131714\n",
      "Epoch 3, Batch 274/462, Loss: 0.7876245379447937\n",
      "Epoch 3, Batch 275/462, Loss: 0.9724779725074768\n",
      "Epoch 3, Batch 276/462, Loss: 0.9079205989837646\n",
      "Epoch 3, Batch 277/462, Loss: 0.9849457740783691\n",
      "Epoch 3, Batch 278/462, Loss: 0.9411889910697937\n",
      "Epoch 3, Batch 279/462, Loss: 1.1071633100509644\n",
      "Epoch 3, Batch 280/462, Loss: 0.9636853337287903\n",
      "Epoch 3, Batch 281/462, Loss: 0.8456293344497681\n",
      "Epoch 3, Batch 282/462, Loss: 0.8660094738006592\n",
      "Epoch 3, Batch 283/462, Loss: 0.9282925128936768\n",
      "Epoch 3, Batch 284/462, Loss: 0.7838913202285767\n",
      "Epoch 3, Batch 285/462, Loss: 0.874323308467865\n",
      "Epoch 3, Batch 286/462, Loss: 0.9587271213531494\n",
      "Epoch 3, Batch 287/462, Loss: 0.7830517888069153\n",
      "Epoch 3, Batch 288/462, Loss: 0.9041669964790344\n",
      "Epoch 3, Batch 289/462, Loss: 0.864700436592102\n",
      "Epoch 3, Batch 290/462, Loss: 0.8857825398445129\n",
      "Epoch 3, Batch 291/462, Loss: 0.9882112741470337\n",
      "Epoch 3, Batch 292/462, Loss: 0.9052391052246094\n",
      "Epoch 3, Batch 293/462, Loss: 0.9057568907737732\n",
      "Epoch 3, Batch 294/462, Loss: 1.02538001537323\n",
      "Epoch 3, Batch 295/462, Loss: 0.7644357085227966\n",
      "Epoch 3, Batch 296/462, Loss: 0.82277911901474\n",
      "Epoch 3, Batch 297/462, Loss: 0.8392367362976074\n",
      "Epoch 3, Batch 298/462, Loss: 0.7733933329582214\n",
      "Epoch 3, Batch 299/462, Loss: 1.0867787599563599\n",
      "Epoch 3, Batch 300/462, Loss: 0.8920102119445801\n",
      "Epoch 3, Batch 301/462, Loss: 0.8718324899673462\n",
      "Epoch 3, Batch 302/462, Loss: 0.9008593559265137\n",
      "Epoch 3, Batch 303/462, Loss: 0.8798781037330627\n",
      "Epoch 3, Batch 304/462, Loss: 0.9347164630889893\n",
      "Epoch 3, Batch 305/462, Loss: 0.8718166947364807\n",
      "Epoch 3, Batch 306/462, Loss: 1.042335033416748\n",
      "Epoch 3, Batch 307/462, Loss: 0.9451149106025696\n",
      "Epoch 3, Batch 308/462, Loss: 0.9668180346488953\n",
      "Epoch 3, Batch 309/462, Loss: 0.8242009282112122\n",
      "Epoch 3, Batch 310/462, Loss: 0.953704297542572\n",
      "Epoch 3, Batch 311/462, Loss: 0.6885026097297668\n",
      "Epoch 3, Batch 312/462, Loss: 1.0280053615570068\n",
      "Epoch 3, Batch 313/462, Loss: 0.9745120406150818\n",
      "Epoch 3, Batch 314/462, Loss: 0.7782699465751648\n",
      "Epoch 3, Batch 315/462, Loss: 0.7876083254814148\n",
      "Epoch 3, Batch 316/462, Loss: 1.0187492370605469\n",
      "Epoch 3, Batch 317/462, Loss: 0.8781324028968811\n",
      "Epoch 3, Batch 318/462, Loss: 0.8950949907302856\n",
      "Epoch 3, Batch 319/462, Loss: 0.9958714842796326\n",
      "Epoch 3, Batch 320/462, Loss: 0.7693490386009216\n",
      "Epoch 3, Batch 321/462, Loss: 0.8260213136672974\n",
      "Epoch 3, Batch 322/462, Loss: 0.802577018737793\n",
      "Epoch 3, Batch 323/462, Loss: 0.8118771910667419\n",
      "Epoch 3, Batch 324/462, Loss: 0.9132211208343506\n",
      "Epoch 3, Batch 325/462, Loss: 0.8794934749603271\n",
      "Epoch 3, Batch 326/462, Loss: 1.026716709136963\n",
      "Epoch 3, Batch 327/462, Loss: 0.8092254996299744\n",
      "Epoch 3, Batch 328/462, Loss: 1.0459541082382202\n",
      "Epoch 3, Batch 329/462, Loss: 0.7919182777404785\n",
      "Epoch 3, Batch 330/462, Loss: 0.9251537919044495\n",
      "Epoch 3, Batch 331/462, Loss: 0.8395141959190369\n",
      "Epoch 3, Batch 332/462, Loss: 0.7838690876960754\n",
      "Epoch 3, Batch 333/462, Loss: 0.9313115477561951\n",
      "Epoch 3, Batch 334/462, Loss: 0.8676574230194092\n",
      "Epoch 3, Batch 335/462, Loss: 0.7930160164833069\n",
      "Epoch 3, Batch 336/462, Loss: 0.8384431600570679\n",
      "Epoch 3, Batch 337/462, Loss: 0.9483181238174438\n",
      "Epoch 3, Batch 338/462, Loss: 0.8537731766700745\n",
      "Epoch 3, Batch 339/462, Loss: 0.9255447387695312\n",
      "Epoch 3, Batch 340/462, Loss: 0.8310796022415161\n",
      "Epoch 3, Batch 341/462, Loss: 0.8351241946220398\n",
      "Epoch 3, Batch 342/462, Loss: 0.8821461796760559\n",
      "Epoch 3, Batch 343/462, Loss: 0.7702367901802063\n",
      "Epoch 3, Batch 344/462, Loss: 0.9327918887138367\n",
      "Epoch 3, Batch 345/462, Loss: 1.0583467483520508\n",
      "Epoch 3, Batch 346/462, Loss: 0.5954464077949524\n",
      "Epoch 3, Batch 347/462, Loss: 1.0170626640319824\n",
      "Epoch 3, Batch 348/462, Loss: 0.9760199189186096\n",
      "Epoch 3, Batch 349/462, Loss: 0.900196373462677\n",
      "Epoch 3, Batch 350/462, Loss: 0.8281725645065308\n",
      "Epoch 3, Batch 351/462, Loss: 0.9736015200614929\n",
      "Epoch 3, Batch 352/462, Loss: 0.9570543766021729\n",
      "Epoch 3, Batch 353/462, Loss: 0.8349427580833435\n",
      "Epoch 3, Batch 354/462, Loss: 0.8408761024475098\n",
      "Epoch 3, Batch 355/462, Loss: 0.8775800466537476\n",
      "Epoch 3, Batch 356/462, Loss: 0.941097617149353\n",
      "Epoch 3, Batch 357/462, Loss: 0.8691247701644897\n",
      "Epoch 3, Batch 358/462, Loss: 1.0127447843551636\n",
      "Epoch 3, Batch 359/462, Loss: 0.7171821594238281\n",
      "Epoch 3, Batch 360/462, Loss: 0.9011633396148682\n",
      "Epoch 3, Batch 361/462, Loss: 0.8580058217048645\n",
      "Epoch 3, Batch 362/462, Loss: 0.7876840233802795\n",
      "Epoch 3, Batch 363/462, Loss: 0.8397658467292786\n",
      "Epoch 3, Batch 364/462, Loss: 1.0055330991744995\n",
      "Epoch 3, Batch 365/462, Loss: 1.0840067863464355\n",
      "Epoch 3, Batch 366/462, Loss: 0.9014847278594971\n",
      "Epoch 3, Batch 367/462, Loss: 0.8692160248756409\n",
      "Epoch 3, Batch 368/462, Loss: 0.972785234451294\n",
      "Epoch 3, Batch 369/462, Loss: 0.8640797734260559\n",
      "Epoch 3, Batch 370/462, Loss: 1.0290886163711548\n",
      "Epoch 3, Batch 371/462, Loss: 0.8967750072479248\n",
      "Epoch 3, Batch 372/462, Loss: 0.8646202683448792\n",
      "Epoch 3, Batch 373/462, Loss: 0.8437684774398804\n",
      "Epoch 3, Batch 374/462, Loss: 0.95050048828125\n",
      "Epoch 3, Batch 375/462, Loss: 1.0526708364486694\n",
      "Epoch 3, Batch 376/462, Loss: 0.8543198704719543\n",
      "Epoch 3, Batch 377/462, Loss: 0.9300245046615601\n",
      "Epoch 3, Batch 378/462, Loss: 0.8431177139282227\n",
      "Epoch 3, Batch 379/462, Loss: 0.9828077554702759\n",
      "Epoch 3, Batch 380/462, Loss: 0.8865519762039185\n",
      "Epoch 3, Batch 381/462, Loss: 1.0128382444381714\n",
      "Epoch 3, Batch 382/462, Loss: 0.9896060228347778\n",
      "Epoch 3, Batch 383/462, Loss: 0.9708376526832581\n",
      "Epoch 3, Batch 384/462, Loss: 0.8371865749359131\n",
      "Epoch 3, Batch 385/462, Loss: 0.9417067170143127\n",
      "Epoch 3, Batch 386/462, Loss: 0.9892794489860535\n",
      "Epoch 3, Batch 387/462, Loss: 0.8530516624450684\n",
      "Epoch 3, Batch 388/462, Loss: 0.7991898655891418\n",
      "Epoch 3, Batch 389/462, Loss: 0.9677913188934326\n",
      "Epoch 3, Batch 390/462, Loss: 0.9767577052116394\n",
      "Epoch 3, Batch 391/462, Loss: 0.9837131500244141\n",
      "Epoch 3, Batch 392/462, Loss: 0.8555231094360352\n",
      "Epoch 3, Batch 393/462, Loss: 0.8569707870483398\n",
      "Epoch 3, Batch 394/462, Loss: 0.8102566003799438\n",
      "Epoch 3, Batch 395/462, Loss: 0.8387293219566345\n",
      "Epoch 3, Batch 396/462, Loss: 0.9684349298477173\n",
      "Epoch 3, Batch 397/462, Loss: 0.9115038514137268\n",
      "Epoch 3, Batch 398/462, Loss: 1.0532758235931396\n",
      "Epoch 3, Batch 399/462, Loss: 0.8898394107818604\n",
      "Epoch 3, Batch 400/462, Loss: 0.9123160243034363\n",
      "Epoch 3, Batch 401/462, Loss: 0.9227468371391296\n",
      "Epoch 3, Batch 402/462, Loss: 0.8308077454566956\n",
      "Epoch 3, Batch 403/462, Loss: 0.804587185382843\n",
      "Epoch 3, Batch 404/462, Loss: 0.9856415390968323\n",
      "Epoch 3, Batch 405/462, Loss: 1.0089824199676514\n",
      "Epoch 3, Batch 406/462, Loss: 0.8936256170272827\n",
      "Epoch 3, Batch 407/462, Loss: 0.919735312461853\n",
      "Epoch 3, Batch 408/462, Loss: 0.9152689576148987\n",
      "Epoch 3, Batch 409/462, Loss: 0.9475774168968201\n",
      "Epoch 3, Batch 410/462, Loss: 0.8236556649208069\n",
      "Epoch 3, Batch 411/462, Loss: 0.8523528575897217\n",
      "Epoch 3, Batch 412/462, Loss: 0.8760596513748169\n",
      "Epoch 3, Batch 413/462, Loss: 0.8833668231964111\n",
      "Epoch 3, Batch 414/462, Loss: 0.9791545867919922\n",
      "Epoch 3, Batch 415/462, Loss: 0.8930855989456177\n",
      "Epoch 3, Batch 416/462, Loss: 0.9009327292442322\n",
      "Epoch 3, Batch 417/462, Loss: 0.8916948437690735\n",
      "Epoch 3, Batch 418/462, Loss: 0.8104794025421143\n",
      "Epoch 3, Batch 419/462, Loss: 0.7068730592727661\n",
      "Epoch 3, Batch 420/462, Loss: 0.8980395793914795\n",
      "Epoch 3, Batch 421/462, Loss: 0.8948447704315186\n",
      "Epoch 3, Batch 422/462, Loss: 0.7956304550170898\n",
      "Epoch 3, Batch 423/462, Loss: 0.9249978065490723\n",
      "Epoch 3, Batch 424/462, Loss: 0.8742812275886536\n",
      "Epoch 3, Batch 425/462, Loss: 0.8509158492088318\n",
      "Epoch 3, Batch 426/462, Loss: 0.8510717153549194\n",
      "Epoch 3, Batch 427/462, Loss: 0.9147652983665466\n",
      "Epoch 3, Batch 428/462, Loss: 0.9434879422187805\n",
      "Epoch 3, Batch 429/462, Loss: 0.927440881729126\n",
      "Epoch 3, Batch 430/462, Loss: 0.8871053457260132\n",
      "Epoch 3, Batch 431/462, Loss: 0.871549665927887\n",
      "Epoch 3, Batch 432/462, Loss: 0.9881384372711182\n",
      "Epoch 3, Batch 433/462, Loss: 0.8586372137069702\n",
      "Epoch 3, Batch 434/462, Loss: 0.9322423338890076\n",
      "Epoch 3, Batch 435/462, Loss: 0.8681344985961914\n",
      "Epoch 3, Batch 436/462, Loss: 0.9075513482093811\n",
      "Epoch 3, Batch 437/462, Loss: 0.9946551322937012\n",
      "Epoch 3, Batch 438/462, Loss: 0.9617553949356079\n",
      "Epoch 3, Batch 439/462, Loss: 0.8311833739280701\n",
      "Epoch 3, Batch 440/462, Loss: 0.7944247126579285\n",
      "Epoch 3, Batch 441/462, Loss: 0.8948132991790771\n",
      "Epoch 3, Batch 442/462, Loss: 0.8579936027526855\n",
      "Epoch 3, Batch 443/462, Loss: 0.8498504161834717\n",
      "Epoch 3, Batch 444/462, Loss: 0.8266484141349792\n",
      "Epoch 3, Batch 445/462, Loss: 0.8662089109420776\n",
      "Epoch 3, Batch 446/462, Loss: 0.9499155879020691\n",
      "Epoch 3, Batch 447/462, Loss: 0.9628258943557739\n",
      "Epoch 3, Batch 448/462, Loss: 1.1648294925689697\n",
      "Epoch 3, Batch 449/462, Loss: 0.9700351357460022\n",
      "Epoch 3, Batch 450/462, Loss: 0.8510309457778931\n",
      "Epoch 3, Batch 451/462, Loss: 0.9008665680885315\n",
      "Epoch 3, Batch 452/462, Loss: 0.9161890149116516\n",
      "Epoch 3, Batch 453/462, Loss: 0.9123784303665161\n",
      "Epoch 3, Batch 454/462, Loss: 1.007158875465393\n",
      "Epoch 3, Batch 455/462, Loss: 0.8713980317115784\n",
      "Epoch 3, Batch 456/462, Loss: 0.8711357712745667\n",
      "Epoch 3, Batch 457/462, Loss: 0.8605115413665771\n",
      "Epoch 3, Batch 458/462, Loss: 1.0594427585601807\n",
      "Epoch 3, Batch 459/462, Loss: 0.8916305303573608\n",
      "Epoch 3, Batch 460/462, Loss: 0.8857962489128113\n",
      "Epoch 3, Batch 461/462, Loss: 0.8909443020820618\n",
      "Epoch 3, Batch 462/462, Loss: 0.9762177467346191\n",
      "Epoch 3, Loss: 413.53223741054535\n",
      "Epoch 4, Batch 1/462, Loss: 0.8480382561683655\n",
      "Epoch 4, Batch 2/462, Loss: 0.8571329712867737\n",
      "Epoch 4, Batch 3/462, Loss: 1.0536748170852661\n",
      "Epoch 4, Batch 4/462, Loss: 0.9217330813407898\n",
      "Epoch 4, Batch 5/462, Loss: 0.9508347511291504\n",
      "Epoch 4, Batch 6/462, Loss: 1.0033478736877441\n",
      "Epoch 4, Batch 7/462, Loss: 0.7780334949493408\n",
      "Epoch 4, Batch 8/462, Loss: 0.8161442279815674\n",
      "Epoch 4, Batch 9/462, Loss: 0.8801367282867432\n",
      "Epoch 4, Batch 10/462, Loss: 0.7901883721351624\n",
      "Epoch 4, Batch 11/462, Loss: 0.9870824813842773\n",
      "Epoch 4, Batch 12/462, Loss: 0.8270219564437866\n",
      "Epoch 4, Batch 13/462, Loss: 0.8412458896636963\n",
      "Epoch 4, Batch 14/462, Loss: 0.9224791526794434\n",
      "Epoch 4, Batch 15/462, Loss: 0.9982268810272217\n",
      "Epoch 4, Batch 16/462, Loss: 0.8632670044898987\n",
      "Epoch 4, Batch 17/462, Loss: 0.8751837015151978\n",
      "Epoch 4, Batch 18/462, Loss: 0.6840788125991821\n",
      "Epoch 4, Batch 19/462, Loss: 0.9964768886566162\n",
      "Epoch 4, Batch 20/462, Loss: 0.875633716583252\n",
      "Epoch 4, Batch 21/462, Loss: 0.7390573024749756\n",
      "Epoch 4, Batch 22/462, Loss: 0.8845774531364441\n",
      "Epoch 4, Batch 23/462, Loss: 0.845291018486023\n",
      "Epoch 4, Batch 24/462, Loss: 0.9654728770256042\n",
      "Epoch 4, Batch 25/462, Loss: 0.8982294201850891\n",
      "Epoch 4, Batch 26/462, Loss: 0.7315059900283813\n",
      "Epoch 4, Batch 27/462, Loss: 0.9411178827285767\n",
      "Epoch 4, Batch 28/462, Loss: 0.7052629590034485\n",
      "Epoch 4, Batch 29/462, Loss: 1.0842578411102295\n",
      "Epoch 4, Batch 30/462, Loss: 0.9218252897262573\n",
      "Epoch 4, Batch 31/462, Loss: 0.9224217534065247\n",
      "Epoch 4, Batch 32/462, Loss: 0.8877525329589844\n",
      "Epoch 4, Batch 33/462, Loss: 0.991095781326294\n",
      "Epoch 4, Batch 34/462, Loss: 0.8564756512641907\n",
      "Epoch 4, Batch 35/462, Loss: 0.7305163145065308\n",
      "Epoch 4, Batch 36/462, Loss: 0.8709205389022827\n",
      "Epoch 4, Batch 37/462, Loss: 0.9401437044143677\n",
      "Epoch 4, Batch 38/462, Loss: 0.8705762624740601\n",
      "Epoch 4, Batch 39/462, Loss: 0.8851556777954102\n",
      "Epoch 4, Batch 40/462, Loss: 0.8517649173736572\n",
      "Epoch 4, Batch 41/462, Loss: 0.8088882565498352\n",
      "Epoch 4, Batch 42/462, Loss: 0.8705698847770691\n",
      "Epoch 4, Batch 43/462, Loss: 0.8575404286384583\n",
      "Epoch 4, Batch 44/462, Loss: 0.815061092376709\n",
      "Epoch 4, Batch 45/462, Loss: 0.88595050573349\n",
      "Epoch 4, Batch 46/462, Loss: 1.0251820087432861\n",
      "Epoch 4, Batch 47/462, Loss: 0.9506720304489136\n",
      "Epoch 4, Batch 48/462, Loss: 0.901080846786499\n",
      "Epoch 4, Batch 49/462, Loss: 0.8436093926429749\n",
      "Epoch 4, Batch 50/462, Loss: 0.9169042110443115\n",
      "Epoch 4, Batch 51/462, Loss: 0.7919027805328369\n",
      "Epoch 4, Batch 52/462, Loss: 0.8538050055503845\n",
      "Epoch 4, Batch 53/462, Loss: 1.007680892944336\n",
      "Epoch 4, Batch 54/462, Loss: 0.9355344176292419\n",
      "Epoch 4, Batch 55/462, Loss: 1.0480165481567383\n",
      "Epoch 4, Batch 56/462, Loss: 0.8348904252052307\n",
      "Epoch 4, Batch 57/462, Loss: 0.8333771228790283\n",
      "Epoch 4, Batch 58/462, Loss: 0.8234788179397583\n",
      "Epoch 4, Batch 59/462, Loss: 0.8302981853485107\n",
      "Epoch 4, Batch 60/462, Loss: 0.8575610518455505\n",
      "Epoch 4, Batch 61/462, Loss: 0.7995098829269409\n",
      "Epoch 4, Batch 62/462, Loss: 0.8485711216926575\n",
      "Epoch 4, Batch 63/462, Loss: 0.8780786395072937\n",
      "Epoch 4, Batch 64/462, Loss: 0.9701245427131653\n",
      "Epoch 4, Batch 65/462, Loss: 0.9342573285102844\n",
      "Epoch 4, Batch 66/462, Loss: 0.8911585807800293\n",
      "Epoch 4, Batch 67/462, Loss: 0.945639431476593\n",
      "Epoch 4, Batch 68/462, Loss: 0.9023705124855042\n",
      "Epoch 4, Batch 69/462, Loss: 0.847206175327301\n",
      "Epoch 4, Batch 70/462, Loss: 0.9555070996284485\n",
      "Epoch 4, Batch 71/462, Loss: 0.8173782229423523\n",
      "Epoch 4, Batch 72/462, Loss: 0.770494818687439\n",
      "Epoch 4, Batch 73/462, Loss: 0.8490223288536072\n",
      "Epoch 4, Batch 74/462, Loss: 0.8437190651893616\n",
      "Epoch 4, Batch 75/462, Loss: 0.8985296487808228\n",
      "Epoch 4, Batch 76/462, Loss: 0.8594717383384705\n",
      "Epoch 4, Batch 77/462, Loss: 0.902586817741394\n",
      "Epoch 4, Batch 78/462, Loss: 0.9623253345489502\n",
      "Epoch 4, Batch 79/462, Loss: 0.9333701133728027\n",
      "Epoch 4, Batch 80/462, Loss: 0.7823730111122131\n",
      "Epoch 4, Batch 81/462, Loss: 0.8090940117835999\n",
      "Epoch 4, Batch 82/462, Loss: 0.8520539999008179\n",
      "Epoch 4, Batch 83/462, Loss: 0.8745028376579285\n",
      "Epoch 4, Batch 84/462, Loss: 0.9224145412445068\n",
      "Epoch 4, Batch 85/462, Loss: 0.8192950487136841\n",
      "Epoch 4, Batch 86/462, Loss: 0.8123428225517273\n",
      "Epoch 4, Batch 87/462, Loss: 0.7791416049003601\n",
      "Epoch 4, Batch 88/462, Loss: 0.8913341164588928\n",
      "Epoch 4, Batch 89/462, Loss: 0.8579626083374023\n",
      "Epoch 4, Batch 90/462, Loss: 0.7160652279853821\n",
      "Epoch 4, Batch 91/462, Loss: 0.889445424079895\n",
      "Epoch 4, Batch 92/462, Loss: 0.7687667012214661\n",
      "Epoch 4, Batch 93/462, Loss: 0.9913525581359863\n",
      "Epoch 4, Batch 94/462, Loss: 0.8617121577262878\n",
      "Epoch 4, Batch 95/462, Loss: 0.9138253927230835\n",
      "Epoch 4, Batch 96/462, Loss: 0.8407968282699585\n",
      "Epoch 4, Batch 97/462, Loss: 0.8931047916412354\n",
      "Epoch 4, Batch 98/462, Loss: 0.8286361694335938\n",
      "Epoch 4, Batch 99/462, Loss: 0.8600658178329468\n",
      "Epoch 4, Batch 100/462, Loss: 0.9626588225364685\n",
      "Epoch 4, Batch 101/462, Loss: 0.8268761038780212\n",
      "Epoch 4, Batch 102/462, Loss: 0.9867019653320312\n",
      "Epoch 4, Batch 103/462, Loss: 0.984765350818634\n",
      "Epoch 4, Batch 104/462, Loss: 0.9571874141693115\n",
      "Epoch 4, Batch 105/462, Loss: 0.8520826697349548\n",
      "Epoch 4, Batch 106/462, Loss: 0.9237802624702454\n",
      "Epoch 4, Batch 107/462, Loss: 0.8343145251274109\n",
      "Epoch 4, Batch 108/462, Loss: 0.846433162689209\n",
      "Epoch 4, Batch 109/462, Loss: 0.9139552712440491\n",
      "Epoch 4, Batch 110/462, Loss: 0.9347155094146729\n",
      "Epoch 4, Batch 111/462, Loss: 0.7655846476554871\n",
      "Epoch 4, Batch 112/462, Loss: 0.9250949025154114\n",
      "Epoch 4, Batch 113/462, Loss: 0.7882729172706604\n",
      "Epoch 4, Batch 114/462, Loss: 0.9698903560638428\n",
      "Epoch 4, Batch 115/462, Loss: 0.7933138608932495\n",
      "Epoch 4, Batch 116/462, Loss: 0.919630765914917\n",
      "Epoch 4, Batch 117/462, Loss: 0.9891675114631653\n",
      "Epoch 4, Batch 118/462, Loss: 0.8893420696258545\n",
      "Epoch 4, Batch 119/462, Loss: 0.869072437286377\n",
      "Epoch 4, Batch 120/462, Loss: 0.7678229808807373\n",
      "Epoch 4, Batch 121/462, Loss: 0.9482111930847168\n",
      "Epoch 4, Batch 122/462, Loss: 0.8397462368011475\n",
      "Epoch 4, Batch 123/462, Loss: 0.917403519153595\n",
      "Epoch 4, Batch 124/462, Loss: 0.8878856301307678\n",
      "Epoch 4, Batch 125/462, Loss: 0.8496108055114746\n",
      "Epoch 4, Batch 126/462, Loss: 0.8167620301246643\n",
      "Epoch 4, Batch 127/462, Loss: 0.7776086926460266\n",
      "Epoch 4, Batch 128/462, Loss: 0.8860182762145996\n",
      "Epoch 4, Batch 129/462, Loss: 0.7841618061065674\n",
      "Epoch 4, Batch 130/462, Loss: 0.8575258255004883\n",
      "Epoch 4, Batch 131/462, Loss: 0.8663153648376465\n",
      "Epoch 4, Batch 132/462, Loss: 0.8126863837242126\n",
      "Epoch 4, Batch 133/462, Loss: 0.8824120759963989\n",
      "Epoch 4, Batch 134/462, Loss: 0.9737085700035095\n",
      "Epoch 4, Batch 135/462, Loss: 0.8724802136421204\n",
      "Epoch 4, Batch 136/462, Loss: 0.8297926187515259\n",
      "Epoch 4, Batch 137/462, Loss: 0.8252154588699341\n",
      "Epoch 4, Batch 138/462, Loss: 0.8899334073066711\n",
      "Epoch 4, Batch 139/462, Loss: 0.9273939728736877\n",
      "Epoch 4, Batch 140/462, Loss: 0.9385661482810974\n",
      "Epoch 4, Batch 141/462, Loss: 0.9627161622047424\n",
      "Epoch 4, Batch 142/462, Loss: 1.0482087135314941\n",
      "Epoch 4, Batch 143/462, Loss: 0.8066925406455994\n",
      "Epoch 4, Batch 144/462, Loss: 0.8066011667251587\n",
      "Epoch 4, Batch 145/462, Loss: 0.750617504119873\n",
      "Epoch 4, Batch 146/462, Loss: 0.9015993475914001\n",
      "Epoch 4, Batch 147/462, Loss: 0.8264335989952087\n",
      "Epoch 4, Batch 148/462, Loss: 1.0194644927978516\n",
      "Epoch 4, Batch 149/462, Loss: 0.9226232171058655\n",
      "Epoch 4, Batch 150/462, Loss: 0.9131600260734558\n",
      "Epoch 4, Batch 151/462, Loss: 0.9470822215080261\n",
      "Epoch 4, Batch 152/462, Loss: 0.8926221132278442\n",
      "Epoch 4, Batch 153/462, Loss: 1.0590143203735352\n",
      "Epoch 4, Batch 154/462, Loss: 1.0157804489135742\n",
      "Epoch 4, Batch 155/462, Loss: 0.93378746509552\n",
      "Epoch 4, Batch 156/462, Loss: 0.8576200008392334\n",
      "Epoch 4, Batch 157/462, Loss: 0.7437951564788818\n",
      "Epoch 4, Batch 158/462, Loss: 0.9076496958732605\n",
      "Epoch 4, Batch 159/462, Loss: 0.880618155002594\n",
      "Epoch 4, Batch 160/462, Loss: 0.7924125790596008\n",
      "Epoch 4, Batch 161/462, Loss: 0.8429920673370361\n",
      "Epoch 4, Batch 162/462, Loss: 0.8467702865600586\n",
      "Epoch 4, Batch 163/462, Loss: 1.1232593059539795\n",
      "Epoch 4, Batch 164/462, Loss: 0.7871803641319275\n",
      "Epoch 4, Batch 165/462, Loss: 0.9192970991134644\n",
      "Epoch 4, Batch 166/462, Loss: 0.8173283338546753\n",
      "Epoch 4, Batch 167/462, Loss: 0.8445162773132324\n",
      "Epoch 4, Batch 168/462, Loss: 0.8349863290786743\n",
      "Epoch 4, Batch 169/462, Loss: 0.9080708622932434\n",
      "Epoch 4, Batch 170/462, Loss: 0.7734835743904114\n",
      "Epoch 4, Batch 171/462, Loss: 0.9740927815437317\n",
      "Epoch 4, Batch 172/462, Loss: 0.874131441116333\n",
      "Epoch 4, Batch 173/462, Loss: 1.0002487897872925\n",
      "Epoch 4, Batch 174/462, Loss: 0.8047804236412048\n",
      "Epoch 4, Batch 175/462, Loss: 0.888022780418396\n",
      "Epoch 4, Batch 176/462, Loss: 1.0255893468856812\n",
      "Epoch 4, Batch 177/462, Loss: 0.8794273138046265\n",
      "Epoch 4, Batch 178/462, Loss: 0.8802049160003662\n",
      "Epoch 4, Batch 179/462, Loss: 0.7832280397415161\n",
      "Epoch 4, Batch 180/462, Loss: 0.9200367331504822\n",
      "Epoch 4, Batch 181/462, Loss: 0.9358644485473633\n",
      "Epoch 4, Batch 182/462, Loss: 0.7832024693489075\n",
      "Epoch 4, Batch 183/462, Loss: 0.9277386665344238\n",
      "Epoch 4, Batch 184/462, Loss: 0.8184422254562378\n",
      "Epoch 4, Batch 185/462, Loss: 1.006122350692749\n",
      "Epoch 4, Batch 186/462, Loss: 0.9906137585639954\n",
      "Epoch 4, Batch 187/462, Loss: 0.8693147301673889\n",
      "Epoch 4, Batch 188/462, Loss: 0.8862852454185486\n",
      "Epoch 4, Batch 189/462, Loss: 0.8767768144607544\n",
      "Epoch 4, Batch 190/462, Loss: 0.7936723232269287\n",
      "Epoch 4, Batch 191/462, Loss: 0.8273599743843079\n",
      "Epoch 4, Batch 192/462, Loss: 0.8806044459342957\n",
      "Epoch 4, Batch 193/462, Loss: 0.9185670614242554\n",
      "Epoch 4, Batch 194/462, Loss: 0.8268987536430359\n",
      "Epoch 4, Batch 195/462, Loss: 0.9392040371894836\n",
      "Epoch 4, Batch 196/462, Loss: 0.9393454790115356\n",
      "Epoch 4, Batch 197/462, Loss: 0.8911034464836121\n",
      "Epoch 4, Batch 198/462, Loss: 0.7324388027191162\n",
      "Epoch 4, Batch 199/462, Loss: 0.8822368383407593\n",
      "Epoch 4, Batch 200/462, Loss: 0.6886118054389954\n",
      "Epoch 4, Batch 201/462, Loss: 0.936955451965332\n",
      "Epoch 4, Batch 202/462, Loss: 0.8616209030151367\n",
      "Epoch 4, Batch 203/462, Loss: 0.8270832300186157\n",
      "Epoch 4, Batch 204/462, Loss: 1.026403546333313\n",
      "Epoch 4, Batch 205/462, Loss: 0.9812390804290771\n",
      "Epoch 4, Batch 206/462, Loss: 0.8787378072738647\n",
      "Epoch 4, Batch 207/462, Loss: 0.8739022612571716\n",
      "Epoch 4, Batch 208/462, Loss: 0.7107819318771362\n",
      "Epoch 4, Batch 209/462, Loss: 0.8197493553161621\n",
      "Epoch 4, Batch 210/462, Loss: 0.7621888518333435\n",
      "Epoch 4, Batch 211/462, Loss: 0.8670101165771484\n",
      "Epoch 4, Batch 212/462, Loss: 1.073671579360962\n",
      "Epoch 4, Batch 213/462, Loss: 0.9269136786460876\n",
      "Epoch 4, Batch 214/462, Loss: 0.8908513188362122\n",
      "Epoch 4, Batch 215/462, Loss: 0.8606483340263367\n",
      "Epoch 4, Batch 216/462, Loss: 0.767053484916687\n",
      "Epoch 4, Batch 217/462, Loss: 0.9352962970733643\n",
      "Epoch 4, Batch 218/462, Loss: 0.8380398750305176\n",
      "Epoch 4, Batch 219/462, Loss: 0.7971033453941345\n",
      "Epoch 4, Batch 220/462, Loss: 0.9206116199493408\n",
      "Epoch 4, Batch 221/462, Loss: 0.7958289980888367\n",
      "Epoch 4, Batch 222/462, Loss: 0.8101661801338196\n",
      "Epoch 4, Batch 223/462, Loss: 0.8754425644874573\n",
      "Epoch 4, Batch 224/462, Loss: 0.9015917181968689\n",
      "Epoch 4, Batch 225/462, Loss: 0.9078278541564941\n",
      "Epoch 4, Batch 226/462, Loss: 0.7752377390861511\n",
      "Epoch 4, Batch 227/462, Loss: 1.0475856065750122\n",
      "Epoch 4, Batch 228/462, Loss: 0.8018442988395691\n",
      "Epoch 4, Batch 229/462, Loss: 0.8098349571228027\n",
      "Epoch 4, Batch 230/462, Loss: 0.9251695871353149\n",
      "Epoch 4, Batch 231/462, Loss: 0.9040316343307495\n",
      "Epoch 4, Batch 232/462, Loss: 0.9702501893043518\n",
      "Epoch 4, Batch 233/462, Loss: 0.7243474721908569\n",
      "Epoch 4, Batch 234/462, Loss: 0.8449662327766418\n",
      "Epoch 4, Batch 235/462, Loss: 0.8900482654571533\n",
      "Epoch 4, Batch 236/462, Loss: 0.8630498647689819\n",
      "Epoch 4, Batch 237/462, Loss: 0.8708986639976501\n",
      "Epoch 4, Batch 238/462, Loss: 0.7922572493553162\n",
      "Epoch 4, Batch 239/462, Loss: 0.8755979537963867\n",
      "Epoch 4, Batch 240/462, Loss: 0.7278276681900024\n",
      "Epoch 4, Batch 241/462, Loss: 0.9755614995956421\n",
      "Epoch 4, Batch 242/462, Loss: 0.7649187445640564\n",
      "Epoch 4, Batch 243/462, Loss: 0.9736053347587585\n",
      "Epoch 4, Batch 244/462, Loss: 0.8532077670097351\n",
      "Epoch 4, Batch 245/462, Loss: 0.8745827078819275\n",
      "Epoch 4, Batch 246/462, Loss: 0.8828315734863281\n",
      "Epoch 4, Batch 247/462, Loss: 0.8122238516807556\n",
      "Epoch 4, Batch 248/462, Loss: 0.8440589904785156\n",
      "Epoch 4, Batch 249/462, Loss: 0.9754337668418884\n",
      "Epoch 4, Batch 250/462, Loss: 0.8001750707626343\n",
      "Epoch 4, Batch 251/462, Loss: 0.8301316499710083\n",
      "Epoch 4, Batch 252/462, Loss: 0.8730403184890747\n",
      "Epoch 4, Batch 253/462, Loss: 0.7943492531776428\n",
      "Epoch 4, Batch 254/462, Loss: 0.9388613700866699\n",
      "Epoch 4, Batch 255/462, Loss: 0.9748756289482117\n",
      "Epoch 4, Batch 256/462, Loss: 0.876727283000946\n",
      "Epoch 4, Batch 257/462, Loss: 0.8325257301330566\n",
      "Epoch 4, Batch 258/462, Loss: 0.7820345163345337\n",
      "Epoch 4, Batch 259/462, Loss: 1.0285472869873047\n",
      "Epoch 4, Batch 260/462, Loss: 0.856806218624115\n",
      "Epoch 4, Batch 261/462, Loss: 0.8970921039581299\n",
      "Epoch 4, Batch 262/462, Loss: 0.8265604376792908\n",
      "Epoch 4, Batch 263/462, Loss: 0.8861897587776184\n",
      "Epoch 4, Batch 264/462, Loss: 0.8537987470626831\n",
      "Epoch 4, Batch 265/462, Loss: 1.024216651916504\n",
      "Epoch 4, Batch 266/462, Loss: 0.8536041378974915\n",
      "Epoch 4, Batch 267/462, Loss: 0.9588947296142578\n",
      "Epoch 4, Batch 268/462, Loss: 0.8720342516899109\n",
      "Epoch 4, Batch 269/462, Loss: 0.6715899705886841\n",
      "Epoch 4, Batch 270/462, Loss: 0.7363309264183044\n",
      "Epoch 4, Batch 271/462, Loss: 0.7777246236801147\n",
      "Epoch 4, Batch 272/462, Loss: 0.826964795589447\n",
      "Epoch 4, Batch 273/462, Loss: 0.9797001481056213\n",
      "Epoch 4, Batch 274/462, Loss: 0.8108965754508972\n",
      "Epoch 4, Batch 275/462, Loss: 0.7990357279777527\n",
      "Epoch 4, Batch 276/462, Loss: 0.8318616151809692\n",
      "Epoch 4, Batch 277/462, Loss: 0.9112520813941956\n",
      "Epoch 4, Batch 278/462, Loss: 0.966164231300354\n",
      "Epoch 4, Batch 279/462, Loss: 0.9055323600769043\n",
      "Epoch 4, Batch 280/462, Loss: 0.842969536781311\n",
      "Epoch 4, Batch 281/462, Loss: 0.917454719543457\n",
      "Epoch 4, Batch 282/462, Loss: 0.9293741583824158\n",
      "Epoch 4, Batch 283/462, Loss: 0.8355406522750854\n",
      "Epoch 4, Batch 284/462, Loss: 0.8221883773803711\n",
      "Epoch 4, Batch 285/462, Loss: 0.8497251272201538\n",
      "Epoch 4, Batch 286/462, Loss: 0.9006000757217407\n",
      "Epoch 4, Batch 287/462, Loss: 0.9909682869911194\n",
      "Epoch 4, Batch 288/462, Loss: 0.7837361097335815\n",
      "Epoch 4, Batch 289/462, Loss: 1.02044677734375\n",
      "Epoch 4, Batch 290/462, Loss: 0.9302753806114197\n",
      "Epoch 4, Batch 291/462, Loss: 1.010589838027954\n",
      "Epoch 4, Batch 292/462, Loss: 0.9493197798728943\n",
      "Epoch 4, Batch 293/462, Loss: 0.9535319209098816\n",
      "Epoch 4, Batch 294/462, Loss: 0.9177318215370178\n",
      "Epoch 4, Batch 295/462, Loss: 0.9118335247039795\n",
      "Epoch 4, Batch 296/462, Loss: 0.7860814332962036\n",
      "Epoch 4, Batch 297/462, Loss: 0.8232508301734924\n",
      "Epoch 4, Batch 298/462, Loss: 0.8098315596580505\n",
      "Epoch 4, Batch 299/462, Loss: 0.7831699848175049\n",
      "Epoch 4, Batch 300/462, Loss: 0.971858024597168\n",
      "Epoch 4, Batch 301/462, Loss: 0.878371000289917\n",
      "Epoch 4, Batch 302/462, Loss: 1.0060206651687622\n",
      "Epoch 4, Batch 303/462, Loss: 0.8292468786239624\n",
      "Epoch 4, Batch 304/462, Loss: 0.8414344787597656\n",
      "Epoch 4, Batch 305/462, Loss: 0.8701627850532532\n",
      "Epoch 4, Batch 306/462, Loss: 0.8088376522064209\n",
      "Epoch 4, Batch 307/462, Loss: 0.8445451259613037\n",
      "Epoch 4, Batch 308/462, Loss: 0.8380125164985657\n",
      "Epoch 4, Batch 309/462, Loss: 0.8153079748153687\n",
      "Epoch 4, Batch 310/462, Loss: 1.018531084060669\n",
      "Epoch 4, Batch 311/462, Loss: 0.8339993953704834\n",
      "Epoch 4, Batch 312/462, Loss: 0.8023265600204468\n",
      "Epoch 4, Batch 313/462, Loss: 0.9110351800918579\n",
      "Epoch 4, Batch 314/462, Loss: 0.8598638772964478\n",
      "Epoch 4, Batch 315/462, Loss: 0.94919353723526\n",
      "Epoch 4, Batch 316/462, Loss: 0.8878921866416931\n",
      "Epoch 4, Batch 317/462, Loss: 0.7822388410568237\n",
      "Epoch 4, Batch 318/462, Loss: 0.707694947719574\n",
      "Epoch 4, Batch 319/462, Loss: 1.0490570068359375\n",
      "Epoch 4, Batch 320/462, Loss: 0.810194194316864\n",
      "Epoch 4, Batch 321/462, Loss: 0.8534519076347351\n",
      "Epoch 4, Batch 322/462, Loss: 0.8641824722290039\n",
      "Epoch 4, Batch 323/462, Loss: 0.9675627946853638\n",
      "Epoch 4, Batch 324/462, Loss: 0.8482195138931274\n",
      "Epoch 4, Batch 325/462, Loss: 0.8374745845794678\n",
      "Epoch 4, Batch 326/462, Loss: 0.8974100947380066\n",
      "Epoch 4, Batch 327/462, Loss: 0.8340131044387817\n",
      "Epoch 4, Batch 328/462, Loss: 0.8137726783752441\n",
      "Epoch 4, Batch 329/462, Loss: 0.797501802444458\n",
      "Epoch 4, Batch 330/462, Loss: 0.916228711605072\n",
      "Epoch 4, Batch 331/462, Loss: 0.8228940367698669\n",
      "Epoch 4, Batch 332/462, Loss: 0.8112362623214722\n",
      "Epoch 4, Batch 333/462, Loss: 0.9867960214614868\n",
      "Epoch 4, Batch 334/462, Loss: 0.7933476567268372\n",
      "Epoch 4, Batch 335/462, Loss: 0.8677253723144531\n",
      "Epoch 4, Batch 336/462, Loss: 0.9520323872566223\n",
      "Epoch 4, Batch 337/462, Loss: 0.940697431564331\n",
      "Epoch 4, Batch 338/462, Loss: 0.8855248689651489\n",
      "Epoch 4, Batch 339/462, Loss: 0.9672036170959473\n",
      "Epoch 4, Batch 340/462, Loss: 0.8804728984832764\n",
      "Epoch 4, Batch 341/462, Loss: 1.0002281665802002\n",
      "Epoch 4, Batch 342/462, Loss: 0.8163750171661377\n",
      "Epoch 4, Batch 343/462, Loss: 0.8231550455093384\n",
      "Epoch 4, Batch 344/462, Loss: 0.8232236504554749\n",
      "Epoch 4, Batch 345/462, Loss: 0.7901018857955933\n",
      "Epoch 4, Batch 346/462, Loss: 0.8448734283447266\n",
      "Epoch 4, Batch 347/462, Loss: 0.865132749080658\n",
      "Epoch 4, Batch 348/462, Loss: 0.8934590220451355\n",
      "Epoch 4, Batch 349/462, Loss: 0.8622406125068665\n",
      "Epoch 4, Batch 350/462, Loss: 0.7891831994056702\n",
      "Epoch 4, Batch 351/462, Loss: 0.8334774374961853\n",
      "Epoch 4, Batch 352/462, Loss: 0.9944742918014526\n",
      "Epoch 4, Batch 353/462, Loss: 0.8284134864807129\n",
      "Epoch 4, Batch 354/462, Loss: 0.9514869451522827\n",
      "Epoch 4, Batch 355/462, Loss: 0.965350329875946\n",
      "Epoch 4, Batch 356/462, Loss: 1.0150816440582275\n",
      "Epoch 4, Batch 357/462, Loss: 0.7464669942855835\n",
      "Epoch 4, Batch 358/462, Loss: 0.8745445609092712\n",
      "Epoch 4, Batch 359/462, Loss: 0.9041529297828674\n",
      "Epoch 4, Batch 360/462, Loss: 0.8396382927894592\n",
      "Epoch 4, Batch 361/462, Loss: 0.9729938507080078\n",
      "Epoch 4, Batch 362/462, Loss: 0.8558170795440674\n",
      "Epoch 4, Batch 363/462, Loss: 0.6803648471832275\n",
      "Epoch 4, Batch 364/462, Loss: 0.8748671412467957\n",
      "Epoch 4, Batch 365/462, Loss: 0.9798728227615356\n",
      "Epoch 4, Batch 366/462, Loss: 0.9564934372901917\n",
      "Epoch 4, Batch 367/462, Loss: 0.8544529676437378\n",
      "Epoch 4, Batch 368/462, Loss: 0.883217990398407\n",
      "Epoch 4, Batch 369/462, Loss: 0.8434165120124817\n",
      "Epoch 4, Batch 370/462, Loss: 0.8298898935317993\n",
      "Epoch 4, Batch 371/462, Loss: 0.8112217783927917\n",
      "Epoch 4, Batch 372/462, Loss: 0.8791349530220032\n",
      "Epoch 4, Batch 373/462, Loss: 0.8722137808799744\n",
      "Epoch 4, Batch 374/462, Loss: 0.7243454456329346\n",
      "Epoch 4, Batch 375/462, Loss: 0.7851542234420776\n",
      "Epoch 4, Batch 376/462, Loss: 0.9394935965538025\n",
      "Epoch 4, Batch 377/462, Loss: 0.9320497512817383\n",
      "Epoch 4, Batch 378/462, Loss: 0.6975301504135132\n",
      "Epoch 4, Batch 379/462, Loss: 0.8792463541030884\n",
      "Epoch 4, Batch 380/462, Loss: 0.8681315183639526\n",
      "Epoch 4, Batch 381/462, Loss: 0.7631508708000183\n",
      "Epoch 4, Batch 382/462, Loss: 0.9628685712814331\n",
      "Epoch 4, Batch 383/462, Loss: 0.7664722204208374\n",
      "Epoch 4, Batch 384/462, Loss: 0.8204876780509949\n",
      "Epoch 4, Batch 385/462, Loss: 1.0824187994003296\n",
      "Epoch 4, Batch 386/462, Loss: 0.8703725934028625\n",
      "Epoch 4, Batch 387/462, Loss: 0.8735255002975464\n",
      "Epoch 4, Batch 388/462, Loss: 0.6582759022712708\n",
      "Epoch 4, Batch 389/462, Loss: 0.8760965466499329\n",
      "Epoch 4, Batch 390/462, Loss: 0.8759058117866516\n",
      "Epoch 4, Batch 391/462, Loss: 0.8895119428634644\n",
      "Epoch 4, Batch 392/462, Loss: 0.7650293111801147\n",
      "Epoch 4, Batch 393/462, Loss: 0.9284543991088867\n",
      "Epoch 4, Batch 394/462, Loss: 0.7030941247940063\n",
      "Epoch 4, Batch 395/462, Loss: 0.817020833492279\n",
      "Epoch 4, Batch 396/462, Loss: 0.8623204231262207\n",
      "Epoch 4, Batch 397/462, Loss: 0.7766157984733582\n",
      "Epoch 4, Batch 398/462, Loss: 0.900250256061554\n",
      "Epoch 4, Batch 399/462, Loss: 0.8535477519035339\n",
      "Epoch 4, Batch 400/462, Loss: 0.9886593818664551\n",
      "Epoch 4, Batch 401/462, Loss: 0.7282025218009949\n",
      "Epoch 4, Batch 402/462, Loss: 0.8912604451179504\n",
      "Epoch 4, Batch 403/462, Loss: 0.9542993307113647\n",
      "Epoch 4, Batch 404/462, Loss: 0.8787838220596313\n",
      "Epoch 4, Batch 405/462, Loss: 1.119092583656311\n",
      "Epoch 4, Batch 406/462, Loss: 0.9354544281959534\n",
      "Epoch 4, Batch 407/462, Loss: 0.7952761054039001\n",
      "Epoch 4, Batch 408/462, Loss: 0.9441362619400024\n",
      "Epoch 4, Batch 409/462, Loss: 0.8879122138023376\n",
      "Epoch 4, Batch 410/462, Loss: 0.9033332467079163\n",
      "Epoch 4, Batch 411/462, Loss: 0.8648515939712524\n",
      "Epoch 4, Batch 412/462, Loss: 0.8506649732589722\n",
      "Epoch 4, Batch 413/462, Loss: 0.8056600093841553\n",
      "Epoch 4, Batch 414/462, Loss: 0.929047703742981\n",
      "Epoch 4, Batch 415/462, Loss: 0.9864416122436523\n",
      "Epoch 4, Batch 416/462, Loss: 0.7599949836730957\n",
      "Epoch 4, Batch 417/462, Loss: 0.8787034749984741\n",
      "Epoch 4, Batch 418/462, Loss: 1.0071606636047363\n",
      "Epoch 4, Batch 419/462, Loss: 0.9045050740242004\n",
      "Epoch 4, Batch 420/462, Loss: 0.8414129018783569\n",
      "Epoch 4, Batch 421/462, Loss: 0.9999653100967407\n",
      "Epoch 4, Batch 422/462, Loss: 0.8903326988220215\n",
      "Epoch 4, Batch 423/462, Loss: 0.9812281131744385\n",
      "Epoch 4, Batch 424/462, Loss: 0.886066198348999\n",
      "Epoch 4, Batch 425/462, Loss: 0.8263054490089417\n",
      "Epoch 4, Batch 426/462, Loss: 1.0790905952453613\n",
      "Epoch 4, Batch 427/462, Loss: 0.8892329931259155\n",
      "Epoch 4, Batch 428/462, Loss: 0.8390664458274841\n",
      "Epoch 4, Batch 429/462, Loss: 0.8073510527610779\n",
      "Epoch 4, Batch 430/462, Loss: 0.8289622068405151\n",
      "Epoch 4, Batch 431/462, Loss: 0.7875018119812012\n",
      "Epoch 4, Batch 432/462, Loss: 0.8978331685066223\n",
      "Epoch 4, Batch 433/462, Loss: 0.9488022923469543\n",
      "Epoch 4, Batch 434/462, Loss: 0.8455764055252075\n",
      "Epoch 4, Batch 435/462, Loss: 0.7935108542442322\n",
      "Epoch 4, Batch 436/462, Loss: 0.8206567764282227\n",
      "Epoch 4, Batch 437/462, Loss: 0.7446010708808899\n",
      "Epoch 4, Batch 438/462, Loss: 0.8269640803337097\n",
      "Epoch 4, Batch 439/462, Loss: 0.8231996893882751\n",
      "Epoch 4, Batch 440/462, Loss: 0.8076419830322266\n",
      "Epoch 4, Batch 441/462, Loss: 1.0315433740615845\n",
      "Epoch 4, Batch 442/462, Loss: 0.9892368316650391\n",
      "Epoch 4, Batch 443/462, Loss: 0.8034051656723022\n",
      "Epoch 4, Batch 444/462, Loss: 0.893459677696228\n",
      "Epoch 4, Batch 445/462, Loss: 0.8291462063789368\n",
      "Epoch 4, Batch 446/462, Loss: 0.8427253365516663\n",
      "Epoch 4, Batch 447/462, Loss: 0.7516504526138306\n",
      "Epoch 4, Batch 448/462, Loss: 0.8832927346229553\n",
      "Epoch 4, Batch 449/462, Loss: 0.8643641471862793\n",
      "Epoch 4, Batch 450/462, Loss: 0.758741557598114\n",
      "Epoch 4, Batch 451/462, Loss: 0.8814458847045898\n",
      "Epoch 4, Batch 452/462, Loss: 0.9722117781639099\n",
      "Epoch 4, Batch 453/462, Loss: 0.8735063076019287\n",
      "Epoch 4, Batch 454/462, Loss: 0.7827576398849487\n",
      "Epoch 4, Batch 455/462, Loss: 0.8967307806015015\n",
      "Epoch 4, Batch 456/462, Loss: 0.7288593053817749\n",
      "Epoch 4, Batch 457/462, Loss: 0.7730290293693542\n",
      "Epoch 4, Batch 458/462, Loss: 0.8956748843193054\n",
      "Epoch 4, Batch 459/462, Loss: 0.8885806202888489\n",
      "Epoch 4, Batch 460/462, Loss: 0.8259267807006836\n",
      "Epoch 4, Batch 461/462, Loss: 0.8151460886001587\n",
      "Epoch 4, Batch 462/462, Loss: 0.8400961756706238\n",
      "Epoch 4, Loss: 403.8261062502861\n",
      "Epoch 5, Batch 1/462, Loss: 0.9140229821205139\n",
      "Epoch 5, Batch 2/462, Loss: 0.7991632223129272\n",
      "Epoch 5, Batch 3/462, Loss: 0.8414707779884338\n",
      "Epoch 5, Batch 4/462, Loss: 0.8636664152145386\n",
      "Epoch 5, Batch 5/462, Loss: 0.7757549285888672\n",
      "Epoch 5, Batch 6/462, Loss: 0.7607147097587585\n",
      "Epoch 5, Batch 7/462, Loss: 0.9294577836990356\n",
      "Epoch 5, Batch 8/462, Loss: 0.9476147294044495\n",
      "Epoch 5, Batch 9/462, Loss: 0.8553523421287537\n",
      "Epoch 5, Batch 10/462, Loss: 0.839031457901001\n",
      "Epoch 5, Batch 11/462, Loss: 0.8436256051063538\n",
      "Epoch 5, Batch 12/462, Loss: 0.7815291285514832\n",
      "Epoch 5, Batch 13/462, Loss: 0.8995223045349121\n",
      "Epoch 5, Batch 14/462, Loss: 0.929092526435852\n",
      "Epoch 5, Batch 15/462, Loss: 0.9396611452102661\n",
      "Epoch 5, Batch 16/462, Loss: 1.0346719026565552\n",
      "Epoch 5, Batch 17/462, Loss: 0.8908647894859314\n",
      "Epoch 5, Batch 18/462, Loss: 0.9194750189781189\n",
      "Epoch 5, Batch 19/462, Loss: 0.9754814505577087\n",
      "Epoch 5, Batch 20/462, Loss: 1.0578044652938843\n",
      "Epoch 5, Batch 21/462, Loss: 0.7588597536087036\n",
      "Epoch 5, Batch 22/462, Loss: 0.7457993626594543\n",
      "Epoch 5, Batch 23/462, Loss: 0.7862923741340637\n",
      "Epoch 5, Batch 24/462, Loss: 0.803894579410553\n",
      "Epoch 5, Batch 25/462, Loss: 1.0167391300201416\n",
      "Epoch 5, Batch 26/462, Loss: 0.8886251449584961\n",
      "Epoch 5, Batch 27/462, Loss: 0.9146305322647095\n",
      "Epoch 5, Batch 28/462, Loss: 0.7660027742385864\n",
      "Epoch 5, Batch 29/462, Loss: 0.8660074472427368\n",
      "Epoch 5, Batch 30/462, Loss: 0.8268780708312988\n",
      "Epoch 5, Batch 31/462, Loss: 0.8441963791847229\n",
      "Epoch 5, Batch 32/462, Loss: 0.8882805109024048\n",
      "Epoch 5, Batch 33/462, Loss: 0.800428569316864\n",
      "Epoch 5, Batch 34/462, Loss: 0.7771100997924805\n",
      "Epoch 5, Batch 35/462, Loss: 0.9321725368499756\n",
      "Epoch 5, Batch 36/462, Loss: 0.8260338306427002\n",
      "Epoch 5, Batch 37/462, Loss: 0.6601428389549255\n",
      "Epoch 5, Batch 38/462, Loss: 0.8860582113265991\n",
      "Epoch 5, Batch 39/462, Loss: 0.8617855310440063\n",
      "Epoch 5, Batch 40/462, Loss: 0.8252725601196289\n",
      "Epoch 5, Batch 41/462, Loss: 0.9925915598869324\n",
      "Epoch 5, Batch 42/462, Loss: 0.9191767573356628\n",
      "Epoch 5, Batch 43/462, Loss: 0.8056263327598572\n",
      "Epoch 5, Batch 44/462, Loss: 0.8626689314842224\n",
      "Epoch 5, Batch 45/462, Loss: 0.6338494420051575\n",
      "Epoch 5, Batch 46/462, Loss: 0.8108652830123901\n",
      "Epoch 5, Batch 47/462, Loss: 0.8584495186805725\n",
      "Epoch 5, Batch 48/462, Loss: 0.7581130266189575\n",
      "Epoch 5, Batch 49/462, Loss: 0.8632091879844666\n",
      "Epoch 5, Batch 50/462, Loss: 0.9270581007003784\n",
      "Epoch 5, Batch 51/462, Loss: 0.8474074602127075\n",
      "Epoch 5, Batch 52/462, Loss: 0.7884617447853088\n",
      "Epoch 5, Batch 53/462, Loss: 0.8100223541259766\n",
      "Epoch 5, Batch 54/462, Loss: 0.8301286697387695\n",
      "Epoch 5, Batch 55/462, Loss: 1.0497792959213257\n",
      "Epoch 5, Batch 56/462, Loss: 0.7622628211975098\n",
      "Epoch 5, Batch 57/462, Loss: 0.9331092834472656\n",
      "Epoch 5, Batch 58/462, Loss: 0.7118954658508301\n",
      "Epoch 5, Batch 59/462, Loss: 0.8287992477416992\n",
      "Epoch 5, Batch 60/462, Loss: 1.0194478034973145\n",
      "Epoch 5, Batch 61/462, Loss: 0.9198166131973267\n",
      "Epoch 5, Batch 62/462, Loss: 0.7490602731704712\n",
      "Epoch 5, Batch 63/462, Loss: 0.9350863695144653\n",
      "Epoch 5, Batch 64/462, Loss: 0.916500985622406\n",
      "Epoch 5, Batch 65/462, Loss: 0.8765142560005188\n",
      "Epoch 5, Batch 66/462, Loss: 0.8677924871444702\n",
      "Epoch 5, Batch 67/462, Loss: 0.7079692482948303\n",
      "Epoch 5, Batch 68/462, Loss: 0.7640964388847351\n",
      "Epoch 5, Batch 69/462, Loss: 0.7647990584373474\n",
      "Epoch 5, Batch 70/462, Loss: 0.8770155906677246\n",
      "Epoch 5, Batch 71/462, Loss: 0.9259312748908997\n",
      "Epoch 5, Batch 72/462, Loss: 0.971203625202179\n",
      "Epoch 5, Batch 73/462, Loss: 0.9503653049468994\n",
      "Epoch 5, Batch 74/462, Loss: 0.8976728320121765\n",
      "Epoch 5, Batch 75/462, Loss: 1.0315499305725098\n",
      "Epoch 5, Batch 76/462, Loss: 0.9281983375549316\n",
      "Epoch 5, Batch 77/462, Loss: 0.8153045177459717\n",
      "Epoch 5, Batch 78/462, Loss: 0.855847179889679\n",
      "Epoch 5, Batch 79/462, Loss: 0.7934324741363525\n",
      "Epoch 5, Batch 80/462, Loss: 0.8590604662895203\n",
      "Epoch 5, Batch 81/462, Loss: 0.7935378551483154\n",
      "Epoch 5, Batch 82/462, Loss: 0.9321831464767456\n",
      "Epoch 5, Batch 83/462, Loss: 0.8293543457984924\n",
      "Epoch 5, Batch 84/462, Loss: 0.8743887543678284\n",
      "Epoch 5, Batch 85/462, Loss: 1.00523042678833\n",
      "Epoch 5, Batch 86/462, Loss: 0.9975002408027649\n",
      "Epoch 5, Batch 87/462, Loss: 0.7166145443916321\n",
      "Epoch 5, Batch 88/462, Loss: 0.8008372783660889\n",
      "Epoch 5, Batch 89/462, Loss: 1.0164874792099\n",
      "Epoch 5, Batch 90/462, Loss: 0.8938074111938477\n",
      "Epoch 5, Batch 91/462, Loss: 1.0366489887237549\n",
      "Epoch 5, Batch 92/462, Loss: 0.9101436734199524\n",
      "Epoch 5, Batch 93/462, Loss: 0.8116472363471985\n",
      "Epoch 5, Batch 94/462, Loss: 0.8572673201560974\n",
      "Epoch 5, Batch 95/462, Loss: 0.9183749556541443\n",
      "Epoch 5, Batch 96/462, Loss: 0.8260223865509033\n",
      "Epoch 5, Batch 97/462, Loss: 0.9265967011451721\n",
      "Epoch 5, Batch 98/462, Loss: 0.8748301267623901\n",
      "Epoch 5, Batch 99/462, Loss: 0.9351809620857239\n",
      "Epoch 5, Batch 100/462, Loss: 0.9485235810279846\n",
      "Epoch 5, Batch 101/462, Loss: 0.8799508810043335\n",
      "Epoch 5, Batch 102/462, Loss: 0.7711628675460815\n",
      "Epoch 5, Batch 103/462, Loss: 0.7531068325042725\n",
      "Epoch 5, Batch 104/462, Loss: 0.8362749814987183\n",
      "Epoch 5, Batch 105/462, Loss: 0.8643997311592102\n",
      "Epoch 5, Batch 106/462, Loss: 0.8754770755767822\n",
      "Epoch 5, Batch 107/462, Loss: 0.8219922780990601\n",
      "Epoch 5, Batch 108/462, Loss: 0.9255691766738892\n",
      "Epoch 5, Batch 109/462, Loss: 0.8842929005622864\n",
      "Epoch 5, Batch 110/462, Loss: 0.8800148963928223\n",
      "Epoch 5, Batch 111/462, Loss: 0.8578451871871948\n",
      "Epoch 5, Batch 112/462, Loss: 0.9150996208190918\n",
      "Epoch 5, Batch 113/462, Loss: 0.9782171845436096\n",
      "Epoch 5, Batch 114/462, Loss: 0.8109036684036255\n",
      "Epoch 5, Batch 115/462, Loss: 0.8165633082389832\n",
      "Epoch 5, Batch 116/462, Loss: 0.8064180612564087\n",
      "Epoch 5, Batch 117/462, Loss: 0.8512243628501892\n",
      "Epoch 5, Batch 118/462, Loss: 0.856613039970398\n",
      "Epoch 5, Batch 119/462, Loss: 0.8270689249038696\n",
      "Epoch 5, Batch 120/462, Loss: 0.8967165946960449\n",
      "Epoch 5, Batch 121/462, Loss: 0.9080729484558105\n",
      "Epoch 5, Batch 122/462, Loss: 0.8228217959403992\n",
      "Epoch 5, Batch 123/462, Loss: 0.9028334617614746\n",
      "Epoch 5, Batch 124/462, Loss: 0.9003027081489563\n",
      "Epoch 5, Batch 125/462, Loss: 0.9152418375015259\n",
      "Epoch 5, Batch 126/462, Loss: 0.8524572849273682\n",
      "Epoch 5, Batch 127/462, Loss: 0.7364194393157959\n",
      "Epoch 5, Batch 128/462, Loss: 0.9318068027496338\n",
      "Epoch 5, Batch 129/462, Loss: 0.7861130833625793\n",
      "Epoch 5, Batch 130/462, Loss: 0.9026187658309937\n",
      "Epoch 5, Batch 131/462, Loss: 0.9059299230575562\n",
      "Epoch 5, Batch 132/462, Loss: 0.8543424010276794\n",
      "Epoch 5, Batch 133/462, Loss: 0.7683455348014832\n",
      "Epoch 5, Batch 134/462, Loss: 0.7832192182540894\n",
      "Epoch 5, Batch 135/462, Loss: 0.8851132392883301\n",
      "Epoch 5, Batch 136/462, Loss: 0.7683391571044922\n",
      "Epoch 5, Batch 137/462, Loss: 0.9252135753631592\n",
      "Epoch 5, Batch 138/462, Loss: 0.9762430787086487\n",
      "Epoch 5, Batch 139/462, Loss: 0.9081985354423523\n",
      "Epoch 5, Batch 140/462, Loss: 0.8918607831001282\n",
      "Epoch 5, Batch 141/462, Loss: 0.8492745161056519\n",
      "Epoch 5, Batch 142/462, Loss: 1.001213550567627\n",
      "Epoch 5, Batch 143/462, Loss: 1.0262449979782104\n",
      "Epoch 5, Batch 144/462, Loss: 0.9348590970039368\n",
      "Epoch 5, Batch 145/462, Loss: 0.7557830810546875\n",
      "Epoch 5, Batch 146/462, Loss: 0.877978503704071\n",
      "Epoch 5, Batch 147/462, Loss: 0.7983362674713135\n",
      "Epoch 5, Batch 148/462, Loss: 0.7575345039367676\n",
      "Epoch 5, Batch 149/462, Loss: 0.9778832197189331\n",
      "Epoch 5, Batch 150/462, Loss: 0.9643538594245911\n",
      "Epoch 5, Batch 151/462, Loss: 0.6584647297859192\n",
      "Epoch 5, Batch 152/462, Loss: 0.9473990797996521\n",
      "Epoch 5, Batch 153/462, Loss: 0.7821685075759888\n",
      "Epoch 5, Batch 154/462, Loss: 0.8073669672012329\n",
      "Epoch 5, Batch 155/462, Loss: 0.803473949432373\n",
      "Epoch 5, Batch 156/462, Loss: 0.8974862098693848\n",
      "Epoch 5, Batch 157/462, Loss: 0.7881676554679871\n",
      "Epoch 5, Batch 158/462, Loss: 0.8140591979026794\n",
      "Epoch 5, Batch 159/462, Loss: 0.7658100724220276\n",
      "Epoch 5, Batch 160/462, Loss: 0.7490158677101135\n",
      "Epoch 5, Batch 161/462, Loss: 0.7710558772087097\n",
      "Epoch 5, Batch 162/462, Loss: 0.9539197683334351\n",
      "Epoch 5, Batch 163/462, Loss: 0.7362959980964661\n",
      "Epoch 5, Batch 164/462, Loss: 0.7949380278587341\n",
      "Epoch 5, Batch 165/462, Loss: 0.8525062203407288\n",
      "Epoch 5, Batch 166/462, Loss: 0.7561933398246765\n",
      "Epoch 5, Batch 167/462, Loss: 0.9886134266853333\n",
      "Epoch 5, Batch 168/462, Loss: 0.9294894337654114\n",
      "Epoch 5, Batch 169/462, Loss: 1.0038795471191406\n",
      "Epoch 5, Batch 170/462, Loss: 0.8402594327926636\n",
      "Epoch 5, Batch 171/462, Loss: 0.8862371444702148\n",
      "Epoch 5, Batch 172/462, Loss: 0.9127312302589417\n",
      "Epoch 5, Batch 173/462, Loss: 0.9502638578414917\n",
      "Epoch 5, Batch 174/462, Loss: 0.9134165644645691\n",
      "Epoch 5, Batch 175/462, Loss: 0.7972981929779053\n",
      "Epoch 5, Batch 176/462, Loss: 0.7763168811798096\n",
      "Epoch 5, Batch 177/462, Loss: 0.8692641854286194\n",
      "Epoch 5, Batch 178/462, Loss: 0.9327951669692993\n",
      "Epoch 5, Batch 179/462, Loss: 0.9712281823158264\n",
      "Epoch 5, Batch 180/462, Loss: 0.813990592956543\n",
      "Epoch 5, Batch 181/462, Loss: 0.8494786620140076\n",
      "Epoch 5, Batch 182/462, Loss: 0.7269979119300842\n",
      "Epoch 5, Batch 183/462, Loss: 0.8251626491546631\n",
      "Epoch 5, Batch 184/462, Loss: 0.8170002698898315\n",
      "Epoch 5, Batch 185/462, Loss: 0.8384140729904175\n",
      "Epoch 5, Batch 186/462, Loss: 0.9432141780853271\n",
      "Epoch 5, Batch 187/462, Loss: 0.781008780002594\n",
      "Epoch 5, Batch 188/462, Loss: 0.8571091294288635\n",
      "Epoch 5, Batch 189/462, Loss: 0.8547468185424805\n",
      "Epoch 5, Batch 190/462, Loss: 0.8902472853660583\n",
      "Epoch 5, Batch 191/462, Loss: 0.9888826608657837\n",
      "Epoch 5, Batch 192/462, Loss: 0.728488028049469\n",
      "Epoch 5, Batch 193/462, Loss: 0.9142184257507324\n",
      "Epoch 5, Batch 194/462, Loss: 0.8245946168899536\n",
      "Epoch 5, Batch 195/462, Loss: 0.9306801557540894\n",
      "Epoch 5, Batch 196/462, Loss: 0.6767953038215637\n",
      "Epoch 5, Batch 197/462, Loss: 0.8057705163955688\n",
      "Epoch 5, Batch 198/462, Loss: 0.8235587477684021\n",
      "Epoch 5, Batch 199/462, Loss: 0.9634100198745728\n",
      "Epoch 5, Batch 200/462, Loss: 0.9164627194404602\n",
      "Epoch 5, Batch 201/462, Loss: 0.8693020939826965\n",
      "Epoch 5, Batch 202/462, Loss: 0.8410435914993286\n",
      "Epoch 5, Batch 203/462, Loss: 0.8624494075775146\n",
      "Epoch 5, Batch 204/462, Loss: 0.7781746983528137\n",
      "Epoch 5, Batch 205/462, Loss: 0.954637885093689\n",
      "Epoch 5, Batch 206/462, Loss: 0.925906777381897\n",
      "Epoch 5, Batch 207/462, Loss: 0.8748505711555481\n",
      "Epoch 5, Batch 208/462, Loss: 0.7988913059234619\n",
      "Epoch 5, Batch 209/462, Loss: 0.8518584966659546\n",
      "Epoch 5, Batch 210/462, Loss: 0.9748872518539429\n",
      "Epoch 5, Batch 211/462, Loss: 0.8181108832359314\n",
      "Epoch 5, Batch 212/462, Loss: 1.0098621845245361\n",
      "Epoch 5, Batch 213/462, Loss: 0.7607624530792236\n",
      "Epoch 5, Batch 214/462, Loss: 0.7731315493583679\n",
      "Epoch 5, Batch 215/462, Loss: 0.8947811126708984\n",
      "Epoch 5, Batch 216/462, Loss: 0.992077648639679\n",
      "Epoch 5, Batch 217/462, Loss: 0.8163143992424011\n",
      "Epoch 5, Batch 218/462, Loss: 0.8601846694946289\n",
      "Epoch 5, Batch 219/462, Loss: 0.7919467091560364\n",
      "Epoch 5, Batch 220/462, Loss: 0.8170303702354431\n",
      "Epoch 5, Batch 221/462, Loss: 0.8294864892959595\n",
      "Epoch 5, Batch 222/462, Loss: 0.8580849170684814\n",
      "Epoch 5, Batch 223/462, Loss: 0.8265579342842102\n",
      "Epoch 5, Batch 224/462, Loss: 0.9734807014465332\n",
      "Epoch 5, Batch 225/462, Loss: 0.8759421110153198\n",
      "Epoch 5, Batch 226/462, Loss: 0.9827691316604614\n",
      "Epoch 5, Batch 227/462, Loss: 0.8021129965782166\n",
      "Epoch 5, Batch 228/462, Loss: 0.8674479126930237\n",
      "Epoch 5, Batch 229/462, Loss: 0.9629446864128113\n",
      "Epoch 5, Batch 230/462, Loss: 0.9095955491065979\n",
      "Epoch 5, Batch 231/462, Loss: 0.912648618221283\n",
      "Epoch 5, Batch 232/462, Loss: 1.012420415878296\n",
      "Epoch 5, Batch 233/462, Loss: 0.864305853843689\n",
      "Epoch 5, Batch 234/462, Loss: 0.8818591237068176\n",
      "Epoch 5, Batch 235/462, Loss: 0.9357170462608337\n",
      "Epoch 5, Batch 236/462, Loss: 0.84894198179245\n",
      "Epoch 5, Batch 237/462, Loss: 0.8830270171165466\n",
      "Epoch 5, Batch 238/462, Loss: 0.8883788585662842\n",
      "Epoch 5, Batch 239/462, Loss: 0.7991176843643188\n",
      "Epoch 5, Batch 240/462, Loss: 0.7239586114883423\n",
      "Epoch 5, Batch 241/462, Loss: 0.7732107043266296\n",
      "Epoch 5, Batch 242/462, Loss: 0.858110249042511\n",
      "Epoch 5, Batch 243/462, Loss: 0.9301141500473022\n",
      "Epoch 5, Batch 244/462, Loss: 0.7893466353416443\n",
      "Epoch 5, Batch 245/462, Loss: 0.9577718377113342\n",
      "Epoch 5, Batch 246/462, Loss: 0.852597713470459\n",
      "Epoch 5, Batch 247/462, Loss: 0.8863111138343811\n",
      "Epoch 5, Batch 248/462, Loss: 0.9024234414100647\n",
      "Epoch 5, Batch 249/462, Loss: 0.8502639532089233\n",
      "Epoch 5, Batch 250/462, Loss: 0.791337788105011\n",
      "Epoch 5, Batch 251/462, Loss: 0.8711003065109253\n",
      "Epoch 5, Batch 252/462, Loss: 0.9052988886833191\n",
      "Epoch 5, Batch 253/462, Loss: 0.8249607682228088\n",
      "Epoch 5, Batch 254/462, Loss: 0.8669909238815308\n",
      "Epoch 5, Batch 255/462, Loss: 0.8620441555976868\n",
      "Epoch 5, Batch 256/462, Loss: 0.9376456141471863\n",
      "Epoch 5, Batch 257/462, Loss: 0.7693979740142822\n",
      "Epoch 5, Batch 258/462, Loss: 0.9257625937461853\n",
      "Epoch 5, Batch 259/462, Loss: 0.8530346751213074\n",
      "Epoch 5, Batch 260/462, Loss: 0.7066396474838257\n",
      "Epoch 5, Batch 261/462, Loss: 0.7707974910736084\n",
      "Epoch 5, Batch 262/462, Loss: 0.871010959148407\n",
      "Epoch 5, Batch 263/462, Loss: 0.9148183465003967\n",
      "Epoch 5, Batch 264/462, Loss: 0.9029220342636108\n",
      "Epoch 5, Batch 265/462, Loss: 0.7605579495429993\n",
      "Epoch 5, Batch 266/462, Loss: 0.8187183737754822\n",
      "Epoch 5, Batch 267/462, Loss: 0.966988742351532\n",
      "Epoch 5, Batch 268/462, Loss: 0.8022564053535461\n",
      "Epoch 5, Batch 269/462, Loss: 0.822700023651123\n",
      "Epoch 5, Batch 270/462, Loss: 0.9510926604270935\n",
      "Epoch 5, Batch 271/462, Loss: 0.7557762861251831\n",
      "Epoch 5, Batch 272/462, Loss: 0.7359331250190735\n",
      "Epoch 5, Batch 273/462, Loss: 0.9099527597427368\n",
      "Epoch 5, Batch 274/462, Loss: 0.7734944820404053\n",
      "Epoch 5, Batch 275/462, Loss: 0.8061587810516357\n",
      "Epoch 5, Batch 276/462, Loss: 0.7665991187095642\n",
      "Epoch 5, Batch 277/462, Loss: 0.8227949142456055\n",
      "Epoch 5, Batch 278/462, Loss: 0.840697705745697\n",
      "Epoch 5, Batch 279/462, Loss: 0.8903446793556213\n",
      "Epoch 5, Batch 280/462, Loss: 0.78154057264328\n",
      "Epoch 5, Batch 281/462, Loss: 0.8860810995101929\n",
      "Epoch 5, Batch 282/462, Loss: 0.8712462782859802\n",
      "Epoch 5, Batch 283/462, Loss: 0.880786120891571\n",
      "Epoch 5, Batch 284/462, Loss: 0.8756904006004333\n",
      "Epoch 5, Batch 285/462, Loss: 0.9596948623657227\n",
      "Epoch 5, Batch 286/462, Loss: 0.782401978969574\n",
      "Epoch 5, Batch 287/462, Loss: 0.9187625050544739\n",
      "Epoch 5, Batch 288/462, Loss: 0.8367288708686829\n",
      "Epoch 5, Batch 289/462, Loss: 0.8112735152244568\n",
      "Epoch 5, Batch 290/462, Loss: 0.9729026556015015\n",
      "Epoch 5, Batch 291/462, Loss: 0.7105902433395386\n",
      "Epoch 5, Batch 292/462, Loss: 0.7478958964347839\n",
      "Epoch 5, Batch 293/462, Loss: 0.9253554344177246\n",
      "Epoch 5, Batch 294/462, Loss: 0.8490208983421326\n",
      "Epoch 5, Batch 295/462, Loss: 0.8845037817955017\n",
      "Epoch 5, Batch 296/462, Loss: 0.7903622984886169\n",
      "Epoch 5, Batch 297/462, Loss: 0.8915524482727051\n",
      "Epoch 5, Batch 298/462, Loss: 1.0275640487670898\n",
      "Epoch 5, Batch 299/462, Loss: 0.8222971558570862\n",
      "Epoch 5, Batch 300/462, Loss: 0.9784432649612427\n",
      "Epoch 5, Batch 301/462, Loss: 0.8722510933876038\n",
      "Epoch 5, Batch 302/462, Loss: 0.9654334187507629\n",
      "Epoch 5, Batch 303/462, Loss: 0.8683847784996033\n",
      "Epoch 5, Batch 304/462, Loss: 0.9423515200614929\n",
      "Epoch 5, Batch 305/462, Loss: 0.9151552319526672\n",
      "Epoch 5, Batch 306/462, Loss: 0.6960582733154297\n",
      "Epoch 5, Batch 307/462, Loss: 0.8159327507019043\n",
      "Epoch 5, Batch 308/462, Loss: 0.9120558500289917\n",
      "Epoch 5, Batch 309/462, Loss: 0.8457422256469727\n",
      "Epoch 5, Batch 310/462, Loss: 0.8456618189811707\n",
      "Epoch 5, Batch 311/462, Loss: 0.9549155235290527\n",
      "Epoch 5, Batch 312/462, Loss: 0.8298143148422241\n",
      "Epoch 5, Batch 313/462, Loss: 0.8482927680015564\n",
      "Epoch 5, Batch 314/462, Loss: 0.8634611368179321\n",
      "Epoch 5, Batch 315/462, Loss: 0.9194525480270386\n",
      "Epoch 5, Batch 316/462, Loss: 0.8808669447898865\n",
      "Epoch 5, Batch 317/462, Loss: 0.8610941767692566\n",
      "Epoch 5, Batch 318/462, Loss: 0.6895177960395813\n",
      "Epoch 5, Batch 319/462, Loss: 0.7284832000732422\n",
      "Epoch 5, Batch 320/462, Loss: 0.7360559701919556\n",
      "Epoch 5, Batch 321/462, Loss: 0.7959636449813843\n",
      "Epoch 5, Batch 322/462, Loss: 0.8720861077308655\n",
      "Epoch 5, Batch 323/462, Loss: 0.8916913866996765\n",
      "Epoch 5, Batch 324/462, Loss: 0.771838903427124\n",
      "Epoch 5, Batch 325/462, Loss: 0.9230490922927856\n",
      "Epoch 5, Batch 326/462, Loss: 0.6773748993873596\n",
      "Epoch 5, Batch 327/462, Loss: 0.9199678897857666\n",
      "Epoch 5, Batch 328/462, Loss: 0.8414763808250427\n",
      "Epoch 5, Batch 329/462, Loss: 0.8550553321838379\n",
      "Epoch 5, Batch 330/462, Loss: 0.9676408767700195\n",
      "Epoch 5, Batch 331/462, Loss: 1.0008882284164429\n",
      "Epoch 5, Batch 332/462, Loss: 0.8519852161407471\n",
      "Epoch 5, Batch 333/462, Loss: 0.8626132011413574\n",
      "Epoch 5, Batch 334/462, Loss: 0.7715762853622437\n",
      "Epoch 5, Batch 335/462, Loss: 0.7738918662071228\n",
      "Epoch 5, Batch 336/462, Loss: 0.8431097269058228\n",
      "Epoch 5, Batch 337/462, Loss: 0.9185319542884827\n",
      "Epoch 5, Batch 338/462, Loss: 0.9330822229385376\n",
      "Epoch 5, Batch 339/462, Loss: 0.9104794263839722\n",
      "Epoch 5, Batch 340/462, Loss: 0.8373362421989441\n",
      "Epoch 5, Batch 341/462, Loss: 0.7720192074775696\n",
      "Epoch 5, Batch 342/462, Loss: 0.801342248916626\n",
      "Epoch 5, Batch 343/462, Loss: 0.9113235473632812\n",
      "Epoch 5, Batch 344/462, Loss: 0.7053163051605225\n",
      "Epoch 5, Batch 345/462, Loss: 0.7248473167419434\n",
      "Epoch 5, Batch 346/462, Loss: 0.8198031783103943\n",
      "Epoch 5, Batch 347/462, Loss: 0.8487834334373474\n",
      "Epoch 5, Batch 348/462, Loss: 0.7323004007339478\n",
      "Epoch 5, Batch 349/462, Loss: 0.8873913884162903\n",
      "Epoch 5, Batch 350/462, Loss: 0.8956843614578247\n",
      "Epoch 5, Batch 351/462, Loss: 0.7532046437263489\n",
      "Epoch 5, Batch 352/462, Loss: 0.7944039702415466\n",
      "Epoch 5, Batch 353/462, Loss: 0.7921591997146606\n",
      "Epoch 5, Batch 354/462, Loss: 0.7359780073165894\n",
      "Epoch 5, Batch 355/462, Loss: 0.890383780002594\n",
      "Epoch 5, Batch 356/462, Loss: 0.770041823387146\n",
      "Epoch 5, Batch 357/462, Loss: 0.9114738702774048\n",
      "Epoch 5, Batch 358/462, Loss: 0.8890470266342163\n",
      "Epoch 5, Batch 359/462, Loss: 0.789018988609314\n",
      "Epoch 5, Batch 360/462, Loss: 0.7445796132087708\n",
      "Epoch 5, Batch 361/462, Loss: 0.8477545380592346\n",
      "Epoch 5, Batch 362/462, Loss: 0.8705847859382629\n",
      "Epoch 5, Batch 363/462, Loss: 0.7118630409240723\n",
      "Epoch 5, Batch 364/462, Loss: 0.8396238684654236\n",
      "Epoch 5, Batch 365/462, Loss: 0.8290203809738159\n",
      "Epoch 5, Batch 366/462, Loss: 0.9008169770240784\n",
      "Epoch 5, Batch 367/462, Loss: 0.8182071447372437\n",
      "Epoch 5, Batch 368/462, Loss: 0.8423062562942505\n",
      "Epoch 5, Batch 369/462, Loss: 0.7989814877510071\n",
      "Epoch 5, Batch 370/462, Loss: 0.9055687785148621\n",
      "Epoch 5, Batch 371/462, Loss: 0.9168248176574707\n",
      "Epoch 5, Batch 372/462, Loss: 0.8102096319198608\n",
      "Epoch 5, Batch 373/462, Loss: 0.7847024202346802\n",
      "Epoch 5, Batch 374/462, Loss: 0.9182396531105042\n",
      "Epoch 5, Batch 375/462, Loss: 0.8187738656997681\n",
      "Epoch 5, Batch 376/462, Loss: 0.8131958842277527\n",
      "Epoch 5, Batch 377/462, Loss: 0.9734826683998108\n",
      "Epoch 5, Batch 378/462, Loss: 0.7716541886329651\n",
      "Epoch 5, Batch 379/462, Loss: 0.9921503067016602\n",
      "Epoch 5, Batch 380/462, Loss: 0.8228928446769714\n",
      "Epoch 5, Batch 381/462, Loss: 0.8025978803634644\n",
      "Epoch 5, Batch 382/462, Loss: 0.6686865091323853\n",
      "Epoch 5, Batch 383/462, Loss: 0.827324390411377\n",
      "Epoch 5, Batch 384/462, Loss: 0.7857556343078613\n",
      "Epoch 5, Batch 385/462, Loss: 0.7798517942428589\n",
      "Epoch 5, Batch 386/462, Loss: 0.8892923593521118\n",
      "Epoch 5, Batch 387/462, Loss: 0.9260426759719849\n",
      "Epoch 5, Batch 388/462, Loss: 0.8678243160247803\n",
      "Epoch 5, Batch 389/462, Loss: 0.9468687176704407\n",
      "Epoch 5, Batch 390/462, Loss: 0.848347544670105\n",
      "Epoch 5, Batch 391/462, Loss: 0.8358510136604309\n",
      "Epoch 5, Batch 392/462, Loss: 0.7735825777053833\n",
      "Epoch 5, Batch 393/462, Loss: 0.8845834732055664\n",
      "Epoch 5, Batch 394/462, Loss: 0.8271949887275696\n",
      "Epoch 5, Batch 395/462, Loss: 0.7780957818031311\n",
      "Epoch 5, Batch 396/462, Loss: 0.9643752574920654\n",
      "Epoch 5, Batch 397/462, Loss: 0.8236529231071472\n",
      "Epoch 5, Batch 398/462, Loss: 0.8028497099876404\n",
      "Epoch 5, Batch 399/462, Loss: 0.8364869356155396\n",
      "Epoch 5, Batch 400/462, Loss: 0.8221098184585571\n",
      "Epoch 5, Batch 401/462, Loss: 0.7481402158737183\n",
      "Epoch 5, Batch 402/462, Loss: 0.9934364557266235\n",
      "Epoch 5, Batch 403/462, Loss: 0.7644301056861877\n",
      "Epoch 5, Batch 404/462, Loss: 0.853827178478241\n",
      "Epoch 5, Batch 405/462, Loss: 0.7664586305618286\n",
      "Epoch 5, Batch 406/462, Loss: 0.8352954983711243\n",
      "Epoch 5, Batch 407/462, Loss: 0.713813841342926\n",
      "Epoch 5, Batch 408/462, Loss: 0.9233238697052002\n",
      "Epoch 5, Batch 409/462, Loss: 0.8932837247848511\n",
      "Epoch 5, Batch 410/462, Loss: 0.9676117300987244\n",
      "Epoch 5, Batch 411/462, Loss: 0.8399620652198792\n",
      "Epoch 5, Batch 412/462, Loss: 0.7970086336135864\n",
      "Epoch 5, Batch 413/462, Loss: 0.8049407005310059\n",
      "Epoch 5, Batch 414/462, Loss: 0.8276930451393127\n",
      "Epoch 5, Batch 415/462, Loss: 1.0205703973770142\n",
      "Epoch 5, Batch 416/462, Loss: 1.0044918060302734\n",
      "Epoch 5, Batch 417/462, Loss: 0.923802375793457\n",
      "Epoch 5, Batch 418/462, Loss: 0.9116697907447815\n",
      "Epoch 5, Batch 419/462, Loss: 0.8748749494552612\n",
      "Epoch 5, Batch 420/462, Loss: 0.9192976355552673\n",
      "Epoch 5, Batch 421/462, Loss: 0.8070061802864075\n",
      "Epoch 5, Batch 422/462, Loss: 0.9196087718009949\n",
      "Epoch 5, Batch 423/462, Loss: 0.9299211502075195\n",
      "Epoch 5, Batch 424/462, Loss: 0.960309624671936\n",
      "Epoch 5, Batch 425/462, Loss: 0.8729222416877747\n",
      "Epoch 5, Batch 426/462, Loss: 0.8220081329345703\n",
      "Epoch 5, Batch 427/462, Loss: 0.7365032434463501\n",
      "Epoch 5, Batch 428/462, Loss: 0.8135151863098145\n",
      "Epoch 5, Batch 429/462, Loss: 0.8037828207015991\n",
      "Epoch 5, Batch 430/462, Loss: 0.862149178981781\n",
      "Epoch 5, Batch 431/462, Loss: 0.8921589851379395\n",
      "Epoch 5, Batch 432/462, Loss: 0.8887906074523926\n",
      "Epoch 5, Batch 433/462, Loss: 0.7004280090332031\n",
      "Epoch 5, Batch 434/462, Loss: 0.8031442761421204\n",
      "Epoch 5, Batch 435/462, Loss: 0.8873655796051025\n",
      "Epoch 5, Batch 436/462, Loss: 0.8409417271614075\n",
      "Epoch 5, Batch 437/462, Loss: 0.7354561686515808\n",
      "Epoch 5, Batch 438/462, Loss: 0.82920902967453\n",
      "Epoch 5, Batch 439/462, Loss: 0.7141349911689758\n",
      "Epoch 5, Batch 440/462, Loss: 0.8163900375366211\n",
      "Epoch 5, Batch 441/462, Loss: 0.8194653391838074\n",
      "Epoch 5, Batch 442/462, Loss: 0.6836588382720947\n",
      "Epoch 5, Batch 443/462, Loss: 0.931738018989563\n",
      "Epoch 5, Batch 444/462, Loss: 0.9228476285934448\n",
      "Epoch 5, Batch 445/462, Loss: 0.7994597554206848\n",
      "Epoch 5, Batch 446/462, Loss: 0.8032505512237549\n",
      "Epoch 5, Batch 447/462, Loss: 0.8750227093696594\n",
      "Epoch 5, Batch 448/462, Loss: 0.8574032187461853\n",
      "Epoch 5, Batch 449/462, Loss: 0.8518478274345398\n",
      "Epoch 5, Batch 450/462, Loss: 0.8573522567749023\n",
      "Epoch 5, Batch 451/462, Loss: 0.8316996097564697\n",
      "Epoch 5, Batch 452/462, Loss: 0.8870382905006409\n",
      "Epoch 5, Batch 453/462, Loss: 0.7821339964866638\n",
      "Epoch 5, Batch 454/462, Loss: 0.8061569929122925\n",
      "Epoch 5, Batch 455/462, Loss: 0.8636032342910767\n",
      "Epoch 5, Batch 456/462, Loss: 0.8648576736450195\n",
      "Epoch 5, Batch 457/462, Loss: 0.9408351182937622\n",
      "Epoch 5, Batch 458/462, Loss: 0.8552393913269043\n",
      "Epoch 5, Batch 459/462, Loss: 0.835459291934967\n",
      "Epoch 5, Batch 460/462, Loss: 0.9768537878990173\n",
      "Epoch 5, Batch 461/462, Loss: 0.860379159450531\n",
      "Epoch 5, Batch 462/462, Loss: 0.8871968388557434\n",
      "Epoch 5, Loss: 395.5755203962326\n",
      "Epoch 6, Batch 1/462, Loss: 0.8160315752029419\n",
      "Epoch 6, Batch 2/462, Loss: 0.8550559282302856\n",
      "Epoch 6, Batch 3/462, Loss: 0.7663569450378418\n",
      "Epoch 6, Batch 4/462, Loss: 0.8422834873199463\n",
      "Epoch 6, Batch 5/462, Loss: 0.9251744151115417\n",
      "Epoch 6, Batch 6/462, Loss: 0.979296863079071\n",
      "Epoch 6, Batch 7/462, Loss: 0.8290219902992249\n",
      "Epoch 6, Batch 8/462, Loss: 0.7706632018089294\n",
      "Epoch 6, Batch 9/462, Loss: 0.7663990259170532\n",
      "Epoch 6, Batch 10/462, Loss: 0.7954955101013184\n",
      "Epoch 6, Batch 11/462, Loss: 0.7055450081825256\n",
      "Epoch 6, Batch 12/462, Loss: 0.7925384640693665\n",
      "Epoch 6, Batch 13/462, Loss: 0.9043689370155334\n",
      "Epoch 6, Batch 14/462, Loss: 0.8134440183639526\n",
      "Epoch 6, Batch 15/462, Loss: 0.79669189453125\n",
      "Epoch 6, Batch 16/462, Loss: 0.9251741766929626\n",
      "Epoch 6, Batch 17/462, Loss: 0.7732254266738892\n",
      "Epoch 6, Batch 18/462, Loss: 0.823026716709137\n",
      "Epoch 6, Batch 19/462, Loss: 0.8932970762252808\n",
      "Epoch 6, Batch 20/462, Loss: 0.7737413644790649\n",
      "Epoch 6, Batch 21/462, Loss: 0.805071234703064\n",
      "Epoch 6, Batch 22/462, Loss: 0.7647179365158081\n",
      "Epoch 6, Batch 23/462, Loss: 0.8692541122436523\n",
      "Epoch 6, Batch 24/462, Loss: 0.8916676044464111\n",
      "Epoch 6, Batch 25/462, Loss: 0.9468613266944885\n",
      "Epoch 6, Batch 26/462, Loss: 0.9452319145202637\n",
      "Epoch 6, Batch 27/462, Loss: 0.8637041449546814\n",
      "Epoch 6, Batch 28/462, Loss: 0.7970077991485596\n",
      "Epoch 6, Batch 29/462, Loss: 0.8255996704101562\n",
      "Epoch 6, Batch 30/462, Loss: 0.847638726234436\n",
      "Epoch 6, Batch 31/462, Loss: 0.8129974603652954\n",
      "Epoch 6, Batch 32/462, Loss: 0.9377142190933228\n",
      "Epoch 6, Batch 33/462, Loss: 0.7932422757148743\n",
      "Epoch 6, Batch 34/462, Loss: 0.9245737791061401\n",
      "Epoch 6, Batch 35/462, Loss: 0.8718196749687195\n",
      "Epoch 6, Batch 36/462, Loss: 0.8149437308311462\n",
      "Epoch 6, Batch 37/462, Loss: 0.8703516125679016\n",
      "Epoch 6, Batch 38/462, Loss: 0.8761362433433533\n",
      "Epoch 6, Batch 39/462, Loss: 0.9587116837501526\n",
      "Epoch 6, Batch 40/462, Loss: 0.7765375971794128\n",
      "Epoch 6, Batch 41/462, Loss: 0.8004773259162903\n",
      "Epoch 6, Batch 42/462, Loss: 0.9347943663597107\n",
      "Epoch 6, Batch 43/462, Loss: 0.79710853099823\n",
      "Epoch 6, Batch 44/462, Loss: 0.9340428113937378\n",
      "Epoch 6, Batch 45/462, Loss: 0.8582480549812317\n",
      "Epoch 6, Batch 46/462, Loss: 0.8190656304359436\n",
      "Epoch 6, Batch 47/462, Loss: 0.9671148657798767\n",
      "Epoch 6, Batch 48/462, Loss: 0.7175397872924805\n",
      "Epoch 6, Batch 49/462, Loss: 0.7637329697608948\n",
      "Epoch 6, Batch 50/462, Loss: 0.7103016972541809\n",
      "Epoch 6, Batch 51/462, Loss: 0.8232955932617188\n",
      "Epoch 6, Batch 52/462, Loss: 0.7532658576965332\n",
      "Epoch 6, Batch 53/462, Loss: 0.8555310964584351\n",
      "Epoch 6, Batch 54/462, Loss: 0.7259952425956726\n",
      "Epoch 6, Batch 55/462, Loss: 0.8655133843421936\n",
      "Epoch 6, Batch 56/462, Loss: 0.8108909130096436\n",
      "Epoch 6, Batch 57/462, Loss: 0.7485272288322449\n",
      "Epoch 6, Batch 58/462, Loss: 0.9860297441482544\n",
      "Epoch 6, Batch 59/462, Loss: 0.6884915828704834\n",
      "Epoch 6, Batch 60/462, Loss: 0.7767201066017151\n",
      "Epoch 6, Batch 61/462, Loss: 0.7747164368629456\n",
      "Epoch 6, Batch 62/462, Loss: 0.8947588801383972\n",
      "Epoch 6, Batch 63/462, Loss: 0.6958783864974976\n",
      "Epoch 6, Batch 64/462, Loss: 0.7535128593444824\n",
      "Epoch 6, Batch 65/462, Loss: 0.8409366607666016\n",
      "Epoch 6, Batch 66/462, Loss: 0.9043306708335876\n",
      "Epoch 6, Batch 67/462, Loss: 0.8663210868835449\n",
      "Epoch 6, Batch 68/462, Loss: 0.8432122468948364\n",
      "Epoch 6, Batch 69/462, Loss: 0.8731520175933838\n",
      "Epoch 6, Batch 70/462, Loss: 0.8514344692230225\n",
      "Epoch 6, Batch 71/462, Loss: 0.7902833223342896\n",
      "Epoch 6, Batch 72/462, Loss: 0.727245569229126\n",
      "Epoch 6, Batch 73/462, Loss: 0.8097435832023621\n",
      "Epoch 6, Batch 74/462, Loss: 0.888943076133728\n",
      "Epoch 6, Batch 75/462, Loss: 0.8697666525840759\n",
      "Epoch 6, Batch 76/462, Loss: 0.9740903377532959\n",
      "Epoch 6, Batch 77/462, Loss: 0.8356212377548218\n",
      "Epoch 6, Batch 78/462, Loss: 0.8546310067176819\n",
      "Epoch 6, Batch 79/462, Loss: 0.7321553230285645\n",
      "Epoch 6, Batch 80/462, Loss: 0.7665025591850281\n",
      "Epoch 6, Batch 81/462, Loss: 0.992120623588562\n",
      "Epoch 6, Batch 82/462, Loss: 0.8886464834213257\n",
      "Epoch 6, Batch 83/462, Loss: 0.8938224911689758\n",
      "Epoch 6, Batch 84/462, Loss: 0.8890458345413208\n",
      "Epoch 6, Batch 85/462, Loss: 0.8803191184997559\n",
      "Epoch 6, Batch 86/462, Loss: 0.7361688613891602\n",
      "Epoch 6, Batch 87/462, Loss: 1.0254839658737183\n",
      "Epoch 6, Batch 88/462, Loss: 0.7390240430831909\n",
      "Epoch 6, Batch 89/462, Loss: 0.7244166731834412\n",
      "Epoch 6, Batch 90/462, Loss: 0.7428786754608154\n",
      "Epoch 6, Batch 91/462, Loss: 0.7243466973304749\n",
      "Epoch 6, Batch 92/462, Loss: 0.7521357536315918\n",
      "Epoch 6, Batch 93/462, Loss: 0.995927095413208\n",
      "Epoch 6, Batch 94/462, Loss: 0.900850236415863\n",
      "Epoch 6, Batch 95/462, Loss: 0.8763835430145264\n",
      "Epoch 6, Batch 96/462, Loss: 0.7967944145202637\n",
      "Epoch 6, Batch 97/462, Loss: 0.6874800324440002\n",
      "Epoch 6, Batch 98/462, Loss: 0.9224857091903687\n",
      "Epoch 6, Batch 99/462, Loss: 0.7761942148208618\n",
      "Epoch 6, Batch 100/462, Loss: 0.8147251605987549\n",
      "Epoch 6, Batch 101/462, Loss: 0.8142573237419128\n",
      "Epoch 6, Batch 102/462, Loss: 0.8066478967666626\n",
      "Epoch 6, Batch 103/462, Loss: 0.848999559879303\n",
      "Epoch 6, Batch 104/462, Loss: 0.7920876741409302\n",
      "Epoch 6, Batch 105/462, Loss: 0.9493102431297302\n",
      "Epoch 6, Batch 106/462, Loss: 0.7548461556434631\n",
      "Epoch 6, Batch 107/462, Loss: 0.7722254395484924\n",
      "Epoch 6, Batch 108/462, Loss: 0.8333614468574524\n",
      "Epoch 6, Batch 109/462, Loss: 0.8856925368309021\n",
      "Epoch 6, Batch 110/462, Loss: 0.8633424043655396\n",
      "Epoch 6, Batch 111/462, Loss: 0.8316989541053772\n",
      "Epoch 6, Batch 112/462, Loss: 0.8780201077461243\n",
      "Epoch 6, Batch 113/462, Loss: 0.9006481170654297\n",
      "Epoch 6, Batch 114/462, Loss: 0.8412794470787048\n",
      "Epoch 6, Batch 115/462, Loss: 0.859839916229248\n",
      "Epoch 6, Batch 116/462, Loss: 0.9113715291023254\n",
      "Epoch 6, Batch 117/462, Loss: 0.906717836856842\n",
      "Epoch 6, Batch 118/462, Loss: 0.8674790263175964\n",
      "Epoch 6, Batch 119/462, Loss: 0.8275411128997803\n",
      "Epoch 6, Batch 120/462, Loss: 0.7994470000267029\n",
      "Epoch 6, Batch 121/462, Loss: 0.8104150295257568\n",
      "Epoch 6, Batch 122/462, Loss: 0.8526646494865417\n",
      "Epoch 6, Batch 123/462, Loss: 0.9165197610855103\n",
      "Epoch 6, Batch 124/462, Loss: 0.8569560647010803\n",
      "Epoch 6, Batch 125/462, Loss: 0.8336827158927917\n",
      "Epoch 6, Batch 126/462, Loss: 0.7835907340049744\n",
      "Epoch 6, Batch 127/462, Loss: 0.9348646402359009\n",
      "Epoch 6, Batch 128/462, Loss: 0.7470815777778625\n",
      "Epoch 6, Batch 129/462, Loss: 0.9038137793540955\n",
      "Epoch 6, Batch 130/462, Loss: 0.8435922265052795\n",
      "Epoch 6, Batch 131/462, Loss: 0.7381225228309631\n",
      "Epoch 6, Batch 132/462, Loss: 0.8464450240135193\n",
      "Epoch 6, Batch 133/462, Loss: 0.9077951908111572\n",
      "Epoch 6, Batch 134/462, Loss: 0.6972501277923584\n",
      "Epoch 6, Batch 135/462, Loss: 0.7611282467842102\n",
      "Epoch 6, Batch 136/462, Loss: 0.8971425294876099\n",
      "Epoch 6, Batch 137/462, Loss: 0.911308765411377\n",
      "Epoch 6, Batch 138/462, Loss: 0.7097107768058777\n",
      "Epoch 6, Batch 139/462, Loss: 0.9443244934082031\n",
      "Epoch 6, Batch 140/462, Loss: 0.7129895687103271\n",
      "Epoch 6, Batch 141/462, Loss: 0.8966746926307678\n",
      "Epoch 6, Batch 142/462, Loss: 0.8466882705688477\n",
      "Epoch 6, Batch 143/462, Loss: 0.8632314205169678\n",
      "Epoch 6, Batch 144/462, Loss: 0.9029578566551208\n",
      "Epoch 6, Batch 145/462, Loss: 0.9822824001312256\n",
      "Epoch 6, Batch 146/462, Loss: 0.7891267538070679\n",
      "Epoch 6, Batch 147/462, Loss: 0.8432236313819885\n",
      "Epoch 6, Batch 148/462, Loss: 0.7679259777069092\n",
      "Epoch 6, Batch 149/462, Loss: 0.8682401180267334\n",
      "Epoch 6, Batch 150/462, Loss: 0.8683505654335022\n",
      "Epoch 6, Batch 151/462, Loss: 0.8602336049079895\n",
      "Epoch 6, Batch 152/462, Loss: 0.7110763788223267\n",
      "Epoch 6, Batch 153/462, Loss: 0.7902035713195801\n",
      "Epoch 6, Batch 154/462, Loss: 0.943398654460907\n",
      "Epoch 6, Batch 155/462, Loss: 0.9360786080360413\n",
      "Epoch 6, Batch 156/462, Loss: 1.0171784162521362\n",
      "Epoch 6, Batch 157/462, Loss: 1.0752273797988892\n",
      "Epoch 6, Batch 158/462, Loss: 0.8145527243614197\n",
      "Epoch 6, Batch 159/462, Loss: 0.7847639322280884\n",
      "Epoch 6, Batch 160/462, Loss: 0.9335048198699951\n",
      "Epoch 6, Batch 161/462, Loss: 0.8068050742149353\n",
      "Epoch 6, Batch 162/462, Loss: 0.9817181825637817\n",
      "Epoch 6, Batch 163/462, Loss: 0.8883603811264038\n",
      "Epoch 6, Batch 164/462, Loss: 0.770969033241272\n",
      "Epoch 6, Batch 165/462, Loss: 0.745021402835846\n",
      "Epoch 6, Batch 166/462, Loss: 0.7939742207527161\n",
      "Epoch 6, Batch 167/462, Loss: 0.7814012765884399\n",
      "Epoch 6, Batch 168/462, Loss: 0.8512452244758606\n",
      "Epoch 6, Batch 169/462, Loss: 0.8845523595809937\n",
      "Epoch 6, Batch 170/462, Loss: 1.0589444637298584\n",
      "Epoch 6, Batch 171/462, Loss: 0.8177916407585144\n",
      "Epoch 6, Batch 172/462, Loss: 0.7119587063789368\n",
      "Epoch 6, Batch 173/462, Loss: 0.8595783710479736\n",
      "Epoch 6, Batch 174/462, Loss: 0.8211615681648254\n",
      "Epoch 6, Batch 175/462, Loss: 0.8636475205421448\n",
      "Epoch 6, Batch 176/462, Loss: 0.9323413968086243\n",
      "Epoch 6, Batch 177/462, Loss: 0.7897242307662964\n",
      "Epoch 6, Batch 178/462, Loss: 0.8021723031997681\n",
      "Epoch 6, Batch 179/462, Loss: 0.9673984050750732\n",
      "Epoch 6, Batch 180/462, Loss: 0.9438213109970093\n",
      "Epoch 6, Batch 181/462, Loss: 0.9499170184135437\n",
      "Epoch 6, Batch 182/462, Loss: 0.78778076171875\n",
      "Epoch 6, Batch 183/462, Loss: 0.8573546409606934\n",
      "Epoch 6, Batch 184/462, Loss: 0.8951060175895691\n",
      "Epoch 6, Batch 185/462, Loss: 0.9352628588676453\n",
      "Epoch 6, Batch 186/462, Loss: 0.8277734518051147\n",
      "Epoch 6, Batch 187/462, Loss: 0.8789690136909485\n",
      "Epoch 6, Batch 188/462, Loss: 0.9112774133682251\n",
      "Epoch 6, Batch 189/462, Loss: 0.7474396824836731\n",
      "Epoch 6, Batch 190/462, Loss: 0.9048552513122559\n",
      "Epoch 6, Batch 191/462, Loss: 0.8145583271980286\n",
      "Epoch 6, Batch 192/462, Loss: 0.8248571753501892\n",
      "Epoch 6, Batch 193/462, Loss: 0.70777827501297\n",
      "Epoch 6, Batch 194/462, Loss: 0.8543960452079773\n",
      "Epoch 6, Batch 195/462, Loss: 0.7229459881782532\n",
      "Epoch 6, Batch 196/462, Loss: 0.7101808190345764\n",
      "Epoch 6, Batch 197/462, Loss: 0.9990480542182922\n",
      "Epoch 6, Batch 198/462, Loss: 0.8852790594100952\n",
      "Epoch 6, Batch 199/462, Loss: 0.8277941346168518\n",
      "Epoch 6, Batch 200/462, Loss: 0.9180768132209778\n",
      "Epoch 6, Batch 201/462, Loss: 0.8989471793174744\n",
      "Epoch 6, Batch 202/462, Loss: 0.8137681484222412\n",
      "Epoch 6, Batch 203/462, Loss: 0.8724146485328674\n",
      "Epoch 6, Batch 204/462, Loss: 0.8196035623550415\n",
      "Epoch 6, Batch 205/462, Loss: 0.8668289184570312\n",
      "Epoch 6, Batch 206/462, Loss: 0.8761008381843567\n",
      "Epoch 6, Batch 207/462, Loss: 0.8253790140151978\n",
      "Epoch 6, Batch 208/462, Loss: 0.9007773995399475\n",
      "Epoch 6, Batch 209/462, Loss: 0.7584313154220581\n",
      "Epoch 6, Batch 210/462, Loss: 0.8795809745788574\n",
      "Epoch 6, Batch 211/462, Loss: 0.9023600816726685\n",
      "Epoch 6, Batch 212/462, Loss: 0.8660179972648621\n",
      "Epoch 6, Batch 213/462, Loss: 0.8238463997840881\n",
      "Epoch 6, Batch 214/462, Loss: 0.7987204790115356\n",
      "Epoch 6, Batch 215/462, Loss: 0.9486144781112671\n",
      "Epoch 6, Batch 216/462, Loss: 0.9276929497718811\n",
      "Epoch 6, Batch 217/462, Loss: 0.8199228048324585\n",
      "Epoch 6, Batch 218/462, Loss: 0.87592613697052\n",
      "Epoch 6, Batch 219/462, Loss: 0.8662906289100647\n",
      "Epoch 6, Batch 220/462, Loss: 0.8503918051719666\n",
      "Epoch 6, Batch 221/462, Loss: 0.7304198145866394\n",
      "Epoch 6, Batch 222/462, Loss: 0.8261650204658508\n",
      "Epoch 6, Batch 223/462, Loss: 0.7713750004768372\n",
      "Epoch 6, Batch 224/462, Loss: 0.9522718191146851\n",
      "Epoch 6, Batch 225/462, Loss: 0.7948302626609802\n",
      "Epoch 6, Batch 226/462, Loss: 0.9053424000740051\n",
      "Epoch 6, Batch 227/462, Loss: 0.9388940930366516\n",
      "Epoch 6, Batch 228/462, Loss: 0.9086644649505615\n",
      "Epoch 6, Batch 229/462, Loss: 0.8803203701972961\n",
      "Epoch 6, Batch 230/462, Loss: 0.8464949131011963\n",
      "Epoch 6, Batch 231/462, Loss: 0.8711078763008118\n",
      "Epoch 6, Batch 232/462, Loss: 0.8136337995529175\n",
      "Epoch 6, Batch 233/462, Loss: 0.8086933493614197\n",
      "Epoch 6, Batch 234/462, Loss: 0.9141823053359985\n",
      "Epoch 6, Batch 235/462, Loss: 0.8445068001747131\n",
      "Epoch 6, Batch 236/462, Loss: 0.9050840139389038\n",
      "Epoch 6, Batch 237/462, Loss: 0.7348425984382629\n",
      "Epoch 6, Batch 238/462, Loss: 0.9116066098213196\n",
      "Epoch 6, Batch 239/462, Loss: 0.7435590624809265\n",
      "Epoch 6, Batch 240/462, Loss: 0.848470151424408\n",
      "Epoch 6, Batch 241/462, Loss: 0.8443941473960876\n",
      "Epoch 6, Batch 242/462, Loss: 0.8340532779693604\n",
      "Epoch 6, Batch 243/462, Loss: 1.0115333795547485\n",
      "Epoch 6, Batch 244/462, Loss: 0.8993510007858276\n",
      "Epoch 6, Batch 245/462, Loss: 0.9069462418556213\n",
      "Epoch 6, Batch 246/462, Loss: 0.7485103011131287\n",
      "Epoch 6, Batch 247/462, Loss: 0.8049476742744446\n",
      "Epoch 6, Batch 248/462, Loss: 0.8932377696037292\n",
      "Epoch 6, Batch 249/462, Loss: 0.8197327256202698\n",
      "Epoch 6, Batch 250/462, Loss: 0.7635071873664856\n",
      "Epoch 6, Batch 251/462, Loss: 0.9203051924705505\n",
      "Epoch 6, Batch 252/462, Loss: 0.9238103628158569\n",
      "Epoch 6, Batch 253/462, Loss: 0.7188495993614197\n",
      "Epoch 6, Batch 254/462, Loss: 0.802049994468689\n",
      "Epoch 6, Batch 255/462, Loss: 0.8249916434288025\n",
      "Epoch 6, Batch 256/462, Loss: 0.9411678910255432\n",
      "Epoch 6, Batch 257/462, Loss: 0.8402673006057739\n",
      "Epoch 6, Batch 258/462, Loss: 0.8637923002243042\n",
      "Epoch 6, Batch 259/462, Loss: 0.7984734177589417\n",
      "Epoch 6, Batch 260/462, Loss: 0.8863250017166138\n",
      "Epoch 6, Batch 261/462, Loss: 0.8790994882583618\n",
      "Epoch 6, Batch 262/462, Loss: 0.7455576062202454\n",
      "Epoch 6, Batch 263/462, Loss: 0.9026631116867065\n",
      "Epoch 6, Batch 264/462, Loss: 0.7047555446624756\n",
      "Epoch 6, Batch 265/462, Loss: 0.8534225821495056\n",
      "Epoch 6, Batch 266/462, Loss: 0.7795716524124146\n",
      "Epoch 6, Batch 267/462, Loss: 1.0692424774169922\n",
      "Epoch 6, Batch 268/462, Loss: 0.733747661113739\n",
      "Epoch 6, Batch 269/462, Loss: 0.836201012134552\n",
      "Epoch 6, Batch 270/462, Loss: 0.6891006231307983\n",
      "Epoch 6, Batch 271/462, Loss: 0.8747721314430237\n",
      "Epoch 6, Batch 272/462, Loss: 0.8020336031913757\n",
      "Epoch 6, Batch 273/462, Loss: 0.8683608174324036\n",
      "Epoch 6, Batch 274/462, Loss: 0.9507462978363037\n",
      "Epoch 6, Batch 275/462, Loss: 0.8766316771507263\n",
      "Epoch 6, Batch 276/462, Loss: 0.9751079678535461\n",
      "Epoch 6, Batch 277/462, Loss: 0.7979998588562012\n",
      "Epoch 6, Batch 278/462, Loss: 0.9627165198326111\n",
      "Epoch 6, Batch 279/462, Loss: 0.8296176195144653\n",
      "Epoch 6, Batch 280/462, Loss: 0.9589724540710449\n",
      "Epoch 6, Batch 281/462, Loss: 0.9010478258132935\n",
      "Epoch 6, Batch 282/462, Loss: 0.8387178778648376\n",
      "Epoch 6, Batch 283/462, Loss: 0.7500571608543396\n",
      "Epoch 6, Batch 284/462, Loss: 0.726090669631958\n",
      "Epoch 6, Batch 285/462, Loss: 0.8314886093139648\n",
      "Epoch 6, Batch 286/462, Loss: 0.8298839926719666\n",
      "Epoch 6, Batch 287/462, Loss: 0.9587656259536743\n",
      "Epoch 6, Batch 288/462, Loss: 0.9109944701194763\n",
      "Epoch 6, Batch 289/462, Loss: 0.941777765750885\n",
      "Epoch 6, Batch 290/462, Loss: 0.9071792364120483\n",
      "Epoch 6, Batch 291/462, Loss: 0.717818558216095\n",
      "Epoch 6, Batch 292/462, Loss: 0.745563805103302\n",
      "Epoch 6, Batch 293/462, Loss: 0.8982639908790588\n",
      "Epoch 6, Batch 294/462, Loss: 0.8364846706390381\n",
      "Epoch 6, Batch 295/462, Loss: 0.8546440601348877\n",
      "Epoch 6, Batch 296/462, Loss: 0.7232345938682556\n",
      "Epoch 6, Batch 297/462, Loss: 0.8416904807090759\n",
      "Epoch 6, Batch 298/462, Loss: 0.9857513904571533\n",
      "Epoch 6, Batch 299/462, Loss: 0.8030906319618225\n",
      "Epoch 6, Batch 300/462, Loss: 0.9306617379188538\n",
      "Epoch 6, Batch 301/462, Loss: 0.8082011938095093\n",
      "Epoch 6, Batch 302/462, Loss: 0.8320598602294922\n",
      "Epoch 6, Batch 303/462, Loss: 0.8769702315330505\n",
      "Epoch 6, Batch 304/462, Loss: 0.8516660332679749\n",
      "Epoch 6, Batch 305/462, Loss: 0.8844867944717407\n",
      "Epoch 6, Batch 306/462, Loss: 0.7428979873657227\n",
      "Epoch 6, Batch 307/462, Loss: 0.8512178063392639\n",
      "Epoch 6, Batch 308/462, Loss: 0.9197347164154053\n",
      "Epoch 6, Batch 309/462, Loss: 0.9625781774520874\n",
      "Epoch 6, Batch 310/462, Loss: 0.8037892580032349\n",
      "Epoch 6, Batch 311/462, Loss: 0.9513841867446899\n",
      "Epoch 6, Batch 312/462, Loss: 0.87117999792099\n",
      "Epoch 6, Batch 313/462, Loss: 0.8401105403900146\n",
      "Epoch 6, Batch 314/462, Loss: 0.870072066783905\n",
      "Epoch 6, Batch 315/462, Loss: 0.890931248664856\n",
      "Epoch 6, Batch 316/462, Loss: 0.7905244827270508\n",
      "Epoch 6, Batch 317/462, Loss: 0.8169425129890442\n",
      "Epoch 6, Batch 318/462, Loss: 0.899914026260376\n",
      "Epoch 6, Batch 319/462, Loss: 0.7914222478866577\n",
      "Epoch 6, Batch 320/462, Loss: 0.8526055812835693\n",
      "Epoch 6, Batch 321/462, Loss: 0.8389996886253357\n",
      "Epoch 6, Batch 322/462, Loss: 0.9179513454437256\n",
      "Epoch 6, Batch 323/462, Loss: 0.7301494479179382\n",
      "Epoch 6, Batch 324/462, Loss: 0.8302968144416809\n",
      "Epoch 6, Batch 325/462, Loss: 0.8424308896064758\n",
      "Epoch 6, Batch 326/462, Loss: 0.7606549859046936\n",
      "Epoch 6, Batch 327/462, Loss: 0.9133857488632202\n",
      "Epoch 6, Batch 328/462, Loss: 0.8333136439323425\n",
      "Epoch 6, Batch 329/462, Loss: 0.7171971201896667\n",
      "Epoch 6, Batch 330/462, Loss: 0.8936335444450378\n",
      "Epoch 6, Batch 331/462, Loss: 0.8513401746749878\n",
      "Epoch 6, Batch 332/462, Loss: 0.8173245191574097\n",
      "Epoch 6, Batch 333/462, Loss: 0.8059760332107544\n",
      "Epoch 6, Batch 334/462, Loss: 0.8835095763206482\n",
      "Epoch 6, Batch 335/462, Loss: 0.9395714998245239\n",
      "Epoch 6, Batch 336/462, Loss: 0.8155068159103394\n",
      "Epoch 6, Batch 337/462, Loss: 0.7522689700126648\n",
      "Epoch 6, Batch 338/462, Loss: 0.7556986212730408\n",
      "Epoch 6, Batch 339/462, Loss: 0.8250831365585327\n",
      "Epoch 6, Batch 340/462, Loss: 0.9417833685874939\n",
      "Epoch 6, Batch 341/462, Loss: 0.8002728819847107\n",
      "Epoch 6, Batch 342/462, Loss: 1.0308325290679932\n",
      "Epoch 6, Batch 343/462, Loss: 1.0008935928344727\n",
      "Epoch 6, Batch 344/462, Loss: 0.8033349514007568\n",
      "Epoch 6, Batch 345/462, Loss: 0.8108842372894287\n",
      "Epoch 6, Batch 346/462, Loss: 0.8286134600639343\n",
      "Epoch 6, Batch 347/462, Loss: 0.7308307886123657\n",
      "Epoch 6, Batch 348/462, Loss: 1.0644272565841675\n",
      "Epoch 6, Batch 349/462, Loss: 0.8023890852928162\n",
      "Epoch 6, Batch 350/462, Loss: 0.8448193669319153\n",
      "Epoch 6, Batch 351/462, Loss: 0.729663074016571\n",
      "Epoch 6, Batch 352/462, Loss: 0.7769010066986084\n",
      "Epoch 6, Batch 353/462, Loss: 0.925823986530304\n",
      "Epoch 6, Batch 354/462, Loss: 0.8186708688735962\n",
      "Epoch 6, Batch 355/462, Loss: 0.7985836863517761\n",
      "Epoch 6, Batch 356/462, Loss: 0.8269386887550354\n",
      "Epoch 6, Batch 357/462, Loss: 0.9205299019813538\n",
      "Epoch 6, Batch 358/462, Loss: 0.7496627569198608\n",
      "Epoch 6, Batch 359/462, Loss: 0.7737164497375488\n",
      "Epoch 6, Batch 360/462, Loss: 0.730797290802002\n",
      "Epoch 6, Batch 361/462, Loss: 0.8110353946685791\n",
      "Epoch 6, Batch 362/462, Loss: 0.6826775074005127\n",
      "Epoch 6, Batch 363/462, Loss: 0.7756878137588501\n",
      "Epoch 6, Batch 364/462, Loss: 0.7422342300415039\n",
      "Epoch 6, Batch 365/462, Loss: 0.7931092977523804\n",
      "Epoch 6, Batch 366/462, Loss: 1.0165321826934814\n",
      "Epoch 6, Batch 367/462, Loss: 0.8409184813499451\n",
      "Epoch 6, Batch 368/462, Loss: 0.8471593856811523\n",
      "Epoch 6, Batch 369/462, Loss: 0.7285965085029602\n",
      "Epoch 6, Batch 370/462, Loss: 0.7554068565368652\n",
      "Epoch 6, Batch 371/462, Loss: 0.7453691959381104\n",
      "Epoch 6, Batch 372/462, Loss: 0.901061475276947\n",
      "Epoch 6, Batch 373/462, Loss: 0.8206899762153625\n",
      "Epoch 6, Batch 374/462, Loss: 0.802715539932251\n",
      "Epoch 6, Batch 375/462, Loss: 0.9429483413696289\n",
      "Epoch 6, Batch 376/462, Loss: 0.7108199000358582\n",
      "Epoch 6, Batch 377/462, Loss: 0.7228962182998657\n",
      "Epoch 6, Batch 378/462, Loss: 0.9119521975517273\n",
      "Epoch 6, Batch 379/462, Loss: 0.8889565467834473\n",
      "Epoch 6, Batch 380/462, Loss: 0.9079397916793823\n",
      "Epoch 6, Batch 381/462, Loss: 0.9875860810279846\n",
      "Epoch 6, Batch 382/462, Loss: 0.752934455871582\n",
      "Epoch 6, Batch 383/462, Loss: 0.7780634760856628\n",
      "Epoch 6, Batch 384/462, Loss: 0.8576270937919617\n",
      "Epoch 6, Batch 385/462, Loss: 0.8191076517105103\n",
      "Epoch 6, Batch 386/462, Loss: 0.9060109853744507\n",
      "Epoch 6, Batch 387/462, Loss: 0.7685434818267822\n",
      "Epoch 6, Batch 388/462, Loss: 0.6968091130256653\n",
      "Epoch 6, Batch 389/462, Loss: 0.949205219745636\n",
      "Epoch 6, Batch 390/462, Loss: 0.925444483757019\n",
      "Epoch 6, Batch 391/462, Loss: 0.9307963848114014\n",
      "Epoch 6, Batch 392/462, Loss: 0.9408513307571411\n",
      "Epoch 6, Batch 393/462, Loss: 0.8234450221061707\n",
      "Epoch 6, Batch 394/462, Loss: 0.7443855404853821\n",
      "Epoch 6, Batch 395/462, Loss: 0.8871665596961975\n",
      "Epoch 6, Batch 396/462, Loss: 0.8077194690704346\n",
      "Epoch 6, Batch 397/462, Loss: 0.738779604434967\n",
      "Epoch 6, Batch 398/462, Loss: 0.9372312426567078\n",
      "Epoch 6, Batch 399/462, Loss: 0.8384767770767212\n",
      "Epoch 6, Batch 400/462, Loss: 0.7694585919380188\n",
      "Epoch 6, Batch 401/462, Loss: 0.8279929161071777\n",
      "Epoch 6, Batch 402/462, Loss: 0.769002377986908\n",
      "Epoch 6, Batch 403/462, Loss: 0.8839582204818726\n",
      "Epoch 6, Batch 404/462, Loss: 0.978326141834259\n",
      "Epoch 6, Batch 405/462, Loss: 0.9027383327484131\n",
      "Epoch 6, Batch 406/462, Loss: 0.8790245056152344\n",
      "Epoch 6, Batch 407/462, Loss: 0.7572674751281738\n",
      "Epoch 6, Batch 408/462, Loss: 0.9830165505409241\n",
      "Epoch 6, Batch 409/462, Loss: 0.824709951877594\n",
      "Epoch 6, Batch 410/462, Loss: 0.9045052528381348\n",
      "Epoch 6, Batch 411/462, Loss: 0.8273563981056213\n",
      "Epoch 6, Batch 412/462, Loss: 0.7820761799812317\n",
      "Epoch 6, Batch 413/462, Loss: 0.811554491519928\n",
      "Epoch 6, Batch 414/462, Loss: 0.8877696394920349\n",
      "Epoch 6, Batch 415/462, Loss: 0.8874395489692688\n",
      "Epoch 6, Batch 416/462, Loss: 0.9491984844207764\n",
      "Epoch 6, Batch 417/462, Loss: 0.8225162029266357\n",
      "Epoch 6, Batch 418/462, Loss: 1.099501132965088\n",
      "Epoch 6, Batch 419/462, Loss: 0.867610514163971\n",
      "Epoch 6, Batch 420/462, Loss: 0.792992353439331\n",
      "Epoch 6, Batch 421/462, Loss: 0.7563649415969849\n",
      "Epoch 6, Batch 422/462, Loss: 0.9131386280059814\n",
      "Epoch 6, Batch 423/462, Loss: 0.9022737145423889\n",
      "Epoch 6, Batch 424/462, Loss: 0.9830169081687927\n",
      "Epoch 6, Batch 425/462, Loss: 0.7679263949394226\n",
      "Epoch 6, Batch 426/462, Loss: 1.0372611284255981\n",
      "Epoch 6, Batch 427/462, Loss: 0.9509507417678833\n",
      "Epoch 6, Batch 428/462, Loss: 0.8739274144172668\n",
      "Epoch 6, Batch 429/462, Loss: 0.9082105159759521\n",
      "Epoch 6, Batch 430/462, Loss: 0.7658858299255371\n",
      "Epoch 6, Batch 431/462, Loss: 0.8302382826805115\n",
      "Epoch 6, Batch 432/462, Loss: 0.9610830545425415\n",
      "Epoch 6, Batch 433/462, Loss: 0.8751026391983032\n",
      "Epoch 6, Batch 434/462, Loss: 0.8111068606376648\n",
      "Epoch 6, Batch 435/462, Loss: 0.7360765933990479\n",
      "Epoch 6, Batch 436/462, Loss: 0.7974486947059631\n",
      "Epoch 6, Batch 437/462, Loss: 0.8756111860275269\n",
      "Epoch 6, Batch 438/462, Loss: 0.7607881426811218\n",
      "Epoch 6, Batch 439/462, Loss: 0.8584349155426025\n",
      "Epoch 6, Batch 440/462, Loss: 0.9502916932106018\n",
      "Epoch 6, Batch 441/462, Loss: 0.9096329212188721\n",
      "Epoch 6, Batch 442/462, Loss: 0.7430092096328735\n",
      "Epoch 6, Batch 443/462, Loss: 1.1608107089996338\n",
      "Epoch 6, Batch 444/462, Loss: 0.8828562498092651\n",
      "Epoch 6, Batch 445/462, Loss: 0.7405567765235901\n",
      "Epoch 6, Batch 446/462, Loss: 0.910244345664978\n",
      "Epoch 6, Batch 447/462, Loss: 0.8097534775733948\n",
      "Epoch 6, Batch 448/462, Loss: 0.8398016691207886\n",
      "Epoch 6, Batch 449/462, Loss: 0.7399585247039795\n",
      "Epoch 6, Batch 450/462, Loss: 0.9343115091323853\n",
      "Epoch 6, Batch 451/462, Loss: 0.7712594866752625\n",
      "Epoch 6, Batch 452/462, Loss: 0.8418174386024475\n",
      "Epoch 6, Batch 453/462, Loss: 0.785713791847229\n",
      "Epoch 6, Batch 454/462, Loss: 0.9282524585723877\n",
      "Epoch 6, Batch 455/462, Loss: 0.8601658940315247\n",
      "Epoch 6, Batch 456/462, Loss: 0.683843195438385\n",
      "Epoch 6, Batch 457/462, Loss: 0.8347375988960266\n",
      "Epoch 6, Batch 458/462, Loss: 0.9131991863250732\n",
      "Epoch 6, Batch 459/462, Loss: 0.824363112449646\n",
      "Epoch 6, Batch 460/462, Loss: 0.8150352835655212\n",
      "Epoch 6, Batch 461/462, Loss: 0.8911184072494507\n",
      "Epoch 6, Batch 462/462, Loss: 0.7706121206283569\n",
      "Epoch 6, Loss: 390.77446061372757\n",
      "Epoch 7, Batch 1/462, Loss: 0.8563353419303894\n",
      "Epoch 7, Batch 2/462, Loss: 0.8229225277900696\n",
      "Epoch 7, Batch 3/462, Loss: 0.9378390312194824\n",
      "Epoch 7, Batch 4/462, Loss: 0.846795380115509\n",
      "Epoch 7, Batch 5/462, Loss: 0.9664552807807922\n",
      "Epoch 7, Batch 6/462, Loss: 0.8088421821594238\n",
      "Epoch 7, Batch 7/462, Loss: 0.7222988605499268\n",
      "Epoch 7, Batch 8/462, Loss: 0.970602810382843\n",
      "Epoch 7, Batch 9/462, Loss: 0.8482898473739624\n",
      "Epoch 7, Batch 10/462, Loss: 0.8503494262695312\n",
      "Epoch 7, Batch 11/462, Loss: 0.8578159213066101\n",
      "Epoch 7, Batch 12/462, Loss: 0.7704936861991882\n",
      "Epoch 7, Batch 13/462, Loss: 0.9068264961242676\n",
      "Epoch 7, Batch 14/462, Loss: 0.8470360040664673\n",
      "Epoch 7, Batch 15/462, Loss: 0.8578879833221436\n",
      "Epoch 7, Batch 16/462, Loss: 0.7576594948768616\n",
      "Epoch 7, Batch 17/462, Loss: 0.7457034587860107\n",
      "Epoch 7, Batch 18/462, Loss: 0.8526845574378967\n",
      "Epoch 7, Batch 19/462, Loss: 0.7480089664459229\n",
      "Epoch 7, Batch 20/462, Loss: 0.8664899468421936\n",
      "Epoch 7, Batch 21/462, Loss: 0.8128070831298828\n",
      "Epoch 7, Batch 22/462, Loss: 0.7652472853660583\n",
      "Epoch 7, Batch 23/462, Loss: 0.8507751822471619\n",
      "Epoch 7, Batch 24/462, Loss: 0.8474622368812561\n",
      "Epoch 7, Batch 25/462, Loss: 0.8131585717201233\n",
      "Epoch 7, Batch 26/462, Loss: 0.8766946792602539\n",
      "Epoch 7, Batch 27/462, Loss: 0.884781539440155\n",
      "Epoch 7, Batch 28/462, Loss: 0.9267618060112\n",
      "Epoch 7, Batch 29/462, Loss: 0.8709721565246582\n",
      "Epoch 7, Batch 30/462, Loss: 0.7790512442588806\n",
      "Epoch 7, Batch 31/462, Loss: 0.9409823417663574\n",
      "Epoch 7, Batch 32/462, Loss: 0.696194052696228\n",
      "Epoch 7, Batch 33/462, Loss: 0.8342337608337402\n",
      "Epoch 7, Batch 34/462, Loss: 0.8792805075645447\n",
      "Epoch 7, Batch 35/462, Loss: 0.7567446827888489\n",
      "Epoch 7, Batch 36/462, Loss: 0.8459488153457642\n",
      "Epoch 7, Batch 37/462, Loss: 0.947310745716095\n",
      "Epoch 7, Batch 38/462, Loss: 0.8355622291564941\n",
      "Epoch 7, Batch 39/462, Loss: 0.7003557085990906\n",
      "Epoch 7, Batch 40/462, Loss: 0.8755831718444824\n",
      "Epoch 7, Batch 41/462, Loss: 0.8817307949066162\n",
      "Epoch 7, Batch 42/462, Loss: 0.7770895957946777\n",
      "Epoch 7, Batch 43/462, Loss: 0.8527083992958069\n",
      "Epoch 7, Batch 44/462, Loss: 0.7226637601852417\n",
      "Epoch 7, Batch 45/462, Loss: 0.9130703210830688\n",
      "Epoch 7, Batch 46/462, Loss: 0.8637818694114685\n",
      "Epoch 7, Batch 47/462, Loss: 0.7677786946296692\n",
      "Epoch 7, Batch 48/462, Loss: 1.0332852602005005\n",
      "Epoch 7, Batch 49/462, Loss: 0.8511714339256287\n",
      "Epoch 7, Batch 50/462, Loss: 0.8556532263755798\n",
      "Epoch 7, Batch 51/462, Loss: 0.7501095533370972\n",
      "Epoch 7, Batch 52/462, Loss: 0.8743079304695129\n",
      "Epoch 7, Batch 53/462, Loss: 0.8013788461685181\n",
      "Epoch 7, Batch 54/462, Loss: 0.9437761306762695\n",
      "Epoch 7, Batch 55/462, Loss: 0.8398062586784363\n",
      "Epoch 7, Batch 56/462, Loss: 0.769425094127655\n",
      "Epoch 7, Batch 57/462, Loss: 0.8938140273094177\n",
      "Epoch 7, Batch 58/462, Loss: 0.7878853678703308\n",
      "Epoch 7, Batch 59/462, Loss: 0.8228991627693176\n",
      "Epoch 7, Batch 60/462, Loss: 0.8098339438438416\n",
      "Epoch 7, Batch 61/462, Loss: 0.7541887164115906\n",
      "Epoch 7, Batch 62/462, Loss: 0.9052197337150574\n",
      "Epoch 7, Batch 63/462, Loss: 0.8575618267059326\n",
      "Epoch 7, Batch 64/462, Loss: 0.7557500600814819\n",
      "Epoch 7, Batch 65/462, Loss: 0.7776062488555908\n",
      "Epoch 7, Batch 66/462, Loss: 0.7354200482368469\n",
      "Epoch 7, Batch 67/462, Loss: 0.8223099708557129\n",
      "Epoch 7, Batch 68/462, Loss: 0.7163058519363403\n",
      "Epoch 7, Batch 69/462, Loss: 0.8290808200836182\n",
      "Epoch 7, Batch 70/462, Loss: 0.7963120341300964\n",
      "Epoch 7, Batch 71/462, Loss: 0.8172555565834045\n",
      "Epoch 7, Batch 72/462, Loss: 0.888172447681427\n",
      "Epoch 7, Batch 73/462, Loss: 0.9892446398735046\n",
      "Epoch 7, Batch 74/462, Loss: 0.806721031665802\n",
      "Epoch 7, Batch 75/462, Loss: 0.7451029419898987\n",
      "Epoch 7, Batch 76/462, Loss: 0.8145586848258972\n",
      "Epoch 7, Batch 77/462, Loss: 0.7617515325546265\n",
      "Epoch 7, Batch 78/462, Loss: 0.8638018369674683\n",
      "Epoch 7, Batch 79/462, Loss: 0.8354021310806274\n",
      "Epoch 7, Batch 80/462, Loss: 0.8456643223762512\n",
      "Epoch 7, Batch 81/462, Loss: 0.7521224617958069\n",
      "Epoch 7, Batch 82/462, Loss: 0.9334418177604675\n",
      "Epoch 7, Batch 83/462, Loss: 0.8913493156433105\n",
      "Epoch 7, Batch 84/462, Loss: 0.7595663666725159\n",
      "Epoch 7, Batch 85/462, Loss: 0.9911093711853027\n",
      "Epoch 7, Batch 86/462, Loss: 0.8531481623649597\n",
      "Epoch 7, Batch 87/462, Loss: 0.799034595489502\n",
      "Epoch 7, Batch 88/462, Loss: 0.8456835746765137\n",
      "Epoch 7, Batch 89/462, Loss: 0.7287499904632568\n",
      "Epoch 7, Batch 90/462, Loss: 0.9672830700874329\n",
      "Epoch 7, Batch 91/462, Loss: 0.8901734352111816\n",
      "Epoch 7, Batch 92/462, Loss: 0.9897369146347046\n",
      "Epoch 7, Batch 93/462, Loss: 0.814375638961792\n",
      "Epoch 7, Batch 94/462, Loss: 0.9258807301521301\n",
      "Epoch 7, Batch 95/462, Loss: 0.8766078352928162\n",
      "Epoch 7, Batch 96/462, Loss: 0.9311145544052124\n",
      "Epoch 7, Batch 97/462, Loss: 0.8190016746520996\n",
      "Epoch 7, Batch 98/462, Loss: 0.7598078846931458\n",
      "Epoch 7, Batch 99/462, Loss: 0.8377414345741272\n",
      "Epoch 7, Batch 100/462, Loss: 0.8969627618789673\n",
      "Epoch 7, Batch 101/462, Loss: 0.8766331076622009\n",
      "Epoch 7, Batch 102/462, Loss: 0.8740954399108887\n",
      "Epoch 7, Batch 103/462, Loss: 0.796061635017395\n",
      "Epoch 7, Batch 104/462, Loss: 0.8180484175682068\n",
      "Epoch 7, Batch 105/462, Loss: 0.805207371711731\n",
      "Epoch 7, Batch 106/462, Loss: 0.8813644051551819\n",
      "Epoch 7, Batch 107/462, Loss: 0.942570149898529\n",
      "Epoch 7, Batch 108/462, Loss: 0.9649310111999512\n",
      "Epoch 7, Batch 109/462, Loss: 0.703223466873169\n",
      "Epoch 7, Batch 110/462, Loss: 0.8721075654029846\n",
      "Epoch 7, Batch 111/462, Loss: 0.8353903293609619\n",
      "Epoch 7, Batch 112/462, Loss: 0.8970664143562317\n",
      "Epoch 7, Batch 113/462, Loss: 0.9091131687164307\n",
      "Epoch 7, Batch 114/462, Loss: 0.8244738578796387\n",
      "Epoch 7, Batch 115/462, Loss: 0.8444230556488037\n",
      "Epoch 7, Batch 116/462, Loss: 0.7793642282485962\n",
      "Epoch 7, Batch 117/462, Loss: 0.8319284915924072\n",
      "Epoch 7, Batch 118/462, Loss: 0.8048343062400818\n",
      "Epoch 7, Batch 119/462, Loss: 0.8461439609527588\n",
      "Epoch 7, Batch 120/462, Loss: 0.867874026298523\n",
      "Epoch 7, Batch 121/462, Loss: 0.7505903244018555\n",
      "Epoch 7, Batch 122/462, Loss: 0.8076640963554382\n",
      "Epoch 7, Batch 123/462, Loss: 0.8401095271110535\n",
      "Epoch 7, Batch 124/462, Loss: 0.8004544377326965\n",
      "Epoch 7, Batch 125/462, Loss: 0.7954206466674805\n",
      "Epoch 7, Batch 126/462, Loss: 0.8076615333557129\n",
      "Epoch 7, Batch 127/462, Loss: 0.8325510621070862\n",
      "Epoch 7, Batch 128/462, Loss: 0.7174034118652344\n",
      "Epoch 7, Batch 129/462, Loss: 0.7380868196487427\n",
      "Epoch 7, Batch 130/462, Loss: 0.9430748820304871\n",
      "Epoch 7, Batch 131/462, Loss: 0.9737213850021362\n",
      "Epoch 7, Batch 132/462, Loss: 0.8412827253341675\n",
      "Epoch 7, Batch 133/462, Loss: 0.8747086524963379\n",
      "Epoch 7, Batch 134/462, Loss: 0.966976523399353\n",
      "Epoch 7, Batch 135/462, Loss: 0.9116253852844238\n",
      "Epoch 7, Batch 136/462, Loss: 0.9108442664146423\n",
      "Epoch 7, Batch 137/462, Loss: 0.8459051847457886\n",
      "Epoch 7, Batch 138/462, Loss: 0.7949045300483704\n",
      "Epoch 7, Batch 139/462, Loss: 0.7748926281929016\n",
      "Epoch 7, Batch 140/462, Loss: 0.8241687417030334\n",
      "Epoch 7, Batch 141/462, Loss: 0.6606466174125671\n",
      "Epoch 7, Batch 142/462, Loss: 0.7420836687088013\n",
      "Epoch 7, Batch 143/462, Loss: 0.7716585993766785\n",
      "Epoch 7, Batch 144/462, Loss: 0.8691838979721069\n",
      "Epoch 7, Batch 145/462, Loss: 0.8330243825912476\n",
      "Epoch 7, Batch 146/462, Loss: 0.9480257630348206\n",
      "Epoch 7, Batch 147/462, Loss: 0.8031754493713379\n",
      "Epoch 7, Batch 148/462, Loss: 0.8102843165397644\n",
      "Epoch 7, Batch 149/462, Loss: 0.7773883938789368\n",
      "Epoch 7, Batch 150/462, Loss: 0.7675188779830933\n",
      "Epoch 7, Batch 151/462, Loss: 0.8979371786117554\n",
      "Epoch 7, Batch 152/462, Loss: 0.7860569357872009\n",
      "Epoch 7, Batch 153/462, Loss: 0.7829285860061646\n",
      "Epoch 7, Batch 154/462, Loss: 0.6837443113327026\n",
      "Epoch 7, Batch 155/462, Loss: 0.6998648643493652\n",
      "Epoch 7, Batch 156/462, Loss: 0.8818656802177429\n",
      "Epoch 7, Batch 157/462, Loss: 0.789093554019928\n",
      "Epoch 7, Batch 158/462, Loss: 0.7259632349014282\n",
      "Epoch 7, Batch 159/462, Loss: 0.7875438332557678\n",
      "Epoch 7, Batch 160/462, Loss: 0.8278729915618896\n",
      "Epoch 7, Batch 161/462, Loss: 0.893496572971344\n",
      "Epoch 7, Batch 162/462, Loss: 0.8472885489463806\n",
      "Epoch 7, Batch 163/462, Loss: 0.8844336867332458\n",
      "Epoch 7, Batch 164/462, Loss: 0.8123657703399658\n",
      "Epoch 7, Batch 165/462, Loss: 0.8358638286590576\n",
      "Epoch 7, Batch 166/462, Loss: 0.7314059138298035\n",
      "Epoch 7, Batch 167/462, Loss: 0.6857868432998657\n",
      "Epoch 7, Batch 168/462, Loss: 0.9229047894477844\n",
      "Epoch 7, Batch 169/462, Loss: 0.8043316006660461\n",
      "Epoch 7, Batch 170/462, Loss: 0.7367318272590637\n",
      "Epoch 7, Batch 171/462, Loss: 0.9361782670021057\n",
      "Epoch 7, Batch 172/462, Loss: 0.8700211048126221\n",
      "Epoch 7, Batch 173/462, Loss: 0.9097553491592407\n",
      "Epoch 7, Batch 174/462, Loss: 0.8269256353378296\n",
      "Epoch 7, Batch 175/462, Loss: 0.7344130873680115\n",
      "Epoch 7, Batch 176/462, Loss: 0.9271137714385986\n",
      "Epoch 7, Batch 177/462, Loss: 0.8058807849884033\n",
      "Epoch 7, Batch 178/462, Loss: 0.7145708203315735\n",
      "Epoch 7, Batch 179/462, Loss: 0.921604335308075\n",
      "Epoch 7, Batch 180/462, Loss: 0.9466139674186707\n",
      "Epoch 7, Batch 181/462, Loss: 0.870773434638977\n",
      "Epoch 7, Batch 182/462, Loss: 0.7920854091644287\n",
      "Epoch 7, Batch 183/462, Loss: 0.8769859075546265\n",
      "Epoch 7, Batch 184/462, Loss: 0.772408664226532\n",
      "Epoch 7, Batch 185/462, Loss: 0.7088263034820557\n",
      "Epoch 7, Batch 186/462, Loss: 0.9174624681472778\n",
      "Epoch 7, Batch 187/462, Loss: 0.854379415512085\n",
      "Epoch 7, Batch 188/462, Loss: 0.8709235191345215\n",
      "Epoch 7, Batch 189/462, Loss: 0.7705349326133728\n",
      "Epoch 7, Batch 190/462, Loss: 0.8500028848648071\n",
      "Epoch 7, Batch 191/462, Loss: 0.8242909908294678\n",
      "Epoch 7, Batch 192/462, Loss: 0.9982298612594604\n",
      "Epoch 7, Batch 193/462, Loss: 0.8377004265785217\n",
      "Epoch 7, Batch 194/462, Loss: 0.7802180051803589\n",
      "Epoch 7, Batch 195/462, Loss: 0.8433723449707031\n",
      "Epoch 7, Batch 196/462, Loss: 0.7753053307533264\n",
      "Epoch 7, Batch 197/462, Loss: 0.7291954159736633\n",
      "Epoch 7, Batch 198/462, Loss: 0.8429098129272461\n",
      "Epoch 7, Batch 199/462, Loss: 0.864578127861023\n",
      "Epoch 7, Batch 200/462, Loss: 0.8246474266052246\n",
      "Epoch 7, Batch 201/462, Loss: 0.6670385599136353\n",
      "Epoch 7, Batch 202/462, Loss: 0.8701110482215881\n",
      "Epoch 7, Batch 203/462, Loss: 0.8653473258018494\n",
      "Epoch 7, Batch 204/462, Loss: 0.7885552644729614\n",
      "Epoch 7, Batch 205/462, Loss: 0.8442116975784302\n",
      "Epoch 7, Batch 206/462, Loss: 0.8787255883216858\n",
      "Epoch 7, Batch 207/462, Loss: 0.8083674311637878\n",
      "Epoch 7, Batch 208/462, Loss: 0.6667997241020203\n",
      "Epoch 7, Batch 209/462, Loss: 0.7222052216529846\n",
      "Epoch 7, Batch 210/462, Loss: 0.8375914096832275\n",
      "Epoch 7, Batch 211/462, Loss: 0.890439510345459\n",
      "Epoch 7, Batch 212/462, Loss: 0.8584082126617432\n",
      "Epoch 7, Batch 213/462, Loss: 0.7516419291496277\n",
      "Epoch 7, Batch 214/462, Loss: 0.7433961629867554\n",
      "Epoch 7, Batch 215/462, Loss: 0.8581082820892334\n",
      "Epoch 7, Batch 216/462, Loss: 0.6679978966712952\n",
      "Epoch 7, Batch 217/462, Loss: 0.7862662672996521\n",
      "Epoch 7, Batch 218/462, Loss: 0.8837800025939941\n",
      "Epoch 7, Batch 219/462, Loss: 0.8401074409484863\n",
      "Epoch 7, Batch 220/462, Loss: 0.9952134490013123\n",
      "Epoch 7, Batch 221/462, Loss: 0.7820600867271423\n",
      "Epoch 7, Batch 222/462, Loss: 0.9446460008621216\n",
      "Epoch 7, Batch 223/462, Loss: 0.8141424059867859\n",
      "Epoch 7, Batch 224/462, Loss: 0.7793527245521545\n",
      "Epoch 7, Batch 225/462, Loss: 0.7993854880332947\n",
      "Epoch 7, Batch 226/462, Loss: 0.9069100618362427\n",
      "Epoch 7, Batch 227/462, Loss: 0.9980673789978027\n",
      "Epoch 7, Batch 228/462, Loss: 0.7323791980743408\n",
      "Epoch 7, Batch 229/462, Loss: 0.8776111602783203\n",
      "Epoch 7, Batch 230/462, Loss: 0.8733924627304077\n",
      "Epoch 7, Batch 231/462, Loss: 0.9113859534263611\n",
      "Epoch 7, Batch 232/462, Loss: 0.9490812420845032\n",
      "Epoch 7, Batch 233/462, Loss: 0.7717533707618713\n",
      "Epoch 7, Batch 234/462, Loss: 0.7481295466423035\n",
      "Epoch 7, Batch 235/462, Loss: 0.7145019769668579\n",
      "Epoch 7, Batch 236/462, Loss: 0.6928713917732239\n",
      "Epoch 7, Batch 237/462, Loss: 0.844476044178009\n",
      "Epoch 7, Batch 238/462, Loss: 0.7346104979515076\n",
      "Epoch 7, Batch 239/462, Loss: 0.7359532713890076\n",
      "Epoch 7, Batch 240/462, Loss: 0.8297237753868103\n",
      "Epoch 7, Batch 241/462, Loss: 0.7366881966590881\n",
      "Epoch 7, Batch 242/462, Loss: 0.85941481590271\n",
      "Epoch 7, Batch 243/462, Loss: 0.940916121006012\n",
      "Epoch 7, Batch 244/462, Loss: 0.7440111637115479\n",
      "Epoch 7, Batch 245/462, Loss: 0.8203241229057312\n",
      "Epoch 7, Batch 246/462, Loss: 0.8430628776550293\n",
      "Epoch 7, Batch 247/462, Loss: 0.7619132399559021\n",
      "Epoch 7, Batch 248/462, Loss: 0.9381887912750244\n",
      "Epoch 7, Batch 249/462, Loss: 0.7693182826042175\n",
      "Epoch 7, Batch 250/462, Loss: 0.7480477690696716\n",
      "Epoch 7, Batch 251/462, Loss: 0.6739096641540527\n",
      "Epoch 7, Batch 252/462, Loss: 0.8134141564369202\n",
      "Epoch 7, Batch 253/462, Loss: 0.8631195425987244\n",
      "Epoch 7, Batch 254/462, Loss: 0.7144372463226318\n",
      "Epoch 7, Batch 255/462, Loss: 0.696962296962738\n",
      "Epoch 7, Batch 256/462, Loss: 1.0248992443084717\n",
      "Epoch 7, Batch 257/462, Loss: 0.911653459072113\n",
      "Epoch 7, Batch 258/462, Loss: 0.9533175230026245\n",
      "Epoch 7, Batch 259/462, Loss: 0.912057638168335\n",
      "Epoch 7, Batch 260/462, Loss: 0.8563198447227478\n",
      "Epoch 7, Batch 261/462, Loss: 0.958007276058197\n",
      "Epoch 7, Batch 262/462, Loss: 0.7279428243637085\n",
      "Epoch 7, Batch 263/462, Loss: 0.8296612501144409\n",
      "Epoch 7, Batch 264/462, Loss: 0.8245074152946472\n",
      "Epoch 7, Batch 265/462, Loss: 0.6783627271652222\n",
      "Epoch 7, Batch 266/462, Loss: 0.9449158906936646\n",
      "Epoch 7, Batch 267/462, Loss: 0.8519710302352905\n",
      "Epoch 7, Batch 268/462, Loss: 0.7608886361122131\n",
      "Epoch 7, Batch 269/462, Loss: 0.8299528956413269\n",
      "Epoch 7, Batch 270/462, Loss: 0.6902672052383423\n",
      "Epoch 7, Batch 271/462, Loss: 0.7781214118003845\n",
      "Epoch 7, Batch 272/462, Loss: 0.7554513812065125\n",
      "Epoch 7, Batch 273/462, Loss: 0.8109525442123413\n",
      "Epoch 7, Batch 274/462, Loss: 0.8444551825523376\n",
      "Epoch 7, Batch 275/462, Loss: 0.9268746972084045\n",
      "Epoch 7, Batch 276/462, Loss: 0.9599910974502563\n",
      "Epoch 7, Batch 277/462, Loss: 0.9961944222450256\n",
      "Epoch 7, Batch 278/462, Loss: 0.9567229747772217\n",
      "Epoch 7, Batch 279/462, Loss: 0.8297300934791565\n",
      "Epoch 7, Batch 280/462, Loss: 0.6122230291366577\n",
      "Epoch 7, Batch 281/462, Loss: 0.8758900761604309\n",
      "Epoch 7, Batch 282/462, Loss: 0.8512937426567078\n",
      "Epoch 7, Batch 283/462, Loss: 0.9963206052780151\n",
      "Epoch 7, Batch 284/462, Loss: 0.8797463774681091\n",
      "Epoch 7, Batch 285/462, Loss: 0.765681266784668\n",
      "Epoch 7, Batch 286/462, Loss: 0.7070853114128113\n",
      "Epoch 7, Batch 287/462, Loss: 0.9132004976272583\n",
      "Epoch 7, Batch 288/462, Loss: 0.6637078523635864\n",
      "Epoch 7, Batch 289/462, Loss: 0.9584918022155762\n",
      "Epoch 7, Batch 290/462, Loss: 0.7672200798988342\n",
      "Epoch 7, Batch 291/462, Loss: 0.7936161160469055\n",
      "Epoch 7, Batch 292/462, Loss: 0.8075048923492432\n",
      "Epoch 7, Batch 293/462, Loss: 0.9141486883163452\n",
      "Epoch 7, Batch 294/462, Loss: 0.9250049591064453\n",
      "Epoch 7, Batch 295/462, Loss: 0.7067043781280518\n",
      "Epoch 7, Batch 296/462, Loss: 0.8550336956977844\n",
      "Epoch 7, Batch 297/462, Loss: 0.87660813331604\n",
      "Epoch 7, Batch 298/462, Loss: 0.8052587509155273\n",
      "Epoch 7, Batch 299/462, Loss: 0.9584306478500366\n",
      "Epoch 7, Batch 300/462, Loss: 0.8469464778900146\n",
      "Epoch 7, Batch 301/462, Loss: 0.8290771842002869\n",
      "Epoch 7, Batch 302/462, Loss: 0.9046777486801147\n",
      "Epoch 7, Batch 303/462, Loss: 0.8224310874938965\n",
      "Epoch 7, Batch 304/462, Loss: 0.6650978326797485\n",
      "Epoch 7, Batch 305/462, Loss: 0.9164651036262512\n",
      "Epoch 7, Batch 306/462, Loss: 0.8846811056137085\n",
      "Epoch 7, Batch 307/462, Loss: 0.8224155902862549\n",
      "Epoch 7, Batch 308/462, Loss: 0.7612000703811646\n",
      "Epoch 7, Batch 309/462, Loss: 0.785071074962616\n",
      "Epoch 7, Batch 310/462, Loss: 0.7210532426834106\n",
      "Epoch 7, Batch 311/462, Loss: 0.8207929730415344\n",
      "Epoch 7, Batch 312/462, Loss: 0.8913584351539612\n",
      "Epoch 7, Batch 313/462, Loss: 0.8333253264427185\n",
      "Epoch 7, Batch 314/462, Loss: 0.8390467166900635\n",
      "Epoch 7, Batch 315/462, Loss: 0.6890143752098083\n",
      "Epoch 7, Batch 316/462, Loss: 0.8277550339698792\n",
      "Epoch 7, Batch 317/462, Loss: 0.7798758149147034\n",
      "Epoch 7, Batch 318/462, Loss: 0.806618869304657\n",
      "Epoch 7, Batch 319/462, Loss: 0.7086134552955627\n",
      "Epoch 7, Batch 320/462, Loss: 0.9128348231315613\n",
      "Epoch 7, Batch 321/462, Loss: 0.8419548869132996\n",
      "Epoch 7, Batch 322/462, Loss: 0.9781281352043152\n",
      "Epoch 7, Batch 323/462, Loss: 0.9348635077476501\n",
      "Epoch 7, Batch 324/462, Loss: 0.7771043181419373\n",
      "Epoch 7, Batch 325/462, Loss: 0.7949678301811218\n",
      "Epoch 7, Batch 326/462, Loss: 0.7596650719642639\n",
      "Epoch 7, Batch 327/462, Loss: 0.8775222897529602\n",
      "Epoch 7, Batch 328/462, Loss: 0.8539305925369263\n",
      "Epoch 7, Batch 329/462, Loss: 0.8179166913032532\n",
      "Epoch 7, Batch 330/462, Loss: 0.8131920099258423\n",
      "Epoch 7, Batch 331/462, Loss: 0.8246188759803772\n",
      "Epoch 7, Batch 332/462, Loss: 0.7228477597236633\n",
      "Epoch 7, Batch 333/462, Loss: 0.7889517545700073\n",
      "Epoch 7, Batch 334/462, Loss: 0.7272664904594421\n",
      "Epoch 7, Batch 335/462, Loss: 0.8780268430709839\n",
      "Epoch 7, Batch 336/462, Loss: 0.7691988945007324\n",
      "Epoch 7, Batch 337/462, Loss: 0.9975045919418335\n",
      "Epoch 7, Batch 338/462, Loss: 0.9457886815071106\n",
      "Epoch 7, Batch 339/462, Loss: 0.7938382625579834\n",
      "Epoch 7, Batch 340/462, Loss: 0.7710760831832886\n",
      "Epoch 7, Batch 341/462, Loss: 0.7762142419815063\n",
      "Epoch 7, Batch 342/462, Loss: 0.7574132084846497\n",
      "Epoch 7, Batch 343/462, Loss: 0.8565669655799866\n",
      "Epoch 7, Batch 344/462, Loss: 0.8681697845458984\n",
      "Epoch 7, Batch 345/462, Loss: 0.796291708946228\n",
      "Epoch 7, Batch 346/462, Loss: 0.9026733636856079\n",
      "Epoch 7, Batch 347/462, Loss: 0.7889707684516907\n",
      "Epoch 7, Batch 348/462, Loss: 0.9463909268379211\n",
      "Epoch 7, Batch 349/462, Loss: 0.6843875050544739\n",
      "Epoch 7, Batch 350/462, Loss: 0.93545001745224\n",
      "Epoch 7, Batch 351/462, Loss: 0.7783007621765137\n",
      "Epoch 7, Batch 352/462, Loss: 0.8029093146324158\n",
      "Epoch 7, Batch 353/462, Loss: 0.7005064487457275\n",
      "Epoch 7, Batch 354/462, Loss: 0.8839252591133118\n",
      "Epoch 7, Batch 355/462, Loss: 0.8237220048904419\n",
      "Epoch 7, Batch 356/462, Loss: 0.86827552318573\n",
      "Epoch 7, Batch 357/462, Loss: 0.8930708169937134\n",
      "Epoch 7, Batch 358/462, Loss: 0.7163392901420593\n",
      "Epoch 7, Batch 359/462, Loss: 0.8515180349349976\n",
      "Epoch 7, Batch 360/462, Loss: 0.9056422710418701\n",
      "Epoch 7, Batch 361/462, Loss: 0.680755078792572\n",
      "Epoch 7, Batch 362/462, Loss: 0.7699955105781555\n",
      "Epoch 7, Batch 363/462, Loss: 0.8590486645698547\n",
      "Epoch 7, Batch 364/462, Loss: 0.8937570452690125\n",
      "Epoch 7, Batch 365/462, Loss: 0.7858723402023315\n",
      "Epoch 7, Batch 366/462, Loss: 0.8905141353607178\n",
      "Epoch 7, Batch 367/462, Loss: 0.8645141124725342\n",
      "Epoch 7, Batch 368/462, Loss: 0.8308385014533997\n",
      "Epoch 7, Batch 369/462, Loss: 0.71335369348526\n",
      "Epoch 7, Batch 370/462, Loss: 0.7763604521751404\n",
      "Epoch 7, Batch 371/462, Loss: 0.6493650674819946\n",
      "Epoch 7, Batch 372/462, Loss: 0.7349175810813904\n",
      "Epoch 7, Batch 373/462, Loss: 0.8154892921447754\n",
      "Epoch 7, Batch 374/462, Loss: 0.8649362921714783\n",
      "Epoch 7, Batch 375/462, Loss: 0.7552148103713989\n",
      "Epoch 7, Batch 376/462, Loss: 0.8655677437782288\n",
      "Epoch 7, Batch 377/462, Loss: 0.9065701961517334\n",
      "Epoch 7, Batch 378/462, Loss: 0.920418918132782\n",
      "Epoch 7, Batch 379/462, Loss: 0.85660719871521\n",
      "Epoch 7, Batch 380/462, Loss: 0.8981873989105225\n",
      "Epoch 7, Batch 381/462, Loss: 0.8167923092842102\n",
      "Epoch 7, Batch 382/462, Loss: 0.6884756684303284\n",
      "Epoch 7, Batch 383/462, Loss: 0.9362053275108337\n",
      "Epoch 7, Batch 384/462, Loss: 0.8863123655319214\n",
      "Epoch 7, Batch 385/462, Loss: 0.8863927125930786\n",
      "Epoch 7, Batch 386/462, Loss: 0.8235304951667786\n",
      "Epoch 7, Batch 387/462, Loss: 0.8941624164581299\n",
      "Epoch 7, Batch 388/462, Loss: 0.6650280356407166\n",
      "Epoch 7, Batch 389/462, Loss: 0.8433296084403992\n",
      "Epoch 7, Batch 390/462, Loss: 0.7988666296005249\n",
      "Epoch 7, Batch 391/462, Loss: 0.9239062666893005\n",
      "Epoch 7, Batch 392/462, Loss: 0.726196825504303\n",
      "Epoch 7, Batch 393/462, Loss: 0.9242750406265259\n",
      "Epoch 7, Batch 394/462, Loss: 0.7898909449577332\n",
      "Epoch 7, Batch 395/462, Loss: 0.8136962056159973\n",
      "Epoch 7, Batch 396/462, Loss: 0.8425034880638123\n",
      "Epoch 7, Batch 397/462, Loss: 0.8295213580131531\n",
      "Epoch 7, Batch 398/462, Loss: 0.9117684960365295\n",
      "Epoch 7, Batch 399/462, Loss: 0.7002479434013367\n",
      "Epoch 7, Batch 400/462, Loss: 0.7991487979888916\n",
      "Epoch 7, Batch 401/462, Loss: 0.9212611317634583\n",
      "Epoch 7, Batch 402/462, Loss: 0.8392229676246643\n",
      "Epoch 7, Batch 403/462, Loss: 0.7232077121734619\n",
      "Epoch 7, Batch 404/462, Loss: 0.7205987572669983\n",
      "Epoch 7, Batch 405/462, Loss: 0.9421321749687195\n",
      "Epoch 7, Batch 406/462, Loss: 0.8463253974914551\n",
      "Epoch 7, Batch 407/462, Loss: 0.734250009059906\n",
      "Epoch 7, Batch 408/462, Loss: 0.8442533016204834\n",
      "Epoch 7, Batch 409/462, Loss: 0.7417418360710144\n",
      "Epoch 7, Batch 410/462, Loss: 0.8088850378990173\n",
      "Epoch 7, Batch 411/462, Loss: 0.7503182888031006\n",
      "Epoch 7, Batch 412/462, Loss: 0.9077355861663818\n",
      "Epoch 7, Batch 413/462, Loss: 0.9792985320091248\n",
      "Epoch 7, Batch 414/462, Loss: 1.0042661428451538\n",
      "Epoch 7, Batch 415/462, Loss: 0.9134394526481628\n",
      "Epoch 7, Batch 416/462, Loss: 0.8516777753829956\n",
      "Epoch 7, Batch 417/462, Loss: 0.9483525156974792\n",
      "Epoch 7, Batch 418/462, Loss: 0.8641015887260437\n",
      "Epoch 7, Batch 419/462, Loss: 0.8664610981941223\n",
      "Epoch 7, Batch 420/462, Loss: 0.8281034231185913\n",
      "Epoch 7, Batch 421/462, Loss: 0.9304885268211365\n",
      "Epoch 7, Batch 422/462, Loss: 0.8188847303390503\n",
      "Epoch 7, Batch 423/462, Loss: 0.882744312286377\n",
      "Epoch 7, Batch 424/462, Loss: 0.8410433530807495\n",
      "Epoch 7, Batch 425/462, Loss: 0.8493491411209106\n",
      "Epoch 7, Batch 426/462, Loss: 0.9215553998947144\n",
      "Epoch 7, Batch 427/462, Loss: 0.8897935748100281\n",
      "Epoch 7, Batch 428/462, Loss: 0.7914904356002808\n",
      "Epoch 7, Batch 429/462, Loss: 0.8223117589950562\n",
      "Epoch 7, Batch 430/462, Loss: 0.9381940960884094\n",
      "Epoch 7, Batch 431/462, Loss: 0.8288647532463074\n",
      "Epoch 7, Batch 432/462, Loss: 0.7201338410377502\n",
      "Epoch 7, Batch 433/462, Loss: 0.7098719477653503\n",
      "Epoch 7, Batch 434/462, Loss: 0.8254157900810242\n",
      "Epoch 7, Batch 435/462, Loss: 0.8771710395812988\n",
      "Epoch 7, Batch 436/462, Loss: 0.7432829737663269\n",
      "Epoch 7, Batch 437/462, Loss: 0.7586663961410522\n",
      "Epoch 7, Batch 438/462, Loss: 0.7085773944854736\n",
      "Epoch 7, Batch 439/462, Loss: 0.886242687702179\n",
      "Epoch 7, Batch 440/462, Loss: 0.8349480628967285\n",
      "Epoch 7, Batch 441/462, Loss: 0.7314130067825317\n",
      "Epoch 7, Batch 442/462, Loss: 0.7002092599868774\n",
      "Epoch 7, Batch 443/462, Loss: 0.9173488020896912\n",
      "Epoch 7, Batch 444/462, Loss: 0.6594288349151611\n",
      "Epoch 7, Batch 445/462, Loss: 0.8275404572486877\n",
      "Epoch 7, Batch 446/462, Loss: 0.8744717836380005\n",
      "Epoch 7, Batch 447/462, Loss: 0.9860265254974365\n",
      "Epoch 7, Batch 448/462, Loss: 0.8722444772720337\n",
      "Epoch 7, Batch 449/462, Loss: 0.9593465328216553\n",
      "Epoch 7, Batch 450/462, Loss: 0.8561718463897705\n",
      "Epoch 7, Batch 451/462, Loss: 0.8437522053718567\n",
      "Epoch 7, Batch 452/462, Loss: 0.946327269077301\n",
      "Epoch 7, Batch 453/462, Loss: 0.8934324383735657\n",
      "Epoch 7, Batch 454/462, Loss: 0.9127082228660583\n",
      "Epoch 7, Batch 455/462, Loss: 0.7669996619224548\n",
      "Epoch 7, Batch 456/462, Loss: 0.711662232875824\n",
      "Epoch 7, Batch 457/462, Loss: 0.6673278212547302\n",
      "Epoch 7, Batch 458/462, Loss: 0.8866115212440491\n",
      "Epoch 7, Batch 459/462, Loss: 0.8658631443977356\n",
      "Epoch 7, Batch 460/462, Loss: 0.7826338410377502\n",
      "Epoch 7, Batch 461/462, Loss: 0.8145614862442017\n",
      "Epoch 7, Batch 462/462, Loss: 0.7647314667701721\n",
      "Epoch 7, Loss: 383.6953454017639\n",
      "Epoch 8, Batch 1/462, Loss: 0.8917438983917236\n",
      "Epoch 8, Batch 2/462, Loss: 0.757190465927124\n",
      "Epoch 8, Batch 3/462, Loss: 0.8700288534164429\n",
      "Epoch 8, Batch 4/462, Loss: 1.010502576828003\n",
      "Epoch 8, Batch 5/462, Loss: 0.793338418006897\n",
      "Epoch 8, Batch 6/462, Loss: 0.7722576856613159\n",
      "Epoch 8, Batch 7/462, Loss: 0.791214644908905\n",
      "Epoch 8, Batch 8/462, Loss: 1.0499401092529297\n",
      "Epoch 8, Batch 9/462, Loss: 0.8108117580413818\n",
      "Epoch 8, Batch 10/462, Loss: 0.8138051629066467\n",
      "Epoch 8, Batch 11/462, Loss: 0.7862623333930969\n",
      "Epoch 8, Batch 12/462, Loss: 0.7617585062980652\n",
      "Epoch 8, Batch 13/462, Loss: 0.8007029891014099\n",
      "Epoch 8, Batch 14/462, Loss: 0.8283947110176086\n",
      "Epoch 8, Batch 15/462, Loss: 0.7682477831840515\n",
      "Epoch 8, Batch 16/462, Loss: 0.9738296270370483\n",
      "Epoch 8, Batch 17/462, Loss: 0.7226530909538269\n",
      "Epoch 8, Batch 18/462, Loss: 0.8533411026000977\n",
      "Epoch 8, Batch 19/462, Loss: 0.7939289808273315\n",
      "Epoch 8, Batch 20/462, Loss: 0.8528849482536316\n",
      "Epoch 8, Batch 21/462, Loss: 1.0026217699050903\n",
      "Epoch 8, Batch 22/462, Loss: 0.740748405456543\n",
      "Epoch 8, Batch 23/462, Loss: 0.809276282787323\n",
      "Epoch 8, Batch 24/462, Loss: 0.8420426845550537\n",
      "Epoch 8, Batch 25/462, Loss: 0.694881796836853\n",
      "Epoch 8, Batch 26/462, Loss: 0.9683335423469543\n",
      "Epoch 8, Batch 27/462, Loss: 0.9410200119018555\n",
      "Epoch 8, Batch 28/462, Loss: 0.8349826335906982\n",
      "Epoch 8, Batch 29/462, Loss: 0.957103431224823\n",
      "Epoch 8, Batch 30/462, Loss: 0.8342109322547913\n",
      "Epoch 8, Batch 31/462, Loss: 0.7452216744422913\n",
      "Epoch 8, Batch 32/462, Loss: 0.7824289202690125\n",
      "Epoch 8, Batch 33/462, Loss: 0.8138415217399597\n",
      "Epoch 8, Batch 34/462, Loss: 1.0774985551834106\n",
      "Epoch 8, Batch 35/462, Loss: 0.8596800565719604\n",
      "Epoch 8, Batch 36/462, Loss: 0.6882794499397278\n",
      "Epoch 8, Batch 37/462, Loss: 0.68852698802948\n",
      "Epoch 8, Batch 38/462, Loss: 0.7948917150497437\n",
      "Epoch 8, Batch 39/462, Loss: 1.1056818962097168\n",
      "Epoch 8, Batch 40/462, Loss: 0.7207331657409668\n",
      "Epoch 8, Batch 41/462, Loss: 0.754568874835968\n",
      "Epoch 8, Batch 42/462, Loss: 0.950065016746521\n",
      "Epoch 8, Batch 43/462, Loss: 0.8561899065971375\n",
      "Epoch 8, Batch 44/462, Loss: 0.7716569304466248\n",
      "Epoch 8, Batch 45/462, Loss: 1.020878553390503\n",
      "Epoch 8, Batch 46/462, Loss: 0.7744277119636536\n",
      "Epoch 8, Batch 47/462, Loss: 0.7396987676620483\n",
      "Epoch 8, Batch 48/462, Loss: 0.9099131226539612\n",
      "Epoch 8, Batch 49/462, Loss: 0.7977921962738037\n",
      "Epoch 8, Batch 50/462, Loss: 0.9034467935562134\n",
      "Epoch 8, Batch 51/462, Loss: 0.8392047882080078\n",
      "Epoch 8, Batch 52/462, Loss: 0.8383984565734863\n",
      "Epoch 8, Batch 53/462, Loss: 0.7952978014945984\n",
      "Epoch 8, Batch 54/462, Loss: 0.9579571485519409\n",
      "Epoch 8, Batch 55/462, Loss: 0.9244982600212097\n",
      "Epoch 8, Batch 56/462, Loss: 0.7605932950973511\n",
      "Epoch 8, Batch 57/462, Loss: 0.8713741302490234\n",
      "Epoch 8, Batch 58/462, Loss: 0.8679396510124207\n",
      "Epoch 8, Batch 59/462, Loss: 0.7283096313476562\n",
      "Epoch 8, Batch 60/462, Loss: 0.7421441078186035\n",
      "Epoch 8, Batch 61/462, Loss: 0.7755621075630188\n",
      "Epoch 8, Batch 62/462, Loss: 0.795179545879364\n",
      "Epoch 8, Batch 63/462, Loss: 0.7558813095092773\n",
      "Epoch 8, Batch 64/462, Loss: 0.7619476914405823\n",
      "Epoch 8, Batch 65/462, Loss: 0.7487136721611023\n",
      "Epoch 8, Batch 66/462, Loss: 0.8162261247634888\n",
      "Epoch 8, Batch 67/462, Loss: 0.7188881635665894\n",
      "Epoch 8, Batch 68/462, Loss: 0.8095487952232361\n",
      "Epoch 8, Batch 69/462, Loss: 0.7384222149848938\n",
      "Epoch 8, Batch 70/462, Loss: 0.8884603977203369\n",
      "Epoch 8, Batch 71/462, Loss: 0.6843761205673218\n",
      "Epoch 8, Batch 72/462, Loss: 0.9769147038459778\n",
      "Epoch 8, Batch 73/462, Loss: 0.8933063745498657\n",
      "Epoch 8, Batch 74/462, Loss: 0.9173588156700134\n",
      "Epoch 8, Batch 75/462, Loss: 0.9027853012084961\n",
      "Epoch 8, Batch 76/462, Loss: 0.8612571954727173\n",
      "Epoch 8, Batch 77/462, Loss: 0.8308917284011841\n",
      "Epoch 8, Batch 78/462, Loss: 0.725145697593689\n",
      "Epoch 8, Batch 79/462, Loss: 0.769814670085907\n",
      "Epoch 8, Batch 80/462, Loss: 0.6813448071479797\n",
      "Epoch 8, Batch 81/462, Loss: 0.7183341383934021\n",
      "Epoch 8, Batch 82/462, Loss: 0.7244930863380432\n",
      "Epoch 8, Batch 83/462, Loss: 0.8028894662857056\n",
      "Epoch 8, Batch 84/462, Loss: 0.7366186380386353\n",
      "Epoch 8, Batch 85/462, Loss: 0.8487043976783752\n",
      "Epoch 8, Batch 86/462, Loss: 0.7940419316291809\n",
      "Epoch 8, Batch 87/462, Loss: 0.8929382562637329\n",
      "Epoch 8, Batch 88/462, Loss: 0.8073891997337341\n",
      "Epoch 8, Batch 89/462, Loss: 0.8610081672668457\n",
      "Epoch 8, Batch 90/462, Loss: 0.8460896015167236\n",
      "Epoch 8, Batch 91/462, Loss: 0.8679590225219727\n",
      "Epoch 8, Batch 92/462, Loss: 0.7628359198570251\n",
      "Epoch 8, Batch 93/462, Loss: 0.9870244264602661\n",
      "Epoch 8, Batch 94/462, Loss: 0.8208562135696411\n",
      "Epoch 8, Batch 95/462, Loss: 0.7763179540634155\n",
      "Epoch 8, Batch 96/462, Loss: 0.805577278137207\n",
      "Epoch 8, Batch 97/462, Loss: 0.8306994438171387\n",
      "Epoch 8, Batch 98/462, Loss: 0.7585302591323853\n",
      "Epoch 8, Batch 99/462, Loss: 0.8431611657142639\n",
      "Epoch 8, Batch 100/462, Loss: 0.9273132681846619\n",
      "Epoch 8, Batch 101/462, Loss: 0.7702078223228455\n",
      "Epoch 8, Batch 102/462, Loss: 0.721298098564148\n",
      "Epoch 8, Batch 103/462, Loss: 0.9640788435935974\n",
      "Epoch 8, Batch 104/462, Loss: 0.694905698299408\n",
      "Epoch 8, Batch 105/462, Loss: 0.7572838664054871\n",
      "Epoch 8, Batch 106/462, Loss: 0.908246636390686\n",
      "Epoch 8, Batch 107/462, Loss: 0.9231698513031006\n",
      "Epoch 8, Batch 108/462, Loss: 1.0637489557266235\n",
      "Epoch 8, Batch 109/462, Loss: 0.8962593674659729\n",
      "Epoch 8, Batch 110/462, Loss: 0.9261818528175354\n",
      "Epoch 8, Batch 111/462, Loss: 0.7731640338897705\n",
      "Epoch 8, Batch 112/462, Loss: 0.8750801086425781\n",
      "Epoch 8, Batch 113/462, Loss: 0.8385659456253052\n",
      "Epoch 8, Batch 114/462, Loss: 0.8080141544342041\n",
      "Epoch 8, Batch 115/462, Loss: 0.7258000373840332\n",
      "Epoch 8, Batch 116/462, Loss: 0.8287234306335449\n",
      "Epoch 8, Batch 117/462, Loss: 0.8085750341415405\n",
      "Epoch 8, Batch 118/462, Loss: 0.8015971183776855\n",
      "Epoch 8, Batch 119/462, Loss: 0.8846431970596313\n",
      "Epoch 8, Batch 120/462, Loss: 0.880579948425293\n",
      "Epoch 8, Batch 121/462, Loss: 0.7977020144462585\n",
      "Epoch 8, Batch 122/462, Loss: 0.880437970161438\n",
      "Epoch 8, Batch 123/462, Loss: 0.862356424331665\n",
      "Epoch 8, Batch 124/462, Loss: 0.9420611262321472\n",
      "Epoch 8, Batch 125/462, Loss: 0.8516981601715088\n",
      "Epoch 8, Batch 126/462, Loss: 0.7768246531486511\n",
      "Epoch 8, Batch 127/462, Loss: 0.7803307175636292\n",
      "Epoch 8, Batch 128/462, Loss: 1.0854343175888062\n",
      "Epoch 8, Batch 129/462, Loss: 0.8491402864456177\n",
      "Epoch 8, Batch 130/462, Loss: 0.9187659621238708\n",
      "Epoch 8, Batch 131/462, Loss: 0.8939215540885925\n",
      "Epoch 8, Batch 132/462, Loss: 0.7263948321342468\n",
      "Epoch 8, Batch 133/462, Loss: 0.8088070154190063\n",
      "Epoch 8, Batch 134/462, Loss: 0.7976830005645752\n",
      "Epoch 8, Batch 135/462, Loss: 0.8446072340011597\n",
      "Epoch 8, Batch 136/462, Loss: 0.6574465036392212\n",
      "Epoch 8, Batch 137/462, Loss: 0.9440596103668213\n",
      "Epoch 8, Batch 138/462, Loss: 0.6548541188240051\n",
      "Epoch 8, Batch 139/462, Loss: 0.8452581167221069\n",
      "Epoch 8, Batch 140/462, Loss: 0.7537021040916443\n",
      "Epoch 8, Batch 141/462, Loss: 0.8932474255561829\n",
      "Epoch 8, Batch 142/462, Loss: 0.9559791088104248\n",
      "Epoch 8, Batch 143/462, Loss: 0.6500712633132935\n",
      "Epoch 8, Batch 144/462, Loss: 0.8513274192810059\n",
      "Epoch 8, Batch 145/462, Loss: 0.8144840002059937\n",
      "Epoch 8, Batch 146/462, Loss: 0.8548310399055481\n",
      "Epoch 8, Batch 147/462, Loss: 0.6619861721992493\n",
      "Epoch 8, Batch 148/462, Loss: 0.7162876129150391\n",
      "Epoch 8, Batch 149/462, Loss: 0.9390876889228821\n",
      "Epoch 8, Batch 150/462, Loss: 0.7885017991065979\n",
      "Epoch 8, Batch 151/462, Loss: 0.7331433296203613\n",
      "Epoch 8, Batch 152/462, Loss: 0.8387059569358826\n",
      "Epoch 8, Batch 153/462, Loss: 0.7826286554336548\n",
      "Epoch 8, Batch 154/462, Loss: 0.8847333788871765\n",
      "Epoch 8, Batch 155/462, Loss: 0.8938787579536438\n",
      "Epoch 8, Batch 156/462, Loss: 0.9291533827781677\n",
      "Epoch 8, Batch 157/462, Loss: 0.7921110391616821\n",
      "Epoch 8, Batch 158/462, Loss: 0.7935965061187744\n",
      "Epoch 8, Batch 159/462, Loss: 0.8172288537025452\n",
      "Epoch 8, Batch 160/462, Loss: 0.8743220567703247\n",
      "Epoch 8, Batch 161/462, Loss: 0.7189403176307678\n",
      "Epoch 8, Batch 162/462, Loss: 0.7425737977027893\n",
      "Epoch 8, Batch 163/462, Loss: 0.6512669920921326\n",
      "Epoch 8, Batch 164/462, Loss: 0.8366190195083618\n",
      "Epoch 8, Batch 165/462, Loss: 0.8401508927345276\n",
      "Epoch 8, Batch 166/462, Loss: 0.7233911752700806\n",
      "Epoch 8, Batch 167/462, Loss: 0.7206553220748901\n",
      "Epoch 8, Batch 168/462, Loss: 0.7312857508659363\n",
      "Epoch 8, Batch 169/462, Loss: 0.7775430679321289\n",
      "Epoch 8, Batch 170/462, Loss: 0.798832356929779\n",
      "Epoch 8, Batch 171/462, Loss: 0.7082226276397705\n",
      "Epoch 8, Batch 172/462, Loss: 0.6871981620788574\n",
      "Epoch 8, Batch 173/462, Loss: 0.7850403189659119\n",
      "Epoch 8, Batch 174/462, Loss: 0.755971372127533\n",
      "Epoch 8, Batch 175/462, Loss: 0.8099443316459656\n",
      "Epoch 8, Batch 176/462, Loss: 0.7766866087913513\n",
      "Epoch 8, Batch 177/462, Loss: 0.8175329566001892\n",
      "Epoch 8, Batch 178/462, Loss: 0.8803477883338928\n",
      "Epoch 8, Batch 179/462, Loss: 0.8136581778526306\n",
      "Epoch 8, Batch 180/462, Loss: 0.8925147652626038\n",
      "Epoch 8, Batch 181/462, Loss: 0.8480312824249268\n",
      "Epoch 8, Batch 182/462, Loss: 0.8354050517082214\n",
      "Epoch 8, Batch 183/462, Loss: 0.8777950406074524\n",
      "Epoch 8, Batch 184/462, Loss: 0.7614227533340454\n",
      "Epoch 8, Batch 185/462, Loss: 0.7157662510871887\n",
      "Epoch 8, Batch 186/462, Loss: 0.7618797421455383\n",
      "Epoch 8, Batch 187/462, Loss: 0.8392467498779297\n",
      "Epoch 8, Batch 188/462, Loss: 0.855414867401123\n",
      "Epoch 8, Batch 189/462, Loss: 0.8030330538749695\n",
      "Epoch 8, Batch 190/462, Loss: 0.7887666821479797\n",
      "Epoch 8, Batch 191/462, Loss: 0.9524994492530823\n",
      "Epoch 8, Batch 192/462, Loss: 0.7665326595306396\n",
      "Epoch 8, Batch 193/462, Loss: 0.8122872710227966\n",
      "Epoch 8, Batch 194/462, Loss: 0.6838417649269104\n",
      "Epoch 8, Batch 195/462, Loss: 0.7901167273521423\n",
      "Epoch 8, Batch 196/462, Loss: 0.7537079453468323\n",
      "Epoch 8, Batch 197/462, Loss: 0.718975305557251\n",
      "Epoch 8, Batch 198/462, Loss: 0.7792049050331116\n",
      "Epoch 8, Batch 199/462, Loss: 0.716840922832489\n",
      "Epoch 8, Batch 200/462, Loss: 0.8697350025177002\n",
      "Epoch 8, Batch 201/462, Loss: 0.8667608499526978\n",
      "Epoch 8, Batch 202/462, Loss: 0.8005976676940918\n",
      "Epoch 8, Batch 203/462, Loss: 0.9437603950500488\n",
      "Epoch 8, Batch 204/462, Loss: 0.7875714302062988\n",
      "Epoch 8, Batch 205/462, Loss: 0.7502233982086182\n",
      "Epoch 8, Batch 206/462, Loss: 0.8129440546035767\n",
      "Epoch 8, Batch 207/462, Loss: 0.8380226492881775\n",
      "Epoch 8, Batch 208/462, Loss: 0.9660446643829346\n",
      "Epoch 8, Batch 209/462, Loss: 0.8591670989990234\n",
      "Epoch 8, Batch 210/462, Loss: 0.8558768630027771\n",
      "Epoch 8, Batch 211/462, Loss: 0.8650460243225098\n",
      "Epoch 8, Batch 212/462, Loss: 0.6877381205558777\n",
      "Epoch 8, Batch 213/462, Loss: 0.7773361206054688\n",
      "Epoch 8, Batch 214/462, Loss: 0.9097694158554077\n",
      "Epoch 8, Batch 215/462, Loss: 0.8380939960479736\n",
      "Epoch 8, Batch 216/462, Loss: 0.8528973460197449\n",
      "Epoch 8, Batch 217/462, Loss: 0.8488967418670654\n",
      "Epoch 8, Batch 218/462, Loss: 0.8056317567825317\n",
      "Epoch 8, Batch 219/462, Loss: 0.7526153922080994\n",
      "Epoch 8, Batch 220/462, Loss: 0.8093298673629761\n",
      "Epoch 8, Batch 221/462, Loss: 0.8332968354225159\n",
      "Epoch 8, Batch 222/462, Loss: 0.8940646052360535\n",
      "Epoch 8, Batch 223/462, Loss: 0.7229530215263367\n",
      "Epoch 8, Batch 224/462, Loss: 0.8327944874763489\n",
      "Epoch 8, Batch 225/462, Loss: 0.9105824828147888\n",
      "Epoch 8, Batch 226/462, Loss: 0.8718287944793701\n",
      "Epoch 8, Batch 227/462, Loss: 0.7508924603462219\n",
      "Epoch 8, Batch 228/462, Loss: 0.8144732713699341\n",
      "Epoch 8, Batch 229/462, Loss: 0.8673495650291443\n",
      "Epoch 8, Batch 230/462, Loss: 0.8412594795227051\n",
      "Epoch 8, Batch 231/462, Loss: 0.626438319683075\n",
      "Epoch 8, Batch 232/462, Loss: 0.7906293272972107\n",
      "Epoch 8, Batch 233/462, Loss: 1.0170319080352783\n",
      "Epoch 8, Batch 234/462, Loss: 0.9059793949127197\n",
      "Epoch 8, Batch 235/462, Loss: 0.9588614106178284\n",
      "Epoch 8, Batch 236/462, Loss: 0.8564053773880005\n",
      "Epoch 8, Batch 237/462, Loss: 0.965018093585968\n",
      "Epoch 8, Batch 238/462, Loss: 0.6686623096466064\n",
      "Epoch 8, Batch 239/462, Loss: 0.7681993842124939\n",
      "Epoch 8, Batch 240/462, Loss: 0.8292514681816101\n",
      "Epoch 8, Batch 241/462, Loss: 0.7488232851028442\n",
      "Epoch 8, Batch 242/462, Loss: 0.8851400017738342\n",
      "Epoch 8, Batch 243/462, Loss: 0.9196694493293762\n",
      "Epoch 8, Batch 244/462, Loss: 0.8336278200149536\n",
      "Epoch 8, Batch 245/462, Loss: 0.6995165348052979\n",
      "Epoch 8, Batch 246/462, Loss: 0.6753924489021301\n",
      "Epoch 8, Batch 247/462, Loss: 0.7817729115486145\n",
      "Epoch 8, Batch 248/462, Loss: 0.7782431244850159\n",
      "Epoch 8, Batch 249/462, Loss: 0.6427158117294312\n",
      "Epoch 8, Batch 250/462, Loss: 0.7857915163040161\n",
      "Epoch 8, Batch 251/462, Loss: 0.8541843891143799\n",
      "Epoch 8, Batch 252/462, Loss: 0.8298540711402893\n",
      "Epoch 8, Batch 253/462, Loss: 0.7195304036140442\n",
      "Epoch 8, Batch 254/462, Loss: 0.789509654045105\n",
      "Epoch 8, Batch 255/462, Loss: 0.6631601452827454\n",
      "Epoch 8, Batch 256/462, Loss: 0.7004204392433167\n",
      "Epoch 8, Batch 257/462, Loss: 0.871123731136322\n",
      "Epoch 8, Batch 258/462, Loss: 0.7484793663024902\n",
      "Epoch 8, Batch 259/462, Loss: 0.6957132816314697\n",
      "Epoch 8, Batch 260/462, Loss: 0.7842919230461121\n",
      "Epoch 8, Batch 261/462, Loss: 0.793468177318573\n",
      "Epoch 8, Batch 262/462, Loss: 0.7396958470344543\n",
      "Epoch 8, Batch 263/462, Loss: 0.7889906764030457\n",
      "Epoch 8, Batch 264/462, Loss: 0.715566873550415\n",
      "Epoch 8, Batch 265/462, Loss: 0.7616086006164551\n",
      "Epoch 8, Batch 266/462, Loss: 0.8039653301239014\n",
      "Epoch 8, Batch 267/462, Loss: 0.7513899207115173\n",
      "Epoch 8, Batch 268/462, Loss: 0.7976584434509277\n",
      "Epoch 8, Batch 269/462, Loss: 0.884096622467041\n",
      "Epoch 8, Batch 270/462, Loss: 0.7476949095726013\n",
      "Epoch 8, Batch 271/462, Loss: 0.6690104007720947\n",
      "Epoch 8, Batch 272/462, Loss: 0.8276944756507874\n",
      "Epoch 8, Batch 273/462, Loss: 0.8130571842193604\n",
      "Epoch 8, Batch 274/462, Loss: 0.8699570894241333\n",
      "Epoch 8, Batch 275/462, Loss: 0.964574933052063\n",
      "Epoch 8, Batch 276/462, Loss: 0.8138001561164856\n",
      "Epoch 8, Batch 277/462, Loss: 0.8024143576622009\n",
      "Epoch 8, Batch 278/462, Loss: 0.7752861380577087\n",
      "Epoch 8, Batch 279/462, Loss: 0.7114403247833252\n",
      "Epoch 8, Batch 280/462, Loss: 0.8993822336196899\n",
      "Epoch 8, Batch 281/462, Loss: 0.9389356970787048\n",
      "Epoch 8, Batch 282/462, Loss: 0.6658136248588562\n",
      "Epoch 8, Batch 283/462, Loss: 0.8453782200813293\n",
      "Epoch 8, Batch 284/462, Loss: 0.8137207627296448\n",
      "Epoch 8, Batch 285/462, Loss: 0.9157009720802307\n",
      "Epoch 8, Batch 286/462, Loss: 0.8800634145736694\n",
      "Epoch 8, Batch 287/462, Loss: 0.8583886623382568\n",
      "Epoch 8, Batch 288/462, Loss: 0.7421382069587708\n",
      "Epoch 8, Batch 289/462, Loss: 0.8871259093284607\n",
      "Epoch 8, Batch 290/462, Loss: 0.8339638113975525\n",
      "Epoch 8, Batch 291/462, Loss: 0.8742842674255371\n",
      "Epoch 8, Batch 292/462, Loss: 0.930959165096283\n",
      "Epoch 8, Batch 293/462, Loss: 0.860217273235321\n",
      "Epoch 8, Batch 294/462, Loss: 0.870323657989502\n",
      "Epoch 8, Batch 295/462, Loss: 0.9654797911643982\n",
      "Epoch 8, Batch 296/462, Loss: 0.851730227470398\n",
      "Epoch 8, Batch 297/462, Loss: 0.7555326223373413\n",
      "Epoch 8, Batch 298/462, Loss: 0.7998215556144714\n",
      "Epoch 8, Batch 299/462, Loss: 0.7819870710372925\n",
      "Epoch 8, Batch 300/462, Loss: 0.8104324340820312\n",
      "Epoch 8, Batch 301/462, Loss: 0.8254151940345764\n",
      "Epoch 8, Batch 302/462, Loss: 0.7519135475158691\n",
      "Epoch 8, Batch 303/462, Loss: 0.7804307341575623\n",
      "Epoch 8, Batch 304/462, Loss: 0.8065869808197021\n",
      "Epoch 8, Batch 305/462, Loss: 0.9836421012878418\n",
      "Epoch 8, Batch 306/462, Loss: 0.7749601006507874\n",
      "Epoch 8, Batch 307/462, Loss: 0.8453688621520996\n",
      "Epoch 8, Batch 308/462, Loss: 0.8024463057518005\n",
      "Epoch 8, Batch 309/462, Loss: 0.8611971139907837\n",
      "Epoch 8, Batch 310/462, Loss: 0.8959194421768188\n",
      "Epoch 8, Batch 311/462, Loss: 0.8950551152229309\n",
      "Epoch 8, Batch 312/462, Loss: 0.8029091358184814\n",
      "Epoch 8, Batch 313/462, Loss: 0.7881588935852051\n",
      "Epoch 8, Batch 314/462, Loss: 0.7559911608695984\n",
      "Epoch 8, Batch 315/462, Loss: 0.9268913865089417\n",
      "Epoch 8, Batch 316/462, Loss: 0.8340293169021606\n",
      "Epoch 8, Batch 317/462, Loss: 0.8861488699913025\n",
      "Epoch 8, Batch 318/462, Loss: 0.8366959095001221\n",
      "Epoch 8, Batch 319/462, Loss: 0.8983079195022583\n",
      "Epoch 8, Batch 320/462, Loss: 0.6049719452857971\n",
      "Epoch 8, Batch 321/462, Loss: 0.7481664419174194\n",
      "Epoch 8, Batch 322/462, Loss: 0.7592322826385498\n",
      "Epoch 8, Batch 323/462, Loss: 0.6723247170448303\n",
      "Epoch 8, Batch 324/462, Loss: 0.9358519911766052\n",
      "Epoch 8, Batch 325/462, Loss: 0.9632269740104675\n",
      "Epoch 8, Batch 326/462, Loss: 0.7260726690292358\n",
      "Epoch 8, Batch 327/462, Loss: 0.9380806684494019\n",
      "Epoch 8, Batch 328/462, Loss: 0.8941487669944763\n",
      "Epoch 8, Batch 329/462, Loss: 0.985865592956543\n",
      "Epoch 8, Batch 330/462, Loss: 0.9458144307136536\n",
      "Epoch 8, Batch 331/462, Loss: 0.8295496702194214\n",
      "Epoch 8, Batch 332/462, Loss: 0.8367888927459717\n",
      "Epoch 8, Batch 333/462, Loss: 0.668938159942627\n",
      "Epoch 8, Batch 334/462, Loss: 0.8410552740097046\n",
      "Epoch 8, Batch 335/462, Loss: 0.8546464443206787\n",
      "Epoch 8, Batch 336/462, Loss: 0.7743197679519653\n",
      "Epoch 8, Batch 337/462, Loss: 0.8321965932846069\n",
      "Epoch 8, Batch 338/462, Loss: 0.7919418811798096\n",
      "Epoch 8, Batch 339/462, Loss: 0.7596808671951294\n",
      "Epoch 8, Batch 340/462, Loss: 0.7561219334602356\n",
      "Epoch 8, Batch 341/462, Loss: 0.898208498954773\n",
      "Epoch 8, Batch 342/462, Loss: 0.8130978345870972\n",
      "Epoch 8, Batch 343/462, Loss: 0.8028020262718201\n",
      "Epoch 8, Batch 344/462, Loss: 0.8736146688461304\n",
      "Epoch 8, Batch 345/462, Loss: 0.7181528210639954\n",
      "Epoch 8, Batch 346/462, Loss: 0.7260988354682922\n",
      "Epoch 8, Batch 347/462, Loss: 0.7975337505340576\n",
      "Epoch 8, Batch 348/462, Loss: 0.7703601121902466\n",
      "Epoch 8, Batch 349/462, Loss: 0.7363197803497314\n",
      "Epoch 8, Batch 350/462, Loss: 0.9789135456085205\n",
      "Epoch 8, Batch 351/462, Loss: 0.7893939018249512\n",
      "Epoch 8, Batch 352/462, Loss: 0.7319684624671936\n",
      "Epoch 8, Batch 353/462, Loss: 0.721703290939331\n",
      "Epoch 8, Batch 354/462, Loss: 0.943368673324585\n",
      "Epoch 8, Batch 355/462, Loss: 0.9363720417022705\n",
      "Epoch 8, Batch 356/462, Loss: 0.7899367809295654\n",
      "Epoch 8, Batch 357/462, Loss: 0.6574386358261108\n",
      "Epoch 8, Batch 358/462, Loss: 0.7771570682525635\n",
      "Epoch 8, Batch 359/462, Loss: 0.6910997033119202\n",
      "Epoch 8, Batch 360/462, Loss: 0.9286078214645386\n",
      "Epoch 8, Batch 361/462, Loss: 0.8275565505027771\n",
      "Epoch 8, Batch 362/462, Loss: 0.7726150155067444\n",
      "Epoch 8, Batch 363/462, Loss: 0.7855628132820129\n",
      "Epoch 8, Batch 364/462, Loss: 0.8147200345993042\n",
      "Epoch 8, Batch 365/462, Loss: 0.7871708869934082\n",
      "Epoch 8, Batch 366/462, Loss: 0.7287867665290833\n",
      "Epoch 8, Batch 367/462, Loss: 0.8150813579559326\n",
      "Epoch 8, Batch 368/462, Loss: 0.9171158075332642\n",
      "Epoch 8, Batch 369/462, Loss: 0.6943188905715942\n",
      "Epoch 8, Batch 370/462, Loss: 0.8902769088745117\n",
      "Epoch 8, Batch 371/462, Loss: 0.8620498776435852\n",
      "Epoch 8, Batch 372/462, Loss: 0.8241179585456848\n",
      "Epoch 8, Batch 373/462, Loss: 0.9682084321975708\n",
      "Epoch 8, Batch 374/462, Loss: 0.8488337993621826\n",
      "Epoch 8, Batch 375/462, Loss: 0.7066611647605896\n",
      "Epoch 8, Batch 376/462, Loss: 0.7382334470748901\n",
      "Epoch 8, Batch 377/462, Loss: 0.7983622550964355\n",
      "Epoch 8, Batch 378/462, Loss: 0.7042204141616821\n",
      "Epoch 8, Batch 379/462, Loss: 0.856866180896759\n",
      "Epoch 8, Batch 380/462, Loss: 0.8523197770118713\n",
      "Epoch 8, Batch 381/462, Loss: 0.8148614168167114\n",
      "Epoch 8, Batch 382/462, Loss: 0.9267954230308533\n",
      "Epoch 8, Batch 383/462, Loss: 0.8846521973609924\n",
      "Epoch 8, Batch 384/462, Loss: 0.7695854306221008\n",
      "Epoch 8, Batch 385/462, Loss: 0.7771097421646118\n",
      "Epoch 8, Batch 386/462, Loss: 0.8129427433013916\n",
      "Epoch 8, Batch 387/462, Loss: 0.7837071418762207\n",
      "Epoch 8, Batch 388/462, Loss: 0.8329468369483948\n",
      "Epoch 8, Batch 389/462, Loss: 0.6767362356185913\n",
      "Epoch 8, Batch 390/462, Loss: 0.8946817517280579\n",
      "Epoch 8, Batch 391/462, Loss: 0.8313227891921997\n",
      "Epoch 8, Batch 392/462, Loss: 0.8269939422607422\n",
      "Epoch 8, Batch 393/462, Loss: 0.7204108238220215\n",
      "Epoch 8, Batch 394/462, Loss: 0.8053045272827148\n",
      "Epoch 8, Batch 395/462, Loss: 0.8501265048980713\n",
      "Epoch 8, Batch 396/462, Loss: 0.8754703998565674\n",
      "Epoch 8, Batch 397/462, Loss: 0.8977588415145874\n",
      "Epoch 8, Batch 398/462, Loss: 0.8763561844825745\n",
      "Epoch 8, Batch 399/462, Loss: 0.7983427047729492\n",
      "Epoch 8, Batch 400/462, Loss: 0.816379189491272\n",
      "Epoch 8, Batch 401/462, Loss: 0.9365766048431396\n",
      "Epoch 8, Batch 402/462, Loss: 0.9130493402481079\n",
      "Epoch 8, Batch 403/462, Loss: 0.799094557762146\n",
      "Epoch 8, Batch 404/462, Loss: 0.9468576312065125\n",
      "Epoch 8, Batch 405/462, Loss: 0.9097501039505005\n",
      "Epoch 8, Batch 406/462, Loss: 0.7885901927947998\n",
      "Epoch 8, Batch 407/462, Loss: 0.7049548029899597\n",
      "Epoch 8, Batch 408/462, Loss: 0.9239034652709961\n",
      "Epoch 8, Batch 409/462, Loss: 0.8019891381263733\n",
      "Epoch 8, Batch 410/462, Loss: 0.8536657094955444\n",
      "Epoch 8, Batch 411/462, Loss: 0.7838996052742004\n",
      "Epoch 8, Batch 412/462, Loss: 0.7114799618721008\n",
      "Epoch 8, Batch 413/462, Loss: 0.935775876045227\n",
      "Epoch 8, Batch 414/462, Loss: 0.868174135684967\n",
      "Epoch 8, Batch 415/462, Loss: 0.8984073996543884\n",
      "Epoch 8, Batch 416/462, Loss: 0.7956994771957397\n",
      "Epoch 8, Batch 417/462, Loss: 0.8309590816497803\n",
      "Epoch 8, Batch 418/462, Loss: 0.8370590209960938\n",
      "Epoch 8, Batch 419/462, Loss: 0.8201122283935547\n",
      "Epoch 8, Batch 420/462, Loss: 0.7561177611351013\n",
      "Epoch 8, Batch 421/462, Loss: 0.7875622510910034\n",
      "Epoch 8, Batch 422/462, Loss: 0.8491197824478149\n",
      "Epoch 8, Batch 423/462, Loss: 0.69231116771698\n",
      "Epoch 8, Batch 424/462, Loss: 0.736632227897644\n",
      "Epoch 8, Batch 425/462, Loss: 0.6267932653427124\n",
      "Epoch 8, Batch 426/462, Loss: 0.6574774384498596\n",
      "Epoch 8, Batch 427/462, Loss: 0.8929697275161743\n",
      "Epoch 8, Batch 428/462, Loss: 0.989841639995575\n",
      "Epoch 8, Batch 429/462, Loss: 0.7752844095230103\n",
      "Epoch 8, Batch 430/462, Loss: 0.7561426758766174\n",
      "Epoch 8, Batch 431/462, Loss: 0.839885413646698\n",
      "Epoch 8, Batch 432/462, Loss: 0.6487724781036377\n",
      "Epoch 8, Batch 433/462, Loss: 0.844476580619812\n",
      "Epoch 8, Batch 434/462, Loss: 0.7497047185897827\n",
      "Epoch 8, Batch 435/462, Loss: 0.8240180015563965\n",
      "Epoch 8, Batch 436/462, Loss: 0.7492390275001526\n",
      "Epoch 8, Batch 437/462, Loss: 0.6985609531402588\n",
      "Epoch 8, Batch 438/462, Loss: 0.8252142667770386\n",
      "Epoch 8, Batch 439/462, Loss: 0.9653393030166626\n",
      "Epoch 8, Batch 440/462, Loss: 0.6867507100105286\n",
      "Epoch 8, Batch 441/462, Loss: 0.8328237533569336\n",
      "Epoch 8, Batch 442/462, Loss: 0.7095847725868225\n",
      "Epoch 8, Batch 443/462, Loss: 0.8462985754013062\n",
      "Epoch 8, Batch 444/462, Loss: 0.8081395030021667\n",
      "Epoch 8, Batch 445/462, Loss: 0.7498323917388916\n",
      "Epoch 8, Batch 446/462, Loss: 0.8299773931503296\n",
      "Epoch 8, Batch 447/462, Loss: 0.778554379940033\n",
      "Epoch 8, Batch 448/462, Loss: 0.9966383576393127\n",
      "Epoch 8, Batch 449/462, Loss: 0.9038034081459045\n",
      "Epoch 8, Batch 450/462, Loss: 0.8561288714408875\n",
      "Epoch 8, Batch 451/462, Loss: 0.843153178691864\n",
      "Epoch 8, Batch 452/462, Loss: 0.8202595114707947\n",
      "Epoch 8, Batch 453/462, Loss: 0.7922521829605103\n",
      "Epoch 8, Batch 454/462, Loss: 0.7055748701095581\n",
      "Epoch 8, Batch 455/462, Loss: 0.7172833681106567\n",
      "Epoch 8, Batch 456/462, Loss: 0.8087207674980164\n",
      "Epoch 8, Batch 457/462, Loss: 0.7476967573165894\n",
      "Epoch 8, Batch 458/462, Loss: 0.664380669593811\n",
      "Epoch 8, Batch 459/462, Loss: 0.8062430024147034\n",
      "Epoch 8, Batch 460/462, Loss: 0.8281676769256592\n",
      "Epoch 8, Batch 461/462, Loss: 1.0913543701171875\n",
      "Epoch 8, Batch 462/462, Loss: 0.8860388994216919\n",
      "Epoch 8, Loss: 378.0877003669739\n",
      "Epoch 9, Batch 1/462, Loss: 0.7994682788848877\n",
      "Epoch 9, Batch 2/462, Loss: 0.792569637298584\n",
      "Epoch 9, Batch 3/462, Loss: 0.8742814660072327\n",
      "Epoch 9, Batch 4/462, Loss: 0.655154287815094\n",
      "Epoch 9, Batch 5/462, Loss: 0.7878700494766235\n",
      "Epoch 9, Batch 6/462, Loss: 0.7564634084701538\n",
      "Epoch 9, Batch 7/462, Loss: 0.950391411781311\n",
      "Epoch 9, Batch 8/462, Loss: 0.9311652183532715\n",
      "Epoch 9, Batch 9/462, Loss: 0.8085805177688599\n",
      "Epoch 9, Batch 10/462, Loss: 0.7568512558937073\n",
      "Epoch 9, Batch 11/462, Loss: 0.7455993294715881\n",
      "Epoch 9, Batch 12/462, Loss: 0.6783358454704285\n",
      "Epoch 9, Batch 13/462, Loss: 0.7419962286949158\n",
      "Epoch 9, Batch 14/462, Loss: 0.9658691883087158\n",
      "Epoch 9, Batch 15/462, Loss: 0.8294237852096558\n",
      "Epoch 9, Batch 16/462, Loss: 0.7700231671333313\n",
      "Epoch 9, Batch 17/462, Loss: 0.6923648118972778\n",
      "Epoch 9, Batch 18/462, Loss: 0.8360654711723328\n",
      "Epoch 9, Batch 19/462, Loss: 0.792899489402771\n",
      "Epoch 9, Batch 20/462, Loss: 0.9158586263656616\n",
      "Epoch 9, Batch 21/462, Loss: 0.7480273842811584\n",
      "Epoch 9, Batch 22/462, Loss: 0.6928743720054626\n",
      "Epoch 9, Batch 23/462, Loss: 0.7232511639595032\n",
      "Epoch 9, Batch 24/462, Loss: 0.9034833908081055\n",
      "Epoch 9, Batch 25/462, Loss: 0.7293673753738403\n",
      "Epoch 9, Batch 26/462, Loss: 0.8673055768013\n",
      "Epoch 9, Batch 27/462, Loss: 0.8557373285293579\n",
      "Epoch 9, Batch 28/462, Loss: 0.7033255100250244\n",
      "Epoch 9, Batch 29/462, Loss: 0.7846828699111938\n",
      "Epoch 9, Batch 30/462, Loss: 0.8652647137641907\n",
      "Epoch 9, Batch 31/462, Loss: 0.8963225483894348\n",
      "Epoch 9, Batch 32/462, Loss: 0.8279550671577454\n",
      "Epoch 9, Batch 33/462, Loss: 0.6685318946838379\n",
      "Epoch 9, Batch 34/462, Loss: 0.985052227973938\n",
      "Epoch 9, Batch 35/462, Loss: 0.9040312767028809\n",
      "Epoch 9, Batch 36/462, Loss: 0.7518883943557739\n",
      "Epoch 9, Batch 37/462, Loss: 0.6711261868476868\n",
      "Epoch 9, Batch 38/462, Loss: 0.960432231426239\n",
      "Epoch 9, Batch 39/462, Loss: 0.8144013285636902\n",
      "Epoch 9, Batch 40/462, Loss: 0.6835491061210632\n",
      "Epoch 9, Batch 41/462, Loss: 0.7690594792366028\n",
      "Epoch 9, Batch 42/462, Loss: 0.7790493369102478\n",
      "Epoch 9, Batch 43/462, Loss: 0.7723381519317627\n",
      "Epoch 9, Batch 44/462, Loss: 0.8600814342498779\n",
      "Epoch 9, Batch 45/462, Loss: 0.9138606786727905\n",
      "Epoch 9, Batch 46/462, Loss: 0.7403298020362854\n",
      "Epoch 9, Batch 47/462, Loss: 0.7724246978759766\n",
      "Epoch 9, Batch 48/462, Loss: 0.6946429014205933\n",
      "Epoch 9, Batch 49/462, Loss: 0.8538601994514465\n",
      "Epoch 9, Batch 50/462, Loss: 0.8920899033546448\n",
      "Epoch 9, Batch 51/462, Loss: 0.8763487935066223\n",
      "Epoch 9, Batch 52/462, Loss: 0.7720061540603638\n",
      "Epoch 9, Batch 53/462, Loss: 0.7105231285095215\n",
      "Epoch 9, Batch 54/462, Loss: 0.9726575613021851\n",
      "Epoch 9, Batch 55/462, Loss: 0.8532936573028564\n",
      "Epoch 9, Batch 56/462, Loss: 0.8698559999465942\n",
      "Epoch 9, Batch 57/462, Loss: 0.8266295194625854\n",
      "Epoch 9, Batch 58/462, Loss: 0.7399346828460693\n",
      "Epoch 9, Batch 59/462, Loss: 0.7857068181037903\n",
      "Epoch 9, Batch 60/462, Loss: 0.8236817717552185\n",
      "Epoch 9, Batch 61/462, Loss: 0.7778733968734741\n",
      "Epoch 9, Batch 62/462, Loss: 0.7981772422790527\n",
      "Epoch 9, Batch 63/462, Loss: 0.79820716381073\n",
      "Epoch 9, Batch 64/462, Loss: 0.8592159152030945\n",
      "Epoch 9, Batch 65/462, Loss: 0.8884297013282776\n",
      "Epoch 9, Batch 66/462, Loss: 0.7862274646759033\n",
      "Epoch 9, Batch 67/462, Loss: 0.9020372033119202\n",
      "Epoch 9, Batch 68/462, Loss: 0.9270289540290833\n",
      "Epoch 9, Batch 69/462, Loss: 0.7727763056755066\n",
      "Epoch 9, Batch 70/462, Loss: 0.7354293465614319\n",
      "Epoch 9, Batch 71/462, Loss: 0.7490925788879395\n",
      "Epoch 9, Batch 72/462, Loss: 0.8662621974945068\n",
      "Epoch 9, Batch 73/462, Loss: 0.7800389528274536\n",
      "Epoch 9, Batch 74/462, Loss: 0.7813259363174438\n",
      "Epoch 9, Batch 75/462, Loss: 0.8527653217315674\n",
      "Epoch 9, Batch 76/462, Loss: 0.8035044074058533\n",
      "Epoch 9, Batch 77/462, Loss: 0.8628343939781189\n",
      "Epoch 9, Batch 78/462, Loss: 0.6377754807472229\n",
      "Epoch 9, Batch 79/462, Loss: 0.7238877415657043\n",
      "Epoch 9, Batch 80/462, Loss: 0.7849512100219727\n",
      "Epoch 9, Batch 81/462, Loss: 0.7832866907119751\n",
      "Epoch 9, Batch 82/462, Loss: 0.7103238105773926\n",
      "Epoch 9, Batch 83/462, Loss: 0.7655868530273438\n",
      "Epoch 9, Batch 84/462, Loss: 0.7721368074417114\n",
      "Epoch 9, Batch 85/462, Loss: 0.8272708654403687\n",
      "Epoch 9, Batch 86/462, Loss: 0.8710330724716187\n",
      "Epoch 9, Batch 87/462, Loss: 0.8455492258071899\n",
      "Epoch 9, Batch 88/462, Loss: 0.7542091608047485\n",
      "Epoch 9, Batch 89/462, Loss: 0.7727354168891907\n",
      "Epoch 9, Batch 90/462, Loss: 0.7818299531936646\n",
      "Epoch 9, Batch 91/462, Loss: 0.6747849583625793\n",
      "Epoch 9, Batch 92/462, Loss: 0.8685401082038879\n",
      "Epoch 9, Batch 93/462, Loss: 0.8845775723457336\n",
      "Epoch 9, Batch 94/462, Loss: 0.8177471160888672\n",
      "Epoch 9, Batch 95/462, Loss: 0.9113253951072693\n",
      "Epoch 9, Batch 96/462, Loss: 0.9445250630378723\n",
      "Epoch 9, Batch 97/462, Loss: 0.7934799194335938\n",
      "Epoch 9, Batch 98/462, Loss: 0.8826987147331238\n",
      "Epoch 9, Batch 99/462, Loss: 0.7083660960197449\n",
      "Epoch 9, Batch 100/462, Loss: 0.8152474761009216\n",
      "Epoch 9, Batch 101/462, Loss: 0.8812945485115051\n",
      "Epoch 9, Batch 102/462, Loss: 0.8403931856155396\n",
      "Epoch 9, Batch 103/462, Loss: 0.7204305529594421\n",
      "Epoch 9, Batch 104/462, Loss: 0.8873817920684814\n",
      "Epoch 9, Batch 105/462, Loss: 0.8181436061859131\n",
      "Epoch 9, Batch 106/462, Loss: 0.7696279883384705\n",
      "Epoch 9, Batch 107/462, Loss: 0.8634145855903625\n",
      "Epoch 9, Batch 108/462, Loss: 0.7650474309921265\n",
      "Epoch 9, Batch 109/462, Loss: 0.7154280543327332\n",
      "Epoch 9, Batch 110/462, Loss: 1.0036293268203735\n",
      "Epoch 9, Batch 111/462, Loss: 0.8563792705535889\n",
      "Epoch 9, Batch 112/462, Loss: 0.6718002557754517\n",
      "Epoch 9, Batch 113/462, Loss: 0.7919813990592957\n",
      "Epoch 9, Batch 114/462, Loss: 0.660353422164917\n",
      "Epoch 9, Batch 115/462, Loss: 0.7130345106124878\n",
      "Epoch 9, Batch 116/462, Loss: 0.7599318027496338\n",
      "Epoch 9, Batch 117/462, Loss: 0.9436864852905273\n",
      "Epoch 9, Batch 118/462, Loss: 0.8867546916007996\n",
      "Epoch 9, Batch 119/462, Loss: 0.8272594809532166\n",
      "Epoch 9, Batch 120/462, Loss: 0.8024107217788696\n",
      "Epoch 9, Batch 121/462, Loss: 0.8252363801002502\n",
      "Epoch 9, Batch 122/462, Loss: 0.7992844581604004\n",
      "Epoch 9, Batch 123/462, Loss: 0.7196304202079773\n",
      "Epoch 9, Batch 124/462, Loss: 0.6993952989578247\n",
      "Epoch 9, Batch 125/462, Loss: 0.6574601531028748\n",
      "Epoch 9, Batch 126/462, Loss: 0.8196766972541809\n",
      "Epoch 9, Batch 127/462, Loss: 0.8011764287948608\n",
      "Epoch 9, Batch 128/462, Loss: 0.7981055974960327\n",
      "Epoch 9, Batch 129/462, Loss: 0.7442244291305542\n",
      "Epoch 9, Batch 130/462, Loss: 0.8906779885292053\n",
      "Epoch 9, Batch 131/462, Loss: 0.7746939659118652\n",
      "Epoch 9, Batch 132/462, Loss: 0.6892247796058655\n",
      "Epoch 9, Batch 133/462, Loss: 0.825989305973053\n",
      "Epoch 9, Batch 134/462, Loss: 0.8414050340652466\n",
      "Epoch 9, Batch 135/462, Loss: 0.9181109070777893\n",
      "Epoch 9, Batch 136/462, Loss: 0.7171342372894287\n",
      "Epoch 9, Batch 137/462, Loss: 0.8784589767456055\n",
      "Epoch 9, Batch 138/462, Loss: 0.8885791301727295\n",
      "Epoch 9, Batch 139/462, Loss: 0.7895293831825256\n",
      "Epoch 9, Batch 140/462, Loss: 0.7161641716957092\n",
      "Epoch 9, Batch 141/462, Loss: 0.6862305402755737\n",
      "Epoch 9, Batch 142/462, Loss: 0.738150954246521\n",
      "Epoch 9, Batch 143/462, Loss: 0.9732541441917419\n",
      "Epoch 9, Batch 144/462, Loss: 0.8474690318107605\n",
      "Epoch 9, Batch 145/462, Loss: 0.8398837447166443\n",
      "Epoch 9, Batch 146/462, Loss: 0.8438622355461121\n",
      "Epoch 9, Batch 147/462, Loss: 0.9298317432403564\n",
      "Epoch 9, Batch 148/462, Loss: 0.9152879118919373\n",
      "Epoch 9, Batch 149/462, Loss: 0.8339551091194153\n",
      "Epoch 9, Batch 150/462, Loss: 0.9447365403175354\n",
      "Epoch 9, Batch 151/462, Loss: 0.8035019636154175\n",
      "Epoch 9, Batch 152/462, Loss: 0.8578324913978577\n",
      "Epoch 9, Batch 153/462, Loss: 0.8002135753631592\n",
      "Epoch 9, Batch 154/462, Loss: 0.9596328735351562\n",
      "Epoch 9, Batch 155/462, Loss: 0.7191936373710632\n",
      "Epoch 9, Batch 156/462, Loss: 0.7743875980377197\n",
      "Epoch 9, Batch 157/462, Loss: 0.8633887767791748\n",
      "Epoch 9, Batch 158/462, Loss: 0.6353532671928406\n",
      "Epoch 9, Batch 159/462, Loss: 0.9676761627197266\n",
      "Epoch 9, Batch 160/462, Loss: 0.7075079679489136\n",
      "Epoch 9, Batch 161/462, Loss: 0.9777794480323792\n",
      "Epoch 9, Batch 162/462, Loss: 0.869588315486908\n",
      "Epoch 9, Batch 163/462, Loss: 0.9556982517242432\n",
      "Epoch 9, Batch 164/462, Loss: 0.6817655563354492\n",
      "Epoch 9, Batch 165/462, Loss: 0.7322311997413635\n",
      "Epoch 9, Batch 166/462, Loss: 0.904683530330658\n",
      "Epoch 9, Batch 167/462, Loss: 0.7292124032974243\n",
      "Epoch 9, Batch 168/462, Loss: 0.8566433191299438\n",
      "Epoch 9, Batch 169/462, Loss: 0.839927077293396\n",
      "Epoch 9, Batch 170/462, Loss: 0.7218919992446899\n",
      "Epoch 9, Batch 171/462, Loss: 0.9527881741523743\n",
      "Epoch 9, Batch 172/462, Loss: 0.8022997975349426\n",
      "Epoch 9, Batch 173/462, Loss: 0.8079196810722351\n",
      "Epoch 9, Batch 174/462, Loss: 0.7740212678909302\n",
      "Epoch 9, Batch 175/462, Loss: 0.7636721134185791\n",
      "Epoch 9, Batch 176/462, Loss: 0.8234954476356506\n",
      "Epoch 9, Batch 177/462, Loss: 0.6946130990982056\n",
      "Epoch 9, Batch 178/462, Loss: 0.793681800365448\n",
      "Epoch 9, Batch 179/462, Loss: 0.8730674982070923\n",
      "Epoch 9, Batch 180/462, Loss: 0.7539700865745544\n",
      "Epoch 9, Batch 181/462, Loss: 0.7944948673248291\n",
      "Epoch 9, Batch 182/462, Loss: 0.7054547667503357\n",
      "Epoch 9, Batch 183/462, Loss: 0.8246427774429321\n",
      "Epoch 9, Batch 184/462, Loss: 0.910674512386322\n",
      "Epoch 9, Batch 185/462, Loss: 0.9423362612724304\n",
      "Epoch 9, Batch 186/462, Loss: 0.7527554035186768\n",
      "Epoch 9, Batch 187/462, Loss: 0.8313595056533813\n",
      "Epoch 9, Batch 188/462, Loss: 1.013108253479004\n",
      "Epoch 9, Batch 189/462, Loss: 0.7266826629638672\n",
      "Epoch 9, Batch 190/462, Loss: 0.7791138291358948\n",
      "Epoch 9, Batch 191/462, Loss: 0.7989515662193298\n",
      "Epoch 9, Batch 192/462, Loss: 0.87605881690979\n",
      "Epoch 9, Batch 193/462, Loss: 0.9764589667320251\n",
      "Epoch 9, Batch 194/462, Loss: 0.906317412853241\n",
      "Epoch 9, Batch 195/462, Loss: 0.80815190076828\n",
      "Epoch 9, Batch 196/462, Loss: 0.8864421844482422\n",
      "Epoch 9, Batch 197/462, Loss: 0.8311759233474731\n",
      "Epoch 9, Batch 198/462, Loss: 0.8284736275672913\n",
      "Epoch 9, Batch 199/462, Loss: 0.7860103845596313\n",
      "Epoch 9, Batch 200/462, Loss: 0.803053081035614\n",
      "Epoch 9, Batch 201/462, Loss: 0.9406307935714722\n",
      "Epoch 9, Batch 202/462, Loss: 0.749420702457428\n",
      "Epoch 9, Batch 203/462, Loss: 0.8047047257423401\n",
      "Epoch 9, Batch 204/462, Loss: 0.7842502593994141\n",
      "Epoch 9, Batch 205/462, Loss: 0.853441596031189\n",
      "Epoch 9, Batch 206/462, Loss: 0.9014061093330383\n",
      "Epoch 9, Batch 207/462, Loss: 0.7552736401557922\n",
      "Epoch 9, Batch 208/462, Loss: 0.9438741207122803\n",
      "Epoch 9, Batch 209/462, Loss: 0.8127647638320923\n",
      "Epoch 9, Batch 210/462, Loss: 0.6605042815208435\n",
      "Epoch 9, Batch 211/462, Loss: 0.8212616443634033\n",
      "Epoch 9, Batch 212/462, Loss: 0.9462856650352478\n",
      "Epoch 9, Batch 213/462, Loss: 0.7858495712280273\n",
      "Epoch 9, Batch 214/462, Loss: 0.8128988146781921\n",
      "Epoch 9, Batch 215/462, Loss: 0.8811784982681274\n",
      "Epoch 9, Batch 216/462, Loss: 0.8427788019180298\n",
      "Epoch 9, Batch 217/462, Loss: 0.8358808755874634\n",
      "Epoch 9, Batch 218/462, Loss: 0.9942801594734192\n",
      "Epoch 9, Batch 219/462, Loss: 0.8366618156433105\n",
      "Epoch 9, Batch 220/462, Loss: 0.9655060172080994\n",
      "Epoch 9, Batch 221/462, Loss: 0.6658892631530762\n",
      "Epoch 9, Batch 222/462, Loss: 0.9212455153465271\n",
      "Epoch 9, Batch 223/462, Loss: 0.8149844408035278\n",
      "Epoch 9, Batch 224/462, Loss: 0.7979453802108765\n",
      "Epoch 9, Batch 225/462, Loss: 0.7965967059135437\n",
      "Epoch 9, Batch 226/462, Loss: 0.8224288821220398\n",
      "Epoch 9, Batch 227/462, Loss: 0.7756351232528687\n",
      "Epoch 9, Batch 228/462, Loss: 0.8311067223548889\n",
      "Epoch 9, Batch 229/462, Loss: 0.7575485706329346\n",
      "Epoch 9, Batch 230/462, Loss: 0.70414137840271\n",
      "Epoch 9, Batch 231/462, Loss: 0.7782624959945679\n",
      "Epoch 9, Batch 232/462, Loss: 0.6946445107460022\n",
      "Epoch 9, Batch 233/462, Loss: 0.7829737067222595\n",
      "Epoch 9, Batch 234/462, Loss: 0.8226251602172852\n",
      "Epoch 9, Batch 235/462, Loss: 0.8929623365402222\n",
      "Epoch 9, Batch 236/462, Loss: 0.785280704498291\n",
      "Epoch 9, Batch 237/462, Loss: 0.8489542007446289\n",
      "Epoch 9, Batch 238/462, Loss: 0.8330802321434021\n",
      "Epoch 9, Batch 239/462, Loss: 0.7887906432151794\n",
      "Epoch 9, Batch 240/462, Loss: 0.8704593777656555\n",
      "Epoch 9, Batch 241/462, Loss: 0.9182922840118408\n",
      "Epoch 9, Batch 242/462, Loss: 0.9229140281677246\n",
      "Epoch 9, Batch 243/462, Loss: 0.8197927474975586\n",
      "Epoch 9, Batch 244/462, Loss: 0.8590560555458069\n",
      "Epoch 9, Batch 245/462, Loss: 0.9886817932128906\n",
      "Epoch 9, Batch 246/462, Loss: 0.8215460777282715\n",
      "Epoch 9, Batch 247/462, Loss: 0.7408177852630615\n",
      "Epoch 9, Batch 248/462, Loss: 0.8257596492767334\n",
      "Epoch 9, Batch 249/462, Loss: 0.8323847651481628\n",
      "Epoch 9, Batch 250/462, Loss: 0.7791855931282043\n",
      "Epoch 9, Batch 251/462, Loss: 0.8404127955436707\n",
      "Epoch 9, Batch 252/462, Loss: 0.7896232604980469\n",
      "Epoch 9, Batch 253/462, Loss: 0.8338279724121094\n",
      "Epoch 9, Batch 254/462, Loss: 0.8701854348182678\n",
      "Epoch 9, Batch 255/462, Loss: 0.76633620262146\n",
      "Epoch 9, Batch 256/462, Loss: 0.8449656963348389\n",
      "Epoch 9, Batch 257/462, Loss: 0.7887653708457947\n",
      "Epoch 9, Batch 258/462, Loss: 0.8621695637702942\n",
      "Epoch 9, Batch 259/462, Loss: 0.7632855176925659\n",
      "Epoch 9, Batch 260/462, Loss: 0.8359752893447876\n",
      "Epoch 9, Batch 261/462, Loss: 0.7289435267448425\n",
      "Epoch 9, Batch 262/462, Loss: 0.8776861429214478\n",
      "Epoch 9, Batch 263/462, Loss: 0.7134402394294739\n",
      "Epoch 9, Batch 264/462, Loss: 0.7866343259811401\n",
      "Epoch 9, Batch 265/462, Loss: 0.728851854801178\n",
      "Epoch 9, Batch 266/462, Loss: 0.830560028553009\n",
      "Epoch 9, Batch 267/462, Loss: 0.8778796792030334\n",
      "Epoch 9, Batch 268/462, Loss: 0.7334992289543152\n",
      "Epoch 9, Batch 269/462, Loss: 0.7809746265411377\n",
      "Epoch 9, Batch 270/462, Loss: 0.6038058400154114\n",
      "Epoch 9, Batch 271/462, Loss: 0.7865543365478516\n",
      "Epoch 9, Batch 272/462, Loss: 0.8275991678237915\n",
      "Epoch 9, Batch 273/462, Loss: 0.7073073387145996\n",
      "Epoch 9, Batch 274/462, Loss: 0.7421561479568481\n",
      "Epoch 9, Batch 275/462, Loss: 0.852850615978241\n",
      "Epoch 9, Batch 276/462, Loss: 0.9189380407333374\n",
      "Epoch 9, Batch 277/462, Loss: 0.7052780389785767\n",
      "Epoch 9, Batch 278/462, Loss: 0.7965880036354065\n",
      "Epoch 9, Batch 279/462, Loss: 0.7174509763717651\n",
      "Epoch 9, Batch 280/462, Loss: 0.6705939769744873\n",
      "Epoch 9, Batch 281/462, Loss: 0.8081313967704773\n",
      "Epoch 9, Batch 282/462, Loss: 0.6964643597602844\n",
      "Epoch 9, Batch 283/462, Loss: 0.96214759349823\n",
      "Epoch 9, Batch 284/462, Loss: 0.6902764439582825\n",
      "Epoch 9, Batch 285/462, Loss: 0.795012891292572\n",
      "Epoch 9, Batch 286/462, Loss: 0.8065537214279175\n",
      "Epoch 9, Batch 287/462, Loss: 0.7664448022842407\n",
      "Epoch 9, Batch 288/462, Loss: 0.7465267181396484\n",
      "Epoch 9, Batch 289/462, Loss: 0.6496448516845703\n",
      "Epoch 9, Batch 290/462, Loss: 0.7633946537971497\n",
      "Epoch 9, Batch 291/462, Loss: 0.8750461339950562\n",
      "Epoch 9, Batch 292/462, Loss: 0.8215073347091675\n",
      "Epoch 9, Batch 293/462, Loss: 0.8986707925796509\n",
      "Epoch 9, Batch 294/462, Loss: 0.7901805639266968\n",
      "Epoch 9, Batch 295/462, Loss: 0.8792784214019775\n",
      "Epoch 9, Batch 296/462, Loss: 0.7784900069236755\n",
      "Epoch 9, Batch 297/462, Loss: 0.8430083990097046\n",
      "Epoch 9, Batch 298/462, Loss: 0.8572790622711182\n",
      "Epoch 9, Batch 299/462, Loss: 0.7133755683898926\n",
      "Epoch 9, Batch 300/462, Loss: 0.7998417019844055\n",
      "Epoch 9, Batch 301/462, Loss: 0.712816059589386\n",
      "Epoch 9, Batch 302/462, Loss: 0.8593222498893738\n",
      "Epoch 9, Batch 303/462, Loss: 0.7738686800003052\n",
      "Epoch 9, Batch 304/462, Loss: 0.7596123814582825\n",
      "Epoch 9, Batch 305/462, Loss: 0.839360237121582\n",
      "Epoch 9, Batch 306/462, Loss: 0.8404850363731384\n",
      "Epoch 9, Batch 307/462, Loss: 0.7765018939971924\n",
      "Epoch 9, Batch 308/462, Loss: 0.7696665525436401\n",
      "Epoch 9, Batch 309/462, Loss: 0.757370114326477\n",
      "Epoch 9, Batch 310/462, Loss: 0.7633150219917297\n",
      "Epoch 9, Batch 311/462, Loss: 0.7347456812858582\n",
      "Epoch 9, Batch 312/462, Loss: 0.8142116665840149\n",
      "Epoch 9, Batch 313/462, Loss: 0.6904157400131226\n",
      "Epoch 9, Batch 314/462, Loss: 0.8780642747879028\n",
      "Epoch 9, Batch 315/462, Loss: 0.9610828161239624\n",
      "Epoch 9, Batch 316/462, Loss: 0.8040733933448792\n",
      "Epoch 9, Batch 317/462, Loss: 0.8323315382003784\n",
      "Epoch 9, Batch 318/462, Loss: 0.8801043033599854\n",
      "Epoch 9, Batch 319/462, Loss: 0.6851848363876343\n",
      "Epoch 9, Batch 320/462, Loss: 0.7435745000839233\n",
      "Epoch 9, Batch 321/462, Loss: 0.8232861757278442\n",
      "Epoch 9, Batch 322/462, Loss: 0.9507015943527222\n",
      "Epoch 9, Batch 323/462, Loss: 0.6230784058570862\n",
      "Epoch 9, Batch 324/462, Loss: 0.80770343542099\n",
      "Epoch 9, Batch 325/462, Loss: 0.7036390900611877\n",
      "Epoch 9, Batch 326/462, Loss: 0.8118628263473511\n",
      "Epoch 9, Batch 327/462, Loss: 0.9295703172683716\n",
      "Epoch 9, Batch 328/462, Loss: 0.9717128872871399\n",
      "Epoch 9, Batch 329/462, Loss: 0.7491662502288818\n",
      "Epoch 9, Batch 330/462, Loss: 0.7981659173965454\n",
      "Epoch 9, Batch 331/462, Loss: 0.8900068998336792\n",
      "Epoch 9, Batch 332/462, Loss: 0.743386447429657\n",
      "Epoch 9, Batch 333/462, Loss: 0.7111573219299316\n",
      "Epoch 9, Batch 334/462, Loss: 0.6808269619941711\n",
      "Epoch 9, Batch 335/462, Loss: 0.8219015598297119\n",
      "Epoch 9, Batch 336/462, Loss: 0.7917392253875732\n",
      "Epoch 9, Batch 337/462, Loss: 0.7233042120933533\n",
      "Epoch 9, Batch 338/462, Loss: 0.7872143983840942\n",
      "Epoch 9, Batch 339/462, Loss: 0.969654381275177\n",
      "Epoch 9, Batch 340/462, Loss: 0.7854079008102417\n",
      "Epoch 9, Batch 341/462, Loss: 0.8502148985862732\n",
      "Epoch 9, Batch 342/462, Loss: 0.7979791760444641\n",
      "Epoch 9, Batch 343/462, Loss: 0.8955481648445129\n",
      "Epoch 9, Batch 344/462, Loss: 1.0649162530899048\n",
      "Epoch 9, Batch 345/462, Loss: 0.7410708665847778\n",
      "Epoch 9, Batch 346/462, Loss: 0.8246827125549316\n",
      "Epoch 9, Batch 347/462, Loss: 0.8948865532875061\n",
      "Epoch 9, Batch 348/462, Loss: 0.8800004720687866\n",
      "Epoch 9, Batch 349/462, Loss: 0.7979007959365845\n",
      "Epoch 9, Batch 350/462, Loss: 0.7477304339408875\n",
      "Epoch 9, Batch 351/462, Loss: 0.777888834476471\n",
      "Epoch 9, Batch 352/462, Loss: 0.8699409365653992\n",
      "Epoch 9, Batch 353/462, Loss: 0.669609010219574\n",
      "Epoch 9, Batch 354/462, Loss: 0.9276987314224243\n",
      "Epoch 9, Batch 355/462, Loss: 0.7714502215385437\n",
      "Epoch 9, Batch 356/462, Loss: 0.9193175435066223\n",
      "Epoch 9, Batch 357/462, Loss: 0.7834139466285706\n",
      "Epoch 9, Batch 358/462, Loss: 0.6860529780387878\n",
      "Epoch 9, Batch 359/462, Loss: 0.8313135504722595\n",
      "Epoch 9, Batch 360/462, Loss: 1.0147114992141724\n",
      "Epoch 9, Batch 361/462, Loss: 0.7672976851463318\n",
      "Epoch 9, Batch 362/462, Loss: 0.9056836366653442\n",
      "Epoch 9, Batch 363/462, Loss: 0.8191594481468201\n",
      "Epoch 9, Batch 364/462, Loss: 0.8359761834144592\n",
      "Epoch 9, Batch 365/462, Loss: 0.880770206451416\n",
      "Epoch 9, Batch 366/462, Loss: 0.7266775369644165\n",
      "Epoch 9, Batch 367/462, Loss: 0.7464998960494995\n",
      "Epoch 9, Batch 368/462, Loss: 0.7713898420333862\n",
      "Epoch 9, Batch 369/462, Loss: 0.6954378485679626\n",
      "Epoch 9, Batch 370/462, Loss: 0.748862087726593\n",
      "Epoch 9, Batch 371/462, Loss: 0.688107967376709\n",
      "Epoch 9, Batch 372/462, Loss: 0.719394326210022\n",
      "Epoch 9, Batch 373/462, Loss: 0.7032244801521301\n",
      "Epoch 9, Batch 374/462, Loss: 0.7726630568504333\n",
      "Epoch 9, Batch 375/462, Loss: 0.6979599595069885\n",
      "Epoch 9, Batch 376/462, Loss: 0.7801271080970764\n",
      "Epoch 9, Batch 377/462, Loss: 0.8072950839996338\n",
      "Epoch 9, Batch 378/462, Loss: 0.8900300860404968\n",
      "Epoch 9, Batch 379/462, Loss: 0.8005832433700562\n",
      "Epoch 9, Batch 380/462, Loss: 0.8643495440483093\n",
      "Epoch 9, Batch 381/462, Loss: 0.7148396968841553\n",
      "Epoch 9, Batch 382/462, Loss: 0.7298476696014404\n",
      "Epoch 9, Batch 383/462, Loss: 0.8854487538337708\n",
      "Epoch 9, Batch 384/462, Loss: 0.8103113174438477\n",
      "Epoch 9, Batch 385/462, Loss: 0.7264953255653381\n",
      "Epoch 9, Batch 386/462, Loss: 0.8265806436538696\n",
      "Epoch 9, Batch 387/462, Loss: 0.6372441053390503\n",
      "Epoch 9, Batch 388/462, Loss: 0.751732349395752\n",
      "Epoch 9, Batch 389/462, Loss: 0.8255676031112671\n",
      "Epoch 9, Batch 390/462, Loss: 0.8619511127471924\n",
      "Epoch 9, Batch 391/462, Loss: 0.8433039784431458\n",
      "Epoch 9, Batch 392/462, Loss: 0.7593222260475159\n",
      "Epoch 9, Batch 393/462, Loss: 0.9562740325927734\n",
      "Epoch 9, Batch 394/462, Loss: 0.6396368145942688\n",
      "Epoch 9, Batch 395/462, Loss: 0.6684288382530212\n",
      "Epoch 9, Batch 396/462, Loss: 0.8553226590156555\n",
      "Epoch 9, Batch 397/462, Loss: 0.7856966257095337\n",
      "Epoch 9, Batch 398/462, Loss: 0.8406850695610046\n",
      "Epoch 9, Batch 399/462, Loss: 0.8315819501876831\n",
      "Epoch 9, Batch 400/462, Loss: 0.7468453049659729\n",
      "Epoch 9, Batch 401/462, Loss: 0.7573473453521729\n",
      "Epoch 9, Batch 402/462, Loss: 0.7548151612281799\n",
      "Epoch 9, Batch 403/462, Loss: 0.7888696193695068\n",
      "Epoch 9, Batch 404/462, Loss: 0.8150964379310608\n",
      "Epoch 9, Batch 405/462, Loss: 0.796005129814148\n",
      "Epoch 9, Batch 406/462, Loss: 0.9725653529167175\n",
      "Epoch 9, Batch 407/462, Loss: 0.7648712396621704\n",
      "Epoch 9, Batch 408/462, Loss: 0.7956421971321106\n",
      "Epoch 9, Batch 409/462, Loss: 0.7938018441200256\n",
      "Epoch 9, Batch 410/462, Loss: 0.803972601890564\n",
      "Epoch 9, Batch 411/462, Loss: 0.8743866086006165\n",
      "Epoch 9, Batch 412/462, Loss: 0.7547280788421631\n",
      "Epoch 9, Batch 413/462, Loss: 0.7172626256942749\n",
      "Epoch 9, Batch 414/462, Loss: 0.8115161657333374\n",
      "Epoch 9, Batch 415/462, Loss: 0.7114110589027405\n",
      "Epoch 9, Batch 416/462, Loss: 0.7582682967185974\n",
      "Epoch 9, Batch 417/462, Loss: 0.8780753016471863\n",
      "Epoch 9, Batch 418/462, Loss: 0.8508548736572266\n",
      "Epoch 9, Batch 419/462, Loss: 0.9521749019622803\n",
      "Epoch 9, Batch 420/462, Loss: 0.8881102204322815\n",
      "Epoch 9, Batch 421/462, Loss: 0.9108660817146301\n",
      "Epoch 9, Batch 422/462, Loss: 0.9690539836883545\n",
      "Epoch 9, Batch 423/462, Loss: 0.7834572196006775\n",
      "Epoch 9, Batch 424/462, Loss: 0.8199371099472046\n",
      "Epoch 9, Batch 425/462, Loss: 0.7287704348564148\n",
      "Epoch 9, Batch 426/462, Loss: 0.9163583517074585\n",
      "Epoch 9, Batch 427/462, Loss: 0.7168523073196411\n",
      "Epoch 9, Batch 428/462, Loss: 0.7280617952346802\n",
      "Epoch 9, Batch 429/462, Loss: 0.7471513152122498\n",
      "Epoch 9, Batch 430/462, Loss: 0.6738818287849426\n",
      "Epoch 9, Batch 431/462, Loss: 0.8740161061286926\n",
      "Epoch 9, Batch 432/462, Loss: 0.823078989982605\n",
      "Epoch 9, Batch 433/462, Loss: 0.7157495617866516\n",
      "Epoch 9, Batch 434/462, Loss: 0.6430445909500122\n",
      "Epoch 9, Batch 435/462, Loss: 0.7760447859764099\n",
      "Epoch 9, Batch 436/462, Loss: 0.7571123242378235\n",
      "Epoch 9, Batch 437/462, Loss: 0.7304391264915466\n",
      "Epoch 9, Batch 438/462, Loss: 0.8059701919555664\n",
      "Epoch 9, Batch 439/462, Loss: 0.8150652647018433\n",
      "Epoch 9, Batch 440/462, Loss: 0.7496424317359924\n",
      "Epoch 9, Batch 441/462, Loss: 0.6535798907279968\n",
      "Epoch 9, Batch 442/462, Loss: 0.7988787293434143\n",
      "Epoch 9, Batch 443/462, Loss: 0.9087525010108948\n",
      "Epoch 9, Batch 444/462, Loss: 0.7720226049423218\n",
      "Epoch 9, Batch 445/462, Loss: 0.8356900215148926\n",
      "Epoch 9, Batch 446/462, Loss: 0.7979023456573486\n",
      "Epoch 9, Batch 447/462, Loss: 0.8864604830741882\n",
      "Epoch 9, Batch 448/462, Loss: 0.6650729179382324\n",
      "Epoch 9, Batch 449/462, Loss: 0.9225310683250427\n",
      "Epoch 9, Batch 450/462, Loss: 0.7871887683868408\n",
      "Epoch 9, Batch 451/462, Loss: 0.9152573347091675\n",
      "Epoch 9, Batch 452/462, Loss: 0.7504521012306213\n",
      "Epoch 9, Batch 453/462, Loss: 0.884509801864624\n",
      "Epoch 9, Batch 454/462, Loss: 0.9694277048110962\n",
      "Epoch 9, Batch 455/462, Loss: 0.7154269814491272\n",
      "Epoch 9, Batch 456/462, Loss: 0.7128015756607056\n",
      "Epoch 9, Batch 457/462, Loss: 0.8207038640975952\n",
      "Epoch 9, Batch 458/462, Loss: 0.7716608047485352\n",
      "Epoch 9, Batch 459/462, Loss: 0.8599141240119934\n",
      "Epoch 9, Batch 460/462, Loss: 0.8541558980941772\n",
      "Epoch 9, Batch 461/462, Loss: 0.7646250128746033\n",
      "Epoch 9, Batch 462/462, Loss: 0.6527627110481262\n",
      "Epoch 9, Loss: 372.9597550034523\n",
      "Epoch 10, Batch 1/462, Loss: 0.8075703382492065\n",
      "Epoch 10, Batch 2/462, Loss: 0.879721999168396\n",
      "Epoch 10, Batch 3/462, Loss: 0.6695857048034668\n",
      "Epoch 10, Batch 4/462, Loss: 0.9489555358886719\n",
      "Epoch 10, Batch 5/462, Loss: 0.6595202684402466\n",
      "Epoch 10, Batch 6/462, Loss: 0.6157187223434448\n",
      "Epoch 10, Batch 7/462, Loss: 0.7548804879188538\n",
      "Epoch 10, Batch 8/462, Loss: 0.8215681910514832\n",
      "Epoch 10, Batch 9/462, Loss: 0.8885642290115356\n",
      "Epoch 10, Batch 10/462, Loss: 0.8774630427360535\n",
      "Epoch 10, Batch 11/462, Loss: 0.7615989446640015\n",
      "Epoch 10, Batch 12/462, Loss: 0.8330636024475098\n",
      "Epoch 10, Batch 13/462, Loss: 0.8355023264884949\n",
      "Epoch 10, Batch 14/462, Loss: 0.7298813462257385\n",
      "Epoch 10, Batch 15/462, Loss: 0.7944143414497375\n",
      "Epoch 10, Batch 16/462, Loss: 0.6618552207946777\n",
      "Epoch 10, Batch 17/462, Loss: 0.867854654788971\n",
      "Epoch 10, Batch 18/462, Loss: 0.8164690732955933\n",
      "Epoch 10, Batch 19/462, Loss: 0.9458409547805786\n",
      "Epoch 10, Batch 20/462, Loss: 0.8378793001174927\n",
      "Epoch 10, Batch 21/462, Loss: 0.7315104603767395\n",
      "Epoch 10, Batch 22/462, Loss: 0.793735921382904\n",
      "Epoch 10, Batch 23/462, Loss: 0.8707208633422852\n",
      "Epoch 10, Batch 24/462, Loss: 0.8772222399711609\n",
      "Epoch 10, Batch 25/462, Loss: 0.8272609114646912\n",
      "Epoch 10, Batch 26/462, Loss: 0.7327075600624084\n",
      "Epoch 10, Batch 27/462, Loss: 0.8054696917533875\n",
      "Epoch 10, Batch 28/462, Loss: 0.7394731640815735\n",
      "Epoch 10, Batch 29/462, Loss: 0.7328803539276123\n",
      "Epoch 10, Batch 30/462, Loss: 0.7554672360420227\n",
      "Epoch 10, Batch 31/462, Loss: 0.7041335701942444\n",
      "Epoch 10, Batch 32/462, Loss: 0.7910255193710327\n",
      "Epoch 10, Batch 33/462, Loss: 0.8623854517936707\n",
      "Epoch 10, Batch 34/462, Loss: 0.7264294624328613\n",
      "Epoch 10, Batch 35/462, Loss: 0.8237934708595276\n",
      "Epoch 10, Batch 36/462, Loss: 0.7720740437507629\n",
      "Epoch 10, Batch 37/462, Loss: 0.8245213031768799\n",
      "Epoch 10, Batch 38/462, Loss: 0.8684789538383484\n",
      "Epoch 10, Batch 39/462, Loss: 0.7452276349067688\n",
      "Epoch 10, Batch 40/462, Loss: 0.9258720874786377\n",
      "Epoch 10, Batch 41/462, Loss: 0.680845320224762\n",
      "Epoch 10, Batch 42/462, Loss: 0.8340821862220764\n",
      "Epoch 10, Batch 43/462, Loss: 0.8463563919067383\n",
      "Epoch 10, Batch 44/462, Loss: 0.7992569208145142\n",
      "Epoch 10, Batch 45/462, Loss: 0.7111705541610718\n",
      "Epoch 10, Batch 46/462, Loss: 0.784370481967926\n",
      "Epoch 10, Batch 47/462, Loss: 0.7178805470466614\n",
      "Epoch 10, Batch 48/462, Loss: 0.7949699759483337\n",
      "Epoch 10, Batch 49/462, Loss: 0.7231546640396118\n",
      "Epoch 10, Batch 50/462, Loss: 0.8057630062103271\n",
      "Epoch 10, Batch 51/462, Loss: 0.6794909238815308\n",
      "Epoch 10, Batch 52/462, Loss: 1.0181024074554443\n",
      "Epoch 10, Batch 53/462, Loss: 0.9415882229804993\n",
      "Epoch 10, Batch 54/462, Loss: 0.6432211399078369\n",
      "Epoch 10, Batch 55/462, Loss: 0.7867816686630249\n",
      "Epoch 10, Batch 56/462, Loss: 0.9179030060768127\n",
      "Epoch 10, Batch 57/462, Loss: 0.6553930640220642\n",
      "Epoch 10, Batch 58/462, Loss: 0.843229353427887\n",
      "Epoch 10, Batch 59/462, Loss: 0.81622713804245\n",
      "Epoch 10, Batch 60/462, Loss: 0.7604013681411743\n",
      "Epoch 10, Batch 61/462, Loss: 0.6912602782249451\n",
      "Epoch 10, Batch 62/462, Loss: 0.8495532870292664\n",
      "Epoch 10, Batch 63/462, Loss: 0.7346482872962952\n",
      "Epoch 10, Batch 64/462, Loss: 0.8488695025444031\n",
      "Epoch 10, Batch 65/462, Loss: 0.8056821823120117\n",
      "Epoch 10, Batch 66/462, Loss: 0.9193851351737976\n",
      "Epoch 10, Batch 67/462, Loss: 0.9880573153495789\n",
      "Epoch 10, Batch 68/462, Loss: 0.775378406047821\n",
      "Epoch 10, Batch 69/462, Loss: 0.8449968099594116\n",
      "Epoch 10, Batch 70/462, Loss: 0.8365113139152527\n",
      "Epoch 10, Batch 71/462, Loss: 1.0231345891952515\n",
      "Epoch 10, Batch 72/462, Loss: 0.7280722856521606\n",
      "Epoch 10, Batch 73/462, Loss: 0.719433069229126\n",
      "Epoch 10, Batch 74/462, Loss: 0.7182492613792419\n",
      "Epoch 10, Batch 75/462, Loss: 0.8289608359336853\n",
      "Epoch 10, Batch 76/462, Loss: 0.7597319483757019\n",
      "Epoch 10, Batch 77/462, Loss: 0.7910099029541016\n",
      "Epoch 10, Batch 78/462, Loss: 0.7728869318962097\n",
      "Epoch 10, Batch 79/462, Loss: 0.7999140620231628\n",
      "Epoch 10, Batch 80/462, Loss: 0.8336206078529358\n",
      "Epoch 10, Batch 81/462, Loss: 0.833276093006134\n",
      "Epoch 10, Batch 82/462, Loss: 0.7101498246192932\n",
      "Epoch 10, Batch 83/462, Loss: 0.6853131055831909\n",
      "Epoch 10, Batch 84/462, Loss: 0.7030162215232849\n",
      "Epoch 10, Batch 85/462, Loss: 0.7485671043395996\n",
      "Epoch 10, Batch 86/462, Loss: 0.8338810801506042\n",
      "Epoch 10, Batch 87/462, Loss: 0.7777519822120667\n",
      "Epoch 10, Batch 88/462, Loss: 0.8227351307868958\n",
      "Epoch 10, Batch 89/462, Loss: 0.9262223243713379\n",
      "Epoch 10, Batch 90/462, Loss: 0.8484940528869629\n",
      "Epoch 10, Batch 91/462, Loss: 0.7572181224822998\n",
      "Epoch 10, Batch 92/462, Loss: 1.0932233333587646\n",
      "Epoch 10, Batch 93/462, Loss: 0.675229549407959\n",
      "Epoch 10, Batch 94/462, Loss: 0.7516490817070007\n",
      "Epoch 10, Batch 95/462, Loss: 0.7665523290634155\n",
      "Epoch 10, Batch 96/462, Loss: 0.8431283831596375\n",
      "Epoch 10, Batch 97/462, Loss: 0.6131867170333862\n",
      "Epoch 10, Batch 98/462, Loss: 0.8468108773231506\n",
      "Epoch 10, Batch 99/462, Loss: 0.7579670548439026\n",
      "Epoch 10, Batch 100/462, Loss: 0.7439590692520142\n",
      "Epoch 10, Batch 101/462, Loss: 0.7055330276489258\n",
      "Epoch 10, Batch 102/462, Loss: 0.8855350613594055\n",
      "Epoch 10, Batch 103/462, Loss: 0.6732190847396851\n",
      "Epoch 10, Batch 104/462, Loss: 0.7740587592124939\n",
      "Epoch 10, Batch 105/462, Loss: 0.7938982248306274\n",
      "Epoch 10, Batch 106/462, Loss: 0.7036253213882446\n",
      "Epoch 10, Batch 107/462, Loss: 0.8376378417015076\n",
      "Epoch 10, Batch 108/462, Loss: 0.6649173498153687\n",
      "Epoch 10, Batch 109/462, Loss: 1.0086650848388672\n",
      "Epoch 10, Batch 110/462, Loss: 0.6817910075187683\n",
      "Epoch 10, Batch 111/462, Loss: 0.7227534651756287\n",
      "Epoch 10, Batch 112/462, Loss: 0.7802022099494934\n",
      "Epoch 10, Batch 113/462, Loss: 0.6814072132110596\n",
      "Epoch 10, Batch 114/462, Loss: 0.8729539513587952\n",
      "Epoch 10, Batch 115/462, Loss: 0.8408153653144836\n",
      "Epoch 10, Batch 116/462, Loss: 0.8314915299415588\n",
      "Epoch 10, Batch 117/462, Loss: 0.8599076867103577\n",
      "Epoch 10, Batch 118/462, Loss: 0.8507241010665894\n",
      "Epoch 10, Batch 119/462, Loss: 0.928793728351593\n",
      "Epoch 10, Batch 120/462, Loss: 0.6853304505348206\n",
      "Epoch 10, Batch 121/462, Loss: 0.785883903503418\n",
      "Epoch 10, Batch 122/462, Loss: 0.7911385297775269\n",
      "Epoch 10, Batch 123/462, Loss: 0.7570553421974182\n",
      "Epoch 10, Batch 124/462, Loss: 0.7512574791908264\n",
      "Epoch 10, Batch 125/462, Loss: 0.8896037340164185\n",
      "Epoch 10, Batch 126/462, Loss: 0.7909765243530273\n",
      "Epoch 10, Batch 127/462, Loss: 0.6844903826713562\n",
      "Epoch 10, Batch 128/462, Loss: 0.7374185919761658\n",
      "Epoch 10, Batch 129/462, Loss: 0.800031840801239\n",
      "Epoch 10, Batch 130/462, Loss: 0.8236082792282104\n",
      "Epoch 10, Batch 131/462, Loss: 0.8204638361930847\n",
      "Epoch 10, Batch 132/462, Loss: 0.8453059196472168\n",
      "Epoch 10, Batch 133/462, Loss: 0.8076255917549133\n",
      "Epoch 10, Batch 134/462, Loss: 0.8523771166801453\n",
      "Epoch 10, Batch 135/462, Loss: 0.8179178833961487\n",
      "Epoch 10, Batch 136/462, Loss: 0.7387580871582031\n",
      "Epoch 10, Batch 137/462, Loss: 0.8966289162635803\n",
      "Epoch 10, Batch 138/462, Loss: 0.7942905426025391\n",
      "Epoch 10, Batch 139/462, Loss: 0.805301308631897\n",
      "Epoch 10, Batch 140/462, Loss: 0.8999302983283997\n",
      "Epoch 10, Batch 141/462, Loss: 0.9022737145423889\n",
      "Epoch 10, Batch 142/462, Loss: 0.7428261041641235\n",
      "Epoch 10, Batch 143/462, Loss: 0.929209291934967\n",
      "Epoch 10, Batch 144/462, Loss: 0.8265200257301331\n",
      "Epoch 10, Batch 145/462, Loss: 0.77115398645401\n",
      "Epoch 10, Batch 146/462, Loss: 0.9686048626899719\n",
      "Epoch 10, Batch 147/462, Loss: 0.7948017716407776\n",
      "Epoch 10, Batch 148/462, Loss: 0.8101016283035278\n",
      "Epoch 10, Batch 149/462, Loss: 0.7422981262207031\n",
      "Epoch 10, Batch 150/462, Loss: 0.759218692779541\n",
      "Epoch 10, Batch 151/462, Loss: 0.9886698722839355\n",
      "Epoch 10, Batch 152/462, Loss: 0.9284553527832031\n",
      "Epoch 10, Batch 153/462, Loss: 0.884994626045227\n",
      "Epoch 10, Batch 154/462, Loss: 0.7872955799102783\n",
      "Epoch 10, Batch 155/462, Loss: 0.8534648418426514\n",
      "Epoch 10, Batch 156/462, Loss: 0.8978272676467896\n",
      "Epoch 10, Batch 157/462, Loss: 0.8284825682640076\n",
      "Epoch 10, Batch 158/462, Loss: 0.8059559464454651\n",
      "Epoch 10, Batch 159/462, Loss: 0.9644235372543335\n",
      "Epoch 10, Batch 160/462, Loss: 0.9095389246940613\n",
      "Epoch 10, Batch 161/462, Loss: 0.8374935388565063\n",
      "Epoch 10, Batch 162/462, Loss: 0.75731360912323\n",
      "Epoch 10, Batch 163/462, Loss: 0.8396803736686707\n",
      "Epoch 10, Batch 164/462, Loss: 0.727728545665741\n",
      "Epoch 10, Batch 165/462, Loss: 0.8019041419029236\n",
      "Epoch 10, Batch 166/462, Loss: 0.840679943561554\n",
      "Epoch 10, Batch 167/462, Loss: 0.6889357566833496\n",
      "Epoch 10, Batch 168/462, Loss: 0.8524416089057922\n",
      "Epoch 10, Batch 169/462, Loss: 0.7150416374206543\n",
      "Epoch 10, Batch 170/462, Loss: 0.8344943523406982\n",
      "Epoch 10, Batch 171/462, Loss: 0.7773670554161072\n",
      "Epoch 10, Batch 172/462, Loss: 0.6635258793830872\n",
      "Epoch 10, Batch 173/462, Loss: 0.8873961567878723\n",
      "Epoch 10, Batch 174/462, Loss: 0.7880541086196899\n",
      "Epoch 10, Batch 175/462, Loss: 0.9174819588661194\n",
      "Epoch 10, Batch 176/462, Loss: 0.8323021531105042\n",
      "Epoch 10, Batch 177/462, Loss: 0.7815743684768677\n",
      "Epoch 10, Batch 178/462, Loss: 0.7394067049026489\n",
      "Epoch 10, Batch 179/462, Loss: 0.9865668416023254\n",
      "Epoch 10, Batch 180/462, Loss: 0.7956616282463074\n",
      "Epoch 10, Batch 181/462, Loss: 0.6926141381263733\n",
      "Epoch 10, Batch 182/462, Loss: 0.7556531429290771\n",
      "Epoch 10, Batch 183/462, Loss: 0.8380321860313416\n",
      "Epoch 10, Batch 184/462, Loss: 0.7101213335990906\n",
      "Epoch 10, Batch 185/462, Loss: 0.7478086948394775\n",
      "Epoch 10, Batch 186/462, Loss: 0.8474746346473694\n",
      "Epoch 10, Batch 187/462, Loss: 1.0017571449279785\n",
      "Epoch 10, Batch 188/462, Loss: 0.8424901366233826\n",
      "Epoch 10, Batch 189/462, Loss: 0.7474076747894287\n",
      "Epoch 10, Batch 190/462, Loss: 0.6788146495819092\n",
      "Epoch 10, Batch 191/462, Loss: 0.67624831199646\n",
      "Epoch 10, Batch 192/462, Loss: 0.8573029637336731\n",
      "Epoch 10, Batch 193/462, Loss: 0.8370209336280823\n",
      "Epoch 10, Batch 194/462, Loss: 0.8375078439712524\n",
      "Epoch 10, Batch 195/462, Loss: 0.9617577791213989\n",
      "Epoch 10, Batch 196/462, Loss: 0.7196289896965027\n",
      "Epoch 10, Batch 197/462, Loss: 0.8466483354568481\n",
      "Epoch 10, Batch 198/462, Loss: 0.6870918273925781\n",
      "Epoch 10, Batch 199/462, Loss: 0.8136889338493347\n",
      "Epoch 10, Batch 200/462, Loss: 0.6588218808174133\n",
      "Epoch 10, Batch 201/462, Loss: 0.948901355266571\n",
      "Epoch 10, Batch 202/462, Loss: 0.8845766186714172\n",
      "Epoch 10, Batch 203/462, Loss: 0.7896929383277893\n",
      "Epoch 10, Batch 204/462, Loss: 0.7765426635742188\n",
      "Epoch 10, Batch 205/462, Loss: 0.8596554398536682\n",
      "Epoch 10, Batch 206/462, Loss: 0.9353658556938171\n",
      "Epoch 10, Batch 207/462, Loss: 0.7806108593940735\n",
      "Epoch 10, Batch 208/462, Loss: 0.8773226141929626\n",
      "Epoch 10, Batch 209/462, Loss: 0.652117908000946\n",
      "Epoch 10, Batch 210/462, Loss: 0.7658904790878296\n",
      "Epoch 10, Batch 211/462, Loss: 0.8011718392372131\n",
      "Epoch 10, Batch 212/462, Loss: 0.8494741916656494\n",
      "Epoch 10, Batch 213/462, Loss: 0.9207906723022461\n",
      "Epoch 10, Batch 214/462, Loss: 0.900224506855011\n",
      "Epoch 10, Batch 215/462, Loss: 0.7674968242645264\n",
      "Epoch 10, Batch 216/462, Loss: 0.7027441263198853\n",
      "Epoch 10, Batch 217/462, Loss: 0.7356493473052979\n",
      "Epoch 10, Batch 218/462, Loss: 0.7365912795066833\n",
      "Epoch 10, Batch 219/462, Loss: 0.7774438858032227\n",
      "Epoch 10, Batch 220/462, Loss: 0.7704138159751892\n",
      "Epoch 10, Batch 221/462, Loss: 0.7606124877929688\n",
      "Epoch 10, Batch 222/462, Loss: 0.7847169637680054\n",
      "Epoch 10, Batch 223/462, Loss: 0.8018590211868286\n",
      "Epoch 10, Batch 224/462, Loss: 0.7296321392059326\n",
      "Epoch 10, Batch 225/462, Loss: 0.8403065204620361\n",
      "Epoch 10, Batch 226/462, Loss: 0.6878947019577026\n",
      "Epoch 10, Batch 227/462, Loss: 0.740996241569519\n",
      "Epoch 10, Batch 228/462, Loss: 0.7657795548439026\n",
      "Epoch 10, Batch 229/462, Loss: 0.8889101147651672\n",
      "Epoch 10, Batch 230/462, Loss: 0.7456027865409851\n",
      "Epoch 10, Batch 231/462, Loss: 0.6782549023628235\n",
      "Epoch 10, Batch 232/462, Loss: 0.8531702160835266\n",
      "Epoch 10, Batch 233/462, Loss: 0.7876415848731995\n",
      "Epoch 10, Batch 234/462, Loss: 0.8556932210922241\n",
      "Epoch 10, Batch 235/462, Loss: 0.6908767223358154\n",
      "Epoch 10, Batch 236/462, Loss: 0.8234637379646301\n",
      "Epoch 10, Batch 237/462, Loss: 0.8842995166778564\n",
      "Epoch 10, Batch 238/462, Loss: 0.7253932356834412\n",
      "Epoch 10, Batch 239/462, Loss: 0.6969243884086609\n",
      "Epoch 10, Batch 240/462, Loss: 0.7758793234825134\n",
      "Epoch 10, Batch 241/462, Loss: 0.6790213584899902\n",
      "Epoch 10, Batch 242/462, Loss: 0.6894950866699219\n",
      "Epoch 10, Batch 243/462, Loss: 0.8557465076446533\n",
      "Epoch 10, Batch 244/462, Loss: 0.7222986221313477\n",
      "Epoch 10, Batch 245/462, Loss: 0.7577826976776123\n",
      "Epoch 10, Batch 246/462, Loss: 0.7802526354789734\n",
      "Epoch 10, Batch 247/462, Loss: 0.8037853240966797\n",
      "Epoch 10, Batch 248/462, Loss: 0.7322475910186768\n",
      "Epoch 10, Batch 249/462, Loss: 0.8305665254592896\n",
      "Epoch 10, Batch 250/462, Loss: 0.7368758916854858\n",
      "Epoch 10, Batch 251/462, Loss: 0.7788824439048767\n",
      "Epoch 10, Batch 252/462, Loss: 0.8652857542037964\n",
      "Epoch 10, Batch 253/462, Loss: 0.8234030604362488\n",
      "Epoch 10, Batch 254/462, Loss: 0.7609685659408569\n",
      "Epoch 10, Batch 255/462, Loss: 0.7560911774635315\n",
      "Epoch 10, Batch 256/462, Loss: 0.8599483966827393\n",
      "Epoch 10, Batch 257/462, Loss: 0.8517805933952332\n",
      "Epoch 10, Batch 258/462, Loss: 0.8957624435424805\n",
      "Epoch 10, Batch 259/462, Loss: 0.7474164366722107\n",
      "Epoch 10, Batch 260/462, Loss: 0.693270206451416\n",
      "Epoch 10, Batch 261/462, Loss: 0.807867705821991\n",
      "Epoch 10, Batch 262/462, Loss: 0.6982956528663635\n",
      "Epoch 10, Batch 263/462, Loss: 0.731381356716156\n",
      "Epoch 10, Batch 264/462, Loss: 0.768869936466217\n",
      "Epoch 10, Batch 265/462, Loss: 0.721910834312439\n",
      "Epoch 10, Batch 266/462, Loss: 0.6542766690254211\n",
      "Epoch 10, Batch 267/462, Loss: 0.9166885614395142\n",
      "Epoch 10, Batch 268/462, Loss: 0.7868450284004211\n",
      "Epoch 10, Batch 269/462, Loss: 0.8023427724838257\n",
      "Epoch 10, Batch 270/462, Loss: 0.7971537113189697\n",
      "Epoch 10, Batch 271/462, Loss: 0.784635603427887\n",
      "Epoch 10, Batch 272/462, Loss: 0.7799469232559204\n",
      "Epoch 10, Batch 273/462, Loss: 0.7160377502441406\n",
      "Epoch 10, Batch 274/462, Loss: 0.7812418341636658\n",
      "Epoch 10, Batch 275/462, Loss: 0.8311446905136108\n",
      "Epoch 10, Batch 276/462, Loss: 0.775878369808197\n",
      "Epoch 10, Batch 277/462, Loss: 0.7511777877807617\n",
      "Epoch 10, Batch 278/462, Loss: 0.7795102000236511\n",
      "Epoch 10, Batch 279/462, Loss: 0.7285007238388062\n",
      "Epoch 10, Batch 280/462, Loss: 0.6759216785430908\n",
      "Epoch 10, Batch 281/462, Loss: 0.9281965494155884\n",
      "Epoch 10, Batch 282/462, Loss: 0.8574378490447998\n",
      "Epoch 10, Batch 283/462, Loss: 0.7304520010948181\n",
      "Epoch 10, Batch 284/462, Loss: 0.8079293370246887\n",
      "Epoch 10, Batch 285/462, Loss: 0.7933083176612854\n",
      "Epoch 10, Batch 286/462, Loss: 0.7401524782180786\n",
      "Epoch 10, Batch 287/462, Loss: 0.7981237173080444\n",
      "Epoch 10, Batch 288/462, Loss: 0.966222882270813\n",
      "Epoch 10, Batch 289/462, Loss: 0.8548232316970825\n",
      "Epoch 10, Batch 290/462, Loss: 0.6107529401779175\n",
      "Epoch 10, Batch 291/462, Loss: 0.8709821105003357\n",
      "Epoch 10, Batch 292/462, Loss: 0.7024869918823242\n",
      "Epoch 10, Batch 293/462, Loss: 0.8566607236862183\n",
      "Epoch 10, Batch 294/462, Loss: 0.7526097297668457\n",
      "Epoch 10, Batch 295/462, Loss: 0.7119184732437134\n",
      "Epoch 10, Batch 296/462, Loss: 0.8863956332206726\n",
      "Epoch 10, Batch 297/462, Loss: 0.8402953147888184\n",
      "Epoch 10, Batch 298/462, Loss: 0.9138309359550476\n",
      "Epoch 10, Batch 299/462, Loss: 0.6819514632225037\n",
      "Epoch 10, Batch 300/462, Loss: 0.739095151424408\n",
      "Epoch 10, Batch 301/462, Loss: 1.0375972986221313\n",
      "Epoch 10, Batch 302/462, Loss: 0.8517655730247498\n",
      "Epoch 10, Batch 303/462, Loss: 0.7420088648796082\n",
      "Epoch 10, Batch 304/462, Loss: 0.768640398979187\n",
      "Epoch 10, Batch 305/462, Loss: 0.8012268543243408\n",
      "Epoch 10, Batch 306/462, Loss: 0.726787805557251\n",
      "Epoch 10, Batch 307/462, Loss: 0.8623371124267578\n",
      "Epoch 10, Batch 308/462, Loss: 0.7241055965423584\n",
      "Epoch 10, Batch 309/462, Loss: 0.947541356086731\n",
      "Epoch 10, Batch 310/462, Loss: 0.9892020225524902\n",
      "Epoch 10, Batch 311/462, Loss: 0.9571300745010376\n",
      "Epoch 10, Batch 312/462, Loss: 0.7658780217170715\n",
      "Epoch 10, Batch 313/462, Loss: 0.7498431205749512\n",
      "Epoch 10, Batch 314/462, Loss: 0.8581857085227966\n",
      "Epoch 10, Batch 315/462, Loss: 0.8216837048530579\n",
      "Epoch 10, Batch 316/462, Loss: 0.8410881161689758\n",
      "Epoch 10, Batch 317/462, Loss: 0.8189870119094849\n",
      "Epoch 10, Batch 318/462, Loss: 0.8590561747550964\n",
      "Epoch 10, Batch 319/462, Loss: 0.8443769812583923\n",
      "Epoch 10, Batch 320/462, Loss: 0.8106573224067688\n",
      "Epoch 10, Batch 321/462, Loss: 0.910213053226471\n",
      "Epoch 10, Batch 322/462, Loss: 0.7001180648803711\n",
      "Epoch 10, Batch 323/462, Loss: 0.8073812127113342\n",
      "Epoch 10, Batch 324/462, Loss: 0.795207679271698\n",
      "Epoch 10, Batch 325/462, Loss: 0.9062299132347107\n",
      "Epoch 10, Batch 326/462, Loss: 0.893339991569519\n",
      "Epoch 10, Batch 327/462, Loss: 0.7666376829147339\n",
      "Epoch 10, Batch 328/462, Loss: 0.7885616421699524\n",
      "Epoch 10, Batch 329/462, Loss: 0.8488519191741943\n",
      "Epoch 10, Batch 330/462, Loss: 0.831874668598175\n",
      "Epoch 10, Batch 331/462, Loss: 0.7107619643211365\n",
      "Epoch 10, Batch 332/462, Loss: 0.7910189032554626\n",
      "Epoch 10, Batch 333/462, Loss: 0.8822705745697021\n",
      "Epoch 10, Batch 334/462, Loss: 0.812618613243103\n",
      "Epoch 10, Batch 335/462, Loss: 0.9130566120147705\n",
      "Epoch 10, Batch 336/462, Loss: 0.8857430815696716\n",
      "Epoch 10, Batch 337/462, Loss: 0.6911411881446838\n",
      "Epoch 10, Batch 338/462, Loss: 0.7955055236816406\n",
      "Epoch 10, Batch 339/462, Loss: 0.9080072045326233\n",
      "Epoch 10, Batch 340/462, Loss: 0.717117965221405\n",
      "Epoch 10, Batch 341/462, Loss: 0.9924371242523193\n",
      "Epoch 10, Batch 342/462, Loss: 0.8502048254013062\n",
      "Epoch 10, Batch 343/462, Loss: 0.8844760656356812\n",
      "Epoch 10, Batch 344/462, Loss: 0.7113231420516968\n",
      "Epoch 10, Batch 345/462, Loss: 0.7569421529769897\n",
      "Epoch 10, Batch 346/462, Loss: 0.7707535624504089\n",
      "Epoch 10, Batch 347/462, Loss: 0.8035281300544739\n",
      "Epoch 10, Batch 348/462, Loss: 0.7338749170303345\n",
      "Epoch 10, Batch 349/462, Loss: 1.0290594100952148\n",
      "Epoch 10, Batch 350/462, Loss: 0.8457423448562622\n",
      "Epoch 10, Batch 351/462, Loss: 0.8681415915489197\n",
      "Epoch 10, Batch 352/462, Loss: 0.7846225500106812\n",
      "Epoch 10, Batch 353/462, Loss: 0.894780695438385\n",
      "Epoch 10, Batch 354/462, Loss: 0.8052237033843994\n",
      "Epoch 10, Batch 355/462, Loss: 0.8756611943244934\n",
      "Epoch 10, Batch 356/462, Loss: 0.7298898100852966\n",
      "Epoch 10, Batch 357/462, Loss: 0.7973024249076843\n",
      "Epoch 10, Batch 358/462, Loss: 0.765114963054657\n",
      "Epoch 10, Batch 359/462, Loss: 0.9069066643714905\n",
      "Epoch 10, Batch 360/462, Loss: 0.7725855112075806\n",
      "Epoch 10, Batch 361/462, Loss: 0.7994356751441956\n",
      "Epoch 10, Batch 362/462, Loss: 0.6943524479866028\n",
      "Epoch 10, Batch 363/462, Loss: 0.810309112071991\n",
      "Epoch 10, Batch 364/462, Loss: 0.7657809853553772\n",
      "Epoch 10, Batch 365/462, Loss: 0.8436441421508789\n",
      "Epoch 10, Batch 366/462, Loss: 0.7563591003417969\n",
      "Epoch 10, Batch 367/462, Loss: 0.7948171496391296\n",
      "Epoch 10, Batch 368/462, Loss: 0.851493239402771\n",
      "Epoch 10, Batch 369/462, Loss: 0.6007798314094543\n",
      "Epoch 10, Batch 370/462, Loss: 0.8223013281822205\n",
      "Epoch 10, Batch 371/462, Loss: 0.7456226944923401\n",
      "Epoch 10, Batch 372/462, Loss: 0.7572677731513977\n",
      "Epoch 10, Batch 373/462, Loss: 0.7625397443771362\n",
      "Epoch 10, Batch 374/462, Loss: 0.7661562561988831\n",
      "Epoch 10, Batch 375/462, Loss: 0.9466195702552795\n",
      "Epoch 10, Batch 376/462, Loss: 0.840620219707489\n",
      "Epoch 10, Batch 377/462, Loss: 0.8860726952552795\n",
      "Epoch 10, Batch 378/462, Loss: 0.858471155166626\n",
      "Epoch 10, Batch 379/462, Loss: 0.7238436937332153\n",
      "Epoch 10, Batch 380/462, Loss: 0.8304478526115417\n",
      "Epoch 10, Batch 381/462, Loss: 0.7540888786315918\n",
      "Epoch 10, Batch 382/462, Loss: 0.748767614364624\n",
      "Epoch 10, Batch 383/462, Loss: 0.7366278767585754\n",
      "Epoch 10, Batch 384/462, Loss: 0.7663989067077637\n",
      "Epoch 10, Batch 385/462, Loss: 0.7676892280578613\n",
      "Epoch 10, Batch 386/462, Loss: 0.7608096599578857\n",
      "Epoch 10, Batch 387/462, Loss: 0.7153478264808655\n",
      "Epoch 10, Batch 388/462, Loss: 0.8737910389900208\n",
      "Epoch 10, Batch 389/462, Loss: 0.8756359815597534\n",
      "Epoch 10, Batch 390/462, Loss: 0.7556661367416382\n",
      "Epoch 10, Batch 391/462, Loss: 0.8969041109085083\n",
      "Epoch 10, Batch 392/462, Loss: 0.7614184021949768\n",
      "Epoch 10, Batch 393/462, Loss: 0.7601809501647949\n",
      "Epoch 10, Batch 394/462, Loss: 0.6508704423904419\n",
      "Epoch 10, Batch 395/462, Loss: 0.908221423625946\n",
      "Epoch 10, Batch 396/462, Loss: 0.7263811230659485\n",
      "Epoch 10, Batch 397/462, Loss: 0.7627565860748291\n",
      "Epoch 10, Batch 398/462, Loss: 0.7811691164970398\n",
      "Epoch 10, Batch 399/462, Loss: 0.7107721567153931\n",
      "Epoch 10, Batch 400/462, Loss: 0.6523800492286682\n",
      "Epoch 10, Batch 401/462, Loss: 0.8736940622329712\n",
      "Epoch 10, Batch 402/462, Loss: 0.8096408247947693\n",
      "Epoch 10, Batch 403/462, Loss: 0.8147932887077332\n",
      "Epoch 10, Batch 404/462, Loss: 0.7481600046157837\n",
      "Epoch 10, Batch 405/462, Loss: 0.7488493323326111\n",
      "Epoch 10, Batch 406/462, Loss: 0.8450968861579895\n",
      "Epoch 10, Batch 407/462, Loss: 0.8364471197128296\n",
      "Epoch 10, Batch 408/462, Loss: 0.8661274313926697\n",
      "Epoch 10, Batch 409/462, Loss: 0.7710953950881958\n",
      "Epoch 10, Batch 410/462, Loss: 0.8161510825157166\n",
      "Epoch 10, Batch 411/462, Loss: 0.7747130393981934\n",
      "Epoch 10, Batch 412/462, Loss: 0.7670705318450928\n",
      "Epoch 10, Batch 413/462, Loss: 0.7547823786735535\n",
      "Epoch 10, Batch 414/462, Loss: 0.80776047706604\n",
      "Epoch 10, Batch 415/462, Loss: 0.7656212449073792\n",
      "Epoch 10, Batch 416/462, Loss: 0.7115029692649841\n",
      "Epoch 10, Batch 417/462, Loss: 0.6947886943817139\n",
      "Epoch 10, Batch 418/462, Loss: 0.9056470394134521\n",
      "Epoch 10, Batch 419/462, Loss: 0.9361879229545593\n",
      "Epoch 10, Batch 420/462, Loss: 0.8064702749252319\n",
      "Epoch 10, Batch 421/462, Loss: 0.7756112813949585\n",
      "Epoch 10, Batch 422/462, Loss: 0.7451263070106506\n",
      "Epoch 10, Batch 423/462, Loss: 0.9181798696517944\n",
      "Epoch 10, Batch 424/462, Loss: 0.8523547649383545\n",
      "Epoch 10, Batch 425/462, Loss: 0.7847203016281128\n",
      "Epoch 10, Batch 426/462, Loss: 0.9209513068199158\n",
      "Epoch 10, Batch 427/462, Loss: 0.9318715929985046\n",
      "Epoch 10, Batch 428/462, Loss: 1.0642365217208862\n",
      "Epoch 10, Batch 429/462, Loss: 0.7460112571716309\n",
      "Epoch 10, Batch 430/462, Loss: 0.8474471569061279\n",
      "Epoch 10, Batch 431/462, Loss: 0.8603079915046692\n",
      "Epoch 10, Batch 432/462, Loss: 0.6291788816452026\n",
      "Epoch 10, Batch 433/462, Loss: 0.6835744380950928\n",
      "Epoch 10, Batch 434/462, Loss: 0.7895882725715637\n",
      "Epoch 10, Batch 435/462, Loss: 0.8548185229301453\n",
      "Epoch 10, Batch 436/462, Loss: 0.8789313435554504\n",
      "Epoch 10, Batch 437/462, Loss: 0.8172406554222107\n",
      "Epoch 10, Batch 438/462, Loss: 0.7505626082420349\n",
      "Epoch 10, Batch 439/462, Loss: 0.7934859395027161\n",
      "Epoch 10, Batch 440/462, Loss: 0.8283478617668152\n",
      "Epoch 10, Batch 441/462, Loss: 0.7681242227554321\n",
      "Epoch 10, Batch 442/462, Loss: 0.8574283719062805\n",
      "Epoch 10, Batch 443/462, Loss: 0.7925037145614624\n",
      "Epoch 10, Batch 444/462, Loss: 0.7819502949714661\n",
      "Epoch 10, Batch 445/462, Loss: 0.7409819960594177\n",
      "Epoch 10, Batch 446/462, Loss: 0.8360055088996887\n",
      "Epoch 10, Batch 447/462, Loss: 0.9070151448249817\n",
      "Epoch 10, Batch 448/462, Loss: 0.7335970997810364\n",
      "Epoch 10, Batch 449/462, Loss: 0.855995774269104\n",
      "Epoch 10, Batch 450/462, Loss: 0.8092485666275024\n",
      "Epoch 10, Batch 451/462, Loss: 0.7626052498817444\n",
      "Epoch 10, Batch 452/462, Loss: 0.8271742463111877\n",
      "Epoch 10, Batch 453/462, Loss: 0.7808040976524353\n",
      "Epoch 10, Batch 454/462, Loss: 0.6934719681739807\n",
      "Epoch 10, Batch 455/462, Loss: 0.8302752375602722\n",
      "Epoch 10, Batch 456/462, Loss: 0.7557955384254456\n",
      "Epoch 10, Batch 457/462, Loss: 0.8356920480728149\n",
      "Epoch 10, Batch 458/462, Loss: 0.6612305045127869\n",
      "Epoch 10, Batch 459/462, Loss: 0.7943681478500366\n",
      "Epoch 10, Batch 460/462, Loss: 0.9977268576622009\n",
      "Epoch 10, Batch 461/462, Loss: 0.9271868467330933\n",
      "Epoch 10, Batch 462/462, Loss: 0.844103217124939\n",
      "Epoch 10, Loss: 370.43884778022766\n",
      "Epoch 11, Batch 1/462, Loss: 0.7612773776054382\n",
      "Epoch 11, Batch 2/462, Loss: 0.8924171328544617\n",
      "Epoch 11, Batch 3/462, Loss: 0.8858205080032349\n",
      "Epoch 11, Batch 4/462, Loss: 0.7372678518295288\n",
      "Epoch 11, Batch 5/462, Loss: 0.8280336260795593\n",
      "Epoch 11, Batch 6/462, Loss: 0.7488937377929688\n",
      "Epoch 11, Batch 7/462, Loss: 0.8701615333557129\n",
      "Epoch 11, Batch 8/462, Loss: 0.8489776849746704\n",
      "Epoch 11, Batch 9/462, Loss: 0.7674518823623657\n",
      "Epoch 11, Batch 10/462, Loss: 0.8104771971702576\n",
      "Epoch 11, Batch 11/462, Loss: 0.8212476372718811\n",
      "Epoch 11, Batch 12/462, Loss: 0.8725287318229675\n",
      "Epoch 11, Batch 13/462, Loss: 0.8640414476394653\n",
      "Epoch 11, Batch 14/462, Loss: 0.8539325594902039\n",
      "Epoch 11, Batch 15/462, Loss: 0.706164538860321\n",
      "Epoch 11, Batch 16/462, Loss: 0.9343262314796448\n",
      "Epoch 11, Batch 17/462, Loss: 0.8426021933555603\n",
      "Epoch 11, Batch 18/462, Loss: 0.8245487809181213\n",
      "Epoch 11, Batch 19/462, Loss: 0.8169028162956238\n",
      "Epoch 11, Batch 20/462, Loss: 0.8431929349899292\n",
      "Epoch 11, Batch 21/462, Loss: 0.8664402365684509\n",
      "Epoch 11, Batch 22/462, Loss: 0.7812724709510803\n",
      "Epoch 11, Batch 23/462, Loss: 0.8587279915809631\n",
      "Epoch 11, Batch 24/462, Loss: 0.9397332668304443\n",
      "Epoch 11, Batch 25/462, Loss: 0.8528448343276978\n",
      "Epoch 11, Batch 26/462, Loss: 0.7488909959793091\n",
      "Epoch 11, Batch 27/462, Loss: 0.9626604318618774\n",
      "Epoch 11, Batch 28/462, Loss: 0.8548623323440552\n",
      "Epoch 11, Batch 29/462, Loss: 0.7264859080314636\n",
      "Epoch 11, Batch 30/462, Loss: 0.8124582171440125\n",
      "Epoch 11, Batch 31/462, Loss: 0.8628724813461304\n",
      "Epoch 11, Batch 32/462, Loss: 0.72700434923172\n",
      "Epoch 11, Batch 33/462, Loss: 0.8447937965393066\n",
      "Epoch 11, Batch 34/462, Loss: 0.8497747182846069\n",
      "Epoch 11, Batch 35/462, Loss: 0.8131722807884216\n",
      "Epoch 11, Batch 36/462, Loss: 0.8680247664451599\n",
      "Epoch 11, Batch 37/462, Loss: 0.9478212594985962\n",
      "Epoch 11, Batch 38/462, Loss: 0.8461672067642212\n",
      "Epoch 11, Batch 39/462, Loss: 0.8813612461090088\n",
      "Epoch 11, Batch 40/462, Loss: 0.8332114219665527\n",
      "Epoch 11, Batch 41/462, Loss: 0.7418786883354187\n",
      "Epoch 11, Batch 42/462, Loss: 0.8110758662223816\n",
      "Epoch 11, Batch 43/462, Loss: 0.961294412612915\n",
      "Epoch 11, Batch 44/462, Loss: 0.8644214868545532\n",
      "Epoch 11, Batch 45/462, Loss: 0.8373645544052124\n",
      "Epoch 11, Batch 46/462, Loss: 0.8122155666351318\n",
      "Epoch 11, Batch 47/462, Loss: 0.821144163608551\n",
      "Epoch 11, Batch 48/462, Loss: 0.8119578957557678\n",
      "Epoch 11, Batch 49/462, Loss: 0.8854436874389648\n",
      "Epoch 11, Batch 50/462, Loss: 0.7243495583534241\n",
      "Epoch 11, Batch 51/462, Loss: 0.8195247650146484\n",
      "Epoch 11, Batch 52/462, Loss: 0.7592734098434448\n",
      "Epoch 11, Batch 53/462, Loss: 0.8733287453651428\n",
      "Epoch 11, Batch 54/462, Loss: 0.7223748564720154\n",
      "Epoch 11, Batch 55/462, Loss: 0.7546440362930298\n",
      "Epoch 11, Batch 56/462, Loss: 0.6960693597793579\n",
      "Epoch 11, Batch 57/462, Loss: 0.9126641154289246\n",
      "Epoch 11, Batch 58/462, Loss: 0.5355812907218933\n",
      "Epoch 11, Batch 59/462, Loss: 0.7609978914260864\n",
      "Epoch 11, Batch 60/462, Loss: 0.7731693983078003\n",
      "Epoch 11, Batch 61/462, Loss: 1.0524650812149048\n",
      "Epoch 11, Batch 62/462, Loss: 0.6973208785057068\n",
      "Epoch 11, Batch 63/462, Loss: 0.9052680134773254\n",
      "Epoch 11, Batch 64/462, Loss: 1.001512885093689\n",
      "Epoch 11, Batch 65/462, Loss: 0.7924888730049133\n",
      "Epoch 11, Batch 66/462, Loss: 0.8718317151069641\n",
      "Epoch 11, Batch 67/462, Loss: 0.8214330673217773\n",
      "Epoch 11, Batch 68/462, Loss: 0.7672662138938904\n",
      "Epoch 11, Batch 69/462, Loss: 0.8002967238426208\n",
      "Epoch 11, Batch 70/462, Loss: 0.7283499836921692\n",
      "Epoch 11, Batch 71/462, Loss: 0.8150726556777954\n",
      "Epoch 11, Batch 72/462, Loss: 0.8880831599235535\n",
      "Epoch 11, Batch 73/462, Loss: 0.9829414486885071\n",
      "Epoch 11, Batch 74/462, Loss: 0.7659322619438171\n",
      "Epoch 11, Batch 75/462, Loss: 0.6987519860267639\n",
      "Epoch 11, Batch 76/462, Loss: 0.7998120784759521\n",
      "Epoch 11, Batch 77/462, Loss: 0.8541626334190369\n",
      "Epoch 11, Batch 78/462, Loss: 0.801220715045929\n",
      "Epoch 11, Batch 79/462, Loss: 0.7617045640945435\n",
      "Epoch 11, Batch 80/462, Loss: 0.8172172904014587\n",
      "Epoch 11, Batch 81/462, Loss: 0.6812279224395752\n",
      "Epoch 11, Batch 82/462, Loss: 0.7794639468193054\n",
      "Epoch 11, Batch 83/462, Loss: 0.832557737827301\n",
      "Epoch 11, Batch 84/462, Loss: 0.8638665080070496\n",
      "Epoch 11, Batch 85/462, Loss: 0.8216302990913391\n",
      "Epoch 11, Batch 86/462, Loss: 0.6802088022232056\n",
      "Epoch 11, Batch 87/462, Loss: 0.8335387706756592\n",
      "Epoch 11, Batch 88/462, Loss: 0.8246170878410339\n",
      "Epoch 11, Batch 89/462, Loss: 0.8428117632865906\n",
      "Epoch 11, Batch 90/462, Loss: 0.7848921418190002\n",
      "Epoch 11, Batch 91/462, Loss: 1.023327350616455\n",
      "Epoch 11, Batch 92/462, Loss: 0.9833493828773499\n",
      "Epoch 11, Batch 93/462, Loss: 0.7324972748756409\n",
      "Epoch 11, Batch 94/462, Loss: 0.7495852708816528\n",
      "Epoch 11, Batch 95/462, Loss: 0.8528748154640198\n",
      "Epoch 11, Batch 96/462, Loss: 0.9696146845817566\n",
      "Epoch 11, Batch 97/462, Loss: 0.7603514194488525\n",
      "Epoch 11, Batch 98/462, Loss: 0.678997814655304\n",
      "Epoch 11, Batch 99/462, Loss: 0.7759082317352295\n",
      "Epoch 11, Batch 100/462, Loss: 0.761891782283783\n",
      "Epoch 11, Batch 101/462, Loss: 0.7328534722328186\n",
      "Epoch 11, Batch 102/462, Loss: 0.9131197333335876\n",
      "Epoch 11, Batch 103/462, Loss: 0.9195958375930786\n",
      "Epoch 11, Batch 104/462, Loss: 0.8002663254737854\n",
      "Epoch 11, Batch 105/462, Loss: 0.9029549360275269\n",
      "Epoch 11, Batch 106/462, Loss: 0.7837355732917786\n",
      "Epoch 11, Batch 107/462, Loss: 0.8372203707695007\n",
      "Epoch 11, Batch 108/462, Loss: 0.7851758003234863\n",
      "Epoch 11, Batch 109/462, Loss: 0.7579970955848694\n",
      "Epoch 11, Batch 110/462, Loss: 0.8169618844985962\n",
      "Epoch 11, Batch 111/462, Loss: 0.9161198139190674\n",
      "Epoch 11, Batch 112/462, Loss: 0.8194737434387207\n",
      "Epoch 11, Batch 113/462, Loss: 0.7165432572364807\n",
      "Epoch 11, Batch 114/462, Loss: 0.7030104398727417\n",
      "Epoch 11, Batch 115/462, Loss: 0.773307204246521\n",
      "Epoch 11, Batch 116/462, Loss: 0.8663957715034485\n",
      "Epoch 11, Batch 117/462, Loss: 0.7938188910484314\n",
      "Epoch 11, Batch 118/462, Loss: 0.8922165036201477\n",
      "Epoch 11, Batch 119/462, Loss: 0.8951233625411987\n",
      "Epoch 11, Batch 120/462, Loss: 0.7068922519683838\n",
      "Epoch 11, Batch 121/462, Loss: 0.8167396783828735\n",
      "Epoch 11, Batch 122/462, Loss: 0.7514092326164246\n",
      "Epoch 11, Batch 123/462, Loss: 0.5801397562026978\n",
      "Epoch 11, Batch 124/462, Loss: 0.6873246431350708\n",
      "Epoch 11, Batch 125/462, Loss: 0.7728620171546936\n",
      "Epoch 11, Batch 126/462, Loss: 0.9464924931526184\n",
      "Epoch 11, Batch 127/462, Loss: 0.7277924418449402\n",
      "Epoch 11, Batch 128/462, Loss: 0.7555291652679443\n",
      "Epoch 11, Batch 129/462, Loss: 0.8117752075195312\n",
      "Epoch 11, Batch 130/462, Loss: 0.8713539838790894\n",
      "Epoch 11, Batch 131/462, Loss: 0.7590456008911133\n",
      "Epoch 11, Batch 132/462, Loss: 0.7984191179275513\n",
      "Epoch 11, Batch 133/462, Loss: 0.8772192001342773\n",
      "Epoch 11, Batch 134/462, Loss: 0.7426803112030029\n",
      "Epoch 11, Batch 135/462, Loss: 0.7120338678359985\n",
      "Epoch 11, Batch 136/462, Loss: 0.922969400882721\n",
      "Epoch 11, Batch 137/462, Loss: 0.8291181921958923\n",
      "Epoch 11, Batch 138/462, Loss: 0.827653706073761\n",
      "Epoch 11, Batch 139/462, Loss: 0.7039957642555237\n",
      "Epoch 11, Batch 140/462, Loss: 0.8905748724937439\n",
      "Epoch 11, Batch 141/462, Loss: 0.7591307759284973\n",
      "Epoch 11, Batch 142/462, Loss: 0.7710023522377014\n",
      "Epoch 11, Batch 143/462, Loss: 0.6710953116416931\n",
      "Epoch 11, Batch 144/462, Loss: 0.7247752547264099\n",
      "Epoch 11, Batch 145/462, Loss: 0.6630423665046692\n",
      "Epoch 11, Batch 146/462, Loss: 0.8333326578140259\n",
      "Epoch 11, Batch 147/462, Loss: 0.7336567640304565\n",
      "Epoch 11, Batch 148/462, Loss: 0.6468152403831482\n",
      "Epoch 11, Batch 149/462, Loss: 0.6087276339530945\n",
      "Epoch 11, Batch 150/462, Loss: 0.6829816699028015\n",
      "Epoch 11, Batch 151/462, Loss: 0.9136327505111694\n",
      "Epoch 11, Batch 152/462, Loss: 0.6725612878799438\n",
      "Epoch 11, Batch 153/462, Loss: 0.7289892435073853\n",
      "Epoch 11, Batch 154/462, Loss: 0.8659542202949524\n",
      "Epoch 11, Batch 155/462, Loss: 0.7944552898406982\n",
      "Epoch 11, Batch 156/462, Loss: 0.7465144991874695\n",
      "Epoch 11, Batch 157/462, Loss: 0.673207700252533\n",
      "Epoch 11, Batch 158/462, Loss: 0.6544549465179443\n",
      "Epoch 11, Batch 159/462, Loss: 0.633263111114502\n",
      "Epoch 11, Batch 160/462, Loss: 0.7180548310279846\n",
      "Epoch 11, Batch 161/462, Loss: 0.6429739594459534\n",
      "Epoch 11, Batch 162/462, Loss: 0.7514389157295227\n",
      "Epoch 11, Batch 163/462, Loss: 0.7095423936843872\n",
      "Epoch 11, Batch 164/462, Loss: 0.8362358212471008\n",
      "Epoch 11, Batch 165/462, Loss: 0.8477733731269836\n",
      "Epoch 11, Batch 166/462, Loss: 0.832648515701294\n",
      "Epoch 11, Batch 167/462, Loss: 0.710580587387085\n",
      "Epoch 11, Batch 168/462, Loss: 0.8158361911773682\n",
      "Epoch 11, Batch 169/462, Loss: 0.8079346418380737\n",
      "Epoch 11, Batch 170/462, Loss: 0.8175212740898132\n",
      "Epoch 11, Batch 171/462, Loss: 0.6617117524147034\n",
      "Epoch 11, Batch 172/462, Loss: 0.7820321917533875\n",
      "Epoch 11, Batch 173/462, Loss: 0.6666666269302368\n",
      "Epoch 11, Batch 174/462, Loss: 0.926442563533783\n",
      "Epoch 11, Batch 175/462, Loss: 0.8888282775878906\n",
      "Epoch 11, Batch 176/462, Loss: 0.8498362302780151\n",
      "Epoch 11, Batch 177/462, Loss: 0.9262104034423828\n",
      "Epoch 11, Batch 178/462, Loss: 0.8150850534439087\n",
      "Epoch 11, Batch 179/462, Loss: 0.7929370403289795\n",
      "Epoch 11, Batch 180/462, Loss: 0.7606377601623535\n",
      "Epoch 11, Batch 181/462, Loss: 0.7622094750404358\n",
      "Epoch 11, Batch 182/462, Loss: 0.7821322083473206\n",
      "Epoch 11, Batch 183/462, Loss: 0.743026614189148\n",
      "Epoch 11, Batch 184/462, Loss: 0.8861663937568665\n",
      "Epoch 11, Batch 185/462, Loss: 0.7223485708236694\n",
      "Epoch 11, Batch 186/462, Loss: 0.8061379790306091\n",
      "Epoch 11, Batch 187/462, Loss: 0.9028120636940002\n",
      "Epoch 11, Batch 188/462, Loss: 0.8141114711761475\n",
      "Epoch 11, Batch 189/462, Loss: 1.0227912664413452\n",
      "Epoch 11, Batch 190/462, Loss: 0.7954291105270386\n",
      "Epoch 11, Batch 191/462, Loss: 0.7480120658874512\n",
      "Epoch 11, Batch 192/462, Loss: 0.8772531747817993\n",
      "Epoch 11, Batch 193/462, Loss: 0.5617595314979553\n",
      "Epoch 11, Batch 194/462, Loss: 0.6653432250022888\n",
      "Epoch 11, Batch 195/462, Loss: 0.6980021595954895\n",
      "Epoch 11, Batch 196/462, Loss: 0.8445085287094116\n",
      "Epoch 11, Batch 197/462, Loss: 0.9130277037620544\n",
      "Epoch 11, Batch 198/462, Loss: 0.8188853859901428\n",
      "Epoch 11, Batch 199/462, Loss: 0.7494389414787292\n",
      "Epoch 11, Batch 200/462, Loss: 0.7360464334487915\n",
      "Epoch 11, Batch 201/462, Loss: 0.8667587637901306\n",
      "Epoch 11, Batch 202/462, Loss: 0.8546757698059082\n",
      "Epoch 11, Batch 203/462, Loss: 0.7018040418624878\n",
      "Epoch 11, Batch 204/462, Loss: 0.977565348148346\n",
      "Epoch 11, Batch 205/462, Loss: 0.7884358167648315\n",
      "Epoch 11, Batch 206/462, Loss: 0.69781893491745\n",
      "Epoch 11, Batch 207/462, Loss: 0.6945398449897766\n",
      "Epoch 11, Batch 208/462, Loss: 0.9433221817016602\n",
      "Epoch 11, Batch 209/462, Loss: 0.8806619048118591\n",
      "Epoch 11, Batch 210/462, Loss: 0.7739366292953491\n",
      "Epoch 11, Batch 211/462, Loss: 0.8640865087509155\n",
      "Epoch 11, Batch 212/462, Loss: 0.8368078470230103\n",
      "Epoch 11, Batch 213/462, Loss: 0.734598696231842\n",
      "Epoch 11, Batch 214/462, Loss: 0.6381962895393372\n",
      "Epoch 11, Batch 215/462, Loss: 0.8077752590179443\n",
      "Epoch 11, Batch 216/462, Loss: 0.7401027083396912\n",
      "Epoch 11, Batch 217/462, Loss: 0.7168762683868408\n",
      "Epoch 11, Batch 218/462, Loss: 0.7264968752861023\n",
      "Epoch 11, Batch 219/462, Loss: 0.7795439958572388\n",
      "Epoch 11, Batch 220/462, Loss: 0.9151853322982788\n",
      "Epoch 11, Batch 221/462, Loss: 0.7773608565330505\n",
      "Epoch 11, Batch 222/462, Loss: 0.8217960000038147\n",
      "Epoch 11, Batch 223/462, Loss: 0.8939056396484375\n",
      "Epoch 11, Batch 224/462, Loss: 0.7525240778923035\n",
      "Epoch 11, Batch 225/462, Loss: 0.7218060493469238\n",
      "Epoch 11, Batch 226/462, Loss: 0.8578378558158875\n",
      "Epoch 11, Batch 227/462, Loss: 0.7841640114784241\n",
      "Epoch 11, Batch 228/462, Loss: 0.9179864525794983\n",
      "Epoch 11, Batch 229/462, Loss: 0.7054855823516846\n",
      "Epoch 11, Batch 230/462, Loss: 0.7920632362365723\n",
      "Epoch 11, Batch 231/462, Loss: 0.6560409069061279\n",
      "Epoch 11, Batch 232/462, Loss: 0.773057758808136\n",
      "Epoch 11, Batch 233/462, Loss: 0.8095476627349854\n",
      "Epoch 11, Batch 234/462, Loss: 0.7597535848617554\n",
      "Epoch 11, Batch 235/462, Loss: 0.8620424270629883\n",
      "Epoch 11, Batch 236/462, Loss: 0.9833574295043945\n",
      "Epoch 11, Batch 237/462, Loss: 0.7920384407043457\n",
      "Epoch 11, Batch 238/462, Loss: 0.7859704494476318\n",
      "Epoch 11, Batch 239/462, Loss: 0.7079241275787354\n",
      "Epoch 11, Batch 240/462, Loss: 0.8154696226119995\n",
      "Epoch 11, Batch 241/462, Loss: 0.8335026502609253\n",
      "Epoch 11, Batch 242/462, Loss: 0.8028994798660278\n",
      "Epoch 11, Batch 243/462, Loss: 0.8349672555923462\n",
      "Epoch 11, Batch 244/462, Loss: 0.9046961069107056\n",
      "Epoch 11, Batch 245/462, Loss: 0.7917937636375427\n",
      "Epoch 11, Batch 246/462, Loss: 0.8985071182250977\n",
      "Epoch 11, Batch 247/462, Loss: 0.7342836856842041\n",
      "Epoch 11, Batch 248/462, Loss: 0.9352172017097473\n",
      "Epoch 11, Batch 249/462, Loss: 0.7577176690101624\n",
      "Epoch 11, Batch 250/462, Loss: 0.7789380550384521\n",
      "Epoch 11, Batch 251/462, Loss: 0.8782802224159241\n",
      "Epoch 11, Batch 252/462, Loss: 0.7554602026939392\n",
      "Epoch 11, Batch 253/462, Loss: 0.7975540161132812\n",
      "Epoch 11, Batch 254/462, Loss: 0.6259135007858276\n",
      "Epoch 11, Batch 255/462, Loss: 0.8038362264633179\n",
      "Epoch 11, Batch 256/462, Loss: 0.9027705788612366\n",
      "Epoch 11, Batch 257/462, Loss: 0.8448331356048584\n",
      "Epoch 11, Batch 258/462, Loss: 0.834446907043457\n",
      "Epoch 11, Batch 259/462, Loss: 0.9863544702529907\n",
      "Epoch 11, Batch 260/462, Loss: 0.8123790621757507\n",
      "Epoch 11, Batch 261/462, Loss: 0.6828262209892273\n",
      "Epoch 11, Batch 262/462, Loss: 0.6157652139663696\n",
      "Epoch 11, Batch 263/462, Loss: 0.9378710389137268\n",
      "Epoch 11, Batch 264/462, Loss: 0.7597297430038452\n",
      "Epoch 11, Batch 265/462, Loss: 0.8266602754592896\n",
      "Epoch 11, Batch 266/462, Loss: 0.7957271337509155\n",
      "Epoch 11, Batch 267/462, Loss: 0.7757920622825623\n",
      "Epoch 11, Batch 268/462, Loss: 0.804401695728302\n",
      "Epoch 11, Batch 269/462, Loss: 0.6914189457893372\n",
      "Epoch 11, Batch 270/462, Loss: 0.8422712087631226\n",
      "Epoch 11, Batch 271/462, Loss: 0.710667073726654\n",
      "Epoch 11, Batch 272/462, Loss: 0.6832163333892822\n",
      "Epoch 11, Batch 273/462, Loss: 0.9583743810653687\n",
      "Epoch 11, Batch 274/462, Loss: 0.8358638882637024\n",
      "Epoch 11, Batch 275/462, Loss: 0.776697039604187\n",
      "Epoch 11, Batch 276/462, Loss: 0.8939247131347656\n",
      "Epoch 11, Batch 277/462, Loss: 0.7357863783836365\n",
      "Epoch 11, Batch 278/462, Loss: 0.7509670257568359\n",
      "Epoch 11, Batch 279/462, Loss: 0.8949145674705505\n",
      "Epoch 11, Batch 280/462, Loss: 0.8688456416130066\n",
      "Epoch 11, Batch 281/462, Loss: 0.8047556281089783\n",
      "Epoch 11, Batch 282/462, Loss: 0.8604854345321655\n",
      "Epoch 11, Batch 283/462, Loss: 0.8651395440101624\n",
      "Epoch 11, Batch 284/462, Loss: 0.8057281970977783\n",
      "Epoch 11, Batch 285/462, Loss: 0.717905580997467\n",
      "Epoch 11, Batch 286/462, Loss: 0.6800190210342407\n",
      "Epoch 11, Batch 287/462, Loss: 0.7865535616874695\n",
      "Epoch 11, Batch 288/462, Loss: 0.7673861384391785\n",
      "Epoch 11, Batch 289/462, Loss: 0.7936720252037048\n",
      "Epoch 11, Batch 290/462, Loss: 0.697032630443573\n",
      "Epoch 11, Batch 291/462, Loss: 0.7496315836906433\n",
      "Epoch 11, Batch 292/462, Loss: 0.8373186588287354\n",
      "Epoch 11, Batch 293/462, Loss: 0.6760051250457764\n",
      "Epoch 11, Batch 294/462, Loss: 0.8782294988632202\n",
      "Epoch 11, Batch 295/462, Loss: 0.7743451595306396\n",
      "Epoch 11, Batch 296/462, Loss: 0.8771721124649048\n",
      "Epoch 11, Batch 297/462, Loss: 0.7891706824302673\n",
      "Epoch 11, Batch 298/462, Loss: 0.7828200459480286\n",
      "Epoch 11, Batch 299/462, Loss: 0.9387636780738831\n",
      "Epoch 11, Batch 300/462, Loss: 0.70795077085495\n",
      "Epoch 11, Batch 301/462, Loss: 0.701284646987915\n",
      "Epoch 11, Batch 302/462, Loss: 0.7869886159896851\n",
      "Epoch 11, Batch 303/462, Loss: 1.0762662887573242\n",
      "Epoch 11, Batch 304/462, Loss: 0.8630595803260803\n",
      "Epoch 11, Batch 305/462, Loss: 0.7913180589675903\n",
      "Epoch 11, Batch 306/462, Loss: 0.6799803376197815\n",
      "Epoch 11, Batch 307/462, Loss: 0.6658780574798584\n",
      "Epoch 11, Batch 308/462, Loss: 0.7339646816253662\n",
      "Epoch 11, Batch 309/462, Loss: 0.818243682384491\n",
      "Epoch 11, Batch 310/462, Loss: 0.7026446461677551\n",
      "Epoch 11, Batch 311/462, Loss: 0.707353413105011\n",
      "Epoch 11, Batch 312/462, Loss: 0.8541760444641113\n",
      "Epoch 11, Batch 313/462, Loss: 0.6656696796417236\n",
      "Epoch 11, Batch 314/462, Loss: 0.7171005010604858\n",
      "Epoch 11, Batch 315/462, Loss: 0.72517329454422\n",
      "Epoch 11, Batch 316/462, Loss: 0.7467754483222961\n",
      "Epoch 11, Batch 317/462, Loss: 0.8042269349098206\n",
      "Epoch 11, Batch 318/462, Loss: 0.6754488945007324\n",
      "Epoch 11, Batch 319/462, Loss: 0.8109069466590881\n",
      "Epoch 11, Batch 320/462, Loss: 0.8528201580047607\n",
      "Epoch 11, Batch 321/462, Loss: 0.7539257407188416\n",
      "Epoch 11, Batch 322/462, Loss: 0.7161325812339783\n",
      "Epoch 11, Batch 323/462, Loss: 0.955658495426178\n",
      "Epoch 11, Batch 324/462, Loss: 0.9363027215003967\n",
      "Epoch 11, Batch 325/462, Loss: 0.838910698890686\n",
      "Epoch 11, Batch 326/462, Loss: 0.9496844410896301\n",
      "Epoch 11, Batch 327/462, Loss: 0.5932647585868835\n",
      "Epoch 11, Batch 328/462, Loss: 0.6403672695159912\n",
      "Epoch 11, Batch 329/462, Loss: 0.7873642444610596\n",
      "Epoch 11, Batch 330/462, Loss: 0.7075204849243164\n",
      "Epoch 11, Batch 331/462, Loss: 0.8859124183654785\n",
      "Epoch 11, Batch 332/462, Loss: 0.8406436443328857\n",
      "Epoch 11, Batch 333/462, Loss: 0.8362410068511963\n",
      "Epoch 11, Batch 334/462, Loss: 0.784961462020874\n",
      "Epoch 11, Batch 335/462, Loss: 0.7331641912460327\n",
      "Epoch 11, Batch 336/462, Loss: 0.8160721063613892\n",
      "Epoch 11, Batch 337/462, Loss: 0.8670313954353333\n",
      "Epoch 11, Batch 338/462, Loss: 0.5613507032394409\n",
      "Epoch 11, Batch 339/462, Loss: 0.8378819823265076\n",
      "Epoch 11, Batch 340/462, Loss: 0.9072747230529785\n",
      "Epoch 11, Batch 341/462, Loss: 0.8094802498817444\n",
      "Epoch 11, Batch 342/462, Loss: 0.8132699131965637\n",
      "Epoch 11, Batch 343/462, Loss: 0.7069104909896851\n",
      "Epoch 11, Batch 344/462, Loss: 0.7450659275054932\n",
      "Epoch 11, Batch 345/462, Loss: 0.815162181854248\n",
      "Epoch 11, Batch 346/462, Loss: 0.8637590408325195\n",
      "Epoch 11, Batch 347/462, Loss: 0.7069393396377563\n",
      "Epoch 11, Batch 348/462, Loss: 0.7199495434761047\n",
      "Epoch 11, Batch 349/462, Loss: 0.7797626852989197\n",
      "Epoch 11, Batch 350/462, Loss: 0.9160626530647278\n",
      "Epoch 11, Batch 351/462, Loss: 0.6958674788475037\n",
      "Epoch 11, Batch 352/462, Loss: 0.79766845703125\n",
      "Epoch 11, Batch 353/462, Loss: 0.8307077884674072\n",
      "Epoch 11, Batch 354/462, Loss: 0.7262380719184875\n",
      "Epoch 11, Batch 355/462, Loss: 0.9406508207321167\n",
      "Epoch 11, Batch 356/462, Loss: 0.8507994413375854\n",
      "Epoch 11, Batch 357/462, Loss: 0.7984099388122559\n",
      "Epoch 11, Batch 358/462, Loss: 0.7356021404266357\n",
      "Epoch 11, Batch 359/462, Loss: 0.7897521257400513\n",
      "Epoch 11, Batch 360/462, Loss: 0.7156304717063904\n",
      "Epoch 11, Batch 361/462, Loss: 0.8783499598503113\n",
      "Epoch 11, Batch 362/462, Loss: 0.7831972241401672\n",
      "Epoch 11, Batch 363/462, Loss: 0.889952540397644\n",
      "Epoch 11, Batch 364/462, Loss: 0.7162808775901794\n",
      "Epoch 11, Batch 365/462, Loss: 0.9924265742301941\n",
      "Epoch 11, Batch 366/462, Loss: 0.9046455025672913\n",
      "Epoch 11, Batch 367/462, Loss: 0.7268369793891907\n",
      "Epoch 11, Batch 368/462, Loss: 0.8704632520675659\n",
      "Epoch 11, Batch 369/462, Loss: 0.8732234239578247\n",
      "Epoch 11, Batch 370/462, Loss: 0.6653298139572144\n",
      "Epoch 11, Batch 371/462, Loss: 0.7777928709983826\n",
      "Epoch 11, Batch 372/462, Loss: 0.7314929366111755\n",
      "Epoch 11, Batch 373/462, Loss: 0.7181792855262756\n",
      "Epoch 11, Batch 374/462, Loss: 0.7050173878669739\n",
      "Epoch 11, Batch 375/462, Loss: 0.728168785572052\n",
      "Epoch 11, Batch 376/462, Loss: 0.9344530701637268\n",
      "Epoch 11, Batch 377/462, Loss: 0.7024164795875549\n",
      "Epoch 11, Batch 378/462, Loss: 0.8960095047950745\n",
      "Epoch 11, Batch 379/462, Loss: 0.6851112842559814\n",
      "Epoch 11, Batch 380/462, Loss: 0.657674252986908\n",
      "Epoch 11, Batch 381/462, Loss: 0.7343756556510925\n",
      "Epoch 11, Batch 382/462, Loss: 0.6453386545181274\n",
      "Epoch 11, Batch 383/462, Loss: 0.8225763440132141\n",
      "Epoch 11, Batch 384/462, Loss: 0.8077736496925354\n",
      "Epoch 11, Batch 385/462, Loss: 0.7641839981079102\n",
      "Epoch 11, Batch 386/462, Loss: 0.8327792882919312\n",
      "Epoch 11, Batch 387/462, Loss: 0.7856109142303467\n",
      "Epoch 11, Batch 388/462, Loss: 0.7218957543373108\n",
      "Epoch 11, Batch 389/462, Loss: 0.7514182925224304\n",
      "Epoch 11, Batch 390/462, Loss: 0.8566612601280212\n",
      "Epoch 11, Batch 391/462, Loss: 0.7333461046218872\n",
      "Epoch 11, Batch 392/462, Loss: 0.899020254611969\n",
      "Epoch 11, Batch 393/462, Loss: 0.8025009632110596\n",
      "Epoch 11, Batch 394/462, Loss: 0.9131606221199036\n",
      "Epoch 11, Batch 395/462, Loss: 0.8552667498588562\n",
      "Epoch 11, Batch 396/462, Loss: 0.796884298324585\n",
      "Epoch 11, Batch 397/462, Loss: 0.8197550773620605\n",
      "Epoch 11, Batch 398/462, Loss: 0.7098279595375061\n",
      "Epoch 11, Batch 399/462, Loss: 0.7547059059143066\n",
      "Epoch 11, Batch 400/462, Loss: 0.823607861995697\n",
      "Epoch 11, Batch 401/462, Loss: 0.7875508666038513\n",
      "Epoch 11, Batch 402/462, Loss: 0.7716542482376099\n",
      "Epoch 11, Batch 403/462, Loss: 0.7930026054382324\n",
      "Epoch 11, Batch 404/462, Loss: 0.6879043579101562\n",
      "Epoch 11, Batch 405/462, Loss: 0.7615340352058411\n",
      "Epoch 11, Batch 406/462, Loss: 0.7307251691818237\n",
      "Epoch 11, Batch 407/462, Loss: 0.6694005131721497\n",
      "Epoch 11, Batch 408/462, Loss: 0.7657631039619446\n",
      "Epoch 11, Batch 409/462, Loss: 0.8054049015045166\n",
      "Epoch 11, Batch 410/462, Loss: 0.6459264159202576\n",
      "Epoch 11, Batch 411/462, Loss: 0.6567888259887695\n",
      "Epoch 11, Batch 412/462, Loss: 0.8095462918281555\n",
      "Epoch 11, Batch 413/462, Loss: 0.798100471496582\n",
      "Epoch 11, Batch 414/462, Loss: 0.8500984907150269\n",
      "Epoch 11, Batch 415/462, Loss: 0.8376753330230713\n",
      "Epoch 11, Batch 416/462, Loss: 0.7283762693405151\n",
      "Epoch 11, Batch 417/462, Loss: 0.8568807244300842\n",
      "Epoch 11, Batch 418/462, Loss: 0.6876225471496582\n",
      "Epoch 11, Batch 419/462, Loss: 0.7342046499252319\n",
      "Epoch 11, Batch 420/462, Loss: 0.772637128829956\n",
      "Epoch 11, Batch 421/462, Loss: 0.7634245157241821\n",
      "Epoch 11, Batch 422/462, Loss: 0.7089739441871643\n",
      "Epoch 11, Batch 423/462, Loss: 0.6093826293945312\n",
      "Epoch 11, Batch 424/462, Loss: 0.5417126417160034\n",
      "Epoch 11, Batch 425/462, Loss: 0.8580270409584045\n",
      "Epoch 11, Batch 426/462, Loss: 0.9171483516693115\n",
      "Epoch 11, Batch 427/462, Loss: 0.8082883358001709\n",
      "Epoch 11, Batch 428/462, Loss: 0.8171088695526123\n",
      "Epoch 11, Batch 429/462, Loss: 0.8436696529388428\n",
      "Epoch 11, Batch 430/462, Loss: 0.7416043281555176\n",
      "Epoch 11, Batch 431/462, Loss: 0.8125016689300537\n",
      "Epoch 11, Batch 432/462, Loss: 0.6463191509246826\n",
      "Epoch 11, Batch 433/462, Loss: 0.7464812397956848\n",
      "Epoch 11, Batch 434/462, Loss: 0.6818047165870667\n",
      "Epoch 11, Batch 435/462, Loss: 0.5206124186515808\n",
      "Epoch 11, Batch 436/462, Loss: 0.7345166206359863\n",
      "Epoch 11, Batch 437/462, Loss: 0.7195079922676086\n",
      "Epoch 11, Batch 438/462, Loss: 0.8802688121795654\n",
      "Epoch 11, Batch 439/462, Loss: 0.7446927428245544\n",
      "Epoch 11, Batch 440/462, Loss: 0.838320255279541\n",
      "Epoch 11, Batch 441/462, Loss: 0.680555522441864\n",
      "Epoch 11, Batch 442/462, Loss: 0.7357316613197327\n",
      "Epoch 11, Batch 443/462, Loss: 0.8230428099632263\n",
      "Epoch 11, Batch 444/462, Loss: 0.6438449025154114\n",
      "Epoch 11, Batch 445/462, Loss: 0.8531075716018677\n",
      "Epoch 11, Batch 446/462, Loss: 0.7159647345542908\n",
      "Epoch 11, Batch 447/462, Loss: 0.973200261592865\n",
      "Epoch 11, Batch 448/462, Loss: 0.8623612523078918\n",
      "Epoch 11, Batch 449/462, Loss: 0.8984249234199524\n",
      "Epoch 11, Batch 450/462, Loss: 0.8704043030738831\n",
      "Epoch 11, Batch 451/462, Loss: 0.7752125859260559\n",
      "Epoch 11, Batch 452/462, Loss: 0.8466892838478088\n",
      "Epoch 11, Batch 453/462, Loss: 0.8663730621337891\n",
      "Epoch 11, Batch 454/462, Loss: 0.8098397254943848\n",
      "Epoch 11, Batch 455/462, Loss: 0.9641326665878296\n",
      "Epoch 11, Batch 456/462, Loss: 0.8120608329772949\n",
      "Epoch 11, Batch 457/462, Loss: 0.713752269744873\n",
      "Epoch 11, Batch 458/462, Loss: 0.8770344257354736\n",
      "Epoch 11, Batch 459/462, Loss: 0.7712298035621643\n",
      "Epoch 11, Batch 460/462, Loss: 0.7327755689620972\n",
      "Epoch 11, Batch 461/462, Loss: 0.6753596663475037\n",
      "Epoch 11, Batch 462/462, Loss: 0.7443507313728333\n",
      "Epoch 11, Loss: 366.9902752637863\n",
      "Epoch 12, Batch 1/462, Loss: 0.8043797612190247\n",
      "Epoch 12, Batch 2/462, Loss: 0.6750009655952454\n",
      "Epoch 12, Batch 3/462, Loss: 0.8721257448196411\n",
      "Epoch 12, Batch 4/462, Loss: 0.6037497520446777\n",
      "Epoch 12, Batch 5/462, Loss: 0.7703025937080383\n",
      "Epoch 12, Batch 6/462, Loss: 0.9919182062149048\n",
      "Epoch 12, Batch 7/462, Loss: 0.9036890864372253\n",
      "Epoch 12, Batch 8/462, Loss: 1.0317156314849854\n",
      "Epoch 12, Batch 9/462, Loss: 0.787338376045227\n",
      "Epoch 12, Batch 10/462, Loss: 0.7652605175971985\n",
      "Epoch 12, Batch 11/462, Loss: 0.6984269618988037\n",
      "Epoch 12, Batch 12/462, Loss: 0.7261229157447815\n",
      "Epoch 12, Batch 13/462, Loss: 0.7253326773643494\n",
      "Epoch 12, Batch 14/462, Loss: 0.6577618718147278\n",
      "Epoch 12, Batch 15/462, Loss: 0.8378974795341492\n",
      "Epoch 12, Batch 16/462, Loss: 0.7676168084144592\n",
      "Epoch 12, Batch 17/462, Loss: 0.7340061068534851\n",
      "Epoch 12, Batch 18/462, Loss: 0.8469932675361633\n",
      "Epoch 12, Batch 19/462, Loss: 0.6022850275039673\n",
      "Epoch 12, Batch 20/462, Loss: 0.8625220060348511\n",
      "Epoch 12, Batch 21/462, Loss: 0.776141881942749\n",
      "Epoch 12, Batch 22/462, Loss: 0.7647993564605713\n",
      "Epoch 12, Batch 23/462, Loss: 0.7727267146110535\n",
      "Epoch 12, Batch 24/462, Loss: 0.7808685898780823\n",
      "Epoch 12, Batch 25/462, Loss: 0.8928207159042358\n",
      "Epoch 12, Batch 26/462, Loss: 0.8617780208587646\n",
      "Epoch 12, Batch 27/462, Loss: 0.811801016330719\n",
      "Epoch 12, Batch 28/462, Loss: 0.7222884297370911\n",
      "Epoch 12, Batch 29/462, Loss: 0.6169453263282776\n",
      "Epoch 12, Batch 30/462, Loss: 0.8065723776817322\n",
      "Epoch 12, Batch 31/462, Loss: 0.7935035824775696\n",
      "Epoch 12, Batch 32/462, Loss: 0.6543729305267334\n",
      "Epoch 12, Batch 33/462, Loss: 0.7841934561729431\n",
      "Epoch 12, Batch 34/462, Loss: 0.6871857047080994\n",
      "Epoch 12, Batch 35/462, Loss: 0.6993459463119507\n",
      "Epoch 12, Batch 36/462, Loss: 0.8182587623596191\n",
      "Epoch 12, Batch 37/462, Loss: 0.9003815650939941\n",
      "Epoch 12, Batch 38/462, Loss: 0.7431580424308777\n",
      "Epoch 12, Batch 39/462, Loss: 0.7063571810722351\n",
      "Epoch 12, Batch 40/462, Loss: 0.7845436930656433\n",
      "Epoch 12, Batch 41/462, Loss: 0.7164428234100342\n",
      "Epoch 12, Batch 42/462, Loss: 0.7632927298545837\n",
      "Epoch 12, Batch 43/462, Loss: 0.8201708197593689\n",
      "Epoch 12, Batch 44/462, Loss: 0.7607647180557251\n",
      "Epoch 12, Batch 45/462, Loss: 0.9203076958656311\n",
      "Epoch 12, Batch 46/462, Loss: 0.8548699617385864\n",
      "Epoch 12, Batch 47/462, Loss: 0.7604897618293762\n",
      "Epoch 12, Batch 48/462, Loss: 0.7913874387741089\n",
      "Epoch 12, Batch 49/462, Loss: 0.8454420566558838\n",
      "Epoch 12, Batch 50/462, Loss: 0.8045092225074768\n",
      "Epoch 12, Batch 51/462, Loss: 0.7136289477348328\n",
      "Epoch 12, Batch 52/462, Loss: 0.8343677520751953\n",
      "Epoch 12, Batch 53/462, Loss: 0.8194759488105774\n",
      "Epoch 12, Batch 54/462, Loss: 0.9044002294540405\n",
      "Epoch 12, Batch 55/462, Loss: 0.764107346534729\n",
      "Epoch 12, Batch 56/462, Loss: 0.7632841467857361\n",
      "Epoch 12, Batch 57/462, Loss: 0.7161325216293335\n",
      "Epoch 12, Batch 58/462, Loss: 0.7012388706207275\n",
      "Epoch 12, Batch 59/462, Loss: 0.7419675588607788\n",
      "Epoch 12, Batch 60/462, Loss: 0.8883981704711914\n",
      "Epoch 12, Batch 61/462, Loss: 0.7525303363800049\n",
      "Epoch 12, Batch 62/462, Loss: 0.6789926886558533\n",
      "Epoch 12, Batch 63/462, Loss: 0.8596048951148987\n",
      "Epoch 12, Batch 64/462, Loss: 0.8443416953086853\n",
      "Epoch 12, Batch 65/462, Loss: 0.8043190240859985\n",
      "Epoch 12, Batch 66/462, Loss: 0.7692601680755615\n",
      "Epoch 12, Batch 67/462, Loss: 0.7823650240898132\n",
      "Epoch 12, Batch 68/462, Loss: 0.6884945034980774\n",
      "Epoch 12, Batch 69/462, Loss: 0.6858848333358765\n",
      "Epoch 12, Batch 70/462, Loss: 0.7672168016433716\n",
      "Epoch 12, Batch 71/462, Loss: 0.7546091079711914\n",
      "Epoch 12, Batch 72/462, Loss: 0.9421412944793701\n",
      "Epoch 12, Batch 73/462, Loss: 0.8365464806556702\n",
      "Epoch 12, Batch 74/462, Loss: 0.7328699231147766\n",
      "Epoch 12, Batch 75/462, Loss: 0.7547064423561096\n",
      "Epoch 12, Batch 76/462, Loss: 0.7230260372161865\n",
      "Epoch 12, Batch 77/462, Loss: 0.7590612173080444\n",
      "Epoch 12, Batch 78/462, Loss: 0.8571453094482422\n",
      "Epoch 12, Batch 79/462, Loss: 0.6822010278701782\n",
      "Epoch 12, Batch 80/462, Loss: 0.8379935622215271\n",
      "Epoch 12, Batch 81/462, Loss: 0.785662055015564\n",
      "Epoch 12, Batch 82/462, Loss: 0.7015594244003296\n",
      "Epoch 12, Batch 83/462, Loss: 0.808977484703064\n",
      "Epoch 12, Batch 84/462, Loss: 0.7840057611465454\n",
      "Epoch 12, Batch 85/462, Loss: 0.7383372783660889\n",
      "Epoch 12, Batch 86/462, Loss: 0.7587486505508423\n",
      "Epoch 12, Batch 87/462, Loss: 0.7114369869232178\n",
      "Epoch 12, Batch 88/462, Loss: 0.9468135833740234\n",
      "Epoch 12, Batch 89/462, Loss: 0.7608813047409058\n",
      "Epoch 12, Batch 90/462, Loss: 0.9012869000434875\n",
      "Epoch 12, Batch 91/462, Loss: 0.941714882850647\n",
      "Epoch 12, Batch 92/462, Loss: 0.7437205910682678\n",
      "Epoch 12, Batch 93/462, Loss: 0.9040470123291016\n",
      "Epoch 12, Batch 94/462, Loss: 0.7579647898674011\n",
      "Epoch 12, Batch 95/462, Loss: 0.6555903553962708\n",
      "Epoch 12, Batch 96/462, Loss: 0.8928477764129639\n",
      "Epoch 12, Batch 97/462, Loss: 0.7935460209846497\n",
      "Epoch 12, Batch 98/462, Loss: 0.7679415345191956\n",
      "Epoch 12, Batch 99/462, Loss: 0.8168020844459534\n",
      "Epoch 12, Batch 100/462, Loss: 0.8894116282463074\n",
      "Epoch 12, Batch 101/462, Loss: 0.7576770782470703\n",
      "Epoch 12, Batch 102/462, Loss: 0.6899616122245789\n",
      "Epoch 12, Batch 103/462, Loss: 0.7523178458213806\n",
      "Epoch 12, Batch 104/462, Loss: 0.718995988368988\n",
      "Epoch 12, Batch 105/462, Loss: 0.7827984690666199\n",
      "Epoch 12, Batch 106/462, Loss: 1.0033564567565918\n",
      "Epoch 12, Batch 107/462, Loss: 0.8729391694068909\n",
      "Epoch 12, Batch 108/462, Loss: 0.8870958685874939\n",
      "Epoch 12, Batch 109/462, Loss: 0.7778496146202087\n",
      "Epoch 12, Batch 110/462, Loss: 0.9040648937225342\n",
      "Epoch 12, Batch 111/462, Loss: 0.8130553364753723\n",
      "Epoch 12, Batch 112/462, Loss: 0.7527883648872375\n",
      "Epoch 12, Batch 113/462, Loss: 0.7313932180404663\n",
      "Epoch 12, Batch 114/462, Loss: 0.8607347011566162\n",
      "Epoch 12, Batch 115/462, Loss: 0.6873920559883118\n",
      "Epoch 12, Batch 116/462, Loss: 0.8640944361686707\n",
      "Epoch 12, Batch 117/462, Loss: 0.7927811145782471\n",
      "Epoch 12, Batch 118/462, Loss: 0.7904009222984314\n",
      "Epoch 12, Batch 119/462, Loss: 0.6976363658905029\n",
      "Epoch 12, Batch 120/462, Loss: 0.9257411360740662\n",
      "Epoch 12, Batch 121/462, Loss: 0.7352079749107361\n",
      "Epoch 12, Batch 122/462, Loss: 0.7127752900123596\n",
      "Epoch 12, Batch 123/462, Loss: 0.8657439947128296\n",
      "Epoch 12, Batch 124/462, Loss: 0.9100071787834167\n",
      "Epoch 12, Batch 125/462, Loss: 0.8392682671546936\n",
      "Epoch 12, Batch 126/462, Loss: 0.7196101546287537\n",
      "Epoch 12, Batch 127/462, Loss: 0.7851870656013489\n",
      "Epoch 12, Batch 128/462, Loss: 0.7115049958229065\n",
      "Epoch 12, Batch 129/462, Loss: 0.8741577863693237\n",
      "Epoch 12, Batch 130/462, Loss: 1.018459439277649\n",
      "Epoch 12, Batch 131/462, Loss: 0.8463544845581055\n",
      "Epoch 12, Batch 132/462, Loss: 0.7351146340370178\n",
      "Epoch 12, Batch 133/462, Loss: 0.8789696097373962\n",
      "Epoch 12, Batch 134/462, Loss: 0.8441248536109924\n",
      "Epoch 12, Batch 135/462, Loss: 0.8112765550613403\n",
      "Epoch 12, Batch 136/462, Loss: 0.7639678120613098\n",
      "Epoch 12, Batch 137/462, Loss: 0.8201712965965271\n",
      "Epoch 12, Batch 138/462, Loss: 0.9863635897636414\n",
      "Epoch 12, Batch 139/462, Loss: 0.8137156367301941\n",
      "Epoch 12, Batch 140/462, Loss: 0.8380676507949829\n",
      "Epoch 12, Batch 141/462, Loss: 0.6993951201438904\n",
      "Epoch 12, Batch 142/462, Loss: 0.8078852891921997\n",
      "Epoch 12, Batch 143/462, Loss: 0.6583998203277588\n",
      "Epoch 12, Batch 144/462, Loss: 0.6805957555770874\n",
      "Epoch 12, Batch 145/462, Loss: 0.7299748659133911\n",
      "Epoch 12, Batch 146/462, Loss: 0.6652705073356628\n",
      "Epoch 12, Batch 147/462, Loss: 0.8905231356620789\n",
      "Epoch 12, Batch 148/462, Loss: 0.895581841468811\n",
      "Epoch 12, Batch 149/462, Loss: 0.7482973337173462\n",
      "Epoch 12, Batch 150/462, Loss: 0.7875821590423584\n",
      "Epoch 12, Batch 151/462, Loss: 0.8751556277275085\n",
      "Epoch 12, Batch 152/462, Loss: 0.7774417996406555\n",
      "Epoch 12, Batch 153/462, Loss: 0.7957726716995239\n",
      "Epoch 12, Batch 154/462, Loss: 0.9329544305801392\n",
      "Epoch 12, Batch 155/462, Loss: 0.8012140393257141\n",
      "Epoch 12, Batch 156/462, Loss: 0.7531229853630066\n",
      "Epoch 12, Batch 157/462, Loss: 0.9151245355606079\n",
      "Epoch 12, Batch 158/462, Loss: 0.7258951663970947\n",
      "Epoch 12, Batch 159/462, Loss: 0.70383620262146\n",
      "Epoch 12, Batch 160/462, Loss: 0.7330862283706665\n",
      "Epoch 12, Batch 161/462, Loss: 0.9285628199577332\n",
      "Epoch 12, Batch 162/462, Loss: 0.6992906928062439\n",
      "Epoch 12, Batch 163/462, Loss: 0.7541678547859192\n",
      "Epoch 12, Batch 164/462, Loss: 0.7620636820793152\n",
      "Epoch 12, Batch 165/462, Loss: 0.7865864038467407\n",
      "Epoch 12, Batch 166/462, Loss: 0.6799731254577637\n",
      "Epoch 12, Batch 167/462, Loss: 0.7931829690933228\n",
      "Epoch 12, Batch 168/462, Loss: 0.7165385484695435\n",
      "Epoch 12, Batch 169/462, Loss: 0.8999189138412476\n",
      "Epoch 12, Batch 170/462, Loss: 0.7794673442840576\n",
      "Epoch 12, Batch 171/462, Loss: 0.7652320265769958\n",
      "Epoch 12, Batch 172/462, Loss: 0.8713279366493225\n",
      "Epoch 12, Batch 173/462, Loss: 0.6704142093658447\n",
      "Epoch 12, Batch 174/462, Loss: 0.8482191562652588\n",
      "Epoch 12, Batch 175/462, Loss: 0.8736974000930786\n",
      "Epoch 12, Batch 176/462, Loss: 0.7070532441139221\n",
      "Epoch 12, Batch 177/462, Loss: 0.8912727236747742\n",
      "Epoch 12, Batch 178/462, Loss: 0.7500010132789612\n",
      "Epoch 12, Batch 179/462, Loss: 0.7124975323677063\n",
      "Epoch 12, Batch 180/462, Loss: 0.737750768661499\n",
      "Epoch 12, Batch 181/462, Loss: 0.7822765111923218\n",
      "Epoch 12, Batch 182/462, Loss: 0.7438594698905945\n",
      "Epoch 12, Batch 183/462, Loss: 0.782023012638092\n",
      "Epoch 12, Batch 184/462, Loss: 0.6665781140327454\n",
      "Epoch 12, Batch 185/462, Loss: 0.7155662178993225\n",
      "Epoch 12, Batch 186/462, Loss: 0.7983677387237549\n",
      "Epoch 12, Batch 187/462, Loss: 0.7831882834434509\n",
      "Epoch 12, Batch 188/462, Loss: 0.8919848203659058\n",
      "Epoch 12, Batch 189/462, Loss: 0.7502232193946838\n",
      "Epoch 12, Batch 190/462, Loss: 0.729063868522644\n",
      "Epoch 12, Batch 191/462, Loss: 0.7746173739433289\n",
      "Epoch 12, Batch 192/462, Loss: 0.7652756571769714\n",
      "Epoch 12, Batch 193/462, Loss: 0.7881119251251221\n",
      "Epoch 12, Batch 194/462, Loss: 0.6591973900794983\n",
      "Epoch 12, Batch 195/462, Loss: 0.8085270524024963\n",
      "Epoch 12, Batch 196/462, Loss: 0.5883813500404358\n",
      "Epoch 12, Batch 197/462, Loss: 0.8294581174850464\n",
      "Epoch 12, Batch 198/462, Loss: 0.678550124168396\n",
      "Epoch 12, Batch 199/462, Loss: 0.7734760046005249\n",
      "Epoch 12, Batch 200/462, Loss: 0.8387209177017212\n",
      "Epoch 12, Batch 201/462, Loss: 0.672335147857666\n",
      "Epoch 12, Batch 202/462, Loss: 0.6743138432502747\n",
      "Epoch 12, Batch 203/462, Loss: 0.8673619031906128\n",
      "Epoch 12, Batch 204/462, Loss: 0.8564738631248474\n",
      "Epoch 12, Batch 205/462, Loss: 0.8159981966018677\n",
      "Epoch 12, Batch 206/462, Loss: 0.7286069393157959\n",
      "Epoch 12, Batch 207/462, Loss: 0.713517427444458\n",
      "Epoch 12, Batch 208/462, Loss: 0.8922757506370544\n",
      "Epoch 12, Batch 209/462, Loss: 0.8710410594940186\n",
      "Epoch 12, Batch 210/462, Loss: 0.7956128716468811\n",
      "Epoch 12, Batch 211/462, Loss: 0.9483478665351868\n",
      "Epoch 12, Batch 212/462, Loss: 0.7473278045654297\n",
      "Epoch 12, Batch 213/462, Loss: 0.8197575807571411\n",
      "Epoch 12, Batch 214/462, Loss: 0.7769326567649841\n",
      "Epoch 12, Batch 215/462, Loss: 0.7535674571990967\n",
      "Epoch 12, Batch 216/462, Loss: 0.7558494806289673\n",
      "Epoch 12, Batch 217/462, Loss: 0.8026254773139954\n",
      "Epoch 12, Batch 218/462, Loss: 0.6164281368255615\n",
      "Epoch 12, Batch 219/462, Loss: 0.7724377512931824\n",
      "Epoch 12, Batch 220/462, Loss: 0.7331684231758118\n",
      "Epoch 12, Batch 221/462, Loss: 0.7987128496170044\n",
      "Epoch 12, Batch 222/462, Loss: 0.7912784814834595\n",
      "Epoch 12, Batch 223/462, Loss: 0.7741945385932922\n",
      "Epoch 12, Batch 224/462, Loss: 0.7686887383460999\n",
      "Epoch 12, Batch 225/462, Loss: 0.8499017953872681\n",
      "Epoch 12, Batch 226/462, Loss: 0.7259497046470642\n",
      "Epoch 12, Batch 227/462, Loss: 0.8627412915229797\n",
      "Epoch 12, Batch 228/462, Loss: 0.7516633868217468\n",
      "Epoch 12, Batch 229/462, Loss: 0.7051976919174194\n",
      "Epoch 12, Batch 230/462, Loss: 0.876900851726532\n",
      "Epoch 12, Batch 231/462, Loss: 0.7900893092155457\n",
      "Epoch 12, Batch 232/462, Loss: 0.7175484299659729\n",
      "Epoch 12, Batch 233/462, Loss: 0.840390682220459\n",
      "Epoch 12, Batch 234/462, Loss: 0.7489148378372192\n",
      "Epoch 12, Batch 235/462, Loss: 0.8360678553581238\n",
      "Epoch 12, Batch 236/462, Loss: 0.8909383416175842\n",
      "Epoch 12, Batch 237/462, Loss: 0.7889814376831055\n",
      "Epoch 12, Batch 238/462, Loss: 0.8610730171203613\n",
      "Epoch 12, Batch 239/462, Loss: 0.8910547494888306\n",
      "Epoch 12, Batch 240/462, Loss: 0.9707948565483093\n",
      "Epoch 12, Batch 241/462, Loss: 0.880305290222168\n",
      "Epoch 12, Batch 242/462, Loss: 0.7798340916633606\n",
      "Epoch 12, Batch 243/462, Loss: 0.8853155970573425\n",
      "Epoch 12, Batch 244/462, Loss: 0.8449681401252747\n",
      "Epoch 12, Batch 245/462, Loss: 0.8385350704193115\n",
      "Epoch 12, Batch 246/462, Loss: 0.6961838603019714\n",
      "Epoch 12, Batch 247/462, Loss: 0.8745105266571045\n",
      "Epoch 12, Batch 248/462, Loss: 0.6764395833015442\n",
      "Epoch 12, Batch 249/462, Loss: 0.7770264148712158\n",
      "Epoch 12, Batch 250/462, Loss: 0.6494734287261963\n",
      "Epoch 12, Batch 251/462, Loss: 0.7430956959724426\n",
      "Epoch 12, Batch 252/462, Loss: 0.907480001449585\n",
      "Epoch 12, Batch 253/462, Loss: 0.830217182636261\n",
      "Epoch 12, Batch 254/462, Loss: 0.8174276947975159\n",
      "Epoch 12, Batch 255/462, Loss: 0.8306211233139038\n",
      "Epoch 12, Batch 256/462, Loss: 0.6641088724136353\n",
      "Epoch 12, Batch 257/462, Loss: 0.9004654288291931\n",
      "Epoch 12, Batch 258/462, Loss: 0.782943069934845\n",
      "Epoch 12, Batch 259/462, Loss: 0.7144489288330078\n",
      "Epoch 12, Batch 260/462, Loss: 0.7149229049682617\n",
      "Epoch 12, Batch 261/462, Loss: 0.7988511323928833\n",
      "Epoch 12, Batch 262/462, Loss: 0.8598040342330933\n",
      "Epoch 12, Batch 263/462, Loss: 0.7661046981811523\n",
      "Epoch 12, Batch 264/462, Loss: 0.8250858187675476\n",
      "Epoch 12, Batch 265/462, Loss: 0.767737090587616\n",
      "Epoch 12, Batch 266/462, Loss: 0.6565097570419312\n",
      "Epoch 12, Batch 267/462, Loss: 0.7045791149139404\n",
      "Epoch 12, Batch 268/462, Loss: 0.9362511038780212\n",
      "Epoch 12, Batch 269/462, Loss: 0.9844043850898743\n",
      "Epoch 12, Batch 270/462, Loss: 0.8418475389480591\n",
      "Epoch 12, Batch 271/462, Loss: 0.8215024471282959\n",
      "Epoch 12, Batch 272/462, Loss: 0.8081405162811279\n",
      "Epoch 12, Batch 273/462, Loss: 0.7247615456581116\n",
      "Epoch 12, Batch 274/462, Loss: 0.6436404585838318\n",
      "Epoch 12, Batch 275/462, Loss: 0.6949332356452942\n",
      "Epoch 12, Batch 276/462, Loss: 0.7304500341415405\n",
      "Epoch 12, Batch 277/462, Loss: 0.9661495685577393\n",
      "Epoch 12, Batch 278/462, Loss: 0.8711243271827698\n",
      "Epoch 12, Batch 279/462, Loss: 0.8172318935394287\n",
      "Epoch 12, Batch 280/462, Loss: 0.79513019323349\n",
      "Epoch 12, Batch 281/462, Loss: 0.7346785664558411\n",
      "Epoch 12, Batch 282/462, Loss: 0.8480541706085205\n",
      "Epoch 12, Batch 283/462, Loss: 0.7040653228759766\n",
      "Epoch 12, Batch 284/462, Loss: 0.7469992637634277\n",
      "Epoch 12, Batch 285/462, Loss: 0.9038789868354797\n",
      "Epoch 12, Batch 286/462, Loss: 0.7923380136489868\n",
      "Epoch 12, Batch 287/462, Loss: 0.7616295218467712\n",
      "Epoch 12, Batch 288/462, Loss: 0.800166130065918\n",
      "Epoch 12, Batch 289/462, Loss: 0.7341828346252441\n",
      "Epoch 12, Batch 290/462, Loss: 0.7308631539344788\n",
      "Epoch 12, Batch 291/462, Loss: 0.7661993503570557\n",
      "Epoch 12, Batch 292/462, Loss: 0.8321577906608582\n",
      "Epoch 12, Batch 293/462, Loss: 0.817790687084198\n",
      "Epoch 12, Batch 294/462, Loss: 0.7583796977996826\n",
      "Epoch 12, Batch 295/462, Loss: 0.7451943159103394\n",
      "Epoch 12, Batch 296/462, Loss: 0.8563998341560364\n",
      "Epoch 12, Batch 297/462, Loss: 0.6800268292427063\n",
      "Epoch 12, Batch 298/462, Loss: 0.7184263467788696\n",
      "Epoch 12, Batch 299/462, Loss: 0.850098192691803\n",
      "Epoch 12, Batch 300/462, Loss: 0.7329682111740112\n",
      "Epoch 12, Batch 301/462, Loss: 0.7555583119392395\n",
      "Epoch 12, Batch 302/462, Loss: 0.6941547989845276\n",
      "Epoch 12, Batch 303/462, Loss: 0.8311858773231506\n",
      "Epoch 12, Batch 304/462, Loss: 0.8307019472122192\n",
      "Epoch 12, Batch 305/462, Loss: 0.8167136311531067\n",
      "Epoch 12, Batch 306/462, Loss: 0.7768495082855225\n",
      "Epoch 12, Batch 307/462, Loss: 0.8577837347984314\n",
      "Epoch 12, Batch 308/462, Loss: 0.7979855537414551\n",
      "Epoch 12, Batch 309/462, Loss: 0.9572309255599976\n",
      "Epoch 12, Batch 310/462, Loss: 0.8381350040435791\n",
      "Epoch 12, Batch 311/462, Loss: 0.9937357306480408\n",
      "Epoch 12, Batch 312/462, Loss: 0.6667429208755493\n",
      "Epoch 12, Batch 313/462, Loss: 0.8134699463844299\n",
      "Epoch 12, Batch 314/462, Loss: 0.7097707390785217\n",
      "Epoch 12, Batch 315/462, Loss: 0.8269791007041931\n",
      "Epoch 12, Batch 316/462, Loss: 0.6193427443504333\n",
      "Epoch 12, Batch 317/462, Loss: 0.7875472903251648\n",
      "Epoch 12, Batch 318/462, Loss: 0.7496991753578186\n",
      "Epoch 12, Batch 319/462, Loss: 0.816605806350708\n",
      "Epoch 12, Batch 320/462, Loss: 0.7277117967605591\n",
      "Epoch 12, Batch 321/462, Loss: 0.6424867510795593\n",
      "Epoch 12, Batch 322/462, Loss: 0.7103278636932373\n",
      "Epoch 12, Batch 323/462, Loss: 0.8084652423858643\n",
      "Epoch 12, Batch 324/462, Loss: 0.768212080001831\n",
      "Epoch 12, Batch 325/462, Loss: 0.6942272782325745\n",
      "Epoch 12, Batch 326/462, Loss: 0.7272651791572571\n",
      "Epoch 12, Batch 327/462, Loss: 0.6918845772743225\n",
      "Epoch 12, Batch 328/462, Loss: 0.8251655697822571\n",
      "Epoch 12, Batch 329/462, Loss: 0.7119889855384827\n",
      "Epoch 12, Batch 330/462, Loss: 0.7969906330108643\n",
      "Epoch 12, Batch 331/462, Loss: 0.7714781761169434\n",
      "Epoch 12, Batch 332/462, Loss: 0.7617802023887634\n",
      "Epoch 12, Batch 333/462, Loss: 0.8594604730606079\n",
      "Epoch 12, Batch 334/462, Loss: 0.778725266456604\n",
      "Epoch 12, Batch 335/462, Loss: 0.6814607977867126\n",
      "Epoch 12, Batch 336/462, Loss: 0.831820547580719\n",
      "Epoch 12, Batch 337/462, Loss: 0.7381154298782349\n",
      "Epoch 12, Batch 338/462, Loss: 0.8586746454238892\n",
      "Epoch 12, Batch 339/462, Loss: 0.7163593769073486\n",
      "Epoch 12, Batch 340/462, Loss: 0.7751585245132446\n",
      "Epoch 12, Batch 341/462, Loss: 0.831716775894165\n",
      "Epoch 12, Batch 342/462, Loss: 0.760010302066803\n",
      "Epoch 12, Batch 343/462, Loss: 0.913282036781311\n",
      "Epoch 12, Batch 344/462, Loss: 0.7281472682952881\n",
      "Epoch 12, Batch 345/462, Loss: 0.7346854209899902\n",
      "Epoch 12, Batch 346/462, Loss: 0.8484019637107849\n",
      "Epoch 12, Batch 347/462, Loss: 0.6783081293106079\n",
      "Epoch 12, Batch 348/462, Loss: 0.7509739398956299\n",
      "Epoch 12, Batch 349/462, Loss: 0.8786860108375549\n",
      "Epoch 12, Batch 350/462, Loss: 0.8638932704925537\n",
      "Epoch 12, Batch 351/462, Loss: 0.8093423843383789\n",
      "Epoch 12, Batch 352/462, Loss: 0.7667297124862671\n",
      "Epoch 12, Batch 353/462, Loss: 0.772092878818512\n",
      "Epoch 12, Batch 354/462, Loss: 0.7794772386550903\n",
      "Epoch 12, Batch 355/462, Loss: 0.8458210229873657\n",
      "Epoch 12, Batch 356/462, Loss: 0.8580918312072754\n",
      "Epoch 12, Batch 357/462, Loss: 0.9292184710502625\n",
      "Epoch 12, Batch 358/462, Loss: 0.8287152647972107\n",
      "Epoch 12, Batch 359/462, Loss: 0.8293598890304565\n",
      "Epoch 12, Batch 360/462, Loss: 0.8362825512886047\n",
      "Epoch 12, Batch 361/462, Loss: 0.7802214622497559\n",
      "Epoch 12, Batch 362/462, Loss: 0.7268193364143372\n",
      "Epoch 12, Batch 363/462, Loss: 0.7438863515853882\n",
      "Epoch 12, Batch 364/462, Loss: 0.912643313407898\n",
      "Epoch 12, Batch 365/462, Loss: 0.7211023569107056\n",
      "Epoch 12, Batch 366/462, Loss: 0.8559982776641846\n",
      "Epoch 12, Batch 367/462, Loss: 0.7150679230690002\n",
      "Epoch 12, Batch 368/462, Loss: 0.6893088221549988\n",
      "Epoch 12, Batch 369/462, Loss: 0.7424085140228271\n",
      "Epoch 12, Batch 370/462, Loss: 0.778166651725769\n",
      "Epoch 12, Batch 371/462, Loss: 0.764310359954834\n",
      "Epoch 12, Batch 372/462, Loss: 0.6957908272743225\n",
      "Epoch 12, Batch 373/462, Loss: 0.8185807466506958\n",
      "Epoch 12, Batch 374/462, Loss: 0.8800345659255981\n",
      "Epoch 12, Batch 375/462, Loss: 0.7675378322601318\n",
      "Epoch 12, Batch 376/462, Loss: 0.7157577276229858\n",
      "Epoch 12, Batch 377/462, Loss: 0.7943621873855591\n",
      "Epoch 12, Batch 378/462, Loss: 0.6239591836929321\n",
      "Epoch 12, Batch 379/462, Loss: 0.783234715461731\n",
      "Epoch 12, Batch 380/462, Loss: 0.7864079475402832\n",
      "Epoch 12, Batch 381/462, Loss: 0.7751621007919312\n",
      "Epoch 12, Batch 382/462, Loss: 0.8250337243080139\n",
      "Epoch 12, Batch 383/462, Loss: 0.8592659831047058\n",
      "Epoch 12, Batch 384/462, Loss: 0.699342668056488\n",
      "Epoch 12, Batch 385/462, Loss: 0.8586744666099548\n",
      "Epoch 12, Batch 386/462, Loss: 0.9589707851409912\n",
      "Epoch 12, Batch 387/462, Loss: 0.8345822691917419\n",
      "Epoch 12, Batch 388/462, Loss: 0.8199043273925781\n",
      "Epoch 12, Batch 389/462, Loss: 0.6616864204406738\n",
      "Epoch 12, Batch 390/462, Loss: 0.7603204250335693\n",
      "Epoch 12, Batch 391/462, Loss: 0.9997624754905701\n",
      "Epoch 12, Batch 392/462, Loss: 0.8586518168449402\n",
      "Epoch 12, Batch 393/462, Loss: 0.7639933228492737\n",
      "Epoch 12, Batch 394/462, Loss: 0.8707959651947021\n",
      "Epoch 12, Batch 395/462, Loss: 0.806684672832489\n",
      "Epoch 12, Batch 396/462, Loss: 0.7820906639099121\n",
      "Epoch 12, Batch 397/462, Loss: 0.902866005897522\n",
      "Epoch 12, Batch 398/462, Loss: 0.8742225766181946\n",
      "Epoch 12, Batch 399/462, Loss: 0.7233317494392395\n",
      "Epoch 12, Batch 400/462, Loss: 0.8810860514640808\n",
      "Epoch 12, Batch 401/462, Loss: 0.9398602843284607\n",
      "Epoch 12, Batch 402/462, Loss: 0.693516731262207\n",
      "Epoch 12, Batch 403/462, Loss: 0.7567561268806458\n",
      "Epoch 12, Batch 404/462, Loss: 0.8279110193252563\n",
      "Epoch 12, Batch 405/462, Loss: 0.584294319152832\n",
      "Epoch 12, Batch 406/462, Loss: 0.7452669739723206\n",
      "Epoch 12, Batch 407/462, Loss: 0.725121259689331\n",
      "Epoch 12, Batch 408/462, Loss: 0.6072220206260681\n",
      "Epoch 12, Batch 409/462, Loss: 0.6995185613632202\n",
      "Epoch 12, Batch 410/462, Loss: 0.785097599029541\n",
      "Epoch 12, Batch 411/462, Loss: 0.6491732597351074\n",
      "Epoch 12, Batch 412/462, Loss: 1.010687232017517\n",
      "Epoch 12, Batch 413/462, Loss: 0.8400579690933228\n",
      "Epoch 12, Batch 414/462, Loss: 0.8845255970954895\n",
      "Epoch 12, Batch 415/462, Loss: 0.7712565064430237\n",
      "Epoch 12, Batch 416/462, Loss: 0.8520253896713257\n",
      "Epoch 12, Batch 417/462, Loss: 0.935122013092041\n",
      "Epoch 12, Batch 418/462, Loss: 0.7466826438903809\n",
      "Epoch 12, Batch 419/462, Loss: 0.7406613230705261\n",
      "Epoch 12, Batch 420/462, Loss: 0.7591346502304077\n",
      "Epoch 12, Batch 421/462, Loss: 0.7881677746772766\n",
      "Epoch 12, Batch 422/462, Loss: 0.7417764663696289\n",
      "Epoch 12, Batch 423/462, Loss: 0.8714715838432312\n",
      "Epoch 12, Batch 424/462, Loss: 0.7972602844238281\n",
      "Epoch 12, Batch 425/462, Loss: 0.7638843655586243\n",
      "Epoch 12, Batch 426/462, Loss: 0.8751834034919739\n",
      "Epoch 12, Batch 427/462, Loss: 0.6862009167671204\n",
      "Epoch 12, Batch 428/462, Loss: 0.7988071441650391\n",
      "Epoch 12, Batch 429/462, Loss: 0.7881162166595459\n",
      "Epoch 12, Batch 430/462, Loss: 0.710573673248291\n",
      "Epoch 12, Batch 431/462, Loss: 0.920378565788269\n",
      "Epoch 12, Batch 432/462, Loss: 0.7583159804344177\n",
      "Epoch 12, Batch 433/462, Loss: 0.6759316921234131\n",
      "Epoch 12, Batch 434/462, Loss: 0.8411081433296204\n",
      "Epoch 12, Batch 435/462, Loss: 0.9145055413246155\n",
      "Epoch 12, Batch 436/462, Loss: 0.8051002621650696\n",
      "Epoch 12, Batch 437/462, Loss: 0.7982710003852844\n",
      "Epoch 12, Batch 438/462, Loss: 0.723453164100647\n",
      "Epoch 12, Batch 439/462, Loss: 0.7522278428077698\n",
      "Epoch 12, Batch 440/462, Loss: 0.6403283476829529\n",
      "Epoch 12, Batch 441/462, Loss: 0.8278133273124695\n",
      "Epoch 12, Batch 442/462, Loss: 0.8665863871574402\n",
      "Epoch 12, Batch 443/462, Loss: 0.7742629647254944\n",
      "Epoch 12, Batch 444/462, Loss: 0.8459203839302063\n",
      "Epoch 12, Batch 445/462, Loss: 0.7672973275184631\n",
      "Epoch 12, Batch 446/462, Loss: 0.6436962485313416\n",
      "Epoch 12, Batch 447/462, Loss: 0.761238157749176\n",
      "Epoch 12, Batch 448/462, Loss: 0.9407156109809875\n",
      "Epoch 12, Batch 449/462, Loss: 0.8138795495033264\n",
      "Epoch 12, Batch 450/462, Loss: 0.7740913033485413\n",
      "Epoch 12, Batch 451/462, Loss: 0.7280761003494263\n",
      "Epoch 12, Batch 452/462, Loss: 0.7521516680717468\n",
      "Epoch 12, Batch 453/462, Loss: 1.0173448324203491\n",
      "Epoch 12, Batch 454/462, Loss: 0.9099522233009338\n",
      "Epoch 12, Batch 455/462, Loss: 0.7866921424865723\n",
      "Epoch 12, Batch 456/462, Loss: 0.8045364618301392\n",
      "Epoch 12, Batch 457/462, Loss: 0.8415473103523254\n",
      "Epoch 12, Batch 458/462, Loss: 0.7979528903961182\n",
      "Epoch 12, Batch 459/462, Loss: 0.8863721489906311\n",
      "Epoch 12, Batch 460/462, Loss: 0.812958836555481\n",
      "Epoch 12, Batch 461/462, Loss: 0.6270758509635925\n",
      "Epoch 12, Batch 462/462, Loss: 0.7010591626167297\n",
      "Epoch 12, Loss: 364.57818454504013\n",
      "Epoch 13, Batch 1/462, Loss: 0.8847981691360474\n",
      "Epoch 13, Batch 2/462, Loss: 0.7468403577804565\n",
      "Epoch 13, Batch 3/462, Loss: 0.8387762904167175\n",
      "Epoch 13, Batch 4/462, Loss: 0.8630372881889343\n",
      "Epoch 13, Batch 5/462, Loss: 0.8030974864959717\n",
      "Epoch 13, Batch 6/462, Loss: 0.7727477550506592\n",
      "Epoch 13, Batch 7/462, Loss: 0.9183273911476135\n",
      "Epoch 13, Batch 8/462, Loss: 0.9135692715644836\n",
      "Epoch 13, Batch 9/462, Loss: 0.923109769821167\n",
      "Epoch 13, Batch 10/462, Loss: 0.736110508441925\n",
      "Epoch 13, Batch 11/462, Loss: 0.8009905815124512\n",
      "Epoch 13, Batch 12/462, Loss: 0.7789862155914307\n",
      "Epoch 13, Batch 13/462, Loss: 0.8788518309593201\n",
      "Epoch 13, Batch 14/462, Loss: 0.8084595203399658\n",
      "Epoch 13, Batch 15/462, Loss: 0.6407334208488464\n",
      "Epoch 13, Batch 16/462, Loss: 0.7715895771980286\n",
      "Epoch 13, Batch 17/462, Loss: 0.7668110132217407\n",
      "Epoch 13, Batch 18/462, Loss: 0.6275736689567566\n",
      "Epoch 13, Batch 19/462, Loss: 0.813554584980011\n",
      "Epoch 13, Batch 20/462, Loss: 0.8365504741668701\n",
      "Epoch 13, Batch 21/462, Loss: 0.7021883130073547\n",
      "Epoch 13, Batch 22/462, Loss: 0.8978687524795532\n",
      "Epoch 13, Batch 23/462, Loss: 0.8323186635971069\n",
      "Epoch 13, Batch 24/462, Loss: 0.8970357179641724\n",
      "Epoch 13, Batch 25/462, Loss: 0.9531232714653015\n",
      "Epoch 13, Batch 26/462, Loss: 0.9612075090408325\n",
      "Epoch 13, Batch 27/462, Loss: 0.7018694877624512\n",
      "Epoch 13, Batch 28/462, Loss: 0.7688116431236267\n",
      "Epoch 13, Batch 29/462, Loss: 0.7408186793327332\n",
      "Epoch 13, Batch 30/462, Loss: 0.9824693202972412\n",
      "Epoch 13, Batch 31/462, Loss: 0.8263192772865295\n",
      "Epoch 13, Batch 32/462, Loss: 0.7450628280639648\n",
      "Epoch 13, Batch 33/462, Loss: 0.7518972754478455\n",
      "Epoch 13, Batch 34/462, Loss: 0.7709739208221436\n",
      "Epoch 13, Batch 35/462, Loss: 0.807389497756958\n",
      "Epoch 13, Batch 36/462, Loss: 0.7754056453704834\n",
      "Epoch 13, Batch 37/462, Loss: 0.6976389288902283\n",
      "Epoch 13, Batch 38/462, Loss: 0.8004928827285767\n",
      "Epoch 13, Batch 39/462, Loss: 0.7182497978210449\n",
      "Epoch 13, Batch 40/462, Loss: 0.6407129764556885\n",
      "Epoch 13, Batch 41/462, Loss: 0.8377457857131958\n",
      "Epoch 13, Batch 42/462, Loss: 1.0244284868240356\n",
      "Epoch 13, Batch 43/462, Loss: 0.8706046342849731\n",
      "Epoch 13, Batch 44/462, Loss: 0.8615062832832336\n",
      "Epoch 13, Batch 45/462, Loss: 0.7237284779548645\n",
      "Epoch 13, Batch 46/462, Loss: 0.7732607126235962\n",
      "Epoch 13, Batch 47/462, Loss: 0.8161852359771729\n",
      "Epoch 13, Batch 48/462, Loss: 0.8157896399497986\n",
      "Epoch 13, Batch 49/462, Loss: 0.7492715716362\n",
      "Epoch 13, Batch 50/462, Loss: 0.8625005483627319\n",
      "Epoch 13, Batch 51/462, Loss: 0.9309573173522949\n",
      "Epoch 13, Batch 52/462, Loss: 0.8252684473991394\n",
      "Epoch 13, Batch 53/462, Loss: 0.8470214605331421\n",
      "Epoch 13, Batch 54/462, Loss: 0.8337459564208984\n",
      "Epoch 13, Batch 55/462, Loss: 0.6980476379394531\n",
      "Epoch 13, Batch 56/462, Loss: 0.8450967669487\n",
      "Epoch 13, Batch 57/462, Loss: 0.778182327747345\n",
      "Epoch 13, Batch 58/462, Loss: 0.6780691742897034\n",
      "Epoch 13, Batch 59/462, Loss: 0.7725396752357483\n",
      "Epoch 13, Batch 60/462, Loss: 0.9025405645370483\n",
      "Epoch 13, Batch 61/462, Loss: 0.6952767968177795\n",
      "Epoch 13, Batch 62/462, Loss: 0.7053653597831726\n",
      "Epoch 13, Batch 63/462, Loss: 0.7431283593177795\n",
      "Epoch 13, Batch 64/462, Loss: 0.7306395173072815\n",
      "Epoch 13, Batch 65/462, Loss: 0.7822777628898621\n",
      "Epoch 13, Batch 66/462, Loss: 0.7141004800796509\n",
      "Epoch 13, Batch 67/462, Loss: 0.9928553104400635\n",
      "Epoch 13, Batch 68/462, Loss: 0.8211236000061035\n",
      "Epoch 13, Batch 69/462, Loss: 0.7562795877456665\n",
      "Epoch 13, Batch 70/462, Loss: 0.836059033870697\n",
      "Epoch 13, Batch 71/462, Loss: 0.8913351893424988\n",
      "Epoch 13, Batch 72/462, Loss: 0.7856909036636353\n",
      "Epoch 13, Batch 73/462, Loss: 0.7738598585128784\n",
      "Epoch 13, Batch 74/462, Loss: 0.8602004647254944\n",
      "Epoch 13, Batch 75/462, Loss: 0.8978749513626099\n",
      "Epoch 13, Batch 76/462, Loss: 0.8304941058158875\n",
      "Epoch 13, Batch 77/462, Loss: 0.781661331653595\n",
      "Epoch 13, Batch 78/462, Loss: 0.7490673661231995\n",
      "Epoch 13, Batch 79/462, Loss: 0.7451125979423523\n",
      "Epoch 13, Batch 80/462, Loss: 0.7908132076263428\n",
      "Epoch 13, Batch 81/462, Loss: 0.9034104347229004\n",
      "Epoch 13, Batch 82/462, Loss: 0.7588964104652405\n",
      "Epoch 13, Batch 83/462, Loss: 0.7867668867111206\n",
      "Epoch 13, Batch 84/462, Loss: 0.8944839239120483\n",
      "Epoch 13, Batch 85/462, Loss: 0.7841341495513916\n",
      "Epoch 13, Batch 86/462, Loss: 0.717142641544342\n",
      "Epoch 13, Batch 87/462, Loss: 0.8690991401672363\n",
      "Epoch 13, Batch 88/462, Loss: 0.7835979461669922\n",
      "Epoch 13, Batch 89/462, Loss: 0.78786700963974\n",
      "Epoch 13, Batch 90/462, Loss: 0.7486109733581543\n",
      "Epoch 13, Batch 91/462, Loss: 0.7788335084915161\n",
      "Epoch 13, Batch 92/462, Loss: 1.0835312604904175\n",
      "Epoch 13, Batch 93/462, Loss: 0.787958025932312\n",
      "Epoch 13, Batch 94/462, Loss: 0.9495163559913635\n",
      "Epoch 13, Batch 95/462, Loss: 0.8549509644508362\n",
      "Epoch 13, Batch 96/462, Loss: 0.8496283292770386\n",
      "Epoch 13, Batch 97/462, Loss: 0.9346387386322021\n",
      "Epoch 13, Batch 98/462, Loss: 0.7672293782234192\n",
      "Epoch 13, Batch 99/462, Loss: 0.788293182849884\n",
      "Epoch 13, Batch 100/462, Loss: 0.7619035840034485\n",
      "Epoch 13, Batch 101/462, Loss: 0.7868685126304626\n",
      "Epoch 13, Batch 102/462, Loss: 0.8687799572944641\n",
      "Epoch 13, Batch 103/462, Loss: 0.6475710272789001\n",
      "Epoch 13, Batch 104/462, Loss: 0.7665879130363464\n",
      "Epoch 13, Batch 105/462, Loss: 0.894294023513794\n",
      "Epoch 13, Batch 106/462, Loss: 0.752500057220459\n",
      "Epoch 13, Batch 107/462, Loss: 0.6892253160476685\n",
      "Epoch 13, Batch 108/462, Loss: 0.7752736806869507\n",
      "Epoch 13, Batch 109/462, Loss: 0.7772769331932068\n",
      "Epoch 13, Batch 110/462, Loss: 0.8322939872741699\n",
      "Epoch 13, Batch 111/462, Loss: 0.7833352088928223\n",
      "Epoch 13, Batch 112/462, Loss: 0.7722994089126587\n",
      "Epoch 13, Batch 113/462, Loss: 0.740131139755249\n",
      "Epoch 13, Batch 114/462, Loss: 0.7521358728408813\n",
      "Epoch 13, Batch 115/462, Loss: 0.7563453912734985\n",
      "Epoch 13, Batch 116/462, Loss: 0.8474393486976624\n",
      "Epoch 13, Batch 117/462, Loss: 0.9290573596954346\n",
      "Epoch 13, Batch 118/462, Loss: 0.8458200097084045\n",
      "Epoch 13, Batch 119/462, Loss: 0.8604500889778137\n",
      "Epoch 13, Batch 120/462, Loss: 0.9035426378250122\n",
      "Epoch 13, Batch 121/462, Loss: 0.6471274495124817\n",
      "Epoch 13, Batch 122/462, Loss: 0.9404409527778625\n",
      "Epoch 13, Batch 123/462, Loss: 0.7230520248413086\n",
      "Epoch 13, Batch 124/462, Loss: 0.7272796630859375\n",
      "Epoch 13, Batch 125/462, Loss: 0.7716675996780396\n",
      "Epoch 13, Batch 126/462, Loss: 0.7332716584205627\n",
      "Epoch 13, Batch 127/462, Loss: 0.692280113697052\n",
      "Epoch 13, Batch 128/462, Loss: 0.584560751914978\n",
      "Epoch 13, Batch 129/462, Loss: 0.7439010739326477\n",
      "Epoch 13, Batch 130/462, Loss: 0.6454800367355347\n",
      "Epoch 13, Batch 131/462, Loss: 0.7060509920120239\n",
      "Epoch 13, Batch 132/462, Loss: 0.8140445947647095\n",
      "Epoch 13, Batch 133/462, Loss: 0.675656259059906\n",
      "Epoch 13, Batch 134/462, Loss: 0.7630307078361511\n",
      "Epoch 13, Batch 135/462, Loss: 0.7930400967597961\n",
      "Epoch 13, Batch 136/462, Loss: 0.8942464590072632\n",
      "Epoch 13, Batch 137/462, Loss: 0.7978110313415527\n",
      "Epoch 13, Batch 138/462, Loss: 0.7436971068382263\n",
      "Epoch 13, Batch 139/462, Loss: 0.9213534593582153\n",
      "Epoch 13, Batch 140/462, Loss: 0.7217725515365601\n",
      "Epoch 13, Batch 141/462, Loss: 0.7381954193115234\n",
      "Epoch 13, Batch 142/462, Loss: 0.9808322191238403\n",
      "Epoch 13, Batch 143/462, Loss: 0.9371591806411743\n",
      "Epoch 13, Batch 144/462, Loss: 0.7599632143974304\n",
      "Epoch 13, Batch 145/462, Loss: 0.7148477435112\n",
      "Epoch 13, Batch 146/462, Loss: 0.8064060807228088\n",
      "Epoch 13, Batch 147/462, Loss: 0.5338585376739502\n",
      "Epoch 13, Batch 148/462, Loss: 0.7780684232711792\n",
      "Epoch 13, Batch 149/462, Loss: 0.7931280136108398\n",
      "Epoch 13, Batch 150/462, Loss: 0.633519172668457\n",
      "Epoch 13, Batch 151/462, Loss: 0.6378181576728821\n",
      "Epoch 13, Batch 152/462, Loss: 0.7745275497436523\n",
      "Epoch 13, Batch 153/462, Loss: 0.6649824976921082\n",
      "Epoch 13, Batch 154/462, Loss: 0.6269685626029968\n",
      "Epoch 13, Batch 155/462, Loss: 0.6877565979957581\n",
      "Epoch 13, Batch 156/462, Loss: 0.9941810369491577\n",
      "Epoch 13, Batch 157/462, Loss: 0.8440775275230408\n",
      "Epoch 13, Batch 158/462, Loss: 0.8339455723762512\n",
      "Epoch 13, Batch 159/462, Loss: 0.7530441284179688\n",
      "Epoch 13, Batch 160/462, Loss: 0.7205517292022705\n",
      "Epoch 13, Batch 161/462, Loss: 0.8082783818244934\n",
      "Epoch 13, Batch 162/462, Loss: 0.7665744423866272\n",
      "Epoch 13, Batch 163/462, Loss: 0.7021645903587341\n",
      "Epoch 13, Batch 164/462, Loss: 0.7651460766792297\n",
      "Epoch 13, Batch 165/462, Loss: 0.8140348792076111\n",
      "Epoch 13, Batch 166/462, Loss: 0.7340430617332458\n",
      "Epoch 13, Batch 167/462, Loss: 0.9099417924880981\n",
      "Epoch 13, Batch 168/462, Loss: 1.0031026601791382\n",
      "Epoch 13, Batch 169/462, Loss: 0.9091683030128479\n",
      "Epoch 13, Batch 170/462, Loss: 0.5503131151199341\n",
      "Epoch 13, Batch 171/462, Loss: 0.7431906461715698\n",
      "Epoch 13, Batch 172/462, Loss: 0.688408374786377\n",
      "Epoch 13, Batch 173/462, Loss: 0.9376529455184937\n",
      "Epoch 13, Batch 174/462, Loss: 0.8075675368309021\n",
      "Epoch 13, Batch 175/462, Loss: 0.8621411919593811\n",
      "Epoch 13, Batch 176/462, Loss: 0.8470531702041626\n",
      "Epoch 13, Batch 177/462, Loss: 0.7473859786987305\n",
      "Epoch 13, Batch 178/462, Loss: 0.8456256985664368\n",
      "Epoch 13, Batch 179/462, Loss: 0.8743728399276733\n",
      "Epoch 13, Batch 180/462, Loss: 0.8387892246246338\n",
      "Epoch 13, Batch 181/462, Loss: 0.655241072177887\n",
      "Epoch 13, Batch 182/462, Loss: 0.5246738791465759\n",
      "Epoch 13, Batch 183/462, Loss: 0.7966030240058899\n",
      "Epoch 13, Batch 184/462, Loss: 0.8452164530754089\n",
      "Epoch 13, Batch 185/462, Loss: 0.8750633001327515\n",
      "Epoch 13, Batch 186/462, Loss: 0.8299741148948669\n",
      "Epoch 13, Batch 187/462, Loss: 0.8435834050178528\n",
      "Epoch 13, Batch 188/462, Loss: 0.6822129487991333\n",
      "Epoch 13, Batch 189/462, Loss: 0.7369575500488281\n",
      "Epoch 13, Batch 190/462, Loss: 0.6860659718513489\n",
      "Epoch 13, Batch 191/462, Loss: 0.8556026220321655\n",
      "Epoch 13, Batch 192/462, Loss: 0.6633309721946716\n",
      "Epoch 13, Batch 193/462, Loss: 0.8601195216178894\n",
      "Epoch 13, Batch 194/462, Loss: 0.7148680090904236\n",
      "Epoch 13, Batch 195/462, Loss: 0.774674654006958\n",
      "Epoch 13, Batch 196/462, Loss: 0.8089337348937988\n",
      "Epoch 13, Batch 197/462, Loss: 0.7652395963668823\n",
      "Epoch 13, Batch 198/462, Loss: 0.7684361338615417\n",
      "Epoch 13, Batch 199/462, Loss: 0.9904693365097046\n",
      "Epoch 13, Batch 200/462, Loss: 0.8040503263473511\n",
      "Epoch 13, Batch 201/462, Loss: 0.7777523994445801\n",
      "Epoch 13, Batch 202/462, Loss: 0.7742632627487183\n",
      "Epoch 13, Batch 203/462, Loss: 0.8473267555236816\n",
      "Epoch 13, Batch 204/462, Loss: 0.8091311454772949\n",
      "Epoch 13, Batch 205/462, Loss: 0.8504374623298645\n",
      "Epoch 13, Batch 206/462, Loss: 0.7216467261314392\n",
      "Epoch 13, Batch 207/462, Loss: 0.8288439512252808\n",
      "Epoch 13, Batch 208/462, Loss: 0.8202139735221863\n",
      "Epoch 13, Batch 209/462, Loss: 0.7321630716323853\n",
      "Epoch 13, Batch 210/462, Loss: 0.8875958919525146\n",
      "Epoch 13, Batch 211/462, Loss: 0.728611171245575\n",
      "Epoch 13, Batch 212/462, Loss: 0.8418540954589844\n",
      "Epoch 13, Batch 213/462, Loss: 0.6712496280670166\n",
      "Epoch 13, Batch 214/462, Loss: 0.8862094283103943\n",
      "Epoch 13, Batch 215/462, Loss: 0.6945751905441284\n",
      "Epoch 13, Batch 216/462, Loss: 0.7870871424674988\n",
      "Epoch 13, Batch 217/462, Loss: 0.6734060645103455\n",
      "Epoch 13, Batch 218/462, Loss: 0.8827022314071655\n",
      "Epoch 13, Batch 219/462, Loss: 0.8276827931404114\n",
      "Epoch 13, Batch 220/462, Loss: 0.715211033821106\n",
      "Epoch 13, Batch 221/462, Loss: 0.7714681625366211\n",
      "Epoch 13, Batch 222/462, Loss: 0.8354692459106445\n",
      "Epoch 13, Batch 223/462, Loss: 0.6893551349639893\n",
      "Epoch 13, Batch 224/462, Loss: 0.7343987822532654\n",
      "Epoch 13, Batch 225/462, Loss: 0.7816470265388489\n",
      "Epoch 13, Batch 226/462, Loss: 0.7115249037742615\n",
      "Epoch 13, Batch 227/462, Loss: 0.8196669816970825\n",
      "Epoch 13, Batch 228/462, Loss: 0.8077356815338135\n",
      "Epoch 13, Batch 229/462, Loss: 0.7712652087211609\n",
      "Epoch 13, Batch 230/462, Loss: 0.7739385366439819\n",
      "Epoch 13, Batch 231/462, Loss: 0.7931572198867798\n",
      "Epoch 13, Batch 232/462, Loss: 0.8564559817314148\n",
      "Epoch 13, Batch 233/462, Loss: 0.785131573677063\n",
      "Epoch 13, Batch 234/462, Loss: 0.7528133392333984\n",
      "Epoch 13, Batch 235/462, Loss: 0.7309740781784058\n",
      "Epoch 13, Batch 236/462, Loss: 0.7013126015663147\n",
      "Epoch 13, Batch 237/462, Loss: 0.7729586958885193\n",
      "Epoch 13, Batch 238/462, Loss: 0.8058209419250488\n",
      "Epoch 13, Batch 239/462, Loss: 0.6700196266174316\n",
      "Epoch 13, Batch 240/462, Loss: 0.7489615678787231\n",
      "Epoch 13, Batch 241/462, Loss: 0.7786779999732971\n",
      "Epoch 13, Batch 242/462, Loss: 0.8180189728736877\n",
      "Epoch 13, Batch 243/462, Loss: 0.7309126853942871\n",
      "Epoch 13, Batch 244/462, Loss: 0.729468822479248\n",
      "Epoch 13, Batch 245/462, Loss: 0.8010627627372742\n",
      "Epoch 13, Batch 246/462, Loss: 0.6289485692977905\n",
      "Epoch 13, Batch 247/462, Loss: 0.834498941898346\n",
      "Epoch 13, Batch 248/462, Loss: 0.847048282623291\n",
      "Epoch 13, Batch 249/462, Loss: 0.9485761523246765\n",
      "Epoch 13, Batch 250/462, Loss: 0.7423791885375977\n",
      "Epoch 13, Batch 251/462, Loss: 0.6931342482566833\n",
      "Epoch 13, Batch 252/462, Loss: 0.672495424747467\n",
      "Epoch 13, Batch 253/462, Loss: 0.7548452615737915\n",
      "Epoch 13, Batch 254/462, Loss: 0.8853792548179626\n",
      "Epoch 13, Batch 255/462, Loss: 0.7204548120498657\n",
      "Epoch 13, Batch 256/462, Loss: 0.8839696049690247\n",
      "Epoch 13, Batch 257/462, Loss: 0.7849210500717163\n",
      "Epoch 13, Batch 258/462, Loss: 0.6935057044029236\n",
      "Epoch 13, Batch 259/462, Loss: 0.7972029447555542\n",
      "Epoch 13, Batch 260/462, Loss: 0.7642539143562317\n",
      "Epoch 13, Batch 261/462, Loss: 0.60130375623703\n",
      "Epoch 13, Batch 262/462, Loss: 0.6834840178489685\n",
      "Epoch 13, Batch 263/462, Loss: 0.8850502967834473\n",
      "Epoch 13, Batch 264/462, Loss: 1.0389468669891357\n",
      "Epoch 13, Batch 265/462, Loss: 0.7660697102546692\n",
      "Epoch 13, Batch 266/462, Loss: 0.7474825978279114\n",
      "Epoch 13, Batch 267/462, Loss: 0.7135720252990723\n",
      "Epoch 13, Batch 268/462, Loss: 0.8672987222671509\n",
      "Epoch 13, Batch 269/462, Loss: 0.701698362827301\n",
      "Epoch 13, Batch 270/462, Loss: 0.7571322321891785\n",
      "Epoch 13, Batch 271/462, Loss: 0.7180135250091553\n",
      "Epoch 13, Batch 272/462, Loss: 0.8135818243026733\n",
      "Epoch 13, Batch 273/462, Loss: 0.820869505405426\n",
      "Epoch 13, Batch 274/462, Loss: 0.7186456322669983\n",
      "Epoch 13, Batch 275/462, Loss: 0.7282012701034546\n",
      "Epoch 13, Batch 276/462, Loss: 0.7690321207046509\n",
      "Epoch 13, Batch 277/462, Loss: 0.8954033255577087\n",
      "Epoch 13, Batch 278/462, Loss: 0.838706374168396\n",
      "Epoch 13, Batch 279/462, Loss: 0.8359378576278687\n",
      "Epoch 13, Batch 280/462, Loss: 0.8563015460968018\n",
      "Epoch 13, Batch 281/462, Loss: 0.728733241558075\n",
      "Epoch 13, Batch 282/462, Loss: 0.7599467039108276\n",
      "Epoch 13, Batch 283/462, Loss: 0.7572581171989441\n",
      "Epoch 13, Batch 284/462, Loss: 0.7370659112930298\n",
      "Epoch 13, Batch 285/462, Loss: 0.7385708689689636\n",
      "Epoch 13, Batch 286/462, Loss: 1.0445661544799805\n",
      "Epoch 13, Batch 287/462, Loss: 0.8115069270133972\n",
      "Epoch 13, Batch 288/462, Loss: 0.8405869007110596\n",
      "Epoch 13, Batch 289/462, Loss: 0.5965582728385925\n",
      "Epoch 13, Batch 290/462, Loss: 0.6942139863967896\n",
      "Epoch 13, Batch 291/462, Loss: 0.5585548877716064\n",
      "Epoch 13, Batch 292/462, Loss: 0.7782316207885742\n",
      "Epoch 13, Batch 293/462, Loss: 0.7658683061599731\n",
      "Epoch 13, Batch 294/462, Loss: 0.8328590393066406\n",
      "Epoch 13, Batch 295/462, Loss: 0.7825230956077576\n",
      "Epoch 13, Batch 296/462, Loss: 0.833700954914093\n",
      "Epoch 13, Batch 297/462, Loss: 0.7227280139923096\n",
      "Epoch 13, Batch 298/462, Loss: 0.5923217535018921\n",
      "Epoch 13, Batch 299/462, Loss: 1.1345864534378052\n",
      "Epoch 13, Batch 300/462, Loss: 0.9792619943618774\n",
      "Epoch 13, Batch 301/462, Loss: 0.7579208612442017\n",
      "Epoch 13, Batch 302/462, Loss: 0.7389875650405884\n",
      "Epoch 13, Batch 303/462, Loss: 0.8148865699768066\n",
      "Epoch 13, Batch 304/462, Loss: 0.8356285691261292\n",
      "Epoch 13, Batch 305/462, Loss: 0.7540503144264221\n",
      "Epoch 13, Batch 306/462, Loss: 0.691845715045929\n",
      "Epoch 13, Batch 307/462, Loss: 0.8168754577636719\n",
      "Epoch 13, Batch 308/462, Loss: 0.7474431395530701\n",
      "Epoch 13, Batch 309/462, Loss: 0.8071900010108948\n",
      "Epoch 13, Batch 310/462, Loss: 0.6714013814926147\n",
      "Epoch 13, Batch 311/462, Loss: 0.760534405708313\n",
      "Epoch 13, Batch 312/462, Loss: 0.7527048587799072\n",
      "Epoch 13, Batch 313/462, Loss: 0.7749600410461426\n",
      "Epoch 13, Batch 314/462, Loss: 0.8363606333732605\n",
      "Epoch 13, Batch 315/462, Loss: 0.9716336131095886\n",
      "Epoch 13, Batch 316/462, Loss: 0.833312451839447\n",
      "Epoch 13, Batch 317/462, Loss: 0.7364650964736938\n",
      "Epoch 13, Batch 318/462, Loss: 0.9157152771949768\n",
      "Epoch 13, Batch 319/462, Loss: 0.7351967096328735\n",
      "Epoch 13, Batch 320/462, Loss: 0.6830808520317078\n",
      "Epoch 13, Batch 321/462, Loss: 0.7440856695175171\n",
      "Epoch 13, Batch 322/462, Loss: 0.7657960057258606\n",
      "Epoch 13, Batch 323/462, Loss: 0.6684132814407349\n",
      "Epoch 13, Batch 324/462, Loss: 0.9151745438575745\n",
      "Epoch 13, Batch 325/462, Loss: 0.6892779469490051\n",
      "Epoch 13, Batch 326/462, Loss: 0.8413671851158142\n",
      "Epoch 13, Batch 327/462, Loss: 0.690089225769043\n",
      "Epoch 13, Batch 328/462, Loss: 0.8035817742347717\n",
      "Epoch 13, Batch 329/462, Loss: 0.7918116450309753\n",
      "Epoch 13, Batch 330/462, Loss: 0.7801265716552734\n",
      "Epoch 13, Batch 331/462, Loss: 0.7289474010467529\n",
      "Epoch 13, Batch 332/462, Loss: 0.81148761510849\n",
      "Epoch 13, Batch 333/462, Loss: 0.7783597707748413\n",
      "Epoch 13, Batch 334/462, Loss: 0.846390426158905\n",
      "Epoch 13, Batch 335/462, Loss: 0.7064061164855957\n",
      "Epoch 13, Batch 336/462, Loss: 0.9160103797912598\n",
      "Epoch 13, Batch 337/462, Loss: 0.7987072467803955\n",
      "Epoch 13, Batch 338/462, Loss: 0.6372042894363403\n",
      "Epoch 13, Batch 339/462, Loss: 0.7108790278434753\n",
      "Epoch 13, Batch 340/462, Loss: 0.710189163684845\n",
      "Epoch 13, Batch 341/462, Loss: 0.9248942732810974\n",
      "Epoch 13, Batch 342/462, Loss: 0.808016836643219\n",
      "Epoch 13, Batch 343/462, Loss: 0.924857497215271\n",
      "Epoch 13, Batch 344/462, Loss: 0.7382307052612305\n",
      "Epoch 13, Batch 345/462, Loss: 0.6780915260314941\n",
      "Epoch 13, Batch 346/462, Loss: 0.6851428151130676\n",
      "Epoch 13, Batch 347/462, Loss: 0.7108782529830933\n",
      "Epoch 13, Batch 348/462, Loss: 0.8334451913833618\n",
      "Epoch 13, Batch 349/462, Loss: 0.6629219055175781\n",
      "Epoch 13, Batch 350/462, Loss: 0.794041633605957\n",
      "Epoch 13, Batch 351/462, Loss: 0.8743780255317688\n",
      "Epoch 13, Batch 352/462, Loss: 0.8112069368362427\n",
      "Epoch 13, Batch 353/462, Loss: 0.9719374775886536\n",
      "Epoch 13, Batch 354/462, Loss: 0.7042607665061951\n",
      "Epoch 13, Batch 355/462, Loss: 0.6016026735305786\n",
      "Epoch 13, Batch 356/462, Loss: 0.7724709510803223\n",
      "Epoch 13, Batch 357/462, Loss: 0.8956709504127502\n",
      "Epoch 13, Batch 358/462, Loss: 0.7957126498222351\n",
      "Epoch 13, Batch 359/462, Loss: 0.817286491394043\n",
      "Epoch 13, Batch 360/462, Loss: 0.8257858157157898\n",
      "Epoch 13, Batch 361/462, Loss: 0.7690470814704895\n",
      "Epoch 13, Batch 362/462, Loss: 0.7780613303184509\n",
      "Epoch 13, Batch 363/462, Loss: 0.7780421376228333\n",
      "Epoch 13, Batch 364/462, Loss: 0.7978734374046326\n",
      "Epoch 13, Batch 365/462, Loss: 0.7752526998519897\n",
      "Epoch 13, Batch 366/462, Loss: 0.8182260394096375\n",
      "Epoch 13, Batch 367/462, Loss: 0.8437278270721436\n",
      "Epoch 13, Batch 368/462, Loss: 0.8301185369491577\n",
      "Epoch 13, Batch 369/462, Loss: 0.6812813878059387\n",
      "Epoch 13, Batch 370/462, Loss: 0.7402033805847168\n",
      "Epoch 13, Batch 371/462, Loss: 0.6923503875732422\n",
      "Epoch 13, Batch 372/462, Loss: 0.82486891746521\n",
      "Epoch 13, Batch 373/462, Loss: 0.7890427112579346\n",
      "Epoch 13, Batch 374/462, Loss: 0.8339832425117493\n",
      "Epoch 13, Batch 375/462, Loss: 0.669489860534668\n",
      "Epoch 13, Batch 376/462, Loss: 0.6988139152526855\n",
      "Epoch 13, Batch 377/462, Loss: 0.8716880679130554\n",
      "Epoch 13, Batch 378/462, Loss: 0.5840007662773132\n",
      "Epoch 13, Batch 379/462, Loss: 0.7514287829399109\n",
      "Epoch 13, Batch 380/462, Loss: 0.8202129006385803\n",
      "Epoch 13, Batch 381/462, Loss: 0.8030440807342529\n",
      "Epoch 13, Batch 382/462, Loss: 0.8195390105247498\n",
      "Epoch 13, Batch 383/462, Loss: 0.7885597348213196\n",
      "Epoch 13, Batch 384/462, Loss: 0.7964892983436584\n",
      "Epoch 13, Batch 385/462, Loss: 0.96388840675354\n",
      "Epoch 13, Batch 386/462, Loss: 0.8887149095535278\n",
      "Epoch 13, Batch 387/462, Loss: 0.717018723487854\n",
      "Epoch 13, Batch 388/462, Loss: 0.6512752175331116\n",
      "Epoch 13, Batch 389/462, Loss: 0.6006039977073669\n",
      "Epoch 13, Batch 390/462, Loss: 0.8893382549285889\n",
      "Epoch 13, Batch 391/462, Loss: 0.7909450531005859\n",
      "Epoch 13, Batch 392/462, Loss: 0.7375237345695496\n",
      "Epoch 13, Batch 393/462, Loss: 0.7311087846755981\n",
      "Epoch 13, Batch 394/462, Loss: 0.7601177096366882\n",
      "Epoch 13, Batch 395/462, Loss: 0.8423604965209961\n",
      "Epoch 13, Batch 396/462, Loss: 0.7422769069671631\n",
      "Epoch 13, Batch 397/462, Loss: 0.8804380297660828\n",
      "Epoch 13, Batch 398/462, Loss: 0.8141834735870361\n",
      "Epoch 13, Batch 399/462, Loss: 0.769496500492096\n",
      "Epoch 13, Batch 400/462, Loss: 0.843429446220398\n",
      "Epoch 13, Batch 401/462, Loss: 0.7102760672569275\n",
      "Epoch 13, Batch 402/462, Loss: 0.7733743190765381\n",
      "Epoch 13, Batch 403/462, Loss: 0.6502949595451355\n",
      "Epoch 13, Batch 404/462, Loss: 0.6981655359268188\n",
      "Epoch 13, Batch 405/462, Loss: 0.7003174424171448\n",
      "Epoch 13, Batch 406/462, Loss: 0.7120971083641052\n",
      "Epoch 13, Batch 407/462, Loss: 0.879287600517273\n",
      "Epoch 13, Batch 408/462, Loss: 0.7976778149604797\n",
      "Epoch 13, Batch 409/462, Loss: 0.5651268362998962\n",
      "Epoch 13, Batch 410/462, Loss: 0.7665789723396301\n",
      "Epoch 13, Batch 411/462, Loss: 0.8102789521217346\n",
      "Epoch 13, Batch 412/462, Loss: 0.8534917831420898\n",
      "Epoch 13, Batch 413/462, Loss: 0.7398645877838135\n",
      "Epoch 13, Batch 414/462, Loss: 0.6728906035423279\n",
      "Epoch 13, Batch 415/462, Loss: 0.879430890083313\n",
      "Epoch 13, Batch 416/462, Loss: 0.8455756902694702\n",
      "Epoch 13, Batch 417/462, Loss: 0.8177188634872437\n",
      "Epoch 13, Batch 418/462, Loss: 0.8266083002090454\n",
      "Epoch 13, Batch 419/462, Loss: 0.7214704155921936\n",
      "Epoch 13, Batch 420/462, Loss: 0.7800532579421997\n",
      "Epoch 13, Batch 421/462, Loss: 0.792195737361908\n",
      "Epoch 13, Batch 422/462, Loss: 0.6332130432128906\n",
      "Epoch 13, Batch 423/462, Loss: 0.7665194869041443\n",
      "Epoch 13, Batch 424/462, Loss: 0.7531940340995789\n",
      "Epoch 13, Batch 425/462, Loss: 0.7438849806785583\n",
      "Epoch 13, Batch 426/462, Loss: 0.71898353099823\n",
      "Epoch 13, Batch 427/462, Loss: 0.6645414233207703\n",
      "Epoch 13, Batch 428/462, Loss: 0.7861437797546387\n",
      "Epoch 13, Batch 429/462, Loss: 0.7071863412857056\n",
      "Epoch 13, Batch 430/462, Loss: 0.7566024661064148\n",
      "Epoch 13, Batch 431/462, Loss: 0.7794126272201538\n",
      "Epoch 13, Batch 432/462, Loss: 0.6004478931427002\n",
      "Epoch 13, Batch 433/462, Loss: 0.727115273475647\n",
      "Epoch 13, Batch 434/462, Loss: 0.9056380391120911\n",
      "Epoch 13, Batch 435/462, Loss: 0.7427964210510254\n",
      "Epoch 13, Batch 436/462, Loss: 0.6948215961456299\n",
      "Epoch 13, Batch 437/462, Loss: 0.8173428177833557\n",
      "Epoch 13, Batch 438/462, Loss: 0.7749391198158264\n",
      "Epoch 13, Batch 439/462, Loss: 0.786432147026062\n",
      "Epoch 13, Batch 440/462, Loss: 0.7350691556930542\n",
      "Epoch 13, Batch 441/462, Loss: 0.8625460267066956\n",
      "Epoch 13, Batch 442/462, Loss: 0.7366151809692383\n",
      "Epoch 13, Batch 443/462, Loss: 0.6361605525016785\n",
      "Epoch 13, Batch 444/462, Loss: 0.8016394376754761\n",
      "Epoch 13, Batch 445/462, Loss: 0.7620724439620972\n",
      "Epoch 13, Batch 446/462, Loss: 0.7494629621505737\n",
      "Epoch 13, Batch 447/462, Loss: 0.8228557109832764\n",
      "Epoch 13, Batch 448/462, Loss: 0.775288462638855\n",
      "Epoch 13, Batch 449/462, Loss: 0.8360472321510315\n",
      "Epoch 13, Batch 450/462, Loss: 0.7577155232429504\n",
      "Epoch 13, Batch 451/462, Loss: 0.9142134785652161\n",
      "Epoch 13, Batch 452/462, Loss: 0.6597875356674194\n",
      "Epoch 13, Batch 453/462, Loss: 0.8229513168334961\n",
      "Epoch 13, Batch 454/462, Loss: 0.848432719707489\n",
      "Epoch 13, Batch 455/462, Loss: 0.8379224538803101\n",
      "Epoch 13, Batch 456/462, Loss: 0.7155612111091614\n",
      "Epoch 13, Batch 457/462, Loss: 0.8527962565422058\n",
      "Epoch 13, Batch 458/462, Loss: 0.7511761784553528\n",
      "Epoch 13, Batch 459/462, Loss: 0.7331225275993347\n",
      "Epoch 13, Batch 460/462, Loss: 0.6336522102355957\n",
      "Epoch 13, Batch 461/462, Loss: 0.6804744601249695\n",
      "Epoch 13, Batch 462/462, Loss: 0.7940569519996643\n",
      "Epoch 13, Loss: 361.96684044599533\n",
      "Epoch 14, Batch 1/462, Loss: 0.6404361724853516\n",
      "Epoch 14, Batch 2/462, Loss: 0.8408478498458862\n",
      "Epoch 14, Batch 3/462, Loss: 0.630248486995697\n",
      "Epoch 14, Batch 4/462, Loss: 0.7272753715515137\n",
      "Epoch 14, Batch 5/462, Loss: 0.7408339977264404\n",
      "Epoch 14, Batch 6/462, Loss: 0.8279020190238953\n",
      "Epoch 14, Batch 7/462, Loss: 0.7914525866508484\n",
      "Epoch 14, Batch 8/462, Loss: 0.6926200985908508\n",
      "Epoch 14, Batch 9/462, Loss: 0.7502286434173584\n",
      "Epoch 14, Batch 10/462, Loss: 1.028335690498352\n",
      "Epoch 14, Batch 11/462, Loss: 0.698573887348175\n",
      "Epoch 14, Batch 12/462, Loss: 0.8345943689346313\n",
      "Epoch 14, Batch 13/462, Loss: 0.9003175497055054\n",
      "Epoch 14, Batch 14/462, Loss: 0.7242652177810669\n",
      "Epoch 14, Batch 15/462, Loss: 0.764435350894928\n",
      "Epoch 14, Batch 16/462, Loss: 0.7482951879501343\n",
      "Epoch 14, Batch 17/462, Loss: 0.6534873843193054\n",
      "Epoch 14, Batch 18/462, Loss: 0.7834188342094421\n",
      "Epoch 14, Batch 19/462, Loss: 0.6876367330551147\n",
      "Epoch 14, Batch 20/462, Loss: 0.7832038998603821\n",
      "Epoch 14, Batch 21/462, Loss: 0.8263295888900757\n",
      "Epoch 14, Batch 22/462, Loss: 0.7360295057296753\n",
      "Epoch 14, Batch 23/462, Loss: 0.878908097743988\n",
      "Epoch 14, Batch 24/462, Loss: 0.9155244827270508\n",
      "Epoch 14, Batch 25/462, Loss: 0.7314164042472839\n",
      "Epoch 14, Batch 26/462, Loss: 0.8539875149726868\n",
      "Epoch 14, Batch 27/462, Loss: 0.9030489325523376\n",
      "Epoch 14, Batch 28/462, Loss: 0.7970256805419922\n",
      "Epoch 14, Batch 29/462, Loss: 0.8154214024543762\n",
      "Epoch 14, Batch 30/462, Loss: 0.7485532164573669\n",
      "Epoch 14, Batch 31/462, Loss: 0.7403356432914734\n",
      "Epoch 14, Batch 32/462, Loss: 0.6212942004203796\n",
      "Epoch 14, Batch 33/462, Loss: 0.675586462020874\n",
      "Epoch 14, Batch 34/462, Loss: 0.7820827960968018\n",
      "Epoch 14, Batch 35/462, Loss: 0.8118156790733337\n",
      "Epoch 14, Batch 36/462, Loss: 0.8018997311592102\n",
      "Epoch 14, Batch 37/462, Loss: 0.7382426261901855\n",
      "Epoch 14, Batch 38/462, Loss: 0.7098173499107361\n",
      "Epoch 14, Batch 39/462, Loss: 0.8793300986289978\n",
      "Epoch 14, Batch 40/462, Loss: 0.9191462993621826\n",
      "Epoch 14, Batch 41/462, Loss: 0.6853848099708557\n",
      "Epoch 14, Batch 42/462, Loss: 0.8216837644577026\n",
      "Epoch 14, Batch 43/462, Loss: 0.8039816617965698\n",
      "Epoch 14, Batch 44/462, Loss: 0.8530015349388123\n",
      "Epoch 14, Batch 45/462, Loss: 0.8236100077629089\n",
      "Epoch 14, Batch 46/462, Loss: 0.8202620148658752\n",
      "Epoch 14, Batch 47/462, Loss: 0.8544255495071411\n",
      "Epoch 14, Batch 48/462, Loss: 0.9031829833984375\n",
      "Epoch 14, Batch 49/462, Loss: 0.8163763880729675\n",
      "Epoch 14, Batch 50/462, Loss: 0.7794856429100037\n",
      "Epoch 14, Batch 51/462, Loss: 0.7304881811141968\n",
      "Epoch 14, Batch 52/462, Loss: 0.8361534476280212\n",
      "Epoch 14, Batch 53/462, Loss: 0.7392388582229614\n",
      "Epoch 14, Batch 54/462, Loss: 0.7303152084350586\n",
      "Epoch 14, Batch 55/462, Loss: 0.6941739916801453\n",
      "Epoch 14, Batch 56/462, Loss: 0.7637024521827698\n",
      "Epoch 14, Batch 57/462, Loss: 0.7788581848144531\n",
      "Epoch 14, Batch 58/462, Loss: 0.7941427230834961\n",
      "Epoch 14, Batch 59/462, Loss: 0.8572896718978882\n",
      "Epoch 14, Batch 60/462, Loss: 1.0193241834640503\n",
      "Epoch 14, Batch 61/462, Loss: 0.7380892038345337\n",
      "Epoch 14, Batch 62/462, Loss: 0.8421503901481628\n",
      "Epoch 14, Batch 63/462, Loss: 0.7164016366004944\n",
      "Epoch 14, Batch 64/462, Loss: 0.9629627466201782\n",
      "Epoch 14, Batch 65/462, Loss: 0.7838221192359924\n",
      "Epoch 14, Batch 66/462, Loss: 0.7173514366149902\n",
      "Epoch 14, Batch 67/462, Loss: 0.7246338129043579\n",
      "Epoch 14, Batch 68/462, Loss: 0.5804327130317688\n",
      "Epoch 14, Batch 69/462, Loss: 0.7793728709220886\n",
      "Epoch 14, Batch 70/462, Loss: 0.7046234607696533\n",
      "Epoch 14, Batch 71/462, Loss: 0.8830377459526062\n",
      "Epoch 14, Batch 72/462, Loss: 0.859462559223175\n",
      "Epoch 14, Batch 73/462, Loss: 0.8408899903297424\n",
      "Epoch 14, Batch 74/462, Loss: 0.7337253093719482\n",
      "Epoch 14, Batch 75/462, Loss: 0.8061628341674805\n",
      "Epoch 14, Batch 76/462, Loss: 0.6876800060272217\n",
      "Epoch 14, Batch 77/462, Loss: 0.6386403441429138\n",
      "Epoch 14, Batch 78/462, Loss: 0.7527675032615662\n",
      "Epoch 14, Batch 79/462, Loss: 0.8454368710517883\n",
      "Epoch 14, Batch 80/462, Loss: 0.7041218876838684\n",
      "Epoch 14, Batch 81/462, Loss: 0.6518135070800781\n",
      "Epoch 14, Batch 82/462, Loss: 0.7842819094657898\n",
      "Epoch 14, Batch 83/462, Loss: 0.6964337825775146\n",
      "Epoch 14, Batch 84/462, Loss: 0.8374612331390381\n",
      "Epoch 14, Batch 85/462, Loss: 0.7034168243408203\n",
      "Epoch 14, Batch 86/462, Loss: 0.8103233575820923\n",
      "Epoch 14, Batch 87/462, Loss: 0.7623504400253296\n",
      "Epoch 14, Batch 88/462, Loss: 0.8293619155883789\n",
      "Epoch 14, Batch 89/462, Loss: 0.8380900621414185\n",
      "Epoch 14, Batch 90/462, Loss: 0.7752133011817932\n",
      "Epoch 14, Batch 91/462, Loss: 0.6541956067085266\n",
      "Epoch 14, Batch 92/462, Loss: 0.8135648965835571\n",
      "Epoch 14, Batch 93/462, Loss: 0.7646850347518921\n",
      "Epoch 14, Batch 94/462, Loss: 0.7017039060592651\n",
      "Epoch 14, Batch 95/462, Loss: 0.6724866628646851\n",
      "Epoch 14, Batch 96/462, Loss: 0.8736754655838013\n",
      "Epoch 14, Batch 97/462, Loss: 0.8284941911697388\n",
      "Epoch 14, Batch 98/462, Loss: 0.7069423198699951\n",
      "Epoch 14, Batch 99/462, Loss: 0.6642833948135376\n",
      "Epoch 14, Batch 100/462, Loss: 0.7817894816398621\n",
      "Epoch 14, Batch 101/462, Loss: 0.6510246396064758\n",
      "Epoch 14, Batch 102/462, Loss: 0.7662500143051147\n",
      "Epoch 14, Batch 103/462, Loss: 0.870838463306427\n",
      "Epoch 14, Batch 104/462, Loss: 0.6966708898544312\n",
      "Epoch 14, Batch 105/462, Loss: 0.7749175429344177\n",
      "Epoch 14, Batch 106/462, Loss: 0.8307331800460815\n",
      "Epoch 14, Batch 107/462, Loss: 0.7705541253089905\n",
      "Epoch 14, Batch 108/462, Loss: 0.8756129741668701\n",
      "Epoch 14, Batch 109/462, Loss: 0.8050351738929749\n",
      "Epoch 14, Batch 110/462, Loss: 0.8059240579605103\n",
      "Epoch 14, Batch 111/462, Loss: 0.8756005764007568\n",
      "Epoch 14, Batch 112/462, Loss: 0.6727513074874878\n",
      "Epoch 14, Batch 113/462, Loss: 0.819787859916687\n",
      "Epoch 14, Batch 114/462, Loss: 0.8169386386871338\n",
      "Epoch 14, Batch 115/462, Loss: 0.7729169726371765\n",
      "Epoch 14, Batch 116/462, Loss: 0.8742834329605103\n",
      "Epoch 14, Batch 117/462, Loss: 0.8545840382575989\n",
      "Epoch 14, Batch 118/462, Loss: 0.754787027835846\n",
      "Epoch 14, Batch 119/462, Loss: 0.6561098098754883\n",
      "Epoch 14, Batch 120/462, Loss: 0.7186420559883118\n",
      "Epoch 14, Batch 121/462, Loss: 0.8002245426177979\n",
      "Epoch 14, Batch 122/462, Loss: 0.6800402402877808\n",
      "Epoch 14, Batch 123/462, Loss: 0.8480753898620605\n",
      "Epoch 14, Batch 124/462, Loss: 0.8261576890945435\n",
      "Epoch 14, Batch 125/462, Loss: 0.9437740445137024\n",
      "Epoch 14, Batch 126/462, Loss: 0.814149796962738\n",
      "Epoch 14, Batch 127/462, Loss: 0.7543138861656189\n",
      "Epoch 14, Batch 128/462, Loss: 0.6949137449264526\n",
      "Epoch 14, Batch 129/462, Loss: 0.7512366771697998\n",
      "Epoch 14, Batch 130/462, Loss: 0.6849650740623474\n",
      "Epoch 14, Batch 131/462, Loss: 0.8092934489250183\n",
      "Epoch 14, Batch 132/462, Loss: 0.738664448261261\n",
      "Epoch 14, Batch 133/462, Loss: 0.7475162148475647\n",
      "Epoch 14, Batch 134/462, Loss: 0.6158714890480042\n",
      "Epoch 14, Batch 135/462, Loss: 0.7277252078056335\n",
      "Epoch 14, Batch 136/462, Loss: 0.6473212242126465\n",
      "Epoch 14, Batch 137/462, Loss: 0.8389081358909607\n",
      "Epoch 14, Batch 138/462, Loss: 0.8779764771461487\n",
      "Epoch 14, Batch 139/462, Loss: 0.7582912445068359\n",
      "Epoch 14, Batch 140/462, Loss: 0.766331672668457\n",
      "Epoch 14, Batch 141/462, Loss: 0.9056318998336792\n",
      "Epoch 14, Batch 142/462, Loss: 0.6845492124557495\n",
      "Epoch 14, Batch 143/462, Loss: 0.7464247345924377\n",
      "Epoch 14, Batch 144/462, Loss: 0.9384990334510803\n",
      "Epoch 14, Batch 145/462, Loss: 1.044856071472168\n",
      "Epoch 14, Batch 146/462, Loss: 0.721383810043335\n",
      "Epoch 14, Batch 147/462, Loss: 0.7765203714370728\n",
      "Epoch 14, Batch 148/462, Loss: 0.7256885170936584\n",
      "Epoch 14, Batch 149/462, Loss: 0.87363600730896\n",
      "Epoch 14, Batch 150/462, Loss: 0.7408434748649597\n",
      "Epoch 14, Batch 151/462, Loss: 0.7169792652130127\n",
      "Epoch 14, Batch 152/462, Loss: 0.8084099888801575\n",
      "Epoch 14, Batch 153/462, Loss: 1.0617725849151611\n",
      "Epoch 14, Batch 154/462, Loss: 0.6173905730247498\n",
      "Epoch 14, Batch 155/462, Loss: 0.783790647983551\n",
      "Epoch 14, Batch 156/462, Loss: 0.7619916200637817\n",
      "Epoch 14, Batch 157/462, Loss: 0.7332412600517273\n",
      "Epoch 14, Batch 158/462, Loss: 0.6979957818984985\n",
      "Epoch 14, Batch 159/462, Loss: 0.8543584942817688\n",
      "Epoch 14, Batch 160/462, Loss: 0.84858238697052\n",
      "Epoch 14, Batch 161/462, Loss: 0.7770211696624756\n",
      "Epoch 14, Batch 162/462, Loss: 0.9326916933059692\n",
      "Epoch 14, Batch 163/462, Loss: 0.6788057684898376\n",
      "Epoch 14, Batch 164/462, Loss: 0.704582929611206\n",
      "Epoch 14, Batch 165/462, Loss: 0.7071496248245239\n",
      "Epoch 14, Batch 166/462, Loss: 0.9474411010742188\n",
      "Epoch 14, Batch 167/462, Loss: 0.7897971272468567\n",
      "Epoch 14, Batch 168/462, Loss: 0.8305059671401978\n",
      "Epoch 14, Batch 169/462, Loss: 0.6474993824958801\n",
      "Epoch 14, Batch 170/462, Loss: 0.7746859192848206\n",
      "Epoch 14, Batch 171/462, Loss: 0.7367357015609741\n",
      "Epoch 14, Batch 172/462, Loss: 0.7483643293380737\n",
      "Epoch 14, Batch 173/462, Loss: 0.7779020071029663\n",
      "Epoch 14, Batch 174/462, Loss: 0.8960396647453308\n",
      "Epoch 14, Batch 175/462, Loss: 0.8289592266082764\n",
      "Epoch 14, Batch 176/462, Loss: 0.7809414267539978\n",
      "Epoch 14, Batch 177/462, Loss: 0.7954428791999817\n",
      "Epoch 14, Batch 178/462, Loss: 0.6778115630149841\n",
      "Epoch 14, Batch 179/462, Loss: 0.8389713764190674\n",
      "Epoch 14, Batch 180/462, Loss: 0.7351090312004089\n",
      "Epoch 14, Batch 181/462, Loss: 0.7845780849456787\n",
      "Epoch 14, Batch 182/462, Loss: 0.7863874435424805\n",
      "Epoch 14, Batch 183/462, Loss: 0.8296127915382385\n",
      "Epoch 14, Batch 184/462, Loss: 0.8092215657234192\n",
      "Epoch 14, Batch 185/462, Loss: 0.7846396565437317\n",
      "Epoch 14, Batch 186/462, Loss: 0.9712929725646973\n",
      "Epoch 14, Batch 187/462, Loss: 0.7972986102104187\n",
      "Epoch 14, Batch 188/462, Loss: 0.8160203695297241\n",
      "Epoch 14, Batch 189/462, Loss: 0.7163210511207581\n",
      "Epoch 14, Batch 190/462, Loss: 0.7263023257255554\n",
      "Epoch 14, Batch 191/462, Loss: 0.8726660013198853\n",
      "Epoch 14, Batch 192/462, Loss: 0.6184886693954468\n",
      "Epoch 14, Batch 193/462, Loss: 0.6469229459762573\n",
      "Epoch 14, Batch 194/462, Loss: 0.8282201886177063\n",
      "Epoch 14, Batch 195/462, Loss: 0.8144420385360718\n",
      "Epoch 14, Batch 196/462, Loss: 0.7765176296234131\n",
      "Epoch 14, Batch 197/462, Loss: 0.5993487238883972\n",
      "Epoch 14, Batch 198/462, Loss: 0.8406401872634888\n",
      "Epoch 14, Batch 199/462, Loss: 0.8361677527427673\n",
      "Epoch 14, Batch 200/462, Loss: 0.8551796674728394\n",
      "Epoch 14, Batch 201/462, Loss: 0.8111602067947388\n",
      "Epoch 14, Batch 202/462, Loss: 0.7845021486282349\n",
      "Epoch 14, Batch 203/462, Loss: 0.7179629802703857\n",
      "Epoch 14, Batch 204/462, Loss: 0.7627924680709839\n",
      "Epoch 14, Batch 205/462, Loss: 0.7217616438865662\n",
      "Epoch 14, Batch 206/462, Loss: 0.5869499444961548\n",
      "Epoch 14, Batch 207/462, Loss: 0.7283124327659607\n",
      "Epoch 14, Batch 208/462, Loss: 0.852692723274231\n",
      "Epoch 14, Batch 209/462, Loss: 0.8719853162765503\n",
      "Epoch 14, Batch 210/462, Loss: 0.7144747972488403\n",
      "Epoch 14, Batch 211/462, Loss: 0.8217728137969971\n",
      "Epoch 14, Batch 212/462, Loss: 0.6823520660400391\n",
      "Epoch 14, Batch 213/462, Loss: 0.7227756977081299\n",
      "Epoch 14, Batch 214/462, Loss: 0.8802867531776428\n",
      "Epoch 14, Batch 215/462, Loss: 0.669908344745636\n",
      "Epoch 14, Batch 216/462, Loss: 0.7506996393203735\n",
      "Epoch 14, Batch 217/462, Loss: 0.745393693447113\n",
      "Epoch 14, Batch 218/462, Loss: 0.7920950651168823\n",
      "Epoch 14, Batch 219/462, Loss: 0.8612128496170044\n",
      "Epoch 14, Batch 220/462, Loss: 0.9760181903839111\n",
      "Epoch 14, Batch 221/462, Loss: 0.7471686005592346\n",
      "Epoch 14, Batch 222/462, Loss: 0.7637248039245605\n",
      "Epoch 14, Batch 223/462, Loss: 0.7927795648574829\n",
      "Epoch 14, Batch 224/462, Loss: 0.9537549018859863\n",
      "Epoch 14, Batch 225/462, Loss: 0.7111566066741943\n",
      "Epoch 14, Batch 226/462, Loss: 0.7207483649253845\n",
      "Epoch 14, Batch 227/462, Loss: 0.8442676067352295\n",
      "Epoch 14, Batch 228/462, Loss: 0.899537205696106\n",
      "Epoch 14, Batch 229/462, Loss: 0.8211714029312134\n",
      "Epoch 14, Batch 230/462, Loss: 0.692631185054779\n",
      "Epoch 14, Batch 231/462, Loss: 0.967369556427002\n",
      "Epoch 14, Batch 232/462, Loss: 0.6970596313476562\n",
      "Epoch 14, Batch 233/462, Loss: 0.6988649368286133\n",
      "Epoch 14, Batch 234/462, Loss: 0.6650463938713074\n",
      "Epoch 14, Batch 235/462, Loss: 0.7429112792015076\n",
      "Epoch 14, Batch 236/462, Loss: 0.6982487440109253\n",
      "Epoch 14, Batch 237/462, Loss: 0.8865270018577576\n",
      "Epoch 14, Batch 238/462, Loss: 0.7533336281776428\n",
      "Epoch 14, Batch 239/462, Loss: 0.7810570001602173\n",
      "Epoch 14, Batch 240/462, Loss: 0.8314366340637207\n",
      "Epoch 14, Batch 241/462, Loss: 0.6263086199760437\n",
      "Epoch 14, Batch 242/462, Loss: 0.7217139601707458\n",
      "Epoch 14, Batch 243/462, Loss: 0.6628928780555725\n",
      "Epoch 14, Batch 244/462, Loss: 0.8503746390342712\n",
      "Epoch 14, Batch 245/462, Loss: 0.680625319480896\n",
      "Epoch 14, Batch 246/462, Loss: 0.8625599145889282\n",
      "Epoch 14, Batch 247/462, Loss: 0.8410559892654419\n",
      "Epoch 14, Batch 248/462, Loss: 0.780150294303894\n",
      "Epoch 14, Batch 249/462, Loss: 0.8537968397140503\n",
      "Epoch 14, Batch 250/462, Loss: 0.6874388456344604\n",
      "Epoch 14, Batch 251/462, Loss: 0.8234392404556274\n",
      "Epoch 14, Batch 252/462, Loss: 0.691774308681488\n",
      "Epoch 14, Batch 253/462, Loss: 0.7411279678344727\n",
      "Epoch 14, Batch 254/462, Loss: 0.8563275933265686\n",
      "Epoch 14, Batch 255/462, Loss: 0.7955805659294128\n",
      "Epoch 14, Batch 256/462, Loss: 0.7741642594337463\n",
      "Epoch 14, Batch 257/462, Loss: 0.6865832209587097\n",
      "Epoch 14, Batch 258/462, Loss: 0.9097722172737122\n",
      "Epoch 14, Batch 259/462, Loss: 0.8666896820068359\n",
      "Epoch 14, Batch 260/462, Loss: 0.6694239974021912\n",
      "Epoch 14, Batch 261/462, Loss: 0.7711306214332581\n",
      "Epoch 14, Batch 262/462, Loss: 0.9112401008605957\n",
      "Epoch 14, Batch 263/462, Loss: 0.8429421186447144\n",
      "Epoch 14, Batch 264/462, Loss: 1.0091702938079834\n",
      "Epoch 14, Batch 265/462, Loss: 0.6468082666397095\n",
      "Epoch 14, Batch 266/462, Loss: 0.9060995578765869\n",
      "Epoch 14, Batch 267/462, Loss: 0.7403411269187927\n",
      "Epoch 14, Batch 268/462, Loss: 0.7856740355491638\n",
      "Epoch 14, Batch 269/462, Loss: 0.922300398349762\n",
      "Epoch 14, Batch 270/462, Loss: 0.7533459067344666\n",
      "Epoch 14, Batch 271/462, Loss: 0.8711479306221008\n",
      "Epoch 14, Batch 272/462, Loss: 0.8344280123710632\n",
      "Epoch 14, Batch 273/462, Loss: 0.8096031546592712\n",
      "Epoch 14, Batch 274/462, Loss: 0.7259160280227661\n",
      "Epoch 14, Batch 275/462, Loss: 0.7620458602905273\n",
      "Epoch 14, Batch 276/462, Loss: 0.8011776804924011\n",
      "Epoch 14, Batch 277/462, Loss: 0.7330862879753113\n",
      "Epoch 14, Batch 278/462, Loss: 0.8185256123542786\n",
      "Epoch 14, Batch 279/462, Loss: 0.9431405663490295\n",
      "Epoch 14, Batch 280/462, Loss: 0.7759881615638733\n",
      "Epoch 14, Batch 281/462, Loss: 0.7686182856559753\n",
      "Epoch 14, Batch 282/462, Loss: 0.8909102082252502\n",
      "Epoch 14, Batch 283/462, Loss: 0.6969908475875854\n",
      "Epoch 14, Batch 284/462, Loss: 0.8052095174789429\n",
      "Epoch 14, Batch 285/462, Loss: 0.7514727711677551\n",
      "Epoch 14, Batch 286/462, Loss: 0.6984657645225525\n",
      "Epoch 14, Batch 287/462, Loss: 0.7453583478927612\n",
      "Epoch 14, Batch 288/462, Loss: 0.8860070109367371\n",
      "Epoch 14, Batch 289/462, Loss: 0.9199205040931702\n",
      "Epoch 14, Batch 290/462, Loss: 0.6301501393318176\n",
      "Epoch 14, Batch 291/462, Loss: 0.6836502552032471\n",
      "Epoch 14, Batch 292/462, Loss: 0.5040716528892517\n",
      "Epoch 14, Batch 293/462, Loss: 0.7274612188339233\n",
      "Epoch 14, Batch 294/462, Loss: 0.7784717679023743\n",
      "Epoch 14, Batch 295/462, Loss: 0.7128453254699707\n",
      "Epoch 14, Batch 296/462, Loss: 0.8465723991394043\n",
      "Epoch 14, Batch 297/462, Loss: 0.7997246384620667\n",
      "Epoch 14, Batch 298/462, Loss: 0.8157530426979065\n",
      "Epoch 14, Batch 299/462, Loss: 0.7162030935287476\n",
      "Epoch 14, Batch 300/462, Loss: 0.7022994160652161\n",
      "Epoch 14, Batch 301/462, Loss: 0.8025758862495422\n",
      "Epoch 14, Batch 302/462, Loss: 0.8861425518989563\n",
      "Epoch 14, Batch 303/462, Loss: 0.7534024119377136\n",
      "Epoch 14, Batch 304/462, Loss: 0.9219849705696106\n",
      "Epoch 14, Batch 305/462, Loss: 0.7438378930091858\n",
      "Epoch 14, Batch 306/462, Loss: 0.8139440417289734\n",
      "Epoch 14, Batch 307/462, Loss: 0.8165894746780396\n",
      "Epoch 14, Batch 308/462, Loss: 0.7950229644775391\n",
      "Epoch 14, Batch 309/462, Loss: 0.7287255525588989\n",
      "Epoch 14, Batch 310/462, Loss: 0.8370984792709351\n",
      "Epoch 14, Batch 311/462, Loss: 0.8045657873153687\n",
      "Epoch 14, Batch 312/462, Loss: 0.7317830920219421\n",
      "Epoch 14, Batch 313/462, Loss: 0.7214429974555969\n",
      "Epoch 14, Batch 314/462, Loss: 0.760858952999115\n",
      "Epoch 14, Batch 315/462, Loss: 0.5856233239173889\n",
      "Epoch 14, Batch 316/462, Loss: 0.7743300795555115\n",
      "Epoch 14, Batch 317/462, Loss: 0.7171348929405212\n",
      "Epoch 14, Batch 318/462, Loss: 0.7308615446090698\n",
      "Epoch 14, Batch 319/462, Loss: 0.8209349513053894\n",
      "Epoch 14, Batch 320/462, Loss: 0.8092061281204224\n",
      "Epoch 14, Batch 321/462, Loss: 0.7541242837905884\n",
      "Epoch 14, Batch 322/462, Loss: 0.8946348428726196\n",
      "Epoch 14, Batch 323/462, Loss: 0.793881893157959\n",
      "Epoch 14, Batch 324/462, Loss: 0.7331976890563965\n",
      "Epoch 14, Batch 325/462, Loss: 0.8415802121162415\n",
      "Epoch 14, Batch 326/462, Loss: 0.8118860125541687\n",
      "Epoch 14, Batch 327/462, Loss: 0.7737586498260498\n",
      "Epoch 14, Batch 328/462, Loss: 0.699320375919342\n",
      "Epoch 14, Batch 329/462, Loss: 0.7276322245597839\n",
      "Epoch 14, Batch 330/462, Loss: 0.742110013961792\n",
      "Epoch 14, Batch 331/462, Loss: 0.8228228092193604\n",
      "Epoch 14, Batch 332/462, Loss: 0.7146934270858765\n",
      "Epoch 14, Batch 333/462, Loss: 0.7794566750526428\n",
      "Epoch 14, Batch 334/462, Loss: 0.7138587236404419\n",
      "Epoch 14, Batch 335/462, Loss: 0.8293339610099792\n",
      "Epoch 14, Batch 336/462, Loss: 0.7489776015281677\n",
      "Epoch 14, Batch 337/462, Loss: 0.8157740831375122\n",
      "Epoch 14, Batch 338/462, Loss: 0.8226670622825623\n",
      "Epoch 14, Batch 339/462, Loss: 0.625758945941925\n",
      "Epoch 14, Batch 340/462, Loss: 0.9285084009170532\n",
      "Epoch 14, Batch 341/462, Loss: 0.7304259538650513\n",
      "Epoch 14, Batch 342/462, Loss: 0.7110460996627808\n",
      "Epoch 14, Batch 343/462, Loss: 0.7849363088607788\n",
      "Epoch 14, Batch 344/462, Loss: 0.7822339534759521\n",
      "Epoch 14, Batch 345/462, Loss: 0.7101773023605347\n",
      "Epoch 14, Batch 346/462, Loss: 0.8515970706939697\n",
      "Epoch 14, Batch 347/462, Loss: 0.7130593061447144\n",
      "Epoch 14, Batch 348/462, Loss: 0.6163965463638306\n",
      "Epoch 14, Batch 349/462, Loss: 0.7450758814811707\n",
      "Epoch 14, Batch 350/462, Loss: 0.6111025214195251\n",
      "Epoch 14, Batch 351/462, Loss: 0.7436524033546448\n",
      "Epoch 14, Batch 352/462, Loss: 0.8960371017456055\n",
      "Epoch 14, Batch 353/462, Loss: 0.8175768256187439\n",
      "Epoch 14, Batch 354/462, Loss: 0.8328835964202881\n",
      "Epoch 14, Batch 355/462, Loss: 1.0016909837722778\n",
      "Epoch 14, Batch 356/462, Loss: 0.7557021975517273\n",
      "Epoch 14, Batch 357/462, Loss: 0.8652406334877014\n",
      "Epoch 14, Batch 358/462, Loss: 0.7693185210227966\n",
      "Epoch 14, Batch 359/462, Loss: 0.8309118151664734\n",
      "Epoch 14, Batch 360/462, Loss: 0.6495116949081421\n",
      "Epoch 14, Batch 361/462, Loss: 0.6630343794822693\n",
      "Epoch 14, Batch 362/462, Loss: 0.7635423541069031\n",
      "Epoch 14, Batch 363/462, Loss: 0.8402867317199707\n",
      "Epoch 14, Batch 364/462, Loss: 0.7102328538894653\n",
      "Epoch 14, Batch 365/462, Loss: 0.7168421745300293\n",
      "Epoch 14, Batch 366/462, Loss: 0.6145766377449036\n",
      "Epoch 14, Batch 367/462, Loss: 0.812107264995575\n",
      "Epoch 14, Batch 368/462, Loss: 0.912279486656189\n",
      "Epoch 14, Batch 369/462, Loss: 0.7130814790725708\n",
      "Epoch 14, Batch 370/462, Loss: 0.7719020843505859\n",
      "Epoch 14, Batch 371/462, Loss: 0.7012147903442383\n",
      "Epoch 14, Batch 372/462, Loss: 0.7603073716163635\n",
      "Epoch 14, Batch 373/462, Loss: 0.8067046999931335\n",
      "Epoch 14, Batch 374/462, Loss: 0.673642635345459\n",
      "Epoch 14, Batch 375/462, Loss: 0.6536518335342407\n",
      "Epoch 14, Batch 376/462, Loss: 0.7557821869850159\n",
      "Epoch 14, Batch 377/462, Loss: 0.9001660346984863\n",
      "Epoch 14, Batch 378/462, Loss: 0.6765879988670349\n",
      "Epoch 14, Batch 379/462, Loss: 0.7697413563728333\n",
      "Epoch 14, Batch 380/462, Loss: 0.8821147680282593\n",
      "Epoch 14, Batch 381/462, Loss: 0.7421982884407043\n",
      "Epoch 14, Batch 382/462, Loss: 0.7934986352920532\n",
      "Epoch 14, Batch 383/462, Loss: 0.9417343735694885\n",
      "Epoch 14, Batch 384/462, Loss: 0.7928634285926819\n",
      "Epoch 14, Batch 385/462, Loss: 0.7665659785270691\n",
      "Epoch 14, Batch 386/462, Loss: 0.7963210940361023\n",
      "Epoch 14, Batch 387/462, Loss: 0.6220892667770386\n",
      "Epoch 14, Batch 388/462, Loss: 0.7460559606552124\n",
      "Epoch 14, Batch 389/462, Loss: 0.8914228081703186\n",
      "Epoch 14, Batch 390/462, Loss: 0.5509442090988159\n",
      "Epoch 14, Batch 391/462, Loss: 0.796159029006958\n",
      "Epoch 14, Batch 392/462, Loss: 0.8082000017166138\n",
      "Epoch 14, Batch 393/462, Loss: 0.6204972863197327\n",
      "Epoch 14, Batch 394/462, Loss: 0.7594249844551086\n",
      "Epoch 14, Batch 395/462, Loss: 0.8017628192901611\n",
      "Epoch 14, Batch 396/462, Loss: 0.6900015473365784\n",
      "Epoch 14, Batch 397/462, Loss: 0.6655211448669434\n",
      "Epoch 14, Batch 398/462, Loss: 0.7879918217658997\n",
      "Epoch 14, Batch 399/462, Loss: 0.7137979865074158\n",
      "Epoch 14, Batch 400/462, Loss: 0.6557073593139648\n",
      "Epoch 14, Batch 401/462, Loss: 0.6606230139732361\n",
      "Epoch 14, Batch 402/462, Loss: 0.9917327165603638\n",
      "Epoch 14, Batch 403/462, Loss: 0.8726558089256287\n",
      "Epoch 14, Batch 404/462, Loss: 0.7470526099205017\n",
      "Epoch 14, Batch 405/462, Loss: 0.7752586007118225\n",
      "Epoch 14, Batch 406/462, Loss: 0.6500558853149414\n",
      "Epoch 14, Batch 407/462, Loss: 0.7580402493476868\n",
      "Epoch 14, Batch 408/462, Loss: 0.6364452838897705\n",
      "Epoch 14, Batch 409/462, Loss: 0.8403785228729248\n",
      "Epoch 14, Batch 410/462, Loss: 0.829483687877655\n",
      "Epoch 14, Batch 411/462, Loss: 0.8947519659996033\n",
      "Epoch 14, Batch 412/462, Loss: 0.6387456655502319\n",
      "Epoch 14, Batch 413/462, Loss: 0.7004973292350769\n",
      "Epoch 14, Batch 414/462, Loss: 0.5463939309120178\n",
      "Epoch 14, Batch 415/462, Loss: 0.9278179407119751\n",
      "Epoch 14, Batch 416/462, Loss: 0.8322905898094177\n",
      "Epoch 14, Batch 417/462, Loss: 0.8075820207595825\n",
      "Epoch 14, Batch 418/462, Loss: 0.88377845287323\n",
      "Epoch 14, Batch 419/462, Loss: 0.7352025508880615\n",
      "Epoch 14, Batch 420/462, Loss: 0.7812065482139587\n",
      "Epoch 14, Batch 421/462, Loss: 0.8817547559738159\n",
      "Epoch 14, Batch 422/462, Loss: 0.8263089656829834\n",
      "Epoch 14, Batch 423/462, Loss: 0.7238671779632568\n",
      "Epoch 14, Batch 424/462, Loss: 0.7279323935508728\n",
      "Epoch 14, Batch 425/462, Loss: 0.7775169014930725\n",
      "Epoch 14, Batch 426/462, Loss: 0.7286554574966431\n",
      "Epoch 14, Batch 427/462, Loss: 0.8787700533866882\n",
      "Epoch 14, Batch 428/462, Loss: 0.7099067568778992\n",
      "Epoch 14, Batch 429/462, Loss: 0.7348592877388\n",
      "Epoch 14, Batch 430/462, Loss: 0.6675341129302979\n",
      "Epoch 14, Batch 431/462, Loss: 0.5452901721000671\n",
      "Epoch 14, Batch 432/462, Loss: 0.8923979997634888\n",
      "Epoch 14, Batch 433/462, Loss: 0.8160864114761353\n",
      "Epoch 14, Batch 434/462, Loss: 0.6613864302635193\n",
      "Epoch 14, Batch 435/462, Loss: 0.9293168783187866\n",
      "Epoch 14, Batch 436/462, Loss: 0.7062288522720337\n",
      "Epoch 14, Batch 437/462, Loss: 0.7200477123260498\n",
      "Epoch 14, Batch 438/462, Loss: 0.8645968437194824\n",
      "Epoch 14, Batch 439/462, Loss: 0.7416961789131165\n",
      "Epoch 14, Batch 440/462, Loss: 0.7252171039581299\n",
      "Epoch 14, Batch 441/462, Loss: 0.8504606485366821\n",
      "Epoch 14, Batch 442/462, Loss: 0.7920800447463989\n",
      "Epoch 14, Batch 443/462, Loss: 0.7458080649375916\n",
      "Epoch 14, Batch 444/462, Loss: 0.8509199619293213\n",
      "Epoch 14, Batch 445/462, Loss: 0.812920093536377\n",
      "Epoch 14, Batch 446/462, Loss: 0.7699156999588013\n",
      "Epoch 14, Batch 447/462, Loss: 0.7983216047286987\n",
      "Epoch 14, Batch 448/462, Loss: 0.7415797114372253\n",
      "Epoch 14, Batch 449/462, Loss: 0.7656466364860535\n",
      "Epoch 14, Batch 450/462, Loss: 0.8516841530799866\n",
      "Epoch 14, Batch 451/462, Loss: 0.8209370374679565\n",
      "Epoch 14, Batch 452/462, Loss: 0.7919895648956299\n",
      "Epoch 14, Batch 453/462, Loss: 0.7686140537261963\n",
      "Epoch 14, Batch 454/462, Loss: 0.7771012783050537\n",
      "Epoch 14, Batch 455/462, Loss: 0.8532567024230957\n",
      "Epoch 14, Batch 456/462, Loss: 0.9630484580993652\n",
      "Epoch 14, Batch 457/462, Loss: 0.7953231334686279\n",
      "Epoch 14, Batch 458/462, Loss: 0.7676081657409668\n",
      "Epoch 14, Batch 459/462, Loss: 0.8261379599571228\n",
      "Epoch 14, Batch 460/462, Loss: 0.7720236778259277\n",
      "Epoch 14, Batch 461/462, Loss: 0.936485767364502\n",
      "Epoch 14, Batch 462/462, Loss: 0.59491366147995\n",
      "Epoch 14, Loss: 358.9958955049515\n",
      "Epoch 15, Batch 1/462, Loss: 0.6133534908294678\n",
      "Epoch 15, Batch 2/462, Loss: 1.0339876413345337\n",
      "Epoch 15, Batch 3/462, Loss: 0.8975276350975037\n",
      "Epoch 15, Batch 4/462, Loss: 0.7384326457977295\n",
      "Epoch 15, Batch 5/462, Loss: 0.827456533908844\n",
      "Epoch 15, Batch 6/462, Loss: 0.8652417063713074\n",
      "Epoch 15, Batch 7/462, Loss: 0.6909716725349426\n",
      "Epoch 15, Batch 8/462, Loss: 0.8387070298194885\n",
      "Epoch 15, Batch 9/462, Loss: 0.889361560344696\n",
      "Epoch 15, Batch 10/462, Loss: 0.6947962641716003\n",
      "Epoch 15, Batch 11/462, Loss: 0.8178659081459045\n",
      "Epoch 15, Batch 12/462, Loss: 0.9227588772773743\n",
      "Epoch 15, Batch 13/462, Loss: 0.8044071197509766\n",
      "Epoch 15, Batch 14/462, Loss: 0.7362239360809326\n",
      "Epoch 15, Batch 15/462, Loss: 0.7033171653747559\n",
      "Epoch 15, Batch 16/462, Loss: 0.7217546701431274\n",
      "Epoch 15, Batch 17/462, Loss: 0.804926335811615\n",
      "Epoch 15, Batch 18/462, Loss: 0.8955222964286804\n",
      "Epoch 15, Batch 19/462, Loss: 0.8306710720062256\n",
      "Epoch 15, Batch 20/462, Loss: 0.8052168488502502\n",
      "Epoch 15, Batch 21/462, Loss: 0.7268251776695251\n",
      "Epoch 15, Batch 22/462, Loss: 0.6992851495742798\n",
      "Epoch 15, Batch 23/462, Loss: 0.8194308280944824\n",
      "Epoch 15, Batch 24/462, Loss: 0.806373655796051\n",
      "Epoch 15, Batch 25/462, Loss: 0.6215986013412476\n",
      "Epoch 15, Batch 26/462, Loss: 0.8853579759597778\n",
      "Epoch 15, Batch 27/462, Loss: 0.6038443446159363\n",
      "Epoch 15, Batch 28/462, Loss: 0.74713534116745\n",
      "Epoch 15, Batch 29/462, Loss: 0.8244248032569885\n",
      "Epoch 15, Batch 30/462, Loss: 0.8431603312492371\n",
      "Epoch 15, Batch 31/462, Loss: 0.6997921466827393\n",
      "Epoch 15, Batch 32/462, Loss: 0.7947843074798584\n",
      "Epoch 15, Batch 33/462, Loss: 0.9191401600837708\n",
      "Epoch 15, Batch 34/462, Loss: 0.9278581142425537\n",
      "Epoch 15, Batch 35/462, Loss: 0.8658693432807922\n",
      "Epoch 15, Batch 36/462, Loss: 0.9011794328689575\n",
      "Epoch 15, Batch 37/462, Loss: 0.7914447784423828\n",
      "Epoch 15, Batch 38/462, Loss: 0.717454731464386\n",
      "Epoch 15, Batch 39/462, Loss: 0.8150460124015808\n",
      "Epoch 15, Batch 40/462, Loss: 0.665749192237854\n",
      "Epoch 15, Batch 41/462, Loss: 0.657341718673706\n",
      "Epoch 15, Batch 42/462, Loss: 0.8771058917045593\n",
      "Epoch 15, Batch 43/462, Loss: 1.016635775566101\n",
      "Epoch 15, Batch 44/462, Loss: 0.8816854357719421\n",
      "Epoch 15, Batch 45/462, Loss: 0.847967803478241\n",
      "Epoch 15, Batch 46/462, Loss: 0.7590077519416809\n",
      "Epoch 15, Batch 47/462, Loss: 0.8543275594711304\n",
      "Epoch 15, Batch 48/462, Loss: 0.8737009763717651\n",
      "Epoch 15, Batch 49/462, Loss: 0.674035370349884\n",
      "Epoch 15, Batch 50/462, Loss: 0.7754942178726196\n",
      "Epoch 15, Batch 51/462, Loss: 0.9287667870521545\n",
      "Epoch 15, Batch 52/462, Loss: 0.8612585663795471\n",
      "Epoch 15, Batch 53/462, Loss: 0.7908008694648743\n",
      "Epoch 15, Batch 54/462, Loss: 0.6710007786750793\n",
      "Epoch 15, Batch 55/462, Loss: 0.7411419153213501\n",
      "Epoch 15, Batch 56/462, Loss: 0.785106897354126\n",
      "Epoch 15, Batch 57/462, Loss: 0.8028629422187805\n",
      "Epoch 15, Batch 58/462, Loss: 0.739369809627533\n",
      "Epoch 15, Batch 59/462, Loss: 0.7244845628738403\n",
      "Epoch 15, Batch 60/462, Loss: 0.6750475764274597\n",
      "Epoch 15, Batch 61/462, Loss: 0.6225546598434448\n",
      "Epoch 15, Batch 62/462, Loss: 0.837317168712616\n",
      "Epoch 15, Batch 63/462, Loss: 0.8117828965187073\n",
      "Epoch 15, Batch 64/462, Loss: 0.6675849556922913\n",
      "Epoch 15, Batch 65/462, Loss: 0.8359049558639526\n",
      "Epoch 15, Batch 66/462, Loss: 0.7928977012634277\n",
      "Epoch 15, Batch 67/462, Loss: 0.6449922919273376\n",
      "Epoch 15, Batch 68/462, Loss: 0.7775497436523438\n",
      "Epoch 15, Batch 69/462, Loss: 0.7421249151229858\n",
      "Epoch 15, Batch 70/462, Loss: 0.7967358827590942\n",
      "Epoch 15, Batch 71/462, Loss: 0.8266102075576782\n",
      "Epoch 15, Batch 72/462, Loss: 0.5987735986709595\n",
      "Epoch 15, Batch 73/462, Loss: 0.6796615719795227\n",
      "Epoch 15, Batch 74/462, Loss: 0.7369180917739868\n",
      "Epoch 15, Batch 75/462, Loss: 0.7015683054924011\n",
      "Epoch 15, Batch 76/462, Loss: 0.7592976689338684\n",
      "Epoch 15, Batch 77/462, Loss: 0.7346923351287842\n",
      "Epoch 15, Batch 78/462, Loss: 0.8571890592575073\n",
      "Epoch 15, Batch 79/462, Loss: 0.8574308156967163\n",
      "Epoch 15, Batch 80/462, Loss: 0.9061906337738037\n",
      "Epoch 15, Batch 81/462, Loss: 0.7921060919761658\n",
      "Epoch 15, Batch 82/462, Loss: 0.7095937132835388\n",
      "Epoch 15, Batch 83/462, Loss: 0.746747612953186\n",
      "Epoch 15, Batch 84/462, Loss: 0.8709943890571594\n",
      "Epoch 15, Batch 85/462, Loss: 0.764972984790802\n",
      "Epoch 15, Batch 86/462, Loss: 0.6881960034370422\n",
      "Epoch 15, Batch 87/462, Loss: 0.7234929800033569\n",
      "Epoch 15, Batch 88/462, Loss: 0.6891710758209229\n",
      "Epoch 15, Batch 89/462, Loss: 0.8349806666374207\n",
      "Epoch 15, Batch 90/462, Loss: 0.6998441219329834\n",
      "Epoch 15, Batch 91/462, Loss: 0.8751323223114014\n",
      "Epoch 15, Batch 92/462, Loss: 0.7766268849372864\n",
      "Epoch 15, Batch 93/462, Loss: 0.6642325520515442\n",
      "Epoch 15, Batch 94/462, Loss: 0.7958253622055054\n",
      "Epoch 15, Batch 95/462, Loss: 1.0713224411010742\n",
      "Epoch 15, Batch 96/462, Loss: 0.7887301445007324\n",
      "Epoch 15, Batch 97/462, Loss: 0.6504642963409424\n",
      "Epoch 15, Batch 98/462, Loss: 0.8112314939498901\n",
      "Epoch 15, Batch 99/462, Loss: 0.613636314868927\n",
      "Epoch 15, Batch 100/462, Loss: 0.7459379434585571\n",
      "Epoch 15, Batch 101/462, Loss: 0.6433828473091125\n",
      "Epoch 15, Batch 102/462, Loss: 0.9030225276947021\n",
      "Epoch 15, Batch 103/462, Loss: 0.7770586609840393\n",
      "Epoch 15, Batch 104/462, Loss: 0.7893288135528564\n",
      "Epoch 15, Batch 105/462, Loss: 0.5737922787666321\n",
      "Epoch 15, Batch 106/462, Loss: 0.5832527875900269\n",
      "Epoch 15, Batch 107/462, Loss: 0.7001481056213379\n",
      "Epoch 15, Batch 108/462, Loss: 0.7869990468025208\n",
      "Epoch 15, Batch 109/462, Loss: 0.7438446283340454\n",
      "Epoch 15, Batch 110/462, Loss: 0.7904619574546814\n",
      "Epoch 15, Batch 111/462, Loss: 0.810397744178772\n",
      "Epoch 15, Batch 112/462, Loss: 0.6753193736076355\n",
      "Epoch 15, Batch 113/462, Loss: 0.9604519009590149\n",
      "Epoch 15, Batch 114/462, Loss: 0.7305671572685242\n",
      "Epoch 15, Batch 115/462, Loss: 0.8179514408111572\n",
      "Epoch 15, Batch 116/462, Loss: 0.8474860787391663\n",
      "Epoch 15, Batch 117/462, Loss: 0.7150912284851074\n",
      "Epoch 15, Batch 118/462, Loss: 0.7563840746879578\n",
      "Epoch 15, Batch 119/462, Loss: 0.6727713346481323\n",
      "Epoch 15, Batch 120/462, Loss: 0.7994951009750366\n",
      "Epoch 15, Batch 121/462, Loss: 0.9405566453933716\n",
      "Epoch 15, Batch 122/462, Loss: 0.6875232458114624\n",
      "Epoch 15, Batch 123/462, Loss: 0.6349052786827087\n",
      "Epoch 15, Batch 124/462, Loss: 0.8114038109779358\n",
      "Epoch 15, Batch 125/462, Loss: 0.7618109583854675\n",
      "Epoch 15, Batch 126/462, Loss: 0.8373228311538696\n",
      "Epoch 15, Batch 127/462, Loss: 0.7404001355171204\n",
      "Epoch 15, Batch 128/462, Loss: 0.6948235034942627\n",
      "Epoch 15, Batch 129/462, Loss: 0.757757306098938\n",
      "Epoch 15, Batch 130/462, Loss: 0.7786715030670166\n",
      "Epoch 15, Batch 131/462, Loss: 0.7064851522445679\n",
      "Epoch 15, Batch 132/462, Loss: 0.7426767349243164\n",
      "Epoch 15, Batch 133/462, Loss: 0.8012587428092957\n",
      "Epoch 15, Batch 134/462, Loss: 0.7792857885360718\n",
      "Epoch 15, Batch 135/462, Loss: 0.7433445453643799\n",
      "Epoch 15, Batch 136/462, Loss: 0.935137152671814\n",
      "Epoch 15, Batch 137/462, Loss: 0.8476091027259827\n",
      "Epoch 15, Batch 138/462, Loss: 0.6736443042755127\n",
      "Epoch 15, Batch 139/462, Loss: 0.8000362515449524\n",
      "Epoch 15, Batch 140/462, Loss: 0.8117077946662903\n",
      "Epoch 15, Batch 141/462, Loss: 0.7669010162353516\n",
      "Epoch 15, Batch 142/462, Loss: 0.7957138419151306\n",
      "Epoch 15, Batch 143/462, Loss: 0.8886311054229736\n",
      "Epoch 15, Batch 144/462, Loss: 0.6699877381324768\n",
      "Epoch 15, Batch 145/462, Loss: 0.8202008008956909\n",
      "Epoch 15, Batch 146/462, Loss: 0.7050785422325134\n",
      "Epoch 15, Batch 147/462, Loss: 0.9339101910591125\n",
      "Epoch 15, Batch 148/462, Loss: 0.9327067732810974\n",
      "Epoch 15, Batch 149/462, Loss: 0.8021188974380493\n",
      "Epoch 15, Batch 150/462, Loss: 0.7078021764755249\n",
      "Epoch 15, Batch 151/462, Loss: 0.8846698999404907\n",
      "Epoch 15, Batch 152/462, Loss: 0.8335350155830383\n",
      "Epoch 15, Batch 153/462, Loss: 0.7539808750152588\n",
      "Epoch 15, Batch 154/462, Loss: 0.8594232797622681\n",
      "Epoch 15, Batch 155/462, Loss: 0.6127654910087585\n",
      "Epoch 15, Batch 156/462, Loss: 0.8363264203071594\n",
      "Epoch 15, Batch 157/462, Loss: 0.8034570813179016\n",
      "Epoch 15, Batch 158/462, Loss: 1.0199190378189087\n",
      "Epoch 15, Batch 159/462, Loss: 0.8411017060279846\n",
      "Epoch 15, Batch 160/462, Loss: 0.8258801698684692\n",
      "Epoch 15, Batch 161/462, Loss: 0.7278940081596375\n",
      "Epoch 15, Batch 162/462, Loss: 0.6216287016868591\n",
      "Epoch 15, Batch 163/462, Loss: 1.0147953033447266\n",
      "Epoch 15, Batch 164/462, Loss: 0.8489709496498108\n",
      "Epoch 15, Batch 165/462, Loss: 1.0257476568222046\n",
      "Epoch 15, Batch 166/462, Loss: 0.7604555487632751\n",
      "Epoch 15, Batch 167/462, Loss: 0.8237700462341309\n",
      "Epoch 15, Batch 168/462, Loss: 0.7274512648582458\n",
      "Epoch 15, Batch 169/462, Loss: 0.6715032458305359\n",
      "Epoch 15, Batch 170/462, Loss: 0.7079063653945923\n",
      "Epoch 15, Batch 171/462, Loss: 0.6628061532974243\n",
      "Epoch 15, Batch 172/462, Loss: 0.8046547174453735\n",
      "Epoch 15, Batch 173/462, Loss: 0.9452130198478699\n",
      "Epoch 15, Batch 174/462, Loss: 0.8414538502693176\n",
      "Epoch 15, Batch 175/462, Loss: 0.7166881561279297\n",
      "Epoch 15, Batch 176/462, Loss: 0.6258835792541504\n",
      "Epoch 15, Batch 177/462, Loss: 0.7084518671035767\n",
      "Epoch 15, Batch 178/462, Loss: 0.6553798913955688\n",
      "Epoch 15, Batch 179/462, Loss: 0.8639522194862366\n",
      "Epoch 15, Batch 180/462, Loss: 0.6172202825546265\n",
      "Epoch 15, Batch 181/462, Loss: 0.728908896446228\n",
      "Epoch 15, Batch 182/462, Loss: 0.6456560492515564\n",
      "Epoch 15, Batch 183/462, Loss: 0.8477909564971924\n",
      "Epoch 15, Batch 184/462, Loss: 0.7222395539283752\n",
      "Epoch 15, Batch 185/462, Loss: 0.7372961640357971\n",
      "Epoch 15, Batch 186/462, Loss: 0.6777291893959045\n",
      "Epoch 15, Batch 187/462, Loss: 0.8697741031646729\n",
      "Epoch 15, Batch 188/462, Loss: 0.7768165469169617\n",
      "Epoch 15, Batch 189/462, Loss: 0.7272943258285522\n",
      "Epoch 15, Batch 190/462, Loss: 0.6346431970596313\n",
      "Epoch 15, Batch 191/462, Loss: 0.7448700070381165\n",
      "Epoch 15, Batch 192/462, Loss: 0.7364333868026733\n",
      "Epoch 15, Batch 193/462, Loss: 0.6983104348182678\n",
      "Epoch 15, Batch 194/462, Loss: 0.7977735996246338\n",
      "Epoch 15, Batch 195/462, Loss: 0.780341625213623\n",
      "Epoch 15, Batch 196/462, Loss: 0.7180124521255493\n",
      "Epoch 15, Batch 197/462, Loss: 0.9334787130355835\n",
      "Epoch 15, Batch 198/462, Loss: 0.7957576513290405\n",
      "Epoch 15, Batch 199/462, Loss: 0.8290451765060425\n",
      "Epoch 15, Batch 200/462, Loss: 0.7674221396446228\n",
      "Epoch 15, Batch 201/462, Loss: 0.7404964566230774\n",
      "Epoch 15, Batch 202/462, Loss: 0.6319453716278076\n",
      "Epoch 15, Batch 203/462, Loss: 0.7774941325187683\n",
      "Epoch 15, Batch 204/462, Loss: 0.7901315689086914\n",
      "Epoch 15, Batch 205/462, Loss: 0.8302386403083801\n",
      "Epoch 15, Batch 206/462, Loss: 0.7653555870056152\n",
      "Epoch 15, Batch 207/462, Loss: 0.7945506572723389\n",
      "Epoch 15, Batch 208/462, Loss: 0.8135976791381836\n",
      "Epoch 15, Batch 209/462, Loss: 0.7306913733482361\n",
      "Epoch 15, Batch 210/462, Loss: 0.5696128606796265\n",
      "Epoch 15, Batch 211/462, Loss: 0.7572283744812012\n",
      "Epoch 15, Batch 212/462, Loss: 0.9582550525665283\n",
      "Epoch 15, Batch 213/462, Loss: 0.7660823464393616\n",
      "Epoch 15, Batch 214/462, Loss: 0.6901277303695679\n",
      "Epoch 15, Batch 215/462, Loss: 0.861447274684906\n",
      "Epoch 15, Batch 216/462, Loss: 0.805808424949646\n",
      "Epoch 15, Batch 217/462, Loss: 0.9147568345069885\n",
      "Epoch 15, Batch 218/462, Loss: 0.7832452654838562\n",
      "Epoch 15, Batch 219/462, Loss: 0.8471627831459045\n",
      "Epoch 15, Batch 220/462, Loss: 0.7290658950805664\n",
      "Epoch 15, Batch 221/462, Loss: 0.8232641816139221\n",
      "Epoch 15, Batch 222/462, Loss: 0.6841229200363159\n",
      "Epoch 15, Batch 223/462, Loss: 0.7315024137496948\n",
      "Epoch 15, Batch 224/462, Loss: 0.6761074066162109\n",
      "Epoch 15, Batch 225/462, Loss: 0.6508190631866455\n",
      "Epoch 15, Batch 226/462, Loss: 0.8050617575645447\n",
      "Epoch 15, Batch 227/462, Loss: 0.8272857666015625\n",
      "Epoch 15, Batch 228/462, Loss: 0.7929751873016357\n",
      "Epoch 15, Batch 229/462, Loss: 0.8182271122932434\n",
      "Epoch 15, Batch 230/462, Loss: 0.759059488773346\n",
      "Epoch 15, Batch 231/462, Loss: 0.7218711376190186\n",
      "Epoch 15, Batch 232/462, Loss: 0.8054217100143433\n",
      "Epoch 15, Batch 233/462, Loss: 0.8712207674980164\n",
      "Epoch 15, Batch 234/462, Loss: 0.7334263324737549\n",
      "Epoch 15, Batch 235/462, Loss: 0.7717379331588745\n",
      "Epoch 15, Batch 236/462, Loss: 0.6243916153907776\n",
      "Epoch 15, Batch 237/462, Loss: 0.7794302701950073\n",
      "Epoch 15, Batch 238/462, Loss: 0.938008725643158\n",
      "Epoch 15, Batch 239/462, Loss: 0.7691531777381897\n",
      "Epoch 15, Batch 240/462, Loss: 0.7632531523704529\n",
      "Epoch 15, Batch 241/462, Loss: 0.9088667631149292\n",
      "Epoch 15, Batch 242/462, Loss: 0.9268582463264465\n",
      "Epoch 15, Batch 243/462, Loss: 0.7776445150375366\n",
      "Epoch 15, Batch 244/462, Loss: 0.7011414766311646\n",
      "Epoch 15, Batch 245/462, Loss: 0.8818459510803223\n",
      "Epoch 15, Batch 246/462, Loss: 0.7929080724716187\n",
      "Epoch 15, Batch 247/462, Loss: 0.8039324283599854\n",
      "Epoch 15, Batch 248/462, Loss: 0.8092923164367676\n",
      "Epoch 15, Batch 249/462, Loss: 0.7947404980659485\n",
      "Epoch 15, Batch 250/462, Loss: 0.7264737486839294\n",
      "Epoch 15, Batch 251/462, Loss: 0.6454080939292908\n",
      "Epoch 15, Batch 252/462, Loss: 0.6861727833747864\n",
      "Epoch 15, Batch 253/462, Loss: 1.0505181550979614\n",
      "Epoch 15, Batch 254/462, Loss: 0.7052371501922607\n",
      "Epoch 15, Batch 255/462, Loss: 0.6714143753051758\n",
      "Epoch 15, Batch 256/462, Loss: 0.8097460269927979\n",
      "Epoch 15, Batch 257/462, Loss: 0.7851206660270691\n",
      "Epoch 15, Batch 258/462, Loss: 0.7612666487693787\n",
      "Epoch 15, Batch 259/462, Loss: 0.6767919659614563\n",
      "Epoch 15, Batch 260/462, Loss: 0.7523273229598999\n",
      "Epoch 15, Batch 261/462, Loss: 1.0353684425354004\n",
      "Epoch 15, Batch 262/462, Loss: 0.684042751789093\n",
      "Epoch 15, Batch 263/462, Loss: 0.9508200287818909\n",
      "Epoch 15, Batch 264/462, Loss: 0.8149802684783936\n",
      "Epoch 15, Batch 265/462, Loss: 0.7711649537086487\n",
      "Epoch 15, Batch 266/462, Loss: 0.9463906288146973\n",
      "Epoch 15, Batch 267/462, Loss: 0.65450519323349\n",
      "Epoch 15, Batch 268/462, Loss: 0.7241709232330322\n",
      "Epoch 15, Batch 269/462, Loss: 0.7937406897544861\n",
      "Epoch 15, Batch 270/462, Loss: 0.7668390870094299\n",
      "Epoch 15, Batch 271/462, Loss: 0.9573079943656921\n",
      "Epoch 15, Batch 272/462, Loss: 0.7578480243682861\n",
      "Epoch 15, Batch 273/462, Loss: 0.8380441665649414\n",
      "Epoch 15, Batch 274/462, Loss: 0.7608291506767273\n",
      "Epoch 15, Batch 275/462, Loss: 0.7634099721908569\n",
      "Epoch 15, Batch 276/462, Loss: 0.663915753364563\n",
      "Epoch 15, Batch 277/462, Loss: 0.7582131624221802\n",
      "Epoch 15, Batch 278/462, Loss: 0.7143980860710144\n",
      "Epoch 15, Batch 279/462, Loss: 0.8666079044342041\n",
      "Epoch 15, Batch 280/462, Loss: 0.7620886564254761\n",
      "Epoch 15, Batch 281/462, Loss: 0.7363839149475098\n",
      "Epoch 15, Batch 282/462, Loss: 0.7605586647987366\n",
      "Epoch 15, Batch 283/462, Loss: 0.6270655989646912\n",
      "Epoch 15, Batch 284/462, Loss: 0.7897394895553589\n",
      "Epoch 15, Batch 285/462, Loss: 0.586305558681488\n",
      "Epoch 15, Batch 286/462, Loss: 0.779830276966095\n",
      "Epoch 15, Batch 287/462, Loss: 0.9060345888137817\n",
      "Epoch 15, Batch 288/462, Loss: 0.6948383450508118\n",
      "Epoch 15, Batch 289/462, Loss: 0.7971388101577759\n",
      "Epoch 15, Batch 290/462, Loss: 0.8420140743255615\n",
      "Epoch 15, Batch 291/462, Loss: 0.7707013487815857\n",
      "Epoch 15, Batch 292/462, Loss: 0.7351228594779968\n",
      "Epoch 15, Batch 293/462, Loss: 0.638275682926178\n",
      "Epoch 15, Batch 294/462, Loss: 0.9567511677742004\n",
      "Epoch 15, Batch 295/462, Loss: 0.7878003120422363\n",
      "Epoch 15, Batch 296/462, Loss: 0.6186463236808777\n",
      "Epoch 15, Batch 297/462, Loss: 0.8021638989448547\n",
      "Epoch 15, Batch 298/462, Loss: 0.7541467547416687\n",
      "Epoch 15, Batch 299/462, Loss: 0.7395586967468262\n",
      "Epoch 15, Batch 300/462, Loss: 0.7981100678443909\n",
      "Epoch 15, Batch 301/462, Loss: 0.6807084679603577\n",
      "Epoch 15, Batch 302/462, Loss: 0.7584509253501892\n",
      "Epoch 15, Batch 303/462, Loss: 0.86756432056427\n",
      "Epoch 15, Batch 304/462, Loss: 0.8299572467803955\n",
      "Epoch 15, Batch 305/462, Loss: 0.7042208909988403\n",
      "Epoch 15, Batch 306/462, Loss: 0.7584574818611145\n",
      "Epoch 15, Batch 307/462, Loss: 0.9154146909713745\n",
      "Epoch 15, Batch 308/462, Loss: 0.8915325999259949\n",
      "Epoch 15, Batch 309/462, Loss: 0.8102718591690063\n",
      "Epoch 15, Batch 310/462, Loss: 0.8190257549285889\n",
      "Epoch 15, Batch 311/462, Loss: 0.7610587477684021\n",
      "Epoch 15, Batch 312/462, Loss: 0.8153099417686462\n",
      "Epoch 15, Batch 313/462, Loss: 0.6570569276809692\n",
      "Epoch 15, Batch 314/462, Loss: 0.7066131830215454\n",
      "Epoch 15, Batch 315/462, Loss: 0.6963785886764526\n",
      "Epoch 15, Batch 316/462, Loss: 0.7099477648735046\n",
      "Epoch 15, Batch 317/462, Loss: 0.9208999872207642\n",
      "Epoch 15, Batch 318/462, Loss: 0.8074464797973633\n",
      "Epoch 15, Batch 319/462, Loss: 0.8332077860832214\n",
      "Epoch 15, Batch 320/462, Loss: 0.6558439135551453\n",
      "Epoch 15, Batch 321/462, Loss: 0.8431298732757568\n",
      "Epoch 15, Batch 322/462, Loss: 0.771709144115448\n",
      "Epoch 15, Batch 323/462, Loss: 0.6257613897323608\n",
      "Epoch 15, Batch 324/462, Loss: 0.7870801687240601\n",
      "Epoch 15, Batch 325/462, Loss: 0.9061155915260315\n",
      "Epoch 15, Batch 326/462, Loss: 0.6538782715797424\n",
      "Epoch 15, Batch 327/462, Loss: 0.8356322646141052\n",
      "Epoch 15, Batch 328/462, Loss: 0.7855673432350159\n",
      "Epoch 15, Batch 329/462, Loss: 0.6876643896102905\n",
      "Epoch 15, Batch 330/462, Loss: 0.6805554628372192\n",
      "Epoch 15, Batch 331/462, Loss: 0.5302088260650635\n",
      "Epoch 15, Batch 332/462, Loss: 0.7303986549377441\n",
      "Epoch 15, Batch 333/462, Loss: 0.8487706780433655\n",
      "Epoch 15, Batch 334/462, Loss: 0.9324679970741272\n",
      "Epoch 15, Batch 335/462, Loss: 0.7331703901290894\n",
      "Epoch 15, Batch 336/462, Loss: 0.8278554081916809\n",
      "Epoch 15, Batch 337/462, Loss: 0.742534875869751\n",
      "Epoch 15, Batch 338/462, Loss: 0.8374882936477661\n",
      "Epoch 15, Batch 339/462, Loss: 0.8734056353569031\n",
      "Epoch 15, Batch 340/462, Loss: 0.7832667231559753\n",
      "Epoch 15, Batch 341/462, Loss: 0.7523289918899536\n",
      "Epoch 15, Batch 342/462, Loss: 0.8501970171928406\n",
      "Epoch 15, Batch 343/462, Loss: 0.775794506072998\n",
      "Epoch 15, Batch 344/462, Loss: 0.8061631321907043\n",
      "Epoch 15, Batch 345/462, Loss: 0.64097660779953\n",
      "Epoch 15, Batch 346/462, Loss: 0.8421373963356018\n",
      "Epoch 15, Batch 347/462, Loss: 0.8139878511428833\n",
      "Epoch 15, Batch 348/462, Loss: 0.6109873652458191\n",
      "Epoch 15, Batch 349/462, Loss: 0.6516848206520081\n",
      "Epoch 15, Batch 350/462, Loss: 0.7614176869392395\n",
      "Epoch 15, Batch 351/462, Loss: 0.6451302170753479\n",
      "Epoch 15, Batch 352/462, Loss: 0.9090792536735535\n",
      "Epoch 15, Batch 353/462, Loss: 0.7046672105789185\n",
      "Epoch 15, Batch 354/462, Loss: 0.7241140007972717\n",
      "Epoch 15, Batch 355/462, Loss: 0.8514829874038696\n",
      "Epoch 15, Batch 356/462, Loss: 0.8934910893440247\n",
      "Epoch 15, Batch 357/462, Loss: 0.7629265785217285\n",
      "Epoch 15, Batch 358/462, Loss: 0.7532463073730469\n",
      "Epoch 15, Batch 359/462, Loss: 0.9112757444381714\n",
      "Epoch 15, Batch 360/462, Loss: 0.8301423788070679\n",
      "Epoch 15, Batch 361/462, Loss: 0.7344526648521423\n",
      "Epoch 15, Batch 362/462, Loss: 0.8499051928520203\n",
      "Epoch 15, Batch 363/462, Loss: 0.7951982021331787\n",
      "Epoch 15, Batch 364/462, Loss: 0.7684418559074402\n",
      "Epoch 15, Batch 365/462, Loss: 0.833732008934021\n",
      "Epoch 15, Batch 366/462, Loss: 0.7525758147239685\n",
      "Epoch 15, Batch 367/462, Loss: 0.6070240139961243\n",
      "Epoch 15, Batch 368/462, Loss: 0.7309029698371887\n",
      "Epoch 15, Batch 369/462, Loss: 0.9813461899757385\n",
      "Epoch 15, Batch 370/462, Loss: 0.7297712564468384\n",
      "Epoch 15, Batch 371/462, Loss: 0.7646927833557129\n",
      "Epoch 15, Batch 372/462, Loss: 0.7852648496627808\n",
      "Epoch 15, Batch 373/462, Loss: 0.7537938356399536\n",
      "Epoch 15, Batch 374/462, Loss: 0.6698294281959534\n",
      "Epoch 15, Batch 375/462, Loss: 0.9104026556015015\n",
      "Epoch 15, Batch 376/462, Loss: 0.7218244075775146\n",
      "Epoch 15, Batch 377/462, Loss: 0.7340613007545471\n",
      "Epoch 15, Batch 378/462, Loss: 0.7822360396385193\n",
      "Epoch 15, Batch 379/462, Loss: 0.7226065397262573\n",
      "Epoch 15, Batch 380/462, Loss: 0.6147926449775696\n",
      "Epoch 15, Batch 381/462, Loss: 0.8503405451774597\n",
      "Epoch 15, Batch 382/462, Loss: 0.7262052893638611\n",
      "Epoch 15, Batch 383/462, Loss: 0.8677313327789307\n",
      "Epoch 15, Batch 384/462, Loss: 0.6664537191390991\n",
      "Epoch 15, Batch 385/462, Loss: 0.863716721534729\n",
      "Epoch 15, Batch 386/462, Loss: 0.6103830337524414\n",
      "Epoch 15, Batch 387/462, Loss: 0.6741721034049988\n",
      "Epoch 15, Batch 388/462, Loss: 0.8038443922996521\n",
      "Epoch 15, Batch 389/462, Loss: 0.8547600507736206\n",
      "Epoch 15, Batch 390/462, Loss: 0.7752103805541992\n",
      "Epoch 15, Batch 391/462, Loss: 0.7229511737823486\n",
      "Epoch 15, Batch 392/462, Loss: 0.8580219149589539\n",
      "Epoch 15, Batch 393/462, Loss: 0.7400822639465332\n",
      "Epoch 15, Batch 394/462, Loss: 0.8355624079704285\n",
      "Epoch 15, Batch 395/462, Loss: 0.7435534000396729\n",
      "Epoch 15, Batch 396/462, Loss: 0.7124610543251038\n",
      "Epoch 15, Batch 397/462, Loss: 0.7531182765960693\n",
      "Epoch 15, Batch 398/462, Loss: 0.8556233644485474\n",
      "Epoch 15, Batch 399/462, Loss: 0.7939414978027344\n",
      "Epoch 15, Batch 400/462, Loss: 0.6210451722145081\n",
      "Epoch 15, Batch 401/462, Loss: 0.7212575078010559\n",
      "Epoch 15, Batch 402/462, Loss: 0.7220240831375122\n",
      "Epoch 15, Batch 403/462, Loss: 0.692238986492157\n",
      "Epoch 15, Batch 404/462, Loss: 0.758844256401062\n",
      "Epoch 15, Batch 405/462, Loss: 0.7485793828964233\n",
      "Epoch 15, Batch 406/462, Loss: 0.7670360803604126\n",
      "Epoch 15, Batch 407/462, Loss: 0.722408652305603\n",
      "Epoch 15, Batch 408/462, Loss: 0.594001293182373\n",
      "Epoch 15, Batch 409/462, Loss: 0.7407554388046265\n",
      "Epoch 15, Batch 410/462, Loss: 0.7678377032279968\n",
      "Epoch 15, Batch 411/462, Loss: 0.7165434956550598\n",
      "Epoch 15, Batch 412/462, Loss: 0.6746307015419006\n",
      "Epoch 15, Batch 413/462, Loss: 0.9509368538856506\n",
      "Epoch 15, Batch 414/462, Loss: 0.7205970287322998\n",
      "Epoch 15, Batch 415/462, Loss: 0.8273640871047974\n",
      "Epoch 15, Batch 416/462, Loss: 0.7033440470695496\n",
      "Epoch 15, Batch 417/462, Loss: 0.8434857130050659\n",
      "Epoch 15, Batch 418/462, Loss: 0.7716264128684998\n",
      "Epoch 15, Batch 419/462, Loss: 0.9488630294799805\n",
      "Epoch 15, Batch 420/462, Loss: 0.7619696259498596\n",
      "Epoch 15, Batch 421/462, Loss: 0.800818145275116\n",
      "Epoch 15, Batch 422/462, Loss: 0.719170093536377\n",
      "Epoch 15, Batch 423/462, Loss: 0.8163425922393799\n",
      "Epoch 15, Batch 424/462, Loss: 0.8051826357841492\n",
      "Epoch 15, Batch 425/462, Loss: 0.7268613576889038\n",
      "Epoch 15, Batch 426/462, Loss: 0.8071845173835754\n",
      "Epoch 15, Batch 427/462, Loss: 0.8967099189758301\n",
      "Epoch 15, Batch 428/462, Loss: 0.7072709798812866\n",
      "Epoch 15, Batch 429/462, Loss: 0.7673031091690063\n",
      "Epoch 15, Batch 430/462, Loss: 0.8820747137069702\n",
      "Epoch 15, Batch 431/462, Loss: 0.9286106824874878\n",
      "Epoch 15, Batch 432/462, Loss: 0.7444294095039368\n",
      "Epoch 15, Batch 433/462, Loss: 0.8487110137939453\n",
      "Epoch 15, Batch 434/462, Loss: 0.645416796207428\n",
      "Epoch 15, Batch 435/462, Loss: 0.8131335377693176\n",
      "Epoch 15, Batch 436/462, Loss: 0.6949465274810791\n",
      "Epoch 15, Batch 437/462, Loss: 0.7706025242805481\n",
      "Epoch 15, Batch 438/462, Loss: 0.7282409071922302\n",
      "Epoch 15, Batch 439/462, Loss: 0.7904883027076721\n",
      "Epoch 15, Batch 440/462, Loss: 0.7063247561454773\n",
      "Epoch 15, Batch 441/462, Loss: 0.5439627766609192\n",
      "Epoch 15, Batch 442/462, Loss: 0.6685821413993835\n",
      "Epoch 15, Batch 443/462, Loss: 0.6919101476669312\n",
      "Epoch 15, Batch 444/462, Loss: 0.8026037812232971\n",
      "Epoch 15, Batch 445/462, Loss: 0.7774866819381714\n",
      "Epoch 15, Batch 446/462, Loss: 0.7098125219345093\n",
      "Epoch 15, Batch 447/462, Loss: 0.8319553136825562\n",
      "Epoch 15, Batch 448/462, Loss: 0.7219547033309937\n",
      "Epoch 15, Batch 449/462, Loss: 0.7364285588264465\n",
      "Epoch 15, Batch 450/462, Loss: 0.6253021359443665\n",
      "Epoch 15, Batch 451/462, Loss: 0.7561264038085938\n",
      "Epoch 15, Batch 452/462, Loss: 0.7906514406204224\n",
      "Epoch 15, Batch 453/462, Loss: 0.803527295589447\n",
      "Epoch 15, Batch 454/462, Loss: 0.7233285307884216\n",
      "Epoch 15, Batch 455/462, Loss: 0.8526208400726318\n",
      "Epoch 15, Batch 456/462, Loss: 0.8055101037025452\n",
      "Epoch 15, Batch 457/462, Loss: 0.7829219698905945\n",
      "Epoch 15, Batch 458/462, Loss: 0.8836657404899597\n",
      "Epoch 15, Batch 459/462, Loss: 0.7550602555274963\n",
      "Epoch 15, Batch 460/462, Loss: 0.6857627630233765\n",
      "Epoch 15, Batch 461/462, Loss: 0.8353179097175598\n",
      "Epoch 15, Batch 462/462, Loss: 1.0929969549179077\n",
      "Epoch 15, Loss: 358.03367644548416\n",
      "Epoch 16, Batch 1/462, Loss: 0.6435466408729553\n",
      "Epoch 16, Batch 2/462, Loss: 0.5974754691123962\n",
      "Epoch 16, Batch 3/462, Loss: 0.7527621984481812\n",
      "Epoch 16, Batch 4/462, Loss: 0.8480017781257629\n",
      "Epoch 16, Batch 5/462, Loss: 0.838564932346344\n",
      "Epoch 16, Batch 6/462, Loss: 0.860100269317627\n",
      "Epoch 16, Batch 7/462, Loss: 0.7383518815040588\n",
      "Epoch 16, Batch 8/462, Loss: 0.6437917351722717\n",
      "Epoch 16, Batch 9/462, Loss: 0.7361979484558105\n",
      "Epoch 16, Batch 10/462, Loss: 0.8452954888343811\n",
      "Epoch 16, Batch 11/462, Loss: 0.7283340692520142\n",
      "Epoch 16, Batch 12/462, Loss: 0.8073543310165405\n",
      "Epoch 16, Batch 13/462, Loss: 0.623246967792511\n",
      "Epoch 16, Batch 14/462, Loss: 0.7885656952857971\n",
      "Epoch 16, Batch 15/462, Loss: 0.8550859093666077\n",
      "Epoch 16, Batch 16/462, Loss: 0.9840242862701416\n",
      "Epoch 16, Batch 17/462, Loss: 0.7799245119094849\n",
      "Epoch 16, Batch 18/462, Loss: 0.8565720319747925\n",
      "Epoch 16, Batch 19/462, Loss: 0.8065980672836304\n",
      "Epoch 16, Batch 20/462, Loss: 0.576371431350708\n",
      "Epoch 16, Batch 21/462, Loss: 0.7775957584381104\n",
      "Epoch 16, Batch 22/462, Loss: 0.6791577935218811\n",
      "Epoch 16, Batch 23/462, Loss: 0.8521904945373535\n",
      "Epoch 16, Batch 24/462, Loss: 0.7687186002731323\n",
      "Epoch 16, Batch 25/462, Loss: 0.7192714810371399\n",
      "Epoch 16, Batch 26/462, Loss: 0.7813203930854797\n",
      "Epoch 16, Batch 27/462, Loss: 0.7370545268058777\n",
      "Epoch 16, Batch 28/462, Loss: 0.7027549147605896\n",
      "Epoch 16, Batch 29/462, Loss: 0.7329796552658081\n",
      "Epoch 16, Batch 30/462, Loss: 0.8410933017730713\n",
      "Epoch 16, Batch 31/462, Loss: 0.6208646297454834\n",
      "Epoch 16, Batch 32/462, Loss: 0.7411654591560364\n",
      "Epoch 16, Batch 33/462, Loss: 0.7147102952003479\n",
      "Epoch 16, Batch 34/462, Loss: 0.8464150428771973\n",
      "Epoch 16, Batch 35/462, Loss: 0.7517534494400024\n",
      "Epoch 16, Batch 36/462, Loss: 0.8026525378227234\n",
      "Epoch 16, Batch 37/462, Loss: 0.6698242425918579\n",
      "Epoch 16, Batch 38/462, Loss: 0.7904720306396484\n",
      "Epoch 16, Batch 39/462, Loss: 0.6464091539382935\n",
      "Epoch 16, Batch 40/462, Loss: 0.7612735033035278\n",
      "Epoch 16, Batch 41/462, Loss: 0.8259565830230713\n",
      "Epoch 16, Batch 42/462, Loss: 0.6818516850471497\n",
      "Epoch 16, Batch 43/462, Loss: 0.8423767685890198\n",
      "Epoch 16, Batch 44/462, Loss: 0.8076440691947937\n",
      "Epoch 16, Batch 45/462, Loss: 0.668749213218689\n",
      "Epoch 16, Batch 46/462, Loss: 0.6046997308731079\n",
      "Epoch 16, Batch 47/462, Loss: 0.8197763562202454\n",
      "Epoch 16, Batch 48/462, Loss: 0.6969436407089233\n",
      "Epoch 16, Batch 49/462, Loss: 0.7270652651786804\n",
      "Epoch 16, Batch 50/462, Loss: 0.8200842142105103\n",
      "Epoch 16, Batch 51/462, Loss: 0.8396483659744263\n",
      "Epoch 16, Batch 52/462, Loss: 0.730908215045929\n",
      "Epoch 16, Batch 53/462, Loss: 0.9542747139930725\n",
      "Epoch 16, Batch 54/462, Loss: 0.7510402202606201\n",
      "Epoch 16, Batch 55/462, Loss: 0.635332465171814\n",
      "Epoch 16, Batch 56/462, Loss: 0.8505660891532898\n",
      "Epoch 16, Batch 57/462, Loss: 0.85593581199646\n",
      "Epoch 16, Batch 58/462, Loss: 0.8169912695884705\n",
      "Epoch 16, Batch 59/462, Loss: 0.7770467400550842\n",
      "Epoch 16, Batch 60/462, Loss: 0.804583728313446\n",
      "Epoch 16, Batch 61/462, Loss: 0.7152127623558044\n",
      "Epoch 16, Batch 62/462, Loss: 0.8731724619865417\n",
      "Epoch 16, Batch 63/462, Loss: 0.8193889260292053\n",
      "Epoch 16, Batch 64/462, Loss: 0.7396591901779175\n",
      "Epoch 16, Batch 65/462, Loss: 0.8532306551933289\n",
      "Epoch 16, Batch 66/462, Loss: 0.8658930659294128\n",
      "Epoch 16, Batch 67/462, Loss: 0.6710129380226135\n",
      "Epoch 16, Batch 68/462, Loss: 0.9109387993812561\n",
      "Epoch 16, Batch 69/462, Loss: 0.925467312335968\n",
      "Epoch 16, Batch 70/462, Loss: 0.6082229018211365\n",
      "Epoch 16, Batch 71/462, Loss: 0.777881383895874\n",
      "Epoch 16, Batch 72/462, Loss: 0.846304178237915\n",
      "Epoch 16, Batch 73/462, Loss: 0.8561124205589294\n",
      "Epoch 16, Batch 74/462, Loss: 0.7009435892105103\n",
      "Epoch 16, Batch 75/462, Loss: 0.7621991634368896\n",
      "Epoch 16, Batch 76/462, Loss: 0.8884949684143066\n",
      "Epoch 16, Batch 77/462, Loss: 0.7726888656616211\n",
      "Epoch 16, Batch 78/462, Loss: 0.7064056992530823\n",
      "Epoch 16, Batch 79/462, Loss: 0.7448232173919678\n",
      "Epoch 16, Batch 80/462, Loss: 1.0195224285125732\n",
      "Epoch 16, Batch 81/462, Loss: 0.7994256615638733\n",
      "Epoch 16, Batch 82/462, Loss: 0.661186933517456\n",
      "Epoch 16, Batch 83/462, Loss: 0.7562068104743958\n",
      "Epoch 16, Batch 84/462, Loss: 0.7683423757553101\n",
      "Epoch 16, Batch 85/462, Loss: 0.7770379781723022\n",
      "Epoch 16, Batch 86/462, Loss: 0.6527042984962463\n",
      "Epoch 16, Batch 87/462, Loss: 0.7423911094665527\n",
      "Epoch 16, Batch 88/462, Loss: 0.7532103061676025\n",
      "Epoch 16, Batch 89/462, Loss: 0.6754885911941528\n",
      "Epoch 16, Batch 90/462, Loss: 0.939887285232544\n",
      "Epoch 16, Batch 91/462, Loss: 0.7991275191307068\n",
      "Epoch 16, Batch 92/462, Loss: 0.6828408241271973\n",
      "Epoch 16, Batch 93/462, Loss: 0.9372292757034302\n",
      "Epoch 16, Batch 94/462, Loss: 0.9215772747993469\n",
      "Epoch 16, Batch 95/462, Loss: 0.819119393825531\n",
      "Epoch 16, Batch 96/462, Loss: 0.804986298084259\n",
      "Epoch 16, Batch 97/462, Loss: 0.7889683842658997\n",
      "Epoch 16, Batch 98/462, Loss: 0.8506348729133606\n",
      "Epoch 16, Batch 99/462, Loss: 0.7860993146896362\n",
      "Epoch 16, Batch 100/462, Loss: 0.8657642006874084\n",
      "Epoch 16, Batch 101/462, Loss: 0.8349412083625793\n",
      "Epoch 16, Batch 102/462, Loss: 0.758674681186676\n",
      "Epoch 16, Batch 103/462, Loss: 0.7598133683204651\n",
      "Epoch 16, Batch 104/462, Loss: 0.7361782193183899\n",
      "Epoch 16, Batch 105/462, Loss: 0.6960671544075012\n",
      "Epoch 16, Batch 106/462, Loss: 0.755221962928772\n",
      "Epoch 16, Batch 107/462, Loss: 0.7232300639152527\n",
      "Epoch 16, Batch 108/462, Loss: 0.7756041884422302\n",
      "Epoch 16, Batch 109/462, Loss: 0.5799732208251953\n",
      "Epoch 16, Batch 110/462, Loss: 0.8019843697547913\n",
      "Epoch 16, Batch 111/462, Loss: 0.7420954704284668\n",
      "Epoch 16, Batch 112/462, Loss: 0.8480419516563416\n",
      "Epoch 16, Batch 113/462, Loss: 0.7436018586158752\n",
      "Epoch 16, Batch 114/462, Loss: 0.7970088720321655\n",
      "Epoch 16, Batch 115/462, Loss: 0.606054425239563\n",
      "Epoch 16, Batch 116/462, Loss: 0.8867917060852051\n",
      "Epoch 16, Batch 117/462, Loss: 0.9104673266410828\n",
      "Epoch 16, Batch 118/462, Loss: 0.8409963846206665\n",
      "Epoch 16, Batch 119/462, Loss: 0.6849480867385864\n",
      "Epoch 16, Batch 120/462, Loss: 0.7155758142471313\n",
      "Epoch 16, Batch 121/462, Loss: 0.7082191705703735\n",
      "Epoch 16, Batch 122/462, Loss: 0.7745000123977661\n",
      "Epoch 16, Batch 123/462, Loss: 0.7208499312400818\n",
      "Epoch 16, Batch 124/462, Loss: 0.7625412940979004\n",
      "Epoch 16, Batch 125/462, Loss: 0.971160888671875\n",
      "Epoch 16, Batch 126/462, Loss: 0.6555451154708862\n",
      "Epoch 16, Batch 127/462, Loss: 0.8150907158851624\n",
      "Epoch 16, Batch 128/462, Loss: 0.830718994140625\n",
      "Epoch 16, Batch 129/462, Loss: 0.7016976475715637\n",
      "Epoch 16, Batch 130/462, Loss: 1.0689659118652344\n",
      "Epoch 16, Batch 131/462, Loss: 0.721648633480072\n",
      "Epoch 16, Batch 132/462, Loss: 0.8075141906738281\n",
      "Epoch 16, Batch 133/462, Loss: 0.8163462281227112\n",
      "Epoch 16, Batch 134/462, Loss: 0.9000127911567688\n",
      "Epoch 16, Batch 135/462, Loss: 0.8919397592544556\n",
      "Epoch 16, Batch 136/462, Loss: 0.8707496523857117\n",
      "Epoch 16, Batch 137/462, Loss: 0.6872996687889099\n",
      "Epoch 16, Batch 138/462, Loss: 0.8923438191413879\n",
      "Epoch 16, Batch 139/462, Loss: 0.9120212197303772\n",
      "Epoch 16, Batch 140/462, Loss: 0.8659250736236572\n",
      "Epoch 16, Batch 141/462, Loss: 0.7955988049507141\n",
      "Epoch 16, Batch 142/462, Loss: 0.8193352818489075\n",
      "Epoch 16, Batch 143/462, Loss: 0.7708526253700256\n",
      "Epoch 16, Batch 144/462, Loss: 0.7198704481124878\n",
      "Epoch 16, Batch 145/462, Loss: 0.6908442974090576\n",
      "Epoch 16, Batch 146/462, Loss: 1.1101073026657104\n",
      "Epoch 16, Batch 147/462, Loss: 0.8371504545211792\n",
      "Epoch 16, Batch 148/462, Loss: 0.7002419829368591\n",
      "Epoch 16, Batch 149/462, Loss: 0.6593266725540161\n",
      "Epoch 16, Batch 150/462, Loss: 0.7230899333953857\n",
      "Epoch 16, Batch 151/462, Loss: 0.7437918186187744\n",
      "Epoch 16, Batch 152/462, Loss: 0.751756489276886\n",
      "Epoch 16, Batch 153/462, Loss: 0.7060040235519409\n",
      "Epoch 16, Batch 154/462, Loss: 0.6752625703811646\n",
      "Epoch 16, Batch 155/462, Loss: 0.7480020523071289\n",
      "Epoch 16, Batch 156/462, Loss: 0.7713637351989746\n",
      "Epoch 16, Batch 157/462, Loss: 0.79749995470047\n",
      "Epoch 16, Batch 158/462, Loss: 0.8282518982887268\n",
      "Epoch 16, Batch 159/462, Loss: 0.6569978594779968\n",
      "Epoch 16, Batch 160/462, Loss: 0.7559712529182434\n",
      "Epoch 16, Batch 161/462, Loss: 0.6479043960571289\n",
      "Epoch 16, Batch 162/462, Loss: 0.60107421875\n",
      "Epoch 16, Batch 163/462, Loss: 0.653452455997467\n",
      "Epoch 16, Batch 164/462, Loss: 0.770697295665741\n",
      "Epoch 16, Batch 165/462, Loss: 0.9199817180633545\n",
      "Epoch 16, Batch 166/462, Loss: 0.9466570019721985\n",
      "Epoch 16, Batch 167/462, Loss: 0.7103050351142883\n",
      "Epoch 16, Batch 168/462, Loss: 0.6734837889671326\n",
      "Epoch 16, Batch 169/462, Loss: 0.6906225681304932\n",
      "Epoch 16, Batch 170/462, Loss: 0.6243370771408081\n",
      "Epoch 16, Batch 171/462, Loss: 0.7376792430877686\n",
      "Epoch 16, Batch 172/462, Loss: 0.9877866506576538\n",
      "Epoch 16, Batch 173/462, Loss: 0.8094960451126099\n",
      "Epoch 16, Batch 174/462, Loss: 0.8695170879364014\n",
      "Epoch 16, Batch 175/462, Loss: 0.6571536064147949\n",
      "Epoch 16, Batch 176/462, Loss: 0.5848775506019592\n",
      "Epoch 16, Batch 177/462, Loss: 0.9013715386390686\n",
      "Epoch 16, Batch 178/462, Loss: 0.7928075790405273\n",
      "Epoch 16, Batch 179/462, Loss: 0.7318934202194214\n",
      "Epoch 16, Batch 180/462, Loss: 0.742229700088501\n",
      "Epoch 16, Batch 181/462, Loss: 0.7801973819732666\n",
      "Epoch 16, Batch 182/462, Loss: 0.7942251563072205\n",
      "Epoch 16, Batch 183/462, Loss: 0.6977288722991943\n",
      "Epoch 16, Batch 184/462, Loss: 0.696406900882721\n",
      "Epoch 16, Batch 185/462, Loss: 0.8009675741195679\n",
      "Epoch 16, Batch 186/462, Loss: 0.7406322956085205\n",
      "Epoch 16, Batch 187/462, Loss: 0.8429450392723083\n",
      "Epoch 16, Batch 188/462, Loss: 0.6333200931549072\n",
      "Epoch 16, Batch 189/462, Loss: 0.9185508489608765\n",
      "Epoch 16, Batch 190/462, Loss: 0.7436879277229309\n",
      "Epoch 16, Batch 191/462, Loss: 0.8828956484794617\n",
      "Epoch 16, Batch 192/462, Loss: 0.7896711230278015\n",
      "Epoch 16, Batch 193/462, Loss: 0.6815822720527649\n",
      "Epoch 16, Batch 194/462, Loss: 0.6224206686019897\n",
      "Epoch 16, Batch 195/462, Loss: 0.7416328191757202\n",
      "Epoch 16, Batch 196/462, Loss: 0.6695950031280518\n",
      "Epoch 16, Batch 197/462, Loss: 0.8525904417037964\n",
      "Epoch 16, Batch 198/462, Loss: 0.8448734879493713\n",
      "Epoch 16, Batch 199/462, Loss: 0.7330957055091858\n",
      "Epoch 16, Batch 200/462, Loss: 0.738437831401825\n",
      "Epoch 16, Batch 201/462, Loss: 0.8116316199302673\n",
      "Epoch 16, Batch 202/462, Loss: 0.9056098461151123\n",
      "Epoch 16, Batch 203/462, Loss: 0.7345705032348633\n",
      "Epoch 16, Batch 204/462, Loss: 0.703513503074646\n",
      "Epoch 16, Batch 205/462, Loss: 0.7108662128448486\n",
      "Epoch 16, Batch 206/462, Loss: 0.8397706151008606\n",
      "Epoch 16, Batch 207/462, Loss: 0.8132918477058411\n",
      "Epoch 16, Batch 208/462, Loss: 0.667605459690094\n",
      "Epoch 16, Batch 209/462, Loss: 0.7208578586578369\n",
      "Epoch 16, Batch 210/462, Loss: 0.7089502811431885\n",
      "Epoch 16, Batch 211/462, Loss: 0.7357082962989807\n",
      "Epoch 16, Batch 212/462, Loss: 0.8043540120124817\n",
      "Epoch 16, Batch 213/462, Loss: 0.9575421214103699\n",
      "Epoch 16, Batch 214/462, Loss: 0.8102328181266785\n",
      "Epoch 16, Batch 215/462, Loss: 0.8917763829231262\n",
      "Epoch 16, Batch 216/462, Loss: 0.8613879680633545\n",
      "Epoch 16, Batch 217/462, Loss: 0.6061261296272278\n",
      "Epoch 16, Batch 218/462, Loss: 0.6236169338226318\n",
      "Epoch 16, Batch 219/462, Loss: 0.7265408635139465\n",
      "Epoch 16, Batch 220/462, Loss: 0.6579490900039673\n",
      "Epoch 16, Batch 221/462, Loss: 0.7187584638595581\n",
      "Epoch 16, Batch 222/462, Loss: 0.8989381194114685\n",
      "Epoch 16, Batch 223/462, Loss: 0.7657827138900757\n",
      "Epoch 16, Batch 224/462, Loss: 0.7509850263595581\n",
      "Epoch 16, Batch 225/462, Loss: 0.7846294045448303\n",
      "Epoch 16, Batch 226/462, Loss: 0.725679337978363\n",
      "Epoch 16, Batch 227/462, Loss: 0.8801868557929993\n",
      "Epoch 16, Batch 228/462, Loss: 0.8397183418273926\n",
      "Epoch 16, Batch 229/462, Loss: 1.028114914894104\n",
      "Epoch 16, Batch 230/462, Loss: 0.6813356876373291\n",
      "Epoch 16, Batch 231/462, Loss: 0.8415238261222839\n",
      "Epoch 16, Batch 232/462, Loss: 0.842944324016571\n",
      "Epoch 16, Batch 233/462, Loss: 0.7485713958740234\n",
      "Epoch 16, Batch 234/462, Loss: 0.9860299825668335\n",
      "Epoch 16, Batch 235/462, Loss: 0.7608513236045837\n",
      "Epoch 16, Batch 236/462, Loss: 0.702097475528717\n",
      "Epoch 16, Batch 237/462, Loss: 0.8784500360488892\n",
      "Epoch 16, Batch 238/462, Loss: 0.7380221486091614\n",
      "Epoch 16, Batch 239/462, Loss: 0.6677754521369934\n",
      "Epoch 16, Batch 240/462, Loss: 0.7585008144378662\n",
      "Epoch 16, Batch 241/462, Loss: 0.7674081921577454\n",
      "Epoch 16, Batch 242/462, Loss: 0.6336246132850647\n",
      "Epoch 16, Batch 243/462, Loss: 0.6752452850341797\n",
      "Epoch 16, Batch 244/462, Loss: 0.6648404002189636\n",
      "Epoch 16, Batch 245/462, Loss: 0.7680736780166626\n",
      "Epoch 16, Batch 246/462, Loss: 0.6385473012924194\n",
      "Epoch 16, Batch 247/462, Loss: 0.6339758038520813\n",
      "Epoch 16, Batch 248/462, Loss: 0.7635483145713806\n",
      "Epoch 16, Batch 249/462, Loss: 0.8416828513145447\n",
      "Epoch 16, Batch 250/462, Loss: 0.7851687669754028\n",
      "Epoch 16, Batch 251/462, Loss: 0.8330100774765015\n",
      "Epoch 16, Batch 252/462, Loss: 0.7608816027641296\n",
      "Epoch 16, Batch 253/462, Loss: 0.7213407158851624\n",
      "Epoch 16, Batch 254/462, Loss: 0.7540811896324158\n",
      "Epoch 16, Batch 255/462, Loss: 0.6458083391189575\n",
      "Epoch 16, Batch 256/462, Loss: 0.7606266140937805\n",
      "Epoch 16, Batch 257/462, Loss: 0.5753098130226135\n",
      "Epoch 16, Batch 258/462, Loss: 0.8107193112373352\n",
      "Epoch 16, Batch 259/462, Loss: 0.8026440739631653\n",
      "Epoch 16, Batch 260/462, Loss: 0.691979706287384\n",
      "Epoch 16, Batch 261/462, Loss: 0.8047545552253723\n",
      "Epoch 16, Batch 262/462, Loss: 0.7392049431800842\n",
      "Epoch 16, Batch 263/462, Loss: 0.6556496620178223\n",
      "Epoch 16, Batch 264/462, Loss: 0.7621119618415833\n",
      "Epoch 16, Batch 265/462, Loss: 0.7975361943244934\n",
      "Epoch 16, Batch 266/462, Loss: 0.7245556712150574\n",
      "Epoch 16, Batch 267/462, Loss: 0.7211580276489258\n",
      "Epoch 16, Batch 268/462, Loss: 0.6994091272354126\n",
      "Epoch 16, Batch 269/462, Loss: 0.8644446730613708\n",
      "Epoch 16, Batch 270/462, Loss: 0.8400135636329651\n",
      "Epoch 16, Batch 271/462, Loss: 0.6955443620681763\n",
      "Epoch 16, Batch 272/462, Loss: 0.6611719727516174\n",
      "Epoch 16, Batch 273/462, Loss: 0.7488060593605042\n",
      "Epoch 16, Batch 274/462, Loss: 0.9871871471405029\n",
      "Epoch 16, Batch 275/462, Loss: 0.7330998182296753\n",
      "Epoch 16, Batch 276/462, Loss: 0.6669133901596069\n",
      "Epoch 16, Batch 277/462, Loss: 0.7460773587226868\n",
      "Epoch 16, Batch 278/462, Loss: 0.7203439474105835\n",
      "Epoch 16, Batch 279/462, Loss: 0.8381994366645813\n",
      "Epoch 16, Batch 280/462, Loss: 0.7453998327255249\n",
      "Epoch 16, Batch 281/462, Loss: 0.6582183241844177\n",
      "Epoch 16, Batch 282/462, Loss: 0.8351902961730957\n",
      "Epoch 16, Batch 283/462, Loss: 0.7280824184417725\n",
      "Epoch 16, Batch 284/462, Loss: 0.8034160137176514\n",
      "Epoch 16, Batch 285/462, Loss: 0.7620944976806641\n",
      "Epoch 16, Batch 286/462, Loss: 0.7798830270767212\n",
      "Epoch 16, Batch 287/462, Loss: 0.7528467774391174\n",
      "Epoch 16, Batch 288/462, Loss: 0.648980975151062\n",
      "Epoch 16, Batch 289/462, Loss: 0.7564617991447449\n",
      "Epoch 16, Batch 290/462, Loss: 0.6344056129455566\n",
      "Epoch 16, Batch 291/462, Loss: 0.6553437113761902\n",
      "Epoch 16, Batch 292/462, Loss: 0.8912763595581055\n",
      "Epoch 16, Batch 293/462, Loss: 0.8466048836708069\n",
      "Epoch 16, Batch 294/462, Loss: 0.6327257752418518\n",
      "Epoch 16, Batch 295/462, Loss: 0.7311611175537109\n",
      "Epoch 16, Batch 296/462, Loss: 0.7111569046974182\n",
      "Epoch 16, Batch 297/462, Loss: 0.5940119028091431\n",
      "Epoch 16, Batch 298/462, Loss: 0.7893078327178955\n",
      "Epoch 16, Batch 299/462, Loss: 0.8652965426445007\n",
      "Epoch 16, Batch 300/462, Loss: 0.7182164192199707\n",
      "Epoch 16, Batch 301/462, Loss: 0.8831243515014648\n",
      "Epoch 16, Batch 302/462, Loss: 0.7634433507919312\n",
      "Epoch 16, Batch 303/462, Loss: 0.6323869228363037\n",
      "Epoch 16, Batch 304/462, Loss: 0.9023193717002869\n",
      "Epoch 16, Batch 305/462, Loss: 0.93025141954422\n",
      "Epoch 16, Batch 306/462, Loss: 0.7189531326293945\n",
      "Epoch 16, Batch 307/462, Loss: 0.6849578022956848\n",
      "Epoch 16, Batch 308/462, Loss: 0.8815546631813049\n",
      "Epoch 16, Batch 309/462, Loss: 0.8082195520401001\n",
      "Epoch 16, Batch 310/462, Loss: 0.7506760358810425\n",
      "Epoch 16, Batch 311/462, Loss: 0.7837620973587036\n",
      "Epoch 16, Batch 312/462, Loss: 0.8229728937149048\n",
      "Epoch 16, Batch 313/462, Loss: 0.667218029499054\n",
      "Epoch 16, Batch 314/462, Loss: 0.8146568536758423\n",
      "Epoch 16, Batch 315/462, Loss: 0.6300217509269714\n",
      "Epoch 16, Batch 316/462, Loss: 0.8093445897102356\n",
      "Epoch 16, Batch 317/462, Loss: 0.812958300113678\n",
      "Epoch 16, Batch 318/462, Loss: 0.9218602776527405\n",
      "Epoch 16, Batch 319/462, Loss: 0.627672553062439\n",
      "Epoch 16, Batch 320/462, Loss: 0.7614105939865112\n",
      "Epoch 16, Batch 321/462, Loss: 0.873576819896698\n",
      "Epoch 16, Batch 322/462, Loss: 0.9847069978713989\n",
      "Epoch 16, Batch 323/462, Loss: 0.6635213494300842\n",
      "Epoch 16, Batch 324/462, Loss: 0.7799967527389526\n",
      "Epoch 16, Batch 325/462, Loss: 0.9173348546028137\n",
      "Epoch 16, Batch 326/462, Loss: 0.7147119045257568\n",
      "Epoch 16, Batch 327/462, Loss: 0.8594992160797119\n",
      "Epoch 16, Batch 328/462, Loss: 0.6960356831550598\n",
      "Epoch 16, Batch 329/462, Loss: 0.7439797520637512\n",
      "Epoch 16, Batch 330/462, Loss: 0.7126602530479431\n",
      "Epoch 16, Batch 331/462, Loss: 0.7158017158508301\n",
      "Epoch 16, Batch 332/462, Loss: 0.8440282344818115\n",
      "Epoch 16, Batch 333/462, Loss: 0.7182514071464539\n",
      "Epoch 16, Batch 334/462, Loss: 0.6759733557701111\n",
      "Epoch 16, Batch 335/462, Loss: 0.8439179062843323\n",
      "Epoch 16, Batch 336/462, Loss: 0.7323114275932312\n",
      "Epoch 16, Batch 337/462, Loss: 0.656777024269104\n",
      "Epoch 16, Batch 338/462, Loss: 0.7822402119636536\n",
      "Epoch 16, Batch 339/462, Loss: 0.7752013802528381\n",
      "Epoch 16, Batch 340/462, Loss: 0.809605598449707\n",
      "Epoch 16, Batch 341/462, Loss: 0.7829239368438721\n",
      "Epoch 16, Batch 342/462, Loss: 0.8025923371315002\n",
      "Epoch 16, Batch 343/462, Loss: 0.8003288507461548\n",
      "Epoch 16, Batch 344/462, Loss: 0.7263544797897339\n",
      "Epoch 16, Batch 345/462, Loss: 0.7844377160072327\n",
      "Epoch 16, Batch 346/462, Loss: 0.7840166687965393\n",
      "Epoch 16, Batch 347/462, Loss: 0.6365222334861755\n",
      "Epoch 16, Batch 348/462, Loss: 0.7445061802864075\n",
      "Epoch 16, Batch 349/462, Loss: 0.7594669461250305\n",
      "Epoch 16, Batch 350/462, Loss: 0.7112329006195068\n",
      "Epoch 16, Batch 351/462, Loss: 0.8426069617271423\n",
      "Epoch 16, Batch 352/462, Loss: 0.7289048433303833\n",
      "Epoch 16, Batch 353/462, Loss: 0.7517353892326355\n",
      "Epoch 16, Batch 354/462, Loss: 0.7437909245491028\n",
      "Epoch 16, Batch 355/462, Loss: 0.839996337890625\n",
      "Epoch 16, Batch 356/462, Loss: 0.8111566305160522\n",
      "Epoch 16, Batch 357/462, Loss: 0.8877260088920593\n",
      "Epoch 16, Batch 358/462, Loss: 0.7686660289764404\n",
      "Epoch 16, Batch 359/462, Loss: 0.7858653664588928\n",
      "Epoch 16, Batch 360/462, Loss: 0.6400851011276245\n",
      "Epoch 16, Batch 361/462, Loss: 0.6838642954826355\n",
      "Epoch 16, Batch 362/462, Loss: 0.5572412014007568\n",
      "Epoch 16, Batch 363/462, Loss: 0.7680597305297852\n",
      "Epoch 16, Batch 364/462, Loss: 0.8817885518074036\n",
      "Epoch 16, Batch 365/462, Loss: 0.7927692532539368\n",
      "Epoch 16, Batch 366/462, Loss: 0.7653458118438721\n",
      "Epoch 16, Batch 367/462, Loss: 0.714335024356842\n",
      "Epoch 16, Batch 368/462, Loss: 0.7309120297431946\n",
      "Epoch 16, Batch 369/462, Loss: 0.7472549080848694\n",
      "Epoch 16, Batch 370/462, Loss: 0.7622183561325073\n",
      "Epoch 16, Batch 371/462, Loss: 0.9269775152206421\n",
      "Epoch 16, Batch 372/462, Loss: 0.6710081696510315\n",
      "Epoch 16, Batch 373/462, Loss: 0.6415001153945923\n",
      "Epoch 16, Batch 374/462, Loss: 0.6713075041770935\n",
      "Epoch 16, Batch 375/462, Loss: 0.777218759059906\n",
      "Epoch 16, Batch 376/462, Loss: 0.6954355835914612\n",
      "Epoch 16, Batch 377/462, Loss: 1.0059168338775635\n",
      "Epoch 16, Batch 378/462, Loss: 0.5542906522750854\n",
      "Epoch 16, Batch 379/462, Loss: 0.9047929048538208\n",
      "Epoch 16, Batch 380/462, Loss: 0.7382403612136841\n",
      "Epoch 16, Batch 381/462, Loss: 0.7854020595550537\n",
      "Epoch 16, Batch 382/462, Loss: 0.7669837474822998\n",
      "Epoch 16, Batch 383/462, Loss: 0.8312345743179321\n",
      "Epoch 16, Batch 384/462, Loss: 0.8353826999664307\n",
      "Epoch 16, Batch 385/462, Loss: 0.7034819722175598\n",
      "Epoch 16, Batch 386/462, Loss: 0.6899855136871338\n",
      "Epoch 16, Batch 387/462, Loss: 0.7784619927406311\n",
      "Epoch 16, Batch 388/462, Loss: 0.9549398422241211\n",
      "Epoch 16, Batch 389/462, Loss: 0.618186354637146\n",
      "Epoch 16, Batch 390/462, Loss: 0.7797438502311707\n",
      "Epoch 16, Batch 391/462, Loss: 0.7520304918289185\n",
      "Epoch 16, Batch 392/462, Loss: 0.7208786606788635\n",
      "Epoch 16, Batch 393/462, Loss: 0.7243627905845642\n",
      "Epoch 16, Batch 394/462, Loss: 0.7586386203765869\n",
      "Epoch 16, Batch 395/462, Loss: 0.6623178124427795\n",
      "Epoch 16, Batch 396/462, Loss: 0.5857864618301392\n",
      "Epoch 16, Batch 397/462, Loss: 0.7072685956954956\n",
      "Epoch 16, Batch 398/462, Loss: 0.760586142539978\n",
      "Epoch 16, Batch 399/462, Loss: 0.663497805595398\n",
      "Epoch 16, Batch 400/462, Loss: 0.7976489067077637\n",
      "Epoch 16, Batch 401/462, Loss: 0.7940065860748291\n",
      "Epoch 16, Batch 402/462, Loss: 0.7455006241798401\n",
      "Epoch 16, Batch 403/462, Loss: 0.7487483620643616\n",
      "Epoch 16, Batch 404/462, Loss: 0.7718302011489868\n",
      "Epoch 16, Batch 405/462, Loss: 0.6656283736228943\n",
      "Epoch 16, Batch 406/462, Loss: 0.726665198802948\n",
      "Epoch 16, Batch 407/462, Loss: 0.7956010699272156\n",
      "Epoch 16, Batch 408/462, Loss: 0.7862886190414429\n",
      "Epoch 16, Batch 409/462, Loss: 0.6517660617828369\n",
      "Epoch 16, Batch 410/462, Loss: 0.7555675506591797\n",
      "Epoch 16, Batch 411/462, Loss: 0.7720520496368408\n",
      "Epoch 16, Batch 412/462, Loss: 0.9114782214164734\n",
      "Epoch 16, Batch 413/462, Loss: 0.7735598087310791\n",
      "Epoch 16, Batch 414/462, Loss: 0.7432039380073547\n",
      "Epoch 16, Batch 415/462, Loss: 0.784980058670044\n",
      "Epoch 16, Batch 416/462, Loss: 0.83613520860672\n",
      "Epoch 16, Batch 417/462, Loss: 0.8740514516830444\n",
      "Epoch 16, Batch 418/462, Loss: 0.7528406381607056\n",
      "Epoch 16, Batch 419/462, Loss: 0.8606445789337158\n",
      "Epoch 16, Batch 420/462, Loss: 0.7320764064788818\n",
      "Epoch 16, Batch 421/462, Loss: 0.66996169090271\n",
      "Epoch 16, Batch 422/462, Loss: 0.7806851863861084\n",
      "Epoch 16, Batch 423/462, Loss: 0.6794925928115845\n",
      "Epoch 16, Batch 424/462, Loss: 0.5939852595329285\n",
      "Epoch 16, Batch 425/462, Loss: 0.9740038514137268\n",
      "Epoch 16, Batch 426/462, Loss: 1.0475900173187256\n",
      "Epoch 16, Batch 427/462, Loss: 0.8009520173072815\n",
      "Epoch 16, Batch 428/462, Loss: 0.6936877965927124\n",
      "Epoch 16, Batch 429/462, Loss: 0.7947531342506409\n",
      "Epoch 16, Batch 430/462, Loss: 0.7723455429077148\n",
      "Epoch 16, Batch 431/462, Loss: 0.7601035237312317\n",
      "Epoch 16, Batch 432/462, Loss: 0.8562578558921814\n",
      "Epoch 16, Batch 433/462, Loss: 0.7006317973136902\n",
      "Epoch 16, Batch 434/462, Loss: 0.6836702227592468\n",
      "Epoch 16, Batch 435/462, Loss: 0.8330032229423523\n",
      "Epoch 16, Batch 436/462, Loss: 0.7521973848342896\n",
      "Epoch 16, Batch 437/462, Loss: 0.8678348660469055\n",
      "Epoch 16, Batch 438/462, Loss: 0.7271761894226074\n",
      "Epoch 16, Batch 439/462, Loss: 0.7068682312965393\n",
      "Epoch 16, Batch 440/462, Loss: 0.8200735449790955\n",
      "Epoch 16, Batch 441/462, Loss: 0.7936729788780212\n",
      "Epoch 16, Batch 442/462, Loss: 0.7613184452056885\n",
      "Epoch 16, Batch 443/462, Loss: 0.890505850315094\n",
      "Epoch 16, Batch 444/462, Loss: 0.722606897354126\n",
      "Epoch 16, Batch 445/462, Loss: 0.7890360355377197\n",
      "Epoch 16, Batch 446/462, Loss: 0.9636055827140808\n",
      "Epoch 16, Batch 447/462, Loss: 0.9167348146438599\n",
      "Epoch 16, Batch 448/462, Loss: 0.6806387305259705\n",
      "Epoch 16, Batch 449/462, Loss: 0.7557753324508667\n",
      "Epoch 16, Batch 450/462, Loss: 0.8384519219398499\n",
      "Epoch 16, Batch 451/462, Loss: 0.6829137206077576\n",
      "Epoch 16, Batch 452/462, Loss: 0.7204641699790955\n",
      "Epoch 16, Batch 453/462, Loss: 0.6774542331695557\n",
      "Epoch 16, Batch 454/462, Loss: 0.7714012861251831\n",
      "Epoch 16, Batch 455/462, Loss: 0.7863712310791016\n",
      "Epoch 16, Batch 456/462, Loss: 0.8064950704574585\n",
      "Epoch 16, Batch 457/462, Loss: 0.7693243026733398\n",
      "Epoch 16, Batch 458/462, Loss: 0.7779916524887085\n",
      "Epoch 16, Batch 459/462, Loss: 0.7579807043075562\n",
      "Epoch 16, Batch 460/462, Loss: 0.7979122996330261\n",
      "Epoch 16, Batch 461/462, Loss: 0.8138267993927002\n",
      "Epoch 16, Batch 462/462, Loss: 0.8482187390327454\n",
      "Epoch 16, Loss: 355.08587324619293\n",
      "Epoch 17, Batch 1/462, Loss: 0.9467512369155884\n",
      "Epoch 17, Batch 2/462, Loss: 0.8742491006851196\n",
      "Epoch 17, Batch 3/462, Loss: 0.7161930799484253\n",
      "Epoch 17, Batch 4/462, Loss: 0.7241869568824768\n",
      "Epoch 17, Batch 5/462, Loss: 0.7148339152336121\n",
      "Epoch 17, Batch 6/462, Loss: 0.6054087281227112\n",
      "Epoch 17, Batch 7/462, Loss: 0.6256039142608643\n",
      "Epoch 17, Batch 8/462, Loss: 0.7912513017654419\n",
      "Epoch 17, Batch 9/462, Loss: 0.7289823889732361\n",
      "Epoch 17, Batch 10/462, Loss: 0.711258590221405\n",
      "Epoch 17, Batch 11/462, Loss: 0.7004505395889282\n",
      "Epoch 17, Batch 12/462, Loss: 0.8919517993927002\n",
      "Epoch 17, Batch 13/462, Loss: 0.8430313467979431\n",
      "Epoch 17, Batch 14/462, Loss: 0.760408878326416\n",
      "Epoch 17, Batch 15/462, Loss: 0.7590851187705994\n",
      "Epoch 17, Batch 16/462, Loss: 0.8192217350006104\n",
      "Epoch 17, Batch 17/462, Loss: 0.8015045523643494\n",
      "Epoch 17, Batch 18/462, Loss: 0.6672466397285461\n",
      "Epoch 17, Batch 19/462, Loss: 0.8333808183670044\n",
      "Epoch 17, Batch 20/462, Loss: 0.6715864539146423\n",
      "Epoch 17, Batch 21/462, Loss: 0.6901944875717163\n",
      "Epoch 17, Batch 22/462, Loss: 0.8744074106216431\n",
      "Epoch 17, Batch 23/462, Loss: 0.7760672569274902\n",
      "Epoch 17, Batch 24/462, Loss: 0.7899934649467468\n",
      "Epoch 17, Batch 25/462, Loss: 0.6716275215148926\n",
      "Epoch 17, Batch 26/462, Loss: 0.6165900230407715\n",
      "Epoch 17, Batch 27/462, Loss: 0.7859870195388794\n",
      "Epoch 17, Batch 28/462, Loss: 0.8053447604179382\n",
      "Epoch 17, Batch 29/462, Loss: 0.631317675113678\n",
      "Epoch 17, Batch 30/462, Loss: 0.8034763932228088\n",
      "Epoch 17, Batch 31/462, Loss: 0.8841874599456787\n",
      "Epoch 17, Batch 32/462, Loss: 0.7305081486701965\n",
      "Epoch 17, Batch 33/462, Loss: 0.7844652533531189\n",
      "Epoch 17, Batch 34/462, Loss: 0.6745128631591797\n",
      "Epoch 17, Batch 35/462, Loss: 0.9790418148040771\n",
      "Epoch 17, Batch 36/462, Loss: 0.9359567761421204\n",
      "Epoch 17, Batch 37/462, Loss: 0.7302281260490417\n",
      "Epoch 17, Batch 38/462, Loss: 0.7311661243438721\n",
      "Epoch 17, Batch 39/462, Loss: 0.6937099099159241\n",
      "Epoch 17, Batch 40/462, Loss: 0.9219244122505188\n",
      "Epoch 17, Batch 41/462, Loss: 0.8189517259597778\n",
      "Epoch 17, Batch 42/462, Loss: 0.776001513004303\n",
      "Epoch 17, Batch 43/462, Loss: 0.7528347969055176\n",
      "Epoch 17, Batch 44/462, Loss: 0.8606114387512207\n",
      "Epoch 17, Batch 45/462, Loss: 0.744724690914154\n",
      "Epoch 17, Batch 46/462, Loss: 0.77077716588974\n",
      "Epoch 17, Batch 47/462, Loss: 0.8057901859283447\n",
      "Epoch 17, Batch 48/462, Loss: 0.5967532992362976\n",
      "Epoch 17, Batch 49/462, Loss: 0.8699958324432373\n",
      "Epoch 17, Batch 50/462, Loss: 0.7012401223182678\n",
      "Epoch 17, Batch 51/462, Loss: 0.9449428915977478\n",
      "Epoch 17, Batch 52/462, Loss: 0.6466252207756042\n",
      "Epoch 17, Batch 53/462, Loss: 0.7538855075836182\n",
      "Epoch 17, Batch 54/462, Loss: 0.7351333498954773\n",
      "Epoch 17, Batch 55/462, Loss: 0.9661649465560913\n",
      "Epoch 17, Batch 56/462, Loss: 0.682926595211029\n",
      "Epoch 17, Batch 57/462, Loss: 0.5526385307312012\n",
      "Epoch 17, Batch 58/462, Loss: 0.6141679286956787\n",
      "Epoch 17, Batch 59/462, Loss: 0.6119855642318726\n",
      "Epoch 17, Batch 60/462, Loss: 0.9009564518928528\n",
      "Epoch 17, Batch 61/462, Loss: 0.9684461355209351\n",
      "Epoch 17, Batch 62/462, Loss: 0.7223489880561829\n",
      "Epoch 17, Batch 63/462, Loss: 0.6357367038726807\n",
      "Epoch 17, Batch 64/462, Loss: 0.7607021331787109\n",
      "Epoch 17, Batch 65/462, Loss: 0.8071543574333191\n",
      "Epoch 17, Batch 66/462, Loss: 0.8291893601417542\n",
      "Epoch 17, Batch 67/462, Loss: 0.8104166984558105\n",
      "Epoch 17, Batch 68/462, Loss: 0.7631746530532837\n",
      "Epoch 17, Batch 69/462, Loss: 0.6951609253883362\n",
      "Epoch 17, Batch 70/462, Loss: 0.6287505626678467\n",
      "Epoch 17, Batch 71/462, Loss: 0.7237933278083801\n",
      "Epoch 17, Batch 72/462, Loss: 0.8306424617767334\n",
      "Epoch 17, Batch 73/462, Loss: 0.9033403396606445\n",
      "Epoch 17, Batch 74/462, Loss: 0.9797260761260986\n",
      "Epoch 17, Batch 75/462, Loss: 0.7178133130073547\n",
      "Epoch 17, Batch 76/462, Loss: 0.6023563742637634\n",
      "Epoch 17, Batch 77/462, Loss: 0.6173593401908875\n",
      "Epoch 17, Batch 78/462, Loss: 0.8184167742729187\n",
      "Epoch 17, Batch 79/462, Loss: 0.727753758430481\n",
      "Epoch 17, Batch 80/462, Loss: 0.9616371989250183\n",
      "Epoch 17, Batch 81/462, Loss: 0.6678555011749268\n",
      "Epoch 17, Batch 82/462, Loss: 0.8466501235961914\n",
      "Epoch 17, Batch 83/462, Loss: 0.7721861600875854\n",
      "Epoch 17, Batch 84/462, Loss: 0.7876496315002441\n",
      "Epoch 17, Batch 85/462, Loss: 0.8468517065048218\n",
      "Epoch 17, Batch 86/462, Loss: 0.7466357946395874\n",
      "Epoch 17, Batch 87/462, Loss: 0.8521460294723511\n",
      "Epoch 17, Batch 88/462, Loss: 0.9766032695770264\n",
      "Epoch 17, Batch 89/462, Loss: 0.8052257895469666\n",
      "Epoch 17, Batch 90/462, Loss: 0.7396835684776306\n",
      "Epoch 17, Batch 91/462, Loss: 0.8225216269493103\n",
      "Epoch 17, Batch 92/462, Loss: 0.7634443044662476\n",
      "Epoch 17, Batch 93/462, Loss: 0.8736583590507507\n",
      "Epoch 17, Batch 94/462, Loss: 0.903652548789978\n",
      "Epoch 17, Batch 95/462, Loss: 0.6896276473999023\n",
      "Epoch 17, Batch 96/462, Loss: 0.6530478000640869\n",
      "Epoch 17, Batch 97/462, Loss: 0.8538317084312439\n",
      "Epoch 17, Batch 98/462, Loss: 0.6954236030578613\n",
      "Epoch 17, Batch 99/462, Loss: 0.720299243927002\n",
      "Epoch 17, Batch 100/462, Loss: 0.9466086030006409\n",
      "Epoch 17, Batch 101/462, Loss: 0.9028298854827881\n",
      "Epoch 17, Batch 102/462, Loss: 0.8375515937805176\n",
      "Epoch 17, Batch 103/462, Loss: 0.7001723647117615\n",
      "Epoch 17, Batch 104/462, Loss: 0.8193439245223999\n",
      "Epoch 17, Batch 105/462, Loss: 0.652897298336029\n",
      "Epoch 17, Batch 106/462, Loss: 0.8211371302604675\n",
      "Epoch 17, Batch 107/462, Loss: 0.5923268795013428\n",
      "Epoch 17, Batch 108/462, Loss: 0.8043407797813416\n",
      "Epoch 17, Batch 109/462, Loss: 0.6901509761810303\n",
      "Epoch 17, Batch 110/462, Loss: 0.7825916409492493\n",
      "Epoch 17, Batch 111/462, Loss: 0.9021541476249695\n",
      "Epoch 17, Batch 112/462, Loss: 0.6963733434677124\n",
      "Epoch 17, Batch 113/462, Loss: 0.6904302835464478\n",
      "Epoch 17, Batch 114/462, Loss: 0.7578983902931213\n",
      "Epoch 17, Batch 115/462, Loss: 0.7428560853004456\n",
      "Epoch 17, Batch 116/462, Loss: 0.7678895592689514\n",
      "Epoch 17, Batch 117/462, Loss: 0.7451735734939575\n",
      "Epoch 17, Batch 118/462, Loss: 0.76555997133255\n",
      "Epoch 17, Batch 119/462, Loss: 0.8301966786384583\n",
      "Epoch 17, Batch 120/462, Loss: 0.6628201007843018\n",
      "Epoch 17, Batch 121/462, Loss: 0.7807012796401978\n",
      "Epoch 17, Batch 122/462, Loss: 0.7834453582763672\n",
      "Epoch 17, Batch 123/462, Loss: 0.7901051044464111\n",
      "Epoch 17, Batch 124/462, Loss: 0.7449600100517273\n",
      "Epoch 17, Batch 125/462, Loss: 0.7406576871871948\n",
      "Epoch 17, Batch 126/462, Loss: 0.7692410349845886\n",
      "Epoch 17, Batch 127/462, Loss: 0.7661230564117432\n",
      "Epoch 17, Batch 128/462, Loss: 0.943605363368988\n",
      "Epoch 17, Batch 129/462, Loss: 0.7028966546058655\n",
      "Epoch 17, Batch 130/462, Loss: 0.6831443309783936\n",
      "Epoch 17, Batch 131/462, Loss: 0.904607355594635\n",
      "Epoch 17, Batch 132/462, Loss: 0.7727095484733582\n",
      "Epoch 17, Batch 133/462, Loss: 0.6374499797821045\n",
      "Epoch 17, Batch 134/462, Loss: 0.7458314299583435\n",
      "Epoch 17, Batch 135/462, Loss: 0.672036349773407\n",
      "Epoch 17, Batch 136/462, Loss: 0.8541760444641113\n",
      "Epoch 17, Batch 137/462, Loss: 0.791660487651825\n",
      "Epoch 17, Batch 138/462, Loss: 0.7092507481575012\n",
      "Epoch 17, Batch 139/462, Loss: 0.8414790630340576\n",
      "Epoch 17, Batch 140/462, Loss: 0.6295820474624634\n",
      "Epoch 17, Batch 141/462, Loss: 0.6217286586761475\n",
      "Epoch 17, Batch 142/462, Loss: 0.7990565299987793\n",
      "Epoch 17, Batch 143/462, Loss: 0.9081634879112244\n",
      "Epoch 17, Batch 144/462, Loss: 0.6978420615196228\n",
      "Epoch 17, Batch 145/462, Loss: 0.6156066656112671\n",
      "Epoch 17, Batch 146/462, Loss: 0.7635834217071533\n",
      "Epoch 17, Batch 147/462, Loss: 1.0252190828323364\n",
      "Epoch 17, Batch 148/462, Loss: 0.787592351436615\n",
      "Epoch 17, Batch 149/462, Loss: 0.8825554847717285\n",
      "Epoch 17, Batch 150/462, Loss: 0.9051884412765503\n",
      "Epoch 17, Batch 151/462, Loss: 0.9369182586669922\n",
      "Epoch 17, Batch 152/462, Loss: 0.8221330642700195\n",
      "Epoch 17, Batch 153/462, Loss: 0.7002642750740051\n",
      "Epoch 17, Batch 154/462, Loss: 0.7509623169898987\n",
      "Epoch 17, Batch 155/462, Loss: 0.8434594869613647\n",
      "Epoch 17, Batch 156/462, Loss: 0.7789327502250671\n",
      "Epoch 17, Batch 157/462, Loss: 0.732646644115448\n",
      "Epoch 17, Batch 158/462, Loss: 0.649172842502594\n",
      "Epoch 17, Batch 159/462, Loss: 0.5928217172622681\n",
      "Epoch 17, Batch 160/462, Loss: 0.7160034775733948\n",
      "Epoch 17, Batch 161/462, Loss: 0.7570023536682129\n",
      "Epoch 17, Batch 162/462, Loss: 0.7923811078071594\n",
      "Epoch 17, Batch 163/462, Loss: 0.7214034199714661\n",
      "Epoch 17, Batch 164/462, Loss: 0.7234105467796326\n",
      "Epoch 17, Batch 165/462, Loss: 0.9651427865028381\n",
      "Epoch 17, Batch 166/462, Loss: 0.9867683053016663\n",
      "Epoch 17, Batch 167/462, Loss: 0.7153582572937012\n",
      "Epoch 17, Batch 168/462, Loss: 0.7780935764312744\n",
      "Epoch 17, Batch 169/462, Loss: 0.8045991063117981\n",
      "Epoch 17, Batch 170/462, Loss: 0.6959661245346069\n",
      "Epoch 17, Batch 171/462, Loss: 0.6125256419181824\n",
      "Epoch 17, Batch 172/462, Loss: 0.7182658314704895\n",
      "Epoch 17, Batch 173/462, Loss: 0.7599825859069824\n",
      "Epoch 17, Batch 174/462, Loss: 0.6243045330047607\n",
      "Epoch 17, Batch 175/462, Loss: 0.7914214134216309\n",
      "Epoch 17, Batch 176/462, Loss: 0.7602176666259766\n",
      "Epoch 17, Batch 177/462, Loss: 0.7065312266349792\n",
      "Epoch 17, Batch 178/462, Loss: 0.6566305756568909\n",
      "Epoch 17, Batch 179/462, Loss: 0.7293968200683594\n",
      "Epoch 17, Batch 180/462, Loss: 0.6117233037948608\n",
      "Epoch 17, Batch 181/462, Loss: 0.8376816511154175\n",
      "Epoch 17, Batch 182/462, Loss: 0.7793558835983276\n",
      "Epoch 17, Batch 183/462, Loss: 0.7805027961730957\n",
      "Epoch 17, Batch 184/462, Loss: 0.6479756832122803\n",
      "Epoch 17, Batch 185/462, Loss: 0.7923771739006042\n",
      "Epoch 17, Batch 186/462, Loss: 0.5922551155090332\n",
      "Epoch 17, Batch 187/462, Loss: 0.7881165146827698\n",
      "Epoch 17, Batch 188/462, Loss: 0.9133042097091675\n",
      "Epoch 17, Batch 189/462, Loss: 0.6904857158660889\n",
      "Epoch 17, Batch 190/462, Loss: 0.998065710067749\n",
      "Epoch 17, Batch 191/462, Loss: 0.7541542053222656\n",
      "Epoch 17, Batch 192/462, Loss: 0.7979150414466858\n",
      "Epoch 17, Batch 193/462, Loss: 0.8374782800674438\n",
      "Epoch 17, Batch 194/462, Loss: 0.845697820186615\n",
      "Epoch 17, Batch 195/462, Loss: 0.7618749141693115\n",
      "Epoch 17, Batch 196/462, Loss: 0.8937899470329285\n",
      "Epoch 17, Batch 197/462, Loss: 0.8254499435424805\n",
      "Epoch 17, Batch 198/462, Loss: 0.611125648021698\n",
      "Epoch 17, Batch 199/462, Loss: 0.7971107363700867\n",
      "Epoch 17, Batch 200/462, Loss: 0.7275899052619934\n",
      "Epoch 17, Batch 201/462, Loss: 0.8323131203651428\n",
      "Epoch 17, Batch 202/462, Loss: 0.6485481858253479\n",
      "Epoch 17, Batch 203/462, Loss: 0.7861548662185669\n",
      "Epoch 17, Batch 204/462, Loss: 0.7830389142036438\n",
      "Epoch 17, Batch 205/462, Loss: 0.8258764743804932\n",
      "Epoch 17, Batch 206/462, Loss: 0.7781673669815063\n",
      "Epoch 17, Batch 207/462, Loss: 0.8547752499580383\n",
      "Epoch 17, Batch 208/462, Loss: 0.8952347040176392\n",
      "Epoch 17, Batch 209/462, Loss: 0.727726936340332\n",
      "Epoch 17, Batch 210/462, Loss: 0.8468313813209534\n",
      "Epoch 17, Batch 211/462, Loss: 0.7205204963684082\n",
      "Epoch 17, Batch 212/462, Loss: 0.7987737059593201\n",
      "Epoch 17, Batch 213/462, Loss: 0.7481377720832825\n",
      "Epoch 17, Batch 214/462, Loss: 0.646175742149353\n",
      "Epoch 17, Batch 215/462, Loss: 1.0253880023956299\n",
      "Epoch 17, Batch 216/462, Loss: 0.7128352522850037\n",
      "Epoch 17, Batch 217/462, Loss: 0.8352424502372742\n",
      "Epoch 17, Batch 218/462, Loss: 0.8453627228736877\n",
      "Epoch 17, Batch 219/462, Loss: 0.6745443940162659\n",
      "Epoch 17, Batch 220/462, Loss: 0.6829420328140259\n",
      "Epoch 17, Batch 221/462, Loss: 0.5969602465629578\n",
      "Epoch 17, Batch 222/462, Loss: 0.7389777302742004\n",
      "Epoch 17, Batch 223/462, Loss: 0.7211874127388\n",
      "Epoch 17, Batch 224/462, Loss: 0.7885720729827881\n",
      "Epoch 17, Batch 225/462, Loss: 0.6703670024871826\n",
      "Epoch 17, Batch 226/462, Loss: 0.6916727423667908\n",
      "Epoch 17, Batch 227/462, Loss: 0.6631971001625061\n",
      "Epoch 17, Batch 228/462, Loss: 0.6705000400543213\n",
      "Epoch 17, Batch 229/462, Loss: 0.850503146648407\n",
      "Epoch 17, Batch 230/462, Loss: 0.7023831605911255\n",
      "Epoch 17, Batch 231/462, Loss: 0.781863272190094\n",
      "Epoch 17, Batch 232/462, Loss: 0.8814003467559814\n",
      "Epoch 17, Batch 233/462, Loss: 0.863429844379425\n",
      "Epoch 17, Batch 234/462, Loss: 0.7581256031990051\n",
      "Epoch 17, Batch 235/462, Loss: 0.716134786605835\n",
      "Epoch 17, Batch 236/462, Loss: 0.7586521506309509\n",
      "Epoch 17, Batch 237/462, Loss: 0.6687741875648499\n",
      "Epoch 17, Batch 238/462, Loss: 0.6599314212799072\n",
      "Epoch 17, Batch 239/462, Loss: 0.7752976417541504\n",
      "Epoch 17, Batch 240/462, Loss: 0.759654700756073\n",
      "Epoch 17, Batch 241/462, Loss: 0.6911842823028564\n",
      "Epoch 17, Batch 242/462, Loss: 0.6755216717720032\n",
      "Epoch 17, Batch 243/462, Loss: 0.7347307205200195\n",
      "Epoch 17, Batch 244/462, Loss: 0.5667557120323181\n",
      "Epoch 17, Batch 245/462, Loss: 0.7955917119979858\n",
      "Epoch 17, Batch 246/462, Loss: 0.8274039030075073\n",
      "Epoch 17, Batch 247/462, Loss: 0.9003123641014099\n",
      "Epoch 17, Batch 248/462, Loss: 0.7733340263366699\n",
      "Epoch 17, Batch 249/462, Loss: 0.7344966530799866\n",
      "Epoch 17, Batch 250/462, Loss: 0.6438115835189819\n",
      "Epoch 17, Batch 251/462, Loss: 0.8649803996086121\n",
      "Epoch 17, Batch 252/462, Loss: 0.8784017562866211\n",
      "Epoch 17, Batch 253/462, Loss: 0.8151682615280151\n",
      "Epoch 17, Batch 254/462, Loss: 0.8230482935905457\n",
      "Epoch 17, Batch 255/462, Loss: 0.8755804300308228\n",
      "Epoch 17, Batch 256/462, Loss: 0.8069130182266235\n",
      "Epoch 17, Batch 257/462, Loss: 0.7023434042930603\n",
      "Epoch 17, Batch 258/462, Loss: 0.659268319606781\n",
      "Epoch 17, Batch 259/462, Loss: 0.8083552718162537\n",
      "Epoch 17, Batch 260/462, Loss: 0.6569793224334717\n",
      "Epoch 17, Batch 261/462, Loss: 0.6800629496574402\n",
      "Epoch 17, Batch 262/462, Loss: 0.715814471244812\n",
      "Epoch 17, Batch 263/462, Loss: 0.72728431224823\n",
      "Epoch 17, Batch 264/462, Loss: 0.7312504649162292\n",
      "Epoch 17, Batch 265/462, Loss: 0.7287908792495728\n",
      "Epoch 17, Batch 266/462, Loss: 0.5977820754051208\n",
      "Epoch 17, Batch 267/462, Loss: 0.7906562089920044\n",
      "Epoch 17, Batch 268/462, Loss: 0.6295307278633118\n",
      "Epoch 17, Batch 269/462, Loss: 0.8459829688072205\n",
      "Epoch 17, Batch 270/462, Loss: 0.8709871768951416\n",
      "Epoch 17, Batch 271/462, Loss: 0.7455689311027527\n",
      "Epoch 17, Batch 272/462, Loss: 0.6641563177108765\n",
      "Epoch 17, Batch 273/462, Loss: 0.7518952488899231\n",
      "Epoch 17, Batch 274/462, Loss: 0.6836897730827332\n",
      "Epoch 17, Batch 275/462, Loss: 0.7183092832565308\n",
      "Epoch 17, Batch 276/462, Loss: 0.7641485333442688\n",
      "Epoch 17, Batch 277/462, Loss: 0.8023079037666321\n",
      "Epoch 17, Batch 278/462, Loss: 0.6819937229156494\n",
      "Epoch 17, Batch 279/462, Loss: 0.6522127389907837\n",
      "Epoch 17, Batch 280/462, Loss: 0.635758101940155\n",
      "Epoch 17, Batch 281/462, Loss: 0.7482913732528687\n",
      "Epoch 17, Batch 282/462, Loss: 0.9576992392539978\n",
      "Epoch 17, Batch 283/462, Loss: 0.7829129695892334\n",
      "Epoch 17, Batch 284/462, Loss: 0.8062824606895447\n",
      "Epoch 17, Batch 285/462, Loss: 0.6634612679481506\n",
      "Epoch 17, Batch 286/462, Loss: 0.8287999629974365\n",
      "Epoch 17, Batch 287/462, Loss: 0.7932629585266113\n",
      "Epoch 17, Batch 288/462, Loss: 0.7208209037780762\n",
      "Epoch 17, Batch 289/462, Loss: 0.8036495447158813\n",
      "Epoch 17, Batch 290/462, Loss: 0.7173849940299988\n",
      "Epoch 17, Batch 291/462, Loss: 0.5781093835830688\n",
      "Epoch 17, Batch 292/462, Loss: 0.8631013631820679\n",
      "Epoch 17, Batch 293/462, Loss: 0.8478084802627563\n",
      "Epoch 17, Batch 294/462, Loss: 0.7479792237281799\n",
      "Epoch 17, Batch 295/462, Loss: 0.7996953725814819\n",
      "Epoch 17, Batch 296/462, Loss: 0.6822469234466553\n",
      "Epoch 17, Batch 297/462, Loss: 0.7269394993782043\n",
      "Epoch 17, Batch 298/462, Loss: 0.886851966381073\n",
      "Epoch 17, Batch 299/462, Loss: 0.6782740950584412\n",
      "Epoch 17, Batch 300/462, Loss: 0.922664999961853\n",
      "Epoch 17, Batch 301/462, Loss: 0.885749876499176\n",
      "Epoch 17, Batch 302/462, Loss: 0.8017331957817078\n",
      "Epoch 17, Batch 303/462, Loss: 0.8833908438682556\n",
      "Epoch 17, Batch 304/462, Loss: 0.8171603679656982\n",
      "Epoch 17, Batch 305/462, Loss: 0.8524206876754761\n",
      "Epoch 17, Batch 306/462, Loss: 0.8915730714797974\n",
      "Epoch 17, Batch 307/462, Loss: 0.6570878028869629\n",
      "Epoch 17, Batch 308/462, Loss: 0.7660529613494873\n",
      "Epoch 17, Batch 309/462, Loss: 0.870693027973175\n",
      "Epoch 17, Batch 310/462, Loss: 0.7123563289642334\n",
      "Epoch 17, Batch 311/462, Loss: 0.7179762125015259\n",
      "Epoch 17, Batch 312/462, Loss: 0.7169688940048218\n",
      "Epoch 17, Batch 313/462, Loss: 0.6580065488815308\n",
      "Epoch 17, Batch 314/462, Loss: 0.9006144404411316\n",
      "Epoch 17, Batch 315/462, Loss: 0.9005747437477112\n",
      "Epoch 17, Batch 316/462, Loss: 0.744890034198761\n",
      "Epoch 17, Batch 317/462, Loss: 0.6745351552963257\n",
      "Epoch 17, Batch 318/462, Loss: 0.8834601640701294\n",
      "Epoch 17, Batch 319/462, Loss: 0.7101724147796631\n",
      "Epoch 17, Batch 320/462, Loss: 0.7998225688934326\n",
      "Epoch 17, Batch 321/462, Loss: 0.7886084914207458\n",
      "Epoch 17, Batch 322/462, Loss: 0.8255305290222168\n",
      "Epoch 17, Batch 323/462, Loss: 0.6660318970680237\n",
      "Epoch 17, Batch 324/462, Loss: 0.7523693442344666\n",
      "Epoch 17, Batch 325/462, Loss: 0.8172425627708435\n",
      "Epoch 17, Batch 326/462, Loss: 0.7824845910072327\n",
      "Epoch 17, Batch 327/462, Loss: 0.6237474679946899\n",
      "Epoch 17, Batch 328/462, Loss: 0.7381426095962524\n",
      "Epoch 17, Batch 329/462, Loss: 0.8482113480567932\n",
      "Epoch 17, Batch 330/462, Loss: 0.7388406991958618\n",
      "Epoch 17, Batch 331/462, Loss: 0.8507804870605469\n",
      "Epoch 17, Batch 332/462, Loss: 0.6827558279037476\n",
      "Epoch 17, Batch 333/462, Loss: 0.6626967191696167\n",
      "Epoch 17, Batch 334/462, Loss: 0.7749545574188232\n",
      "Epoch 17, Batch 335/462, Loss: 0.7666907906532288\n",
      "Epoch 17, Batch 336/462, Loss: 0.7615350484848022\n",
      "Epoch 17, Batch 337/462, Loss: 0.8678908348083496\n",
      "Epoch 17, Batch 338/462, Loss: 0.7774814367294312\n",
      "Epoch 17, Batch 339/462, Loss: 0.5994678735733032\n",
      "Epoch 17, Batch 340/462, Loss: 0.7920194864273071\n",
      "Epoch 17, Batch 341/462, Loss: 0.6387243866920471\n",
      "Epoch 17, Batch 342/462, Loss: 0.8310763835906982\n",
      "Epoch 17, Batch 343/462, Loss: 0.7521511912345886\n",
      "Epoch 17, Batch 344/462, Loss: 0.9110344648361206\n",
      "Epoch 17, Batch 345/462, Loss: 0.8619940876960754\n",
      "Epoch 17, Batch 346/462, Loss: 0.7276405692100525\n",
      "Epoch 17, Batch 347/462, Loss: 0.5963624715805054\n",
      "Epoch 17, Batch 348/462, Loss: 0.7268449068069458\n",
      "Epoch 17, Batch 349/462, Loss: 0.7102479934692383\n",
      "Epoch 17, Batch 350/462, Loss: 0.699367880821228\n",
      "Epoch 17, Batch 351/462, Loss: 0.6894805431365967\n",
      "Epoch 17, Batch 352/462, Loss: 0.7769461274147034\n",
      "Epoch 17, Batch 353/462, Loss: 1.03684401512146\n",
      "Epoch 17, Batch 354/462, Loss: 0.6652981638908386\n",
      "Epoch 17, Batch 355/462, Loss: 0.7041120529174805\n",
      "Epoch 17, Batch 356/462, Loss: 0.7935148477554321\n",
      "Epoch 17, Batch 357/462, Loss: 0.7389333248138428\n",
      "Epoch 17, Batch 358/462, Loss: 0.7215960025787354\n",
      "Epoch 17, Batch 359/462, Loss: 0.72642582654953\n",
      "Epoch 17, Batch 360/462, Loss: 0.7852612733840942\n",
      "Epoch 17, Batch 361/462, Loss: 0.7522823214530945\n",
      "Epoch 17, Batch 362/462, Loss: 0.7926936745643616\n",
      "Epoch 17, Batch 363/462, Loss: 0.685657799243927\n",
      "Epoch 17, Batch 364/462, Loss: 0.7048541307449341\n",
      "Epoch 17, Batch 365/462, Loss: 0.7882118225097656\n",
      "Epoch 17, Batch 366/462, Loss: 0.672450602054596\n",
      "Epoch 17, Batch 367/462, Loss: 0.7001503705978394\n",
      "Epoch 17, Batch 368/462, Loss: 0.675667405128479\n",
      "Epoch 17, Batch 369/462, Loss: 0.6447295546531677\n",
      "Epoch 17, Batch 370/462, Loss: 0.7921685576438904\n",
      "Epoch 17, Batch 371/462, Loss: 0.7953837513923645\n",
      "Epoch 17, Batch 372/462, Loss: 0.7053351998329163\n",
      "Epoch 17, Batch 373/462, Loss: 0.7312491536140442\n",
      "Epoch 17, Batch 374/462, Loss: 0.6854091882705688\n",
      "Epoch 17, Batch 375/462, Loss: 0.8359901309013367\n",
      "Epoch 17, Batch 376/462, Loss: 0.688589334487915\n",
      "Epoch 17, Batch 377/462, Loss: 0.6212323904037476\n",
      "Epoch 17, Batch 378/462, Loss: 0.889779806137085\n",
      "Epoch 17, Batch 379/462, Loss: 0.7130146622657776\n",
      "Epoch 17, Batch 380/462, Loss: 0.6749390363693237\n",
      "Epoch 17, Batch 381/462, Loss: 0.8467473387718201\n",
      "Epoch 17, Batch 382/462, Loss: 0.6361657381057739\n",
      "Epoch 17, Batch 383/462, Loss: 0.7521505355834961\n",
      "Epoch 17, Batch 384/462, Loss: 0.9497868418693542\n",
      "Epoch 17, Batch 385/462, Loss: 0.6627539396286011\n",
      "Epoch 17, Batch 386/462, Loss: 0.6308510899543762\n",
      "Epoch 17, Batch 387/462, Loss: 0.5706742405891418\n",
      "Epoch 17, Batch 388/462, Loss: 0.7821069359779358\n",
      "Epoch 17, Batch 389/462, Loss: 0.8163240551948547\n",
      "Epoch 17, Batch 390/462, Loss: 0.578352689743042\n",
      "Epoch 17, Batch 391/462, Loss: 0.7597342133522034\n",
      "Epoch 17, Batch 392/462, Loss: 0.8388338685035706\n",
      "Epoch 17, Batch 393/462, Loss: 0.7339974045753479\n",
      "Epoch 17, Batch 394/462, Loss: 0.7976306676864624\n",
      "Epoch 17, Batch 395/462, Loss: 0.782087504863739\n",
      "Epoch 17, Batch 396/462, Loss: 1.0007007122039795\n",
      "Epoch 17, Batch 397/462, Loss: 0.6866296529769897\n",
      "Epoch 17, Batch 398/462, Loss: 0.8481035828590393\n",
      "Epoch 17, Batch 399/462, Loss: 0.9100103974342346\n",
      "Epoch 17, Batch 400/462, Loss: 0.7022018432617188\n",
      "Epoch 17, Batch 401/462, Loss: 0.8473695516586304\n",
      "Epoch 17, Batch 402/462, Loss: 0.753493070602417\n",
      "Epoch 17, Batch 403/462, Loss: 0.6127949953079224\n",
      "Epoch 17, Batch 404/462, Loss: 0.7142442464828491\n",
      "Epoch 17, Batch 405/462, Loss: 0.7251215577125549\n",
      "Epoch 17, Batch 406/462, Loss: 0.7154670357704163\n",
      "Epoch 17, Batch 407/462, Loss: 0.7691791653633118\n",
      "Epoch 17, Batch 408/462, Loss: 0.8138145804405212\n",
      "Epoch 17, Batch 409/462, Loss: 0.5842269062995911\n",
      "Epoch 17, Batch 410/462, Loss: 0.8879338502883911\n",
      "Epoch 17, Batch 411/462, Loss: 0.6785449385643005\n",
      "Epoch 17, Batch 412/462, Loss: 0.8349207639694214\n",
      "Epoch 17, Batch 413/462, Loss: 0.8335998058319092\n",
      "Epoch 17, Batch 414/462, Loss: 0.7001328468322754\n",
      "Epoch 17, Batch 415/462, Loss: 0.6658200621604919\n",
      "Epoch 17, Batch 416/462, Loss: 0.7778822779655457\n",
      "Epoch 17, Batch 417/462, Loss: 0.783951997756958\n",
      "Epoch 17, Batch 418/462, Loss: 0.7346440553665161\n",
      "Epoch 17, Batch 419/462, Loss: 0.8757498860359192\n",
      "Epoch 17, Batch 420/462, Loss: 0.7115459442138672\n",
      "Epoch 17, Batch 421/462, Loss: 0.7215095162391663\n",
      "Epoch 17, Batch 422/462, Loss: 0.8846277594566345\n",
      "Epoch 17, Batch 423/462, Loss: 0.8638492226600647\n",
      "Epoch 17, Batch 424/462, Loss: 0.7617254257202148\n",
      "Epoch 17, Batch 425/462, Loss: 0.6470850110054016\n",
      "Epoch 17, Batch 426/462, Loss: 0.6236003637313843\n",
      "Epoch 17, Batch 427/462, Loss: 0.8833323121070862\n",
      "Epoch 17, Batch 428/462, Loss: 0.827419102191925\n",
      "Epoch 17, Batch 429/462, Loss: 0.6480796933174133\n",
      "Epoch 17, Batch 430/462, Loss: 0.6998456120491028\n",
      "Epoch 17, Batch 431/462, Loss: 0.6202259063720703\n",
      "Epoch 17, Batch 432/462, Loss: 0.6827095746994019\n",
      "Epoch 17, Batch 433/462, Loss: 0.7787801623344421\n",
      "Epoch 17, Batch 434/462, Loss: 0.6067432761192322\n",
      "Epoch 17, Batch 435/462, Loss: 0.7499893307685852\n",
      "Epoch 17, Batch 436/462, Loss: 0.6674420833587646\n",
      "Epoch 17, Batch 437/462, Loss: 0.7209740281105042\n",
      "Epoch 17, Batch 438/462, Loss: 0.7221632599830627\n",
      "Epoch 17, Batch 439/462, Loss: 0.6981120705604553\n",
      "Epoch 17, Batch 440/462, Loss: 0.698723316192627\n",
      "Epoch 17, Batch 441/462, Loss: 0.8365061283111572\n",
      "Epoch 17, Batch 442/462, Loss: 0.7316985130310059\n",
      "Epoch 17, Batch 443/462, Loss: 0.8010748624801636\n",
      "Epoch 17, Batch 444/462, Loss: 0.7372710108757019\n",
      "Epoch 17, Batch 445/462, Loss: 0.6573188304901123\n",
      "Epoch 17, Batch 446/462, Loss: 0.7398104667663574\n",
      "Epoch 17, Batch 447/462, Loss: 0.9038201570510864\n",
      "Epoch 17, Batch 448/462, Loss: 0.7784541845321655\n",
      "Epoch 17, Batch 449/462, Loss: 0.5979849100112915\n",
      "Epoch 17, Batch 450/462, Loss: 0.8895652890205383\n",
      "Epoch 17, Batch 451/462, Loss: 0.7491035461425781\n",
      "Epoch 17, Batch 452/462, Loss: 0.780973494052887\n",
      "Epoch 17, Batch 453/462, Loss: 0.7977819442749023\n",
      "Epoch 17, Batch 454/462, Loss: 0.6588194370269775\n",
      "Epoch 17, Batch 455/462, Loss: 0.8792705535888672\n",
      "Epoch 17, Batch 456/462, Loss: 0.7306548357009888\n",
      "Epoch 17, Batch 457/462, Loss: 0.6723496913909912\n",
      "Epoch 17, Batch 458/462, Loss: 0.7766214609146118\n",
      "Epoch 17, Batch 459/462, Loss: 0.8722773790359497\n",
      "Epoch 17, Batch 460/462, Loss: 0.9158815145492554\n",
      "Epoch 17, Batch 461/462, Loss: 0.6472493410110474\n",
      "Epoch 17, Batch 462/462, Loss: 0.7442895174026489\n",
      "Epoch 17, Loss: 351.7267023921013\n",
      "Epoch 18, Batch 1/462, Loss: 0.7489277720451355\n",
      "Epoch 18, Batch 2/462, Loss: 0.7698718309402466\n",
      "Epoch 18, Batch 3/462, Loss: 0.7619756460189819\n",
      "Epoch 18, Batch 4/462, Loss: 0.7181597948074341\n",
      "Epoch 18, Batch 5/462, Loss: 0.7926022410392761\n",
      "Epoch 18, Batch 6/462, Loss: 0.7836848497390747\n",
      "Epoch 18, Batch 7/462, Loss: 0.8048286437988281\n",
      "Epoch 18, Batch 8/462, Loss: 0.9046453833580017\n",
      "Epoch 18, Batch 9/462, Loss: 0.780148446559906\n",
      "Epoch 18, Batch 10/462, Loss: 0.6399237513542175\n",
      "Epoch 18, Batch 11/462, Loss: 0.7449971437454224\n",
      "Epoch 18, Batch 12/462, Loss: 0.8185735940933228\n",
      "Epoch 18, Batch 13/462, Loss: 0.7031441330909729\n",
      "Epoch 18, Batch 14/462, Loss: 0.6996947526931763\n",
      "Epoch 18, Batch 15/462, Loss: 0.8273005485534668\n",
      "Epoch 18, Batch 16/462, Loss: 0.5166041851043701\n",
      "Epoch 18, Batch 17/462, Loss: 0.6305274367332458\n",
      "Epoch 18, Batch 18/462, Loss: 0.8343105912208557\n",
      "Epoch 18, Batch 19/462, Loss: 0.7536919116973877\n",
      "Epoch 18, Batch 20/462, Loss: 0.7130048871040344\n",
      "Epoch 18, Batch 21/462, Loss: 0.9197137355804443\n",
      "Epoch 18, Batch 22/462, Loss: 0.73506098985672\n",
      "Epoch 18, Batch 23/462, Loss: 0.7559468150138855\n",
      "Epoch 18, Batch 24/462, Loss: 0.6557294130325317\n",
      "Epoch 18, Batch 25/462, Loss: 0.8091505765914917\n",
      "Epoch 18, Batch 26/462, Loss: 0.6622898578643799\n",
      "Epoch 18, Batch 27/462, Loss: 0.7617999911308289\n",
      "Epoch 18, Batch 28/462, Loss: 0.8390304446220398\n",
      "Epoch 18, Batch 29/462, Loss: 0.6676714420318604\n",
      "Epoch 18, Batch 30/462, Loss: 0.7986854910850525\n",
      "Epoch 18, Batch 31/462, Loss: 0.5900116562843323\n",
      "Epoch 18, Batch 32/462, Loss: 0.9051041603088379\n",
      "Epoch 18, Batch 33/462, Loss: 0.6546725034713745\n",
      "Epoch 18, Batch 34/462, Loss: 0.8156878352165222\n",
      "Epoch 18, Batch 35/462, Loss: 1.1019095182418823\n",
      "Epoch 18, Batch 36/462, Loss: 0.8382223844528198\n",
      "Epoch 18, Batch 37/462, Loss: 0.8459500074386597\n",
      "Epoch 18, Batch 38/462, Loss: 0.5371515154838562\n",
      "Epoch 18, Batch 39/462, Loss: 0.8866787552833557\n",
      "Epoch 18, Batch 40/462, Loss: 0.741051197052002\n",
      "Epoch 18, Batch 41/462, Loss: 0.8595302700996399\n",
      "Epoch 18, Batch 42/462, Loss: 0.6999993324279785\n",
      "Epoch 18, Batch 43/462, Loss: 0.6815724968910217\n",
      "Epoch 18, Batch 44/462, Loss: 0.6676443219184875\n",
      "Epoch 18, Batch 45/462, Loss: 0.7973586320877075\n",
      "Epoch 18, Batch 46/462, Loss: 0.7684714198112488\n",
      "Epoch 18, Batch 47/462, Loss: 0.6470388174057007\n",
      "Epoch 18, Batch 48/462, Loss: 0.7518019080162048\n",
      "Epoch 18, Batch 49/462, Loss: 0.641974925994873\n",
      "Epoch 18, Batch 50/462, Loss: 0.7262071967124939\n",
      "Epoch 18, Batch 51/462, Loss: 0.8139066696166992\n",
      "Epoch 18, Batch 52/462, Loss: 0.8221136927604675\n",
      "Epoch 18, Batch 53/462, Loss: 0.6327495574951172\n",
      "Epoch 18, Batch 54/462, Loss: 0.8222014307975769\n",
      "Epoch 18, Batch 55/462, Loss: 0.8480663895606995\n",
      "Epoch 18, Batch 56/462, Loss: 0.6556909084320068\n",
      "Epoch 18, Batch 57/462, Loss: 0.7611792087554932\n",
      "Epoch 18, Batch 58/462, Loss: 0.7705500721931458\n",
      "Epoch 18, Batch 59/462, Loss: 0.6581010818481445\n",
      "Epoch 18, Batch 60/462, Loss: 0.7344401478767395\n",
      "Epoch 18, Batch 61/462, Loss: 0.750689685344696\n",
      "Epoch 18, Batch 62/462, Loss: 0.765475332736969\n",
      "Epoch 18, Batch 63/462, Loss: 0.7328236103057861\n",
      "Epoch 18, Batch 64/462, Loss: 0.8385125994682312\n",
      "Epoch 18, Batch 65/462, Loss: 0.7310311198234558\n",
      "Epoch 18, Batch 66/462, Loss: 0.7430310845375061\n",
      "Epoch 18, Batch 67/462, Loss: 0.6080880165100098\n",
      "Epoch 18, Batch 68/462, Loss: 0.8457375168800354\n",
      "Epoch 18, Batch 69/462, Loss: 0.7725903987884521\n",
      "Epoch 18, Batch 70/462, Loss: 0.761009931564331\n",
      "Epoch 18, Batch 71/462, Loss: 0.8017553687095642\n",
      "Epoch 18, Batch 72/462, Loss: 0.6492779850959778\n",
      "Epoch 18, Batch 73/462, Loss: 0.699470043182373\n",
      "Epoch 18, Batch 74/462, Loss: 0.6924359798431396\n",
      "Epoch 18, Batch 75/462, Loss: 0.7721445560455322\n",
      "Epoch 18, Batch 76/462, Loss: 0.8606073260307312\n",
      "Epoch 18, Batch 77/462, Loss: 0.7394922971725464\n",
      "Epoch 18, Batch 78/462, Loss: 0.7178283929824829\n",
      "Epoch 18, Batch 79/462, Loss: 0.7400583028793335\n",
      "Epoch 18, Batch 80/462, Loss: 0.7274925112724304\n",
      "Epoch 18, Batch 81/462, Loss: 0.8446998000144958\n",
      "Epoch 18, Batch 82/462, Loss: 0.8707072734832764\n",
      "Epoch 18, Batch 83/462, Loss: 0.8639883995056152\n",
      "Epoch 18, Batch 84/462, Loss: 0.8076840043067932\n",
      "Epoch 18, Batch 85/462, Loss: 0.8365001082420349\n",
      "Epoch 18, Batch 86/462, Loss: 0.6364675164222717\n",
      "Epoch 18, Batch 87/462, Loss: 0.8514969944953918\n",
      "Epoch 18, Batch 88/462, Loss: 0.7674013376235962\n",
      "Epoch 18, Batch 89/462, Loss: 0.8859816789627075\n",
      "Epoch 18, Batch 90/462, Loss: 0.9204716682434082\n",
      "Epoch 18, Batch 91/462, Loss: 0.6855091452598572\n",
      "Epoch 18, Batch 92/462, Loss: 0.8397592902183533\n",
      "Epoch 18, Batch 93/462, Loss: 0.6766409873962402\n",
      "Epoch 18, Batch 94/462, Loss: 0.9289897084236145\n",
      "Epoch 18, Batch 95/462, Loss: 0.7476326823234558\n",
      "Epoch 18, Batch 96/462, Loss: 0.7942322492599487\n",
      "Epoch 18, Batch 97/462, Loss: 0.8149588704109192\n",
      "Epoch 18, Batch 98/462, Loss: 0.833269476890564\n",
      "Epoch 18, Batch 99/462, Loss: 0.7033542990684509\n",
      "Epoch 18, Batch 100/462, Loss: 0.5336640477180481\n",
      "Epoch 18, Batch 101/462, Loss: 0.9447989463806152\n",
      "Epoch 18, Batch 102/462, Loss: 0.717180073261261\n",
      "Epoch 18, Batch 103/462, Loss: 0.6796625852584839\n",
      "Epoch 18, Batch 104/462, Loss: 0.8111191987991333\n",
      "Epoch 18, Batch 105/462, Loss: 0.7355387806892395\n",
      "Epoch 18, Batch 106/462, Loss: 0.8266927003860474\n",
      "Epoch 18, Batch 107/462, Loss: 0.8325432538986206\n",
      "Epoch 18, Batch 108/462, Loss: 0.5705450177192688\n",
      "Epoch 18, Batch 109/462, Loss: 0.661013662815094\n",
      "Epoch 18, Batch 110/462, Loss: 0.8669666647911072\n",
      "Epoch 18, Batch 111/462, Loss: 0.8082354068756104\n",
      "Epoch 18, Batch 112/462, Loss: 0.846403956413269\n",
      "Epoch 18, Batch 113/462, Loss: 0.6470609903335571\n",
      "Epoch 18, Batch 114/462, Loss: 0.737236499786377\n",
      "Epoch 18, Batch 115/462, Loss: 0.7516249418258667\n",
      "Epoch 18, Batch 116/462, Loss: 0.7729711532592773\n",
      "Epoch 18, Batch 117/462, Loss: 0.8482685685157776\n",
      "Epoch 18, Batch 118/462, Loss: 0.6732587218284607\n",
      "Epoch 18, Batch 119/462, Loss: 0.8501322865486145\n",
      "Epoch 18, Batch 120/462, Loss: 0.8599560856819153\n",
      "Epoch 18, Batch 121/462, Loss: 0.7738626003265381\n",
      "Epoch 18, Batch 122/462, Loss: 0.6858665347099304\n",
      "Epoch 18, Batch 123/462, Loss: 0.7460986971855164\n",
      "Epoch 18, Batch 124/462, Loss: 0.8392629027366638\n",
      "Epoch 18, Batch 125/462, Loss: 0.679375171661377\n",
      "Epoch 18, Batch 126/462, Loss: 0.8133803606033325\n",
      "Epoch 18, Batch 127/462, Loss: 0.7873196601867676\n",
      "Epoch 18, Batch 128/462, Loss: 0.8215206861495972\n",
      "Epoch 18, Batch 129/462, Loss: 0.6917595267295837\n",
      "Epoch 18, Batch 130/462, Loss: 0.5922723412513733\n",
      "Epoch 18, Batch 131/462, Loss: 0.8055455088615417\n",
      "Epoch 18, Batch 132/462, Loss: 0.7520051598548889\n",
      "Epoch 18, Batch 133/462, Loss: 0.7089568972587585\n",
      "Epoch 18, Batch 134/462, Loss: 0.8335792422294617\n",
      "Epoch 18, Batch 135/462, Loss: 0.7967010140419006\n",
      "Epoch 18, Batch 136/462, Loss: 0.6516861319541931\n",
      "Epoch 18, Batch 137/462, Loss: 0.883350133895874\n",
      "Epoch 18, Batch 138/462, Loss: 1.0029693841934204\n",
      "Epoch 18, Batch 139/462, Loss: 0.8709946870803833\n",
      "Epoch 18, Batch 140/462, Loss: 0.9582542181015015\n",
      "Epoch 18, Batch 141/462, Loss: 1.0568420886993408\n",
      "Epoch 18, Batch 142/462, Loss: 0.7698377370834351\n",
      "Epoch 18, Batch 143/462, Loss: 0.6406362056732178\n",
      "Epoch 18, Batch 144/462, Loss: 0.8339813947677612\n",
      "Epoch 18, Batch 145/462, Loss: 0.8214235901832581\n",
      "Epoch 18, Batch 146/462, Loss: 0.7757048606872559\n",
      "Epoch 18, Batch 147/462, Loss: 0.7813079953193665\n",
      "Epoch 18, Batch 148/462, Loss: 0.7479742765426636\n",
      "Epoch 18, Batch 149/462, Loss: 0.6471056342124939\n",
      "Epoch 18, Batch 150/462, Loss: 0.8359206914901733\n",
      "Epoch 18, Batch 151/462, Loss: 0.858771800994873\n",
      "Epoch 18, Batch 152/462, Loss: 0.7996760606765747\n",
      "Epoch 18, Batch 153/462, Loss: 0.8731399774551392\n",
      "Epoch 18, Batch 154/462, Loss: 0.8533987998962402\n",
      "Epoch 18, Batch 155/462, Loss: 0.7985438108444214\n",
      "Epoch 18, Batch 156/462, Loss: 0.6581594944000244\n",
      "Epoch 18, Batch 157/462, Loss: 0.6865251064300537\n",
      "Epoch 18, Batch 158/462, Loss: 0.7464543581008911\n",
      "Epoch 18, Batch 159/462, Loss: 0.8525409698486328\n",
      "Epoch 18, Batch 160/462, Loss: 0.9243279099464417\n",
      "Epoch 18, Batch 161/462, Loss: 0.7503290176391602\n",
      "Epoch 18, Batch 162/462, Loss: 0.8269954919815063\n",
      "Epoch 18, Batch 163/462, Loss: 0.8435449004173279\n",
      "Epoch 18, Batch 164/462, Loss: 0.599424421787262\n",
      "Epoch 18, Batch 165/462, Loss: 0.7712695598602295\n",
      "Epoch 18, Batch 166/462, Loss: 0.65357506275177\n",
      "Epoch 18, Batch 167/462, Loss: 0.6877190470695496\n",
      "Epoch 18, Batch 168/462, Loss: 0.6121820211410522\n",
      "Epoch 18, Batch 169/462, Loss: 0.6305428743362427\n",
      "Epoch 18, Batch 170/462, Loss: 0.8146089315414429\n",
      "Epoch 18, Batch 171/462, Loss: 0.6933148503303528\n",
      "Epoch 18, Batch 172/462, Loss: 0.8221468925476074\n",
      "Epoch 18, Batch 173/462, Loss: 0.6336672902107239\n",
      "Epoch 18, Batch 174/462, Loss: 0.7530536651611328\n",
      "Epoch 18, Batch 175/462, Loss: 0.5983080863952637\n",
      "Epoch 18, Batch 176/462, Loss: 0.7897829413414001\n",
      "Epoch 18, Batch 177/462, Loss: 0.5982086658477783\n",
      "Epoch 18, Batch 178/462, Loss: 0.7893993258476257\n",
      "Epoch 18, Batch 179/462, Loss: 0.9011001586914062\n",
      "Epoch 18, Batch 180/462, Loss: 0.5168660283088684\n",
      "Epoch 18, Batch 181/462, Loss: 0.7266401648521423\n",
      "Epoch 18, Batch 182/462, Loss: 0.7307910323143005\n",
      "Epoch 18, Batch 183/462, Loss: 0.8094905018806458\n",
      "Epoch 18, Batch 184/462, Loss: 0.6761018633842468\n",
      "Epoch 18, Batch 185/462, Loss: 0.8095684051513672\n",
      "Epoch 18, Batch 186/462, Loss: 0.8213846683502197\n",
      "Epoch 18, Batch 187/462, Loss: 0.7888236045837402\n",
      "Epoch 18, Batch 188/462, Loss: 0.6737263202667236\n",
      "Epoch 18, Batch 189/462, Loss: 0.6661425232887268\n",
      "Epoch 18, Batch 190/462, Loss: 0.691189706325531\n",
      "Epoch 18, Batch 191/462, Loss: 0.8170409798622131\n",
      "Epoch 18, Batch 192/462, Loss: 0.6560538411140442\n",
      "Epoch 18, Batch 193/462, Loss: 0.8560611605644226\n",
      "Epoch 18, Batch 194/462, Loss: 0.6866645216941833\n",
      "Epoch 18, Batch 195/462, Loss: 0.7939119935035706\n",
      "Epoch 18, Batch 196/462, Loss: 0.7306311130523682\n",
      "Epoch 18, Batch 197/462, Loss: 0.7502011060714722\n",
      "Epoch 18, Batch 198/462, Loss: 0.7523615956306458\n",
      "Epoch 18, Batch 199/462, Loss: 0.7197006940841675\n",
      "Epoch 18, Batch 200/462, Loss: 0.835616946220398\n",
      "Epoch 18, Batch 201/462, Loss: 0.6692679524421692\n",
      "Epoch 18, Batch 202/462, Loss: 0.7509094476699829\n",
      "Epoch 18, Batch 203/462, Loss: 0.6724188327789307\n",
      "Epoch 18, Batch 204/462, Loss: 0.7303248643875122\n",
      "Epoch 18, Batch 205/462, Loss: 0.7064363956451416\n",
      "Epoch 18, Batch 206/462, Loss: 0.8593757152557373\n",
      "Epoch 18, Batch 207/462, Loss: 0.6619455218315125\n",
      "Epoch 18, Batch 208/462, Loss: 0.8152993321418762\n",
      "Epoch 18, Batch 209/462, Loss: 0.6557995080947876\n",
      "Epoch 18, Batch 210/462, Loss: 0.8365325927734375\n",
      "Epoch 18, Batch 211/462, Loss: 0.6936559677124023\n",
      "Epoch 18, Batch 212/462, Loss: 0.7788429260253906\n",
      "Epoch 18, Batch 213/462, Loss: 0.7985304594039917\n",
      "Epoch 18, Batch 214/462, Loss: 0.7334622144699097\n",
      "Epoch 18, Batch 215/462, Loss: 0.7903674840927124\n",
      "Epoch 18, Batch 216/462, Loss: 0.7243317365646362\n",
      "Epoch 18, Batch 217/462, Loss: 0.6996262669563293\n",
      "Epoch 18, Batch 218/462, Loss: 0.6008181571960449\n",
      "Epoch 18, Batch 219/462, Loss: 0.6569768190383911\n",
      "Epoch 18, Batch 220/462, Loss: 0.7844586372375488\n",
      "Epoch 18, Batch 221/462, Loss: 0.6650443077087402\n",
      "Epoch 18, Batch 222/462, Loss: 0.9082820415496826\n",
      "Epoch 18, Batch 223/462, Loss: 0.7185869216918945\n",
      "Epoch 18, Batch 224/462, Loss: 0.7795965075492859\n",
      "Epoch 18, Batch 225/462, Loss: 0.689135730266571\n",
      "Epoch 18, Batch 226/462, Loss: 0.6742740273475647\n",
      "Epoch 18, Batch 227/462, Loss: 0.8148351907730103\n",
      "Epoch 18, Batch 228/462, Loss: 0.6140019297599792\n",
      "Epoch 18, Batch 229/462, Loss: 0.7571852803230286\n",
      "Epoch 18, Batch 230/462, Loss: 0.8219627141952515\n",
      "Epoch 18, Batch 231/462, Loss: 0.7350271940231323\n",
      "Epoch 18, Batch 232/462, Loss: 0.8190833926200867\n",
      "Epoch 18, Batch 233/462, Loss: 0.6264365911483765\n",
      "Epoch 18, Batch 234/462, Loss: 0.7836347222328186\n",
      "Epoch 18, Batch 235/462, Loss: 0.7491121888160706\n",
      "Epoch 18, Batch 236/462, Loss: 0.7582238912582397\n",
      "Epoch 18, Batch 237/462, Loss: 0.701513409614563\n",
      "Epoch 18, Batch 238/462, Loss: 0.6381095051765442\n",
      "Epoch 18, Batch 239/462, Loss: 0.7303175330162048\n",
      "Epoch 18, Batch 240/462, Loss: 0.7116629481315613\n",
      "Epoch 18, Batch 241/462, Loss: 0.8412482738494873\n",
      "Epoch 18, Batch 242/462, Loss: 0.7834305167198181\n",
      "Epoch 18, Batch 243/462, Loss: 0.7330385446548462\n",
      "Epoch 18, Batch 244/462, Loss: 0.8597159385681152\n",
      "Epoch 18, Batch 245/462, Loss: 0.7640987634658813\n",
      "Epoch 18, Batch 246/462, Loss: 0.6280429363250732\n",
      "Epoch 18, Batch 247/462, Loss: 0.7810545563697815\n",
      "Epoch 18, Batch 248/462, Loss: 0.7748937606811523\n",
      "Epoch 18, Batch 249/462, Loss: 0.7424581050872803\n",
      "Epoch 18, Batch 250/462, Loss: 0.7584640979766846\n",
      "Epoch 18, Batch 251/462, Loss: 0.6952030658721924\n",
      "Epoch 18, Batch 252/462, Loss: 0.6831998229026794\n",
      "Epoch 18, Batch 253/462, Loss: 1.0481197834014893\n",
      "Epoch 18, Batch 254/462, Loss: 0.875375509262085\n",
      "Epoch 18, Batch 255/462, Loss: 0.7406319975852966\n",
      "Epoch 18, Batch 256/462, Loss: 0.7163394689559937\n",
      "Epoch 18, Batch 257/462, Loss: 0.7210153937339783\n",
      "Epoch 18, Batch 258/462, Loss: 0.8871742486953735\n",
      "Epoch 18, Batch 259/462, Loss: 0.5301762819290161\n",
      "Epoch 18, Batch 260/462, Loss: 0.6821681261062622\n",
      "Epoch 18, Batch 261/462, Loss: 0.6066482067108154\n",
      "Epoch 18, Batch 262/462, Loss: 0.7988640069961548\n",
      "Epoch 18, Batch 263/462, Loss: 0.6973879337310791\n",
      "Epoch 18, Batch 264/462, Loss: 0.6844071745872498\n",
      "Epoch 18, Batch 265/462, Loss: 0.7436248064041138\n",
      "Epoch 18, Batch 266/462, Loss: 0.7462438344955444\n",
      "Epoch 18, Batch 267/462, Loss: 0.831973135471344\n",
      "Epoch 18, Batch 268/462, Loss: 0.6209728717803955\n",
      "Epoch 18, Batch 269/462, Loss: 0.9061084985733032\n",
      "Epoch 18, Batch 270/462, Loss: 0.7823401093482971\n",
      "Epoch 18, Batch 271/462, Loss: 0.7392386794090271\n",
      "Epoch 18, Batch 272/462, Loss: 0.6371344327926636\n",
      "Epoch 18, Batch 273/462, Loss: 0.7622781991958618\n",
      "Epoch 18, Batch 274/462, Loss: 0.71379554271698\n",
      "Epoch 18, Batch 275/462, Loss: 0.6751013994216919\n",
      "Epoch 18, Batch 276/462, Loss: 0.818719744682312\n",
      "Epoch 18, Batch 277/462, Loss: 0.6094133257865906\n",
      "Epoch 18, Batch 278/462, Loss: 0.9250689744949341\n",
      "Epoch 18, Batch 279/462, Loss: 0.792356014251709\n",
      "Epoch 18, Batch 280/462, Loss: 0.8237058520317078\n",
      "Epoch 18, Batch 281/462, Loss: 0.7862700819969177\n",
      "Epoch 18, Batch 282/462, Loss: 0.6694751977920532\n",
      "Epoch 18, Batch 283/462, Loss: 0.8516732454299927\n",
      "Epoch 18, Batch 284/462, Loss: 0.8038870096206665\n",
      "Epoch 18, Batch 285/462, Loss: 0.8713878989219666\n",
      "Epoch 18, Batch 286/462, Loss: 0.757429838180542\n",
      "Epoch 18, Batch 287/462, Loss: 0.9482272267341614\n",
      "Epoch 18, Batch 288/462, Loss: 0.7260879278182983\n",
      "Epoch 18, Batch 289/462, Loss: 0.7242170572280884\n",
      "Epoch 18, Batch 290/462, Loss: 0.6396417021751404\n",
      "Epoch 18, Batch 291/462, Loss: 0.7331337332725525\n",
      "Epoch 18, Batch 292/462, Loss: 0.757439136505127\n",
      "Epoch 18, Batch 293/462, Loss: 0.8508301973342896\n",
      "Epoch 18, Batch 294/462, Loss: 0.7482709288597107\n",
      "Epoch 18, Batch 295/462, Loss: 0.6903547048568726\n",
      "Epoch 18, Batch 296/462, Loss: 0.6995272040367126\n",
      "Epoch 18, Batch 297/462, Loss: 0.7996041774749756\n",
      "Epoch 18, Batch 298/462, Loss: 0.826960027217865\n",
      "Epoch 18, Batch 299/462, Loss: 0.8058217167854309\n",
      "Epoch 18, Batch 300/462, Loss: 0.7048032283782959\n",
      "Epoch 18, Batch 301/462, Loss: 0.6336437463760376\n",
      "Epoch 18, Batch 302/462, Loss: 0.7244901657104492\n",
      "Epoch 18, Batch 303/462, Loss: 0.804561972618103\n",
      "Epoch 18, Batch 304/462, Loss: 0.8303406834602356\n",
      "Epoch 18, Batch 305/462, Loss: 0.7919931411743164\n",
      "Epoch 18, Batch 306/462, Loss: 0.7401280403137207\n",
      "Epoch 18, Batch 307/462, Loss: 0.6888300776481628\n",
      "Epoch 18, Batch 308/462, Loss: 0.7664334774017334\n",
      "Epoch 18, Batch 309/462, Loss: 0.800565779209137\n",
      "Epoch 18, Batch 310/462, Loss: 0.7108471989631653\n",
      "Epoch 18, Batch 311/462, Loss: 0.6396594047546387\n",
      "Epoch 18, Batch 312/462, Loss: 0.7751722931861877\n",
      "Epoch 18, Batch 313/462, Loss: 0.7026680707931519\n",
      "Epoch 18, Batch 314/462, Loss: 0.7315444350242615\n",
      "Epoch 18, Batch 315/462, Loss: 0.6864528059959412\n",
      "Epoch 18, Batch 316/462, Loss: 0.7655684947967529\n",
      "Epoch 18, Batch 317/462, Loss: 0.8347585201263428\n",
      "Epoch 18, Batch 318/462, Loss: 0.8696730136871338\n",
      "Epoch 18, Batch 319/462, Loss: 0.712817370891571\n",
      "Epoch 18, Batch 320/462, Loss: 0.9074825644493103\n",
      "Epoch 18, Batch 321/462, Loss: 0.6680098176002502\n",
      "Epoch 18, Batch 322/462, Loss: 0.8179593086242676\n",
      "Epoch 18, Batch 323/462, Loss: 0.6772202253341675\n",
      "Epoch 18, Batch 324/462, Loss: 0.730846107006073\n",
      "Epoch 18, Batch 325/462, Loss: 0.5790497064590454\n",
      "Epoch 18, Batch 326/462, Loss: 0.9362831711769104\n",
      "Epoch 18, Batch 327/462, Loss: 0.775052547454834\n",
      "Epoch 18, Batch 328/462, Loss: 0.6622979640960693\n",
      "Epoch 18, Batch 329/462, Loss: 0.8113595247268677\n",
      "Epoch 18, Batch 330/462, Loss: 0.8054426908493042\n",
      "Epoch 18, Batch 331/462, Loss: 0.7914152145385742\n",
      "Epoch 18, Batch 332/462, Loss: 0.8025898933410645\n",
      "Epoch 18, Batch 333/462, Loss: 0.6933773756027222\n",
      "Epoch 18, Batch 334/462, Loss: 0.990181028842926\n",
      "Epoch 18, Batch 335/462, Loss: 0.8959097266197205\n",
      "Epoch 18, Batch 336/462, Loss: 0.7390626668930054\n",
      "Epoch 18, Batch 337/462, Loss: 0.6928114295005798\n",
      "Epoch 18, Batch 338/462, Loss: 0.7562566995620728\n",
      "Epoch 18, Batch 339/462, Loss: 0.8065900802612305\n",
      "Epoch 18, Batch 340/462, Loss: 0.7069733142852783\n",
      "Epoch 18, Batch 341/462, Loss: 0.8568570017814636\n",
      "Epoch 18, Batch 342/462, Loss: 0.6753992438316345\n",
      "Epoch 18, Batch 343/462, Loss: 0.9830979704856873\n",
      "Epoch 18, Batch 344/462, Loss: 0.9754471182823181\n",
      "Epoch 18, Batch 345/462, Loss: 0.7359634637832642\n",
      "Epoch 18, Batch 346/462, Loss: 0.511286735534668\n",
      "Epoch 18, Batch 347/462, Loss: 0.7105984091758728\n",
      "Epoch 18, Batch 348/462, Loss: 0.8727602362632751\n",
      "Epoch 18, Batch 349/462, Loss: 0.6213798522949219\n",
      "Epoch 18, Batch 350/462, Loss: 0.6626452803611755\n",
      "Epoch 18, Batch 351/462, Loss: 0.6951808333396912\n",
      "Epoch 18, Batch 352/462, Loss: 0.8829318284988403\n",
      "Epoch 18, Batch 353/462, Loss: 0.7746240496635437\n",
      "Epoch 18, Batch 354/462, Loss: 0.5905943512916565\n",
      "Epoch 18, Batch 355/462, Loss: 0.6679566502571106\n",
      "Epoch 18, Batch 356/462, Loss: 0.8728074431419373\n",
      "Epoch 18, Batch 357/462, Loss: 0.7797778248786926\n",
      "Epoch 18, Batch 358/462, Loss: 0.9783259630203247\n",
      "Epoch 18, Batch 359/462, Loss: 0.7042837738990784\n",
      "Epoch 18, Batch 360/462, Loss: 0.6862743496894836\n",
      "Epoch 18, Batch 361/462, Loss: 0.6357106566429138\n",
      "Epoch 18, Batch 362/462, Loss: 0.6829100251197815\n",
      "Epoch 18, Batch 363/462, Loss: 0.6848616003990173\n",
      "Epoch 18, Batch 364/462, Loss: 0.8134117126464844\n",
      "Epoch 18, Batch 365/462, Loss: 0.7551409602165222\n",
      "Epoch 18, Batch 366/462, Loss: 0.7839845418930054\n",
      "Epoch 18, Batch 367/462, Loss: 0.8411405086517334\n",
      "Epoch 18, Batch 368/462, Loss: 0.5916871428489685\n",
      "Epoch 18, Batch 369/462, Loss: 0.7226332426071167\n",
      "Epoch 18, Batch 370/462, Loss: 0.714114785194397\n",
      "Epoch 18, Batch 371/462, Loss: 0.8948503732681274\n",
      "Epoch 18, Batch 372/462, Loss: 0.6302773952484131\n",
      "Epoch 18, Batch 373/462, Loss: 0.8997154831886292\n",
      "Epoch 18, Batch 374/462, Loss: 0.9039487838745117\n",
      "Epoch 18, Batch 375/462, Loss: 0.9786111116409302\n",
      "Epoch 18, Batch 376/462, Loss: 0.8708270192146301\n",
      "Epoch 18, Batch 377/462, Loss: 0.8096140623092651\n",
      "Epoch 18, Batch 378/462, Loss: 0.7802847027778625\n",
      "Epoch 18, Batch 379/462, Loss: 0.967938244342804\n",
      "Epoch 18, Batch 380/462, Loss: 0.8357290029525757\n",
      "Epoch 18, Batch 381/462, Loss: 0.7068845629692078\n",
      "Epoch 18, Batch 382/462, Loss: 0.7877746224403381\n",
      "Epoch 18, Batch 383/462, Loss: 0.8557921051979065\n",
      "Epoch 18, Batch 384/462, Loss: 0.7879898548126221\n",
      "Epoch 18, Batch 385/462, Loss: 0.8578932881355286\n",
      "Epoch 18, Batch 386/462, Loss: 0.8514649868011475\n",
      "Epoch 18, Batch 387/462, Loss: 0.7625523209571838\n",
      "Epoch 18, Batch 388/462, Loss: 0.7717891931533813\n",
      "Epoch 18, Batch 389/462, Loss: 0.7405186295509338\n",
      "Epoch 18, Batch 390/462, Loss: 0.7325974702835083\n",
      "Epoch 18, Batch 391/462, Loss: 0.6364249587059021\n",
      "Epoch 18, Batch 392/462, Loss: 0.6342382431030273\n",
      "Epoch 18, Batch 393/462, Loss: 0.6943923830986023\n",
      "Epoch 18, Batch 394/462, Loss: 0.7314236760139465\n",
      "Epoch 18, Batch 395/462, Loss: 0.876314640045166\n",
      "Epoch 18, Batch 396/462, Loss: 0.7444876432418823\n",
      "Epoch 18, Batch 397/462, Loss: 0.778153121471405\n",
      "Epoch 18, Batch 398/462, Loss: 0.7823674082756042\n",
      "Epoch 18, Batch 399/462, Loss: 0.6603814959526062\n",
      "Epoch 18, Batch 400/462, Loss: 0.6900202631950378\n",
      "Epoch 18, Batch 401/462, Loss: 0.5844140648841858\n",
      "Epoch 18, Batch 402/462, Loss: 0.8979806303977966\n",
      "Epoch 18, Batch 403/462, Loss: 0.6668035387992859\n",
      "Epoch 18, Batch 404/462, Loss: 0.9713758826255798\n",
      "Epoch 18, Batch 405/462, Loss: 0.7487022876739502\n",
      "Epoch 18, Batch 406/462, Loss: 1.020285964012146\n",
      "Epoch 18, Batch 407/462, Loss: 0.8014975190162659\n",
      "Epoch 18, Batch 408/462, Loss: 0.8728638291358948\n",
      "Epoch 18, Batch 409/462, Loss: 0.7645127773284912\n",
      "Epoch 18, Batch 410/462, Loss: 0.8857065439224243\n",
      "Epoch 18, Batch 411/462, Loss: 0.8004786372184753\n",
      "Epoch 18, Batch 412/462, Loss: 1.0654432773590088\n",
      "Epoch 18, Batch 413/462, Loss: 0.7922099232673645\n",
      "Epoch 18, Batch 414/462, Loss: 0.6168096661567688\n",
      "Epoch 18, Batch 415/462, Loss: 0.6811774373054504\n",
      "Epoch 18, Batch 416/462, Loss: 0.6852561831474304\n",
      "Epoch 18, Batch 417/462, Loss: 0.7603100538253784\n",
      "Epoch 18, Batch 418/462, Loss: 0.800275981426239\n",
      "Epoch 18, Batch 419/462, Loss: 0.8458356857299805\n",
      "Epoch 18, Batch 420/462, Loss: 0.7349804043769836\n",
      "Epoch 18, Batch 421/462, Loss: 0.7734034061431885\n",
      "Epoch 18, Batch 422/462, Loss: 0.782427191734314\n",
      "Epoch 18, Batch 423/462, Loss: 0.8641178607940674\n",
      "Epoch 18, Batch 424/462, Loss: 0.8345950245857239\n",
      "Epoch 18, Batch 425/462, Loss: 0.7985877990722656\n",
      "Epoch 18, Batch 426/462, Loss: 0.6269665956497192\n",
      "Epoch 18, Batch 427/462, Loss: 0.6639259457588196\n",
      "Epoch 18, Batch 428/462, Loss: 0.9117146134376526\n",
      "Epoch 18, Batch 429/462, Loss: 0.8072657585144043\n",
      "Epoch 18, Batch 430/462, Loss: 0.8327661752700806\n",
      "Epoch 18, Batch 431/462, Loss: 0.6910106539726257\n",
      "Epoch 18, Batch 432/462, Loss: 0.7715461850166321\n",
      "Epoch 18, Batch 433/462, Loss: 0.9638665318489075\n",
      "Epoch 18, Batch 434/462, Loss: 0.8012377619743347\n",
      "Epoch 18, Batch 435/462, Loss: 0.7565681338310242\n",
      "Epoch 18, Batch 436/462, Loss: 0.8118405342102051\n",
      "Epoch 18, Batch 437/462, Loss: 0.6431264281272888\n",
      "Epoch 18, Batch 438/462, Loss: 0.8786576986312866\n",
      "Epoch 18, Batch 439/462, Loss: 0.7237679958343506\n",
      "Epoch 18, Batch 440/462, Loss: 0.8639042973518372\n",
      "Epoch 18, Batch 441/462, Loss: 0.6329695582389832\n",
      "Epoch 18, Batch 442/462, Loss: 0.828004777431488\n",
      "Epoch 18, Batch 443/462, Loss: 0.5838049054145813\n",
      "Epoch 18, Batch 444/462, Loss: 0.6786207556724548\n",
      "Epoch 18, Batch 445/462, Loss: 0.7218825221061707\n",
      "Epoch 18, Batch 446/462, Loss: 0.7797250151634216\n",
      "Epoch 18, Batch 447/462, Loss: 0.6931009888648987\n",
      "Epoch 18, Batch 448/462, Loss: 0.8985936045646667\n",
      "Epoch 18, Batch 449/462, Loss: 0.7963151335716248\n",
      "Epoch 18, Batch 450/462, Loss: 0.6664889454841614\n",
      "Epoch 18, Batch 451/462, Loss: 0.7127729058265686\n",
      "Epoch 18, Batch 452/462, Loss: 0.7640668749809265\n",
      "Epoch 18, Batch 453/462, Loss: 0.8519830107688904\n",
      "Epoch 18, Batch 454/462, Loss: 0.6222535967826843\n",
      "Epoch 18, Batch 455/462, Loss: 0.9526536464691162\n",
      "Epoch 18, Batch 456/462, Loss: 0.6768975257873535\n",
      "Epoch 18, Batch 457/462, Loss: 0.7471472024917603\n",
      "Epoch 18, Batch 458/462, Loss: 0.6474935412406921\n",
      "Epoch 18, Batch 459/462, Loss: 0.6996648907661438\n",
      "Epoch 18, Batch 460/462, Loss: 0.699177086353302\n",
      "Epoch 18, Batch 461/462, Loss: 0.7887094020843506\n",
      "Epoch 18, Batch 462/462, Loss: 0.7645873427391052\n",
      "Epoch 18, Loss: 351.8502565026283\n",
      "Epoch 19, Batch 1/462, Loss: 0.8817893266677856\n",
      "Epoch 19, Batch 2/462, Loss: 0.7062880992889404\n",
      "Epoch 19, Batch 3/462, Loss: 0.6502953767776489\n",
      "Epoch 19, Batch 4/462, Loss: 0.6189522743225098\n",
      "Epoch 19, Batch 5/462, Loss: 0.716359555721283\n",
      "Epoch 19, Batch 6/462, Loss: 0.7264533042907715\n",
      "Epoch 19, Batch 7/462, Loss: 0.6850891709327698\n",
      "Epoch 19, Batch 8/462, Loss: 0.6398446559906006\n",
      "Epoch 19, Batch 9/462, Loss: 0.6825704574584961\n",
      "Epoch 19, Batch 10/462, Loss: 0.5989038348197937\n",
      "Epoch 19, Batch 11/462, Loss: 0.6850711703300476\n",
      "Epoch 19, Batch 12/462, Loss: 0.6772094964981079\n",
      "Epoch 19, Batch 13/462, Loss: 0.824855625629425\n",
      "Epoch 19, Batch 14/462, Loss: 0.9244241714477539\n",
      "Epoch 19, Batch 15/462, Loss: 0.7004757523536682\n",
      "Epoch 19, Batch 16/462, Loss: 0.5837856531143188\n",
      "Epoch 19, Batch 17/462, Loss: 0.8412333130836487\n",
      "Epoch 19, Batch 18/462, Loss: 0.8641089200973511\n",
      "Epoch 19, Batch 19/462, Loss: 0.6852999925613403\n",
      "Epoch 19, Batch 20/462, Loss: 0.7084646821022034\n",
      "Epoch 19, Batch 21/462, Loss: 0.6648176312446594\n",
      "Epoch 19, Batch 22/462, Loss: 0.8810964822769165\n",
      "Epoch 19, Batch 23/462, Loss: 0.6335172653198242\n",
      "Epoch 19, Batch 24/462, Loss: 0.6982545852661133\n",
      "Epoch 19, Batch 25/462, Loss: 0.6638293862342834\n",
      "Epoch 19, Batch 26/462, Loss: 0.6730242967605591\n",
      "Epoch 19, Batch 27/462, Loss: 0.8401528596878052\n",
      "Epoch 19, Batch 28/462, Loss: 0.7158940434455872\n",
      "Epoch 19, Batch 29/462, Loss: 0.8232807517051697\n",
      "Epoch 19, Batch 30/462, Loss: 0.7747812867164612\n",
      "Epoch 19, Batch 31/462, Loss: 0.710340142250061\n",
      "Epoch 19, Batch 32/462, Loss: 0.6830464601516724\n",
      "Epoch 19, Batch 33/462, Loss: 0.9204771518707275\n",
      "Epoch 19, Batch 34/462, Loss: 0.704777717590332\n",
      "Epoch 19, Batch 35/462, Loss: 0.765746533870697\n",
      "Epoch 19, Batch 36/462, Loss: 0.7461723685264587\n",
      "Epoch 19, Batch 37/462, Loss: 0.9601517915725708\n",
      "Epoch 19, Batch 38/462, Loss: 0.6795373558998108\n",
      "Epoch 19, Batch 39/462, Loss: 0.8539782166481018\n",
      "Epoch 19, Batch 40/462, Loss: 0.7860394716262817\n",
      "Epoch 19, Batch 41/462, Loss: 0.7512737512588501\n",
      "Epoch 19, Batch 42/462, Loss: 0.5737976431846619\n",
      "Epoch 19, Batch 43/462, Loss: 0.7274901866912842\n",
      "Epoch 19, Batch 44/462, Loss: 0.6593152284622192\n",
      "Epoch 19, Batch 45/462, Loss: 0.8096460103988647\n",
      "Epoch 19, Batch 46/462, Loss: 0.7126733660697937\n",
      "Epoch 19, Batch 47/462, Loss: 0.8034241795539856\n",
      "Epoch 19, Batch 48/462, Loss: 0.684452474117279\n",
      "Epoch 19, Batch 49/462, Loss: 0.6996362209320068\n",
      "Epoch 19, Batch 50/462, Loss: 0.7840030193328857\n",
      "Epoch 19, Batch 51/462, Loss: 0.7287487387657166\n",
      "Epoch 19, Batch 52/462, Loss: 0.6906043291091919\n",
      "Epoch 19, Batch 53/462, Loss: 0.760327935218811\n",
      "Epoch 19, Batch 54/462, Loss: 0.9778469800949097\n",
      "Epoch 19, Batch 55/462, Loss: 0.9069615006446838\n",
      "Epoch 19, Batch 56/462, Loss: 0.764295756816864\n",
      "Epoch 19, Batch 57/462, Loss: 0.6615298986434937\n",
      "Epoch 19, Batch 58/462, Loss: 0.8452824354171753\n",
      "Epoch 19, Batch 59/462, Loss: 0.6904625296592712\n",
      "Epoch 19, Batch 60/462, Loss: 0.7637486457824707\n",
      "Epoch 19, Batch 61/462, Loss: 0.847743809223175\n",
      "Epoch 19, Batch 62/462, Loss: 0.7746447920799255\n",
      "Epoch 19, Batch 63/462, Loss: 0.7762317657470703\n",
      "Epoch 19, Batch 64/462, Loss: 0.8865697979927063\n",
      "Epoch 19, Batch 65/462, Loss: 0.7947198748588562\n",
      "Epoch 19, Batch 66/462, Loss: 0.6922034621238708\n",
      "Epoch 19, Batch 67/462, Loss: 0.8886582255363464\n",
      "Epoch 19, Batch 68/462, Loss: 0.8915908336639404\n",
      "Epoch 19, Batch 69/462, Loss: 0.80540531873703\n",
      "Epoch 19, Batch 70/462, Loss: 0.6844343543052673\n",
      "Epoch 19, Batch 71/462, Loss: 0.9908546209335327\n",
      "Epoch 19, Batch 72/462, Loss: 0.8095199465751648\n",
      "Epoch 19, Batch 73/462, Loss: 0.6302411556243896\n",
      "Epoch 19, Batch 74/462, Loss: 0.7697661519050598\n",
      "Epoch 19, Batch 75/462, Loss: 0.6290414929389954\n",
      "Epoch 19, Batch 76/462, Loss: 0.7525644302368164\n",
      "Epoch 19, Batch 77/462, Loss: 0.8308202624320984\n",
      "Epoch 19, Batch 78/462, Loss: 0.8436064124107361\n",
      "Epoch 19, Batch 79/462, Loss: 0.6866388916969299\n",
      "Epoch 19, Batch 80/462, Loss: 0.6984969973564148\n",
      "Epoch 19, Batch 81/462, Loss: 0.8591030836105347\n",
      "Epoch 19, Batch 82/462, Loss: 0.9122050404548645\n",
      "Epoch 19, Batch 83/462, Loss: 0.9549660682678223\n",
      "Epoch 19, Batch 84/462, Loss: 0.7656054496765137\n",
      "Epoch 19, Batch 85/462, Loss: 0.823956310749054\n",
      "Epoch 19, Batch 86/462, Loss: 0.6921208500862122\n",
      "Epoch 19, Batch 87/462, Loss: 0.6765980124473572\n",
      "Epoch 19, Batch 88/462, Loss: 0.9018742442131042\n",
      "Epoch 19, Batch 89/462, Loss: 0.6145657896995544\n",
      "Epoch 19, Batch 90/462, Loss: 0.8133600354194641\n",
      "Epoch 19, Batch 91/462, Loss: 0.8059388399124146\n",
      "Epoch 19, Batch 92/462, Loss: 0.6944856643676758\n",
      "Epoch 19, Batch 93/462, Loss: 0.8595613241195679\n",
      "Epoch 19, Batch 94/462, Loss: 0.8175493478775024\n",
      "Epoch 19, Batch 95/462, Loss: 0.7816358804702759\n",
      "Epoch 19, Batch 96/462, Loss: 0.6964183449745178\n",
      "Epoch 19, Batch 97/462, Loss: 0.6770184636116028\n",
      "Epoch 19, Batch 98/462, Loss: 0.6940210461616516\n",
      "Epoch 19, Batch 99/462, Loss: 0.8122956156730652\n",
      "Epoch 19, Batch 100/462, Loss: 0.8966056108474731\n",
      "Epoch 19, Batch 101/462, Loss: 0.6263530254364014\n",
      "Epoch 19, Batch 102/462, Loss: 0.8287820219993591\n",
      "Epoch 19, Batch 103/462, Loss: 0.7645795941352844\n",
      "Epoch 19, Batch 104/462, Loss: 0.6522248983383179\n",
      "Epoch 19, Batch 105/462, Loss: 0.7238821387290955\n",
      "Epoch 19, Batch 106/462, Loss: 0.7213218808174133\n",
      "Epoch 19, Batch 107/462, Loss: 0.7752043008804321\n",
      "Epoch 19, Batch 108/462, Loss: 0.7233250737190247\n",
      "Epoch 19, Batch 109/462, Loss: 0.7699582576751709\n",
      "Epoch 19, Batch 110/462, Loss: 0.7717546820640564\n",
      "Epoch 19, Batch 111/462, Loss: 0.9037018418312073\n",
      "Epoch 19, Batch 112/462, Loss: 0.9328895807266235\n",
      "Epoch 19, Batch 113/462, Loss: 0.6186099052429199\n",
      "Epoch 19, Batch 114/462, Loss: 0.8300075531005859\n",
      "Epoch 19, Batch 115/462, Loss: 0.8416473269462585\n",
      "Epoch 19, Batch 116/462, Loss: 0.8801290392875671\n",
      "Epoch 19, Batch 117/462, Loss: 0.8071296811103821\n",
      "Epoch 19, Batch 118/462, Loss: 0.8164721727371216\n",
      "Epoch 19, Batch 119/462, Loss: 0.7171259522438049\n",
      "Epoch 19, Batch 120/462, Loss: 0.9375935792922974\n",
      "Epoch 19, Batch 121/462, Loss: 0.7754801511764526\n",
      "Epoch 19, Batch 122/462, Loss: 1.1172279119491577\n",
      "Epoch 19, Batch 123/462, Loss: 0.7761286497116089\n",
      "Epoch 19, Batch 124/462, Loss: 0.8336034417152405\n",
      "Epoch 19, Batch 125/462, Loss: 0.8367868661880493\n",
      "Epoch 19, Batch 126/462, Loss: 0.7493639588356018\n",
      "Epoch 19, Batch 127/462, Loss: 0.710151195526123\n",
      "Epoch 19, Batch 128/462, Loss: 0.8640121221542358\n",
      "Epoch 19, Batch 129/462, Loss: 0.7793676257133484\n",
      "Epoch 19, Batch 130/462, Loss: 0.8923232555389404\n",
      "Epoch 19, Batch 131/462, Loss: 0.7476934194564819\n",
      "Epoch 19, Batch 132/462, Loss: 0.819431483745575\n",
      "Epoch 19, Batch 133/462, Loss: 0.7879276871681213\n",
      "Epoch 19, Batch 134/462, Loss: 0.6012024879455566\n",
      "Epoch 19, Batch 135/462, Loss: 0.6407166719436646\n",
      "Epoch 19, Batch 136/462, Loss: 0.7574734091758728\n",
      "Epoch 19, Batch 137/462, Loss: 0.7803668975830078\n",
      "Epoch 19, Batch 138/462, Loss: 0.7819404006004333\n",
      "Epoch 19, Batch 139/462, Loss: 0.7827732563018799\n",
      "Epoch 19, Batch 140/462, Loss: 0.7970446348190308\n",
      "Epoch 19, Batch 141/462, Loss: 0.6468819975852966\n",
      "Epoch 19, Batch 142/462, Loss: 0.6312923431396484\n",
      "Epoch 19, Batch 143/462, Loss: 0.8626750111579895\n",
      "Epoch 19, Batch 144/462, Loss: 0.6941596865653992\n",
      "Epoch 19, Batch 145/462, Loss: 0.7461361289024353\n",
      "Epoch 19, Batch 146/462, Loss: 0.7360222935676575\n",
      "Epoch 19, Batch 147/462, Loss: 0.8101544380187988\n",
      "Epoch 19, Batch 148/462, Loss: 0.6970358490943909\n",
      "Epoch 19, Batch 149/462, Loss: 0.800776481628418\n",
      "Epoch 19, Batch 150/462, Loss: 0.6115321516990662\n",
      "Epoch 19, Batch 151/462, Loss: 0.6091107130050659\n",
      "Epoch 19, Batch 152/462, Loss: 0.6234435439109802\n",
      "Epoch 19, Batch 153/462, Loss: 0.7895684242248535\n",
      "Epoch 19, Batch 154/462, Loss: 0.6638506054878235\n",
      "Epoch 19, Batch 155/462, Loss: 0.774681568145752\n",
      "Epoch 19, Batch 156/462, Loss: 0.8852710127830505\n",
      "Epoch 19, Batch 157/462, Loss: 0.8365556597709656\n",
      "Epoch 19, Batch 158/462, Loss: 0.67824387550354\n",
      "Epoch 19, Batch 159/462, Loss: 0.7573953866958618\n",
      "Epoch 19, Batch 160/462, Loss: 0.8406721949577332\n",
      "Epoch 19, Batch 161/462, Loss: 0.777772843837738\n",
      "Epoch 19, Batch 162/462, Loss: 0.7579910755157471\n",
      "Epoch 19, Batch 163/462, Loss: 0.7372163534164429\n",
      "Epoch 19, Batch 164/462, Loss: 0.8730853796005249\n",
      "Epoch 19, Batch 165/462, Loss: 0.7328482270240784\n",
      "Epoch 19, Batch 166/462, Loss: 0.8956945538520813\n",
      "Epoch 19, Batch 167/462, Loss: 0.6803954839706421\n",
      "Epoch 19, Batch 168/462, Loss: 0.7184698581695557\n",
      "Epoch 19, Batch 169/462, Loss: 0.7897228002548218\n",
      "Epoch 19, Batch 170/462, Loss: 0.8748235702514648\n",
      "Epoch 19, Batch 171/462, Loss: 0.7779352068901062\n",
      "Epoch 19, Batch 172/462, Loss: 0.7382006049156189\n",
      "Epoch 19, Batch 173/462, Loss: 0.6681348085403442\n",
      "Epoch 19, Batch 174/462, Loss: 0.716732919216156\n",
      "Epoch 19, Batch 175/462, Loss: 0.7342563271522522\n",
      "Epoch 19, Batch 176/462, Loss: 0.7329654693603516\n",
      "Epoch 19, Batch 177/462, Loss: 0.6904593706130981\n",
      "Epoch 19, Batch 178/462, Loss: 0.7320179343223572\n",
      "Epoch 19, Batch 179/462, Loss: 0.5905563831329346\n",
      "Epoch 19, Batch 180/462, Loss: 0.7820038199424744\n",
      "Epoch 19, Batch 181/462, Loss: 0.7721046805381775\n",
      "Epoch 19, Batch 182/462, Loss: 0.6642350554466248\n",
      "Epoch 19, Batch 183/462, Loss: 0.7919150590896606\n",
      "Epoch 19, Batch 184/462, Loss: 0.7954702377319336\n",
      "Epoch 19, Batch 185/462, Loss: 0.6388983130455017\n",
      "Epoch 19, Batch 186/462, Loss: 0.7434732913970947\n",
      "Epoch 19, Batch 187/462, Loss: 0.6791156530380249\n",
      "Epoch 19, Batch 188/462, Loss: 0.6500118374824524\n",
      "Epoch 19, Batch 189/462, Loss: 0.8484798073768616\n",
      "Epoch 19, Batch 190/462, Loss: 0.5898457169532776\n",
      "Epoch 19, Batch 191/462, Loss: 0.6143378615379333\n",
      "Epoch 19, Batch 192/462, Loss: 0.7644999623298645\n",
      "Epoch 19, Batch 193/462, Loss: 0.8184810280799866\n",
      "Epoch 19, Batch 194/462, Loss: 0.8859307169914246\n",
      "Epoch 19, Batch 195/462, Loss: 0.618332028388977\n",
      "Epoch 19, Batch 196/462, Loss: 0.6876945495605469\n",
      "Epoch 19, Batch 197/462, Loss: 0.9142299890518188\n",
      "Epoch 19, Batch 198/462, Loss: 0.7674651741981506\n",
      "Epoch 19, Batch 199/462, Loss: 0.7411115765571594\n",
      "Epoch 19, Batch 200/462, Loss: 0.5933842658996582\n",
      "Epoch 19, Batch 201/462, Loss: 0.7136801481246948\n",
      "Epoch 19, Batch 202/462, Loss: 0.7447608113288879\n",
      "Epoch 19, Batch 203/462, Loss: 0.721670925617218\n",
      "Epoch 19, Batch 204/462, Loss: 0.7795127034187317\n",
      "Epoch 19, Batch 205/462, Loss: 0.7270464301109314\n",
      "Epoch 19, Batch 206/462, Loss: 0.7286781072616577\n",
      "Epoch 19, Batch 207/462, Loss: 0.7762593626976013\n",
      "Epoch 19, Batch 208/462, Loss: 0.857849657535553\n",
      "Epoch 19, Batch 209/462, Loss: 0.7304705381393433\n",
      "Epoch 19, Batch 210/462, Loss: 0.7428155541419983\n",
      "Epoch 19, Batch 211/462, Loss: 0.7832923531532288\n",
      "Epoch 19, Batch 212/462, Loss: 0.8131123185157776\n",
      "Epoch 19, Batch 213/462, Loss: 0.7345802187919617\n",
      "Epoch 19, Batch 214/462, Loss: 0.7718650102615356\n",
      "Epoch 19, Batch 215/462, Loss: 0.7169840335845947\n",
      "Epoch 19, Batch 216/462, Loss: 0.6509371399879456\n",
      "Epoch 19, Batch 217/462, Loss: 0.9659597277641296\n",
      "Epoch 19, Batch 218/462, Loss: 0.752554178237915\n",
      "Epoch 19, Batch 219/462, Loss: 0.6667703986167908\n",
      "Epoch 19, Batch 220/462, Loss: 0.7567334771156311\n",
      "Epoch 19, Batch 221/462, Loss: 0.8100904226303101\n",
      "Epoch 19, Batch 222/462, Loss: 0.858485996723175\n",
      "Epoch 19, Batch 223/462, Loss: 0.7080487012863159\n",
      "Epoch 19, Batch 224/462, Loss: 0.6146891117095947\n",
      "Epoch 19, Batch 225/462, Loss: 0.738476037979126\n",
      "Epoch 19, Batch 226/462, Loss: 0.6995629668235779\n",
      "Epoch 19, Batch 227/462, Loss: 0.6837391257286072\n",
      "Epoch 19, Batch 228/462, Loss: 0.6361857056617737\n",
      "Epoch 19, Batch 229/462, Loss: 0.6240251660346985\n",
      "Epoch 19, Batch 230/462, Loss: 0.5974609851837158\n",
      "Epoch 19, Batch 231/462, Loss: 0.839181125164032\n",
      "Epoch 19, Batch 232/462, Loss: 0.7910280823707581\n",
      "Epoch 19, Batch 233/462, Loss: 0.885124683380127\n",
      "Epoch 19, Batch 234/462, Loss: 0.5512223839759827\n",
      "Epoch 19, Batch 235/462, Loss: 0.7435197830200195\n",
      "Epoch 19, Batch 236/462, Loss: 1.058423638343811\n",
      "Epoch 19, Batch 237/462, Loss: 0.6776428818702698\n",
      "Epoch 19, Batch 238/462, Loss: 0.712324321269989\n",
      "Epoch 19, Batch 239/462, Loss: 0.7711817026138306\n",
      "Epoch 19, Batch 240/462, Loss: 0.7161017656326294\n",
      "Epoch 19, Batch 241/462, Loss: 0.6888176202774048\n",
      "Epoch 19, Batch 242/462, Loss: 0.9181811213493347\n",
      "Epoch 19, Batch 243/462, Loss: 0.6521165370941162\n",
      "Epoch 19, Batch 244/462, Loss: 0.6759470105171204\n",
      "Epoch 19, Batch 245/462, Loss: 0.7047102451324463\n",
      "Epoch 19, Batch 246/462, Loss: 0.7088333368301392\n",
      "Epoch 19, Batch 247/462, Loss: 0.9072738289833069\n",
      "Epoch 19, Batch 248/462, Loss: 0.5709078907966614\n",
      "Epoch 19, Batch 249/462, Loss: 0.7905777096748352\n",
      "Epoch 19, Batch 250/462, Loss: 0.773733377456665\n",
      "Epoch 19, Batch 251/462, Loss: 0.8902338743209839\n",
      "Epoch 19, Batch 252/462, Loss: 0.7391182780265808\n",
      "Epoch 19, Batch 253/462, Loss: 0.5919948220252991\n",
      "Epoch 19, Batch 254/462, Loss: 0.7832119464874268\n",
      "Epoch 19, Batch 255/462, Loss: 0.8532211184501648\n",
      "Epoch 19, Batch 256/462, Loss: 0.7250797748565674\n",
      "Epoch 19, Batch 257/462, Loss: 0.8152071833610535\n",
      "Epoch 19, Batch 258/462, Loss: 0.6629464626312256\n",
      "Epoch 19, Batch 259/462, Loss: 0.679200291633606\n",
      "Epoch 19, Batch 260/462, Loss: 0.7095414996147156\n",
      "Epoch 19, Batch 261/462, Loss: 0.8470507860183716\n",
      "Epoch 19, Batch 262/462, Loss: 0.6993143558502197\n",
      "Epoch 19, Batch 263/462, Loss: 0.7311710715293884\n",
      "Epoch 19, Batch 264/462, Loss: 0.7248432636260986\n",
      "Epoch 19, Batch 265/462, Loss: 0.9002001881599426\n",
      "Epoch 19, Batch 266/462, Loss: 0.7561112642288208\n",
      "Epoch 19, Batch 267/462, Loss: 0.7637092471122742\n",
      "Epoch 19, Batch 268/462, Loss: 0.7168252468109131\n",
      "Epoch 19, Batch 269/462, Loss: 0.7004449367523193\n",
      "Epoch 19, Batch 270/462, Loss: 0.8520811200141907\n",
      "Epoch 19, Batch 271/462, Loss: 0.7942798733711243\n",
      "Epoch 19, Batch 272/462, Loss: 0.8178614377975464\n",
      "Epoch 19, Batch 273/462, Loss: 0.708099901676178\n",
      "Epoch 19, Batch 274/462, Loss: 0.8272121548652649\n",
      "Epoch 19, Batch 275/462, Loss: 0.8321849703788757\n",
      "Epoch 19, Batch 276/462, Loss: 0.6777158379554749\n",
      "Epoch 19, Batch 277/462, Loss: 0.6847029328346252\n",
      "Epoch 19, Batch 278/462, Loss: 0.8971803188323975\n",
      "Epoch 19, Batch 279/462, Loss: 0.7771798372268677\n",
      "Epoch 19, Batch 280/462, Loss: 0.7964135408401489\n",
      "Epoch 19, Batch 281/462, Loss: 0.7476220726966858\n",
      "Epoch 19, Batch 282/462, Loss: 0.8336206078529358\n",
      "Epoch 19, Batch 283/462, Loss: 0.795006513595581\n",
      "Epoch 19, Batch 284/462, Loss: 0.8396177291870117\n",
      "Epoch 19, Batch 285/462, Loss: 0.6256454586982727\n",
      "Epoch 19, Batch 286/462, Loss: 0.724043071269989\n",
      "Epoch 19, Batch 287/462, Loss: 0.6877346038818359\n",
      "Epoch 19, Batch 288/462, Loss: 0.8173418641090393\n",
      "Epoch 19, Batch 289/462, Loss: 0.7138276696205139\n",
      "Epoch 19, Batch 290/462, Loss: 0.8098054528236389\n",
      "Epoch 19, Batch 291/462, Loss: 0.8498854637145996\n",
      "Epoch 19, Batch 292/462, Loss: 0.799028754234314\n",
      "Epoch 19, Batch 293/462, Loss: 0.7793617844581604\n",
      "Epoch 19, Batch 294/462, Loss: 0.6771231293678284\n",
      "Epoch 19, Batch 295/462, Loss: 0.8108491897583008\n",
      "Epoch 19, Batch 296/462, Loss: 0.6516832113265991\n",
      "Epoch 19, Batch 297/462, Loss: 0.6017293334007263\n",
      "Epoch 19, Batch 298/462, Loss: 0.8188861012458801\n",
      "Epoch 19, Batch 299/462, Loss: 0.6707896590232849\n",
      "Epoch 19, Batch 300/462, Loss: 0.8893601298332214\n",
      "Epoch 19, Batch 301/462, Loss: 0.7659104466438293\n",
      "Epoch 19, Batch 302/462, Loss: 0.7405968308448792\n",
      "Epoch 19, Batch 303/462, Loss: 0.5730430483818054\n",
      "Epoch 19, Batch 304/462, Loss: 0.8648797869682312\n",
      "Epoch 19, Batch 305/462, Loss: 0.8280302882194519\n",
      "Epoch 19, Batch 306/462, Loss: 0.7308162450790405\n",
      "Epoch 19, Batch 307/462, Loss: 0.5867299437522888\n",
      "Epoch 19, Batch 308/462, Loss: 0.8098210096359253\n",
      "Epoch 19, Batch 309/462, Loss: 0.7310958504676819\n",
      "Epoch 19, Batch 310/462, Loss: 0.7344399690628052\n",
      "Epoch 19, Batch 311/462, Loss: 0.7429879307746887\n",
      "Epoch 19, Batch 312/462, Loss: 0.9262526631355286\n",
      "Epoch 19, Batch 313/462, Loss: 0.7401516437530518\n",
      "Epoch 19, Batch 314/462, Loss: 0.7316612005233765\n",
      "Epoch 19, Batch 315/462, Loss: 0.7881503105163574\n",
      "Epoch 19, Batch 316/462, Loss: 0.7358269095420837\n",
      "Epoch 19, Batch 317/462, Loss: 0.7102154493331909\n",
      "Epoch 19, Batch 318/462, Loss: 0.8896355032920837\n",
      "Epoch 19, Batch 319/462, Loss: 0.7446205019950867\n",
      "Epoch 19, Batch 320/462, Loss: 0.6673867702484131\n",
      "Epoch 19, Batch 321/462, Loss: 0.7969762086868286\n",
      "Epoch 19, Batch 322/462, Loss: 0.6537226438522339\n",
      "Epoch 19, Batch 323/462, Loss: 0.8381458520889282\n",
      "Epoch 19, Batch 324/462, Loss: 0.7253913879394531\n",
      "Epoch 19, Batch 325/462, Loss: 0.7760781049728394\n",
      "Epoch 19, Batch 326/462, Loss: 0.7794860005378723\n",
      "Epoch 19, Batch 327/462, Loss: 0.703831136226654\n",
      "Epoch 19, Batch 328/462, Loss: 0.7281488180160522\n",
      "Epoch 19, Batch 329/462, Loss: 0.848274827003479\n",
      "Epoch 19, Batch 330/462, Loss: 0.750883162021637\n",
      "Epoch 19, Batch 331/462, Loss: 0.5958680510520935\n",
      "Epoch 19, Batch 332/462, Loss: 0.8542662858963013\n",
      "Epoch 19, Batch 333/462, Loss: 0.7648133039474487\n",
      "Epoch 19, Batch 334/462, Loss: 0.6295602917671204\n",
      "Epoch 19, Batch 335/462, Loss: 0.9254699349403381\n",
      "Epoch 19, Batch 336/462, Loss: 0.6235983967781067\n",
      "Epoch 19, Batch 337/462, Loss: 0.6760637760162354\n",
      "Epoch 19, Batch 338/462, Loss: 0.785982608795166\n",
      "Epoch 19, Batch 339/462, Loss: 0.7279002666473389\n",
      "Epoch 19, Batch 340/462, Loss: 0.7883539199829102\n",
      "Epoch 19, Batch 341/462, Loss: 0.8453173637390137\n",
      "Epoch 19, Batch 342/462, Loss: 0.6965360641479492\n",
      "Epoch 19, Batch 343/462, Loss: 0.6382049918174744\n",
      "Epoch 19, Batch 344/462, Loss: 0.7231796383857727\n",
      "Epoch 19, Batch 345/462, Loss: 0.7412658333778381\n",
      "Epoch 19, Batch 346/462, Loss: 0.8257763981819153\n",
      "Epoch 19, Batch 347/462, Loss: 0.7712998390197754\n",
      "Epoch 19, Batch 348/462, Loss: 0.6131713390350342\n",
      "Epoch 19, Batch 349/462, Loss: 0.9012287855148315\n",
      "Epoch 19, Batch 350/462, Loss: 0.6273072957992554\n",
      "Epoch 19, Batch 351/462, Loss: 0.6159166097640991\n",
      "Epoch 19, Batch 352/462, Loss: 0.714509129524231\n",
      "Epoch 19, Batch 353/462, Loss: 0.867807149887085\n",
      "Epoch 19, Batch 354/462, Loss: 0.6904774904251099\n",
      "Epoch 19, Batch 355/462, Loss: 0.8162573575973511\n",
      "Epoch 19, Batch 356/462, Loss: 0.7465332746505737\n",
      "Epoch 19, Batch 357/462, Loss: 0.8990453481674194\n",
      "Epoch 19, Batch 358/462, Loss: 0.7691819071769714\n",
      "Epoch 19, Batch 359/462, Loss: 0.5927996039390564\n",
      "Epoch 19, Batch 360/462, Loss: 0.7931334376335144\n",
      "Epoch 19, Batch 361/462, Loss: 0.6630021333694458\n",
      "Epoch 19, Batch 362/462, Loss: 0.7491918206214905\n",
      "Epoch 19, Batch 363/462, Loss: 0.7794277667999268\n",
      "Epoch 19, Batch 364/462, Loss: 0.7249764800071716\n",
      "Epoch 19, Batch 365/462, Loss: 0.7653084993362427\n",
      "Epoch 19, Batch 366/462, Loss: 0.8267699480056763\n",
      "Epoch 19, Batch 367/462, Loss: 0.8094738721847534\n",
      "Epoch 19, Batch 368/462, Loss: 0.8073061108589172\n",
      "Epoch 19, Batch 369/462, Loss: 0.7828489542007446\n",
      "Epoch 19, Batch 370/462, Loss: 0.8416414260864258\n",
      "Epoch 19, Batch 371/462, Loss: 0.6816534399986267\n",
      "Epoch 19, Batch 372/462, Loss: 0.6402860879898071\n",
      "Epoch 19, Batch 373/462, Loss: 0.7884238362312317\n",
      "Epoch 19, Batch 374/462, Loss: 0.6282829642295837\n",
      "Epoch 19, Batch 375/462, Loss: 0.7749133110046387\n",
      "Epoch 19, Batch 376/462, Loss: 0.6438535451889038\n",
      "Epoch 19, Batch 377/462, Loss: 0.641572117805481\n",
      "Epoch 19, Batch 378/462, Loss: 0.7313132286071777\n",
      "Epoch 19, Batch 379/462, Loss: 0.8146251440048218\n",
      "Epoch 19, Batch 380/462, Loss: 0.675945520401001\n",
      "Epoch 19, Batch 381/462, Loss: 0.8017280101776123\n",
      "Epoch 19, Batch 382/462, Loss: 0.7699921727180481\n",
      "Epoch 19, Batch 383/462, Loss: 0.6300455927848816\n",
      "Epoch 19, Batch 384/462, Loss: 0.8498516082763672\n",
      "Epoch 19, Batch 385/462, Loss: 0.7377542853355408\n",
      "Epoch 19, Batch 386/462, Loss: 0.9074928760528564\n",
      "Epoch 19, Batch 387/462, Loss: 0.76395183801651\n",
      "Epoch 19, Batch 388/462, Loss: 0.7674828767776489\n",
      "Epoch 19, Batch 389/462, Loss: 0.6327741742134094\n",
      "Epoch 19, Batch 390/462, Loss: 0.7914024591445923\n",
      "Epoch 19, Batch 391/462, Loss: 0.6857331991195679\n",
      "Epoch 19, Batch 392/462, Loss: 0.6979402303695679\n",
      "Epoch 19, Batch 393/462, Loss: 0.6552608013153076\n",
      "Epoch 19, Batch 394/462, Loss: 0.9169381260871887\n",
      "Epoch 19, Batch 395/462, Loss: 0.6726993322372437\n",
      "Epoch 19, Batch 396/462, Loss: 0.6760498881340027\n",
      "Epoch 19, Batch 397/462, Loss: 0.6757672429084778\n",
      "Epoch 19, Batch 398/462, Loss: 0.7323544025421143\n",
      "Epoch 19, Batch 399/462, Loss: 0.8621992468833923\n",
      "Epoch 19, Batch 400/462, Loss: 0.7266039848327637\n",
      "Epoch 19, Batch 401/462, Loss: 0.7927480936050415\n",
      "Epoch 19, Batch 402/462, Loss: 0.7892212867736816\n",
      "Epoch 19, Batch 403/462, Loss: 1.0478469133377075\n",
      "Epoch 19, Batch 404/462, Loss: 0.6698333024978638\n",
      "Epoch 19, Batch 405/462, Loss: 0.7938879728317261\n",
      "Epoch 19, Batch 406/462, Loss: 0.6923474669456482\n",
      "Epoch 19, Batch 407/462, Loss: 0.6691877841949463\n",
      "Epoch 19, Batch 408/462, Loss: 0.7237018942832947\n",
      "Epoch 19, Batch 409/462, Loss: 0.9034275412559509\n",
      "Epoch 19, Batch 410/462, Loss: 0.8819735050201416\n",
      "Epoch 19, Batch 411/462, Loss: 0.6865447163581848\n",
      "Epoch 19, Batch 412/462, Loss: 0.7287545800209045\n",
      "Epoch 19, Batch 413/462, Loss: 0.6458309888839722\n",
      "Epoch 19, Batch 414/462, Loss: 0.8095571398735046\n",
      "Epoch 19, Batch 415/462, Loss: 1.021221399307251\n",
      "Epoch 19, Batch 416/462, Loss: 0.7386616468429565\n",
      "Epoch 19, Batch 417/462, Loss: 0.7651737928390503\n",
      "Epoch 19, Batch 418/462, Loss: 0.7655892372131348\n",
      "Epoch 19, Batch 419/462, Loss: 0.74587482213974\n",
      "Epoch 19, Batch 420/462, Loss: 0.7590383887290955\n",
      "Epoch 19, Batch 421/462, Loss: 0.7959613800048828\n",
      "Epoch 19, Batch 422/462, Loss: 0.6562004685401917\n",
      "Epoch 19, Batch 423/462, Loss: 0.794465184211731\n",
      "Epoch 19, Batch 424/462, Loss: 0.6589049696922302\n",
      "Epoch 19, Batch 425/462, Loss: 0.8364259600639343\n",
      "Epoch 19, Batch 426/462, Loss: 0.7181329131126404\n",
      "Epoch 19, Batch 427/462, Loss: 0.7941938638687134\n",
      "Epoch 19, Batch 428/462, Loss: 0.6976500749588013\n",
      "Epoch 19, Batch 429/462, Loss: 0.6825035214424133\n",
      "Epoch 19, Batch 430/462, Loss: 0.6397542357444763\n",
      "Epoch 19, Batch 431/462, Loss: 0.8816506266593933\n",
      "Epoch 19, Batch 432/462, Loss: 0.6999496221542358\n",
      "Epoch 19, Batch 433/462, Loss: 0.7506440877914429\n",
      "Epoch 19, Batch 434/462, Loss: 0.6605893969535828\n",
      "Epoch 19, Batch 435/462, Loss: 0.7560343742370605\n",
      "Epoch 19, Batch 436/462, Loss: 0.7342503666877747\n",
      "Epoch 19, Batch 437/462, Loss: 0.8578807711601257\n",
      "Epoch 19, Batch 438/462, Loss: 0.7120374441146851\n",
      "Epoch 19, Batch 439/462, Loss: 0.9704799652099609\n",
      "Epoch 19, Batch 440/462, Loss: 0.7291870713233948\n",
      "Epoch 19, Batch 441/462, Loss: 0.7961969375610352\n",
      "Epoch 19, Batch 442/462, Loss: 0.9393311142921448\n",
      "Epoch 19, Batch 443/462, Loss: 0.7975587844848633\n",
      "Epoch 19, Batch 444/462, Loss: 0.8088210225105286\n",
      "Epoch 19, Batch 445/462, Loss: 0.7409572005271912\n",
      "Epoch 19, Batch 446/462, Loss: 0.7945531010627747\n",
      "Epoch 19, Batch 447/462, Loss: 0.667321503162384\n",
      "Epoch 19, Batch 448/462, Loss: 0.6659979820251465\n",
      "Epoch 19, Batch 449/462, Loss: 0.6497132778167725\n",
      "Epoch 19, Batch 450/462, Loss: 0.7540128827095032\n",
      "Epoch 19, Batch 451/462, Loss: 0.727471649646759\n",
      "Epoch 19, Batch 452/462, Loss: 0.6903061270713806\n",
      "Epoch 19, Batch 453/462, Loss: 0.6743311882019043\n",
      "Epoch 19, Batch 454/462, Loss: 0.7022221684455872\n",
      "Epoch 19, Batch 455/462, Loss: 0.7907019853591919\n",
      "Epoch 19, Batch 456/462, Loss: 0.8366632461547852\n",
      "Epoch 19, Batch 457/462, Loss: 0.8255209922790527\n",
      "Epoch 19, Batch 458/462, Loss: 0.6697109937667847\n",
      "Epoch 19, Batch 459/462, Loss: 0.6445572376251221\n",
      "Epoch 19, Batch 460/462, Loss: 0.8808088302612305\n",
      "Epoch 19, Batch 461/462, Loss: 0.7557290196418762\n",
      "Epoch 19, Batch 462/462, Loss: 0.8421130776405334\n",
      "Epoch 19, Loss: 349.04023575782776\n",
      "Epoch 20, Batch 1/462, Loss: 0.6798148155212402\n",
      "Epoch 20, Batch 2/462, Loss: 0.8206635117530823\n",
      "Epoch 20, Batch 3/462, Loss: 0.7689014077186584\n",
      "Epoch 20, Batch 4/462, Loss: 0.9073036313056946\n",
      "Epoch 20, Batch 5/462, Loss: 0.680152952671051\n",
      "Epoch 20, Batch 6/462, Loss: 0.7629344463348389\n",
      "Epoch 20, Batch 7/462, Loss: 0.9243895411491394\n",
      "Epoch 20, Batch 8/462, Loss: 1.0172327756881714\n",
      "Epoch 20, Batch 9/462, Loss: 0.7159305810928345\n",
      "Epoch 20, Batch 10/462, Loss: 0.7400495409965515\n",
      "Epoch 20, Batch 11/462, Loss: 0.8059604167938232\n",
      "Epoch 20, Batch 12/462, Loss: 0.684357762336731\n",
      "Epoch 20, Batch 13/462, Loss: 0.8374338150024414\n",
      "Epoch 20, Batch 14/462, Loss: 0.7459293007850647\n",
      "Epoch 20, Batch 15/462, Loss: 0.6986624002456665\n",
      "Epoch 20, Batch 16/462, Loss: 0.6271529793739319\n",
      "Epoch 20, Batch 17/462, Loss: 0.7051501274108887\n",
      "Epoch 20, Batch 18/462, Loss: 0.7676780223846436\n",
      "Epoch 20, Batch 19/462, Loss: 0.701103150844574\n",
      "Epoch 20, Batch 20/462, Loss: 0.6615182757377625\n",
      "Epoch 20, Batch 21/462, Loss: 0.5459418892860413\n",
      "Epoch 20, Batch 22/462, Loss: 0.8464997410774231\n",
      "Epoch 20, Batch 23/462, Loss: 0.659069836139679\n",
      "Epoch 20, Batch 24/462, Loss: 0.8922292590141296\n",
      "Epoch 20, Batch 25/462, Loss: 0.6593478322029114\n",
      "Epoch 20, Batch 26/462, Loss: 0.7425841093063354\n",
      "Epoch 20, Batch 27/462, Loss: 0.6922184228897095\n",
      "Epoch 20, Batch 28/462, Loss: 0.7583693265914917\n",
      "Epoch 20, Batch 29/462, Loss: 0.8042721152305603\n",
      "Epoch 20, Batch 30/462, Loss: 0.7567797899246216\n",
      "Epoch 20, Batch 31/462, Loss: 0.8138467669487\n",
      "Epoch 20, Batch 32/462, Loss: 0.7698043584823608\n",
      "Epoch 20, Batch 33/462, Loss: 0.848800778388977\n",
      "Epoch 20, Batch 34/462, Loss: 0.7719300389289856\n",
      "Epoch 20, Batch 35/462, Loss: 0.7556648254394531\n",
      "Epoch 20, Batch 36/462, Loss: 0.8129452466964722\n",
      "Epoch 20, Batch 37/462, Loss: 0.8374382257461548\n",
      "Epoch 20, Batch 38/462, Loss: 0.832571268081665\n",
      "Epoch 20, Batch 39/462, Loss: 0.6297116875648499\n",
      "Epoch 20, Batch 40/462, Loss: 0.7787637710571289\n",
      "Epoch 20, Batch 41/462, Loss: 0.6864835619926453\n",
      "Epoch 20, Batch 42/462, Loss: 0.7578594088554382\n",
      "Epoch 20, Batch 43/462, Loss: 0.8848459720611572\n",
      "Epoch 20, Batch 44/462, Loss: 0.6405847072601318\n",
      "Epoch 20, Batch 45/462, Loss: 0.736649751663208\n",
      "Epoch 20, Batch 46/462, Loss: 0.5476915836334229\n",
      "Epoch 20, Batch 47/462, Loss: 0.7984259128570557\n",
      "Epoch 20, Batch 48/462, Loss: 0.6463617086410522\n",
      "Epoch 20, Batch 49/462, Loss: 0.7713969945907593\n",
      "Epoch 20, Batch 50/462, Loss: 0.6351464986801147\n",
      "Epoch 20, Batch 51/462, Loss: 0.940222978591919\n",
      "Epoch 20, Batch 52/462, Loss: 0.6076198816299438\n",
      "Epoch 20, Batch 53/462, Loss: 0.8324159383773804\n",
      "Epoch 20, Batch 54/462, Loss: 0.7267280220985413\n",
      "Epoch 20, Batch 55/462, Loss: 0.7336804270744324\n",
      "Epoch 20, Batch 56/462, Loss: 0.8821123838424683\n",
      "Epoch 20, Batch 57/462, Loss: 0.5933915376663208\n",
      "Epoch 20, Batch 58/462, Loss: 0.7555488348007202\n",
      "Epoch 20, Batch 59/462, Loss: 0.5812038779258728\n",
      "Epoch 20, Batch 60/462, Loss: 0.7233405709266663\n",
      "Epoch 20, Batch 61/462, Loss: 0.7923852205276489\n",
      "Epoch 20, Batch 62/462, Loss: 0.710363507270813\n",
      "Epoch 20, Batch 63/462, Loss: 0.7468717098236084\n",
      "Epoch 20, Batch 64/462, Loss: 0.6057032346725464\n",
      "Epoch 20, Batch 65/462, Loss: 0.7451661229133606\n",
      "Epoch 20, Batch 66/462, Loss: 0.7347819805145264\n",
      "Epoch 20, Batch 67/462, Loss: 0.7931668758392334\n",
      "Epoch 20, Batch 68/462, Loss: 0.8685325980186462\n",
      "Epoch 20, Batch 69/462, Loss: 0.8814840912818909\n",
      "Epoch 20, Batch 70/462, Loss: 0.6996700763702393\n",
      "Epoch 20, Batch 71/462, Loss: 0.860905110836029\n",
      "Epoch 20, Batch 72/462, Loss: 0.7494209408760071\n",
      "Epoch 20, Batch 73/462, Loss: 0.7324428558349609\n",
      "Epoch 20, Batch 74/462, Loss: 1.020310640335083\n",
      "Epoch 20, Batch 75/462, Loss: 0.6227639317512512\n",
      "Epoch 20, Batch 76/462, Loss: 0.9345058798789978\n",
      "Epoch 20, Batch 77/462, Loss: 0.5642498135566711\n",
      "Epoch 20, Batch 78/462, Loss: 0.6901203989982605\n",
      "Epoch 20, Batch 79/462, Loss: 0.8280275464057922\n",
      "Epoch 20, Batch 80/462, Loss: 0.7834274172782898\n",
      "Epoch 20, Batch 81/462, Loss: 0.8554678559303284\n",
      "Epoch 20, Batch 82/462, Loss: 0.7192503809928894\n",
      "Epoch 20, Batch 83/462, Loss: 0.9633100628852844\n",
      "Epoch 20, Batch 84/462, Loss: 0.9081325531005859\n",
      "Epoch 20, Batch 85/462, Loss: 0.9047704339027405\n",
      "Epoch 20, Batch 86/462, Loss: 0.8789489269256592\n",
      "Epoch 20, Batch 87/462, Loss: 0.5461297631263733\n",
      "Epoch 20, Batch 88/462, Loss: 0.7815210819244385\n",
      "Epoch 20, Batch 89/462, Loss: 0.5300589799880981\n",
      "Epoch 20, Batch 90/462, Loss: 0.7241498827934265\n",
      "Epoch 20, Batch 91/462, Loss: 0.7889612317085266\n",
      "Epoch 20, Batch 92/462, Loss: 0.7664299607276917\n",
      "Epoch 20, Batch 93/462, Loss: 0.6717264652252197\n",
      "Epoch 20, Batch 94/462, Loss: 0.7916386723518372\n",
      "Epoch 20, Batch 95/462, Loss: 0.7950078845024109\n",
      "Epoch 20, Batch 96/462, Loss: 0.7377313375473022\n",
      "Epoch 20, Batch 97/462, Loss: 0.8279035687446594\n",
      "Epoch 20, Batch 98/462, Loss: 0.9498146176338196\n",
      "Epoch 20, Batch 99/462, Loss: 0.8394449949264526\n",
      "Epoch 20, Batch 100/462, Loss: 0.6632782816886902\n",
      "Epoch 20, Batch 101/462, Loss: 0.6530804634094238\n",
      "Epoch 20, Batch 102/462, Loss: 0.7367197871208191\n",
      "Epoch 20, Batch 103/462, Loss: 0.8449853658676147\n",
      "Epoch 20, Batch 104/462, Loss: 0.634299099445343\n",
      "Epoch 20, Batch 105/462, Loss: 0.8478364944458008\n",
      "Epoch 20, Batch 106/462, Loss: 0.7630806565284729\n",
      "Epoch 20, Batch 107/462, Loss: 0.7509928941726685\n",
      "Epoch 20, Batch 108/462, Loss: 0.8624873161315918\n",
      "Epoch 20, Batch 109/462, Loss: 0.7794902920722961\n",
      "Epoch 20, Batch 110/462, Loss: 0.6163557171821594\n",
      "Epoch 20, Batch 111/462, Loss: 0.8470070362091064\n",
      "Epoch 20, Batch 112/462, Loss: 0.6565281748771667\n",
      "Epoch 20, Batch 113/462, Loss: 0.6527191400527954\n",
      "Epoch 20, Batch 114/462, Loss: 0.6914680600166321\n",
      "Epoch 20, Batch 115/462, Loss: 0.8421915173530579\n",
      "Epoch 20, Batch 116/462, Loss: 0.6579931378364563\n",
      "Epoch 20, Batch 117/462, Loss: 0.8588751554489136\n",
      "Epoch 20, Batch 118/462, Loss: 0.6505423784255981\n",
      "Epoch 20, Batch 119/462, Loss: 0.8180537223815918\n",
      "Epoch 20, Batch 120/462, Loss: 0.7719645500183105\n",
      "Epoch 20, Batch 121/462, Loss: 0.5597858428955078\n",
      "Epoch 20, Batch 122/462, Loss: 0.719159722328186\n",
      "Epoch 20, Batch 123/462, Loss: 0.6828339099884033\n",
      "Epoch 20, Batch 124/462, Loss: 0.7727584838867188\n",
      "Epoch 20, Batch 125/462, Loss: 0.6078266501426697\n",
      "Epoch 20, Batch 126/462, Loss: 0.7266523241996765\n",
      "Epoch 20, Batch 127/462, Loss: 0.703714907169342\n",
      "Epoch 20, Batch 128/462, Loss: 0.792102575302124\n",
      "Epoch 20, Batch 129/462, Loss: 0.8253885507583618\n",
      "Epoch 20, Batch 130/462, Loss: 0.7977862358093262\n",
      "Epoch 20, Batch 131/462, Loss: 0.8076101541519165\n",
      "Epoch 20, Batch 132/462, Loss: 0.7805560827255249\n",
      "Epoch 20, Batch 133/462, Loss: 0.7998597621917725\n",
      "Epoch 20, Batch 134/462, Loss: 0.7245603203773499\n",
      "Epoch 20, Batch 135/462, Loss: 0.7883079648017883\n",
      "Epoch 20, Batch 136/462, Loss: 0.5431600213050842\n",
      "Epoch 20, Batch 137/462, Loss: 0.670644998550415\n",
      "Epoch 20, Batch 138/462, Loss: 0.7636871933937073\n",
      "Epoch 20, Batch 139/462, Loss: 0.8444159626960754\n",
      "Epoch 20, Batch 140/462, Loss: 0.7804388999938965\n",
      "Epoch 20, Batch 141/462, Loss: 0.8168874979019165\n",
      "Epoch 20, Batch 142/462, Loss: 0.8285526037216187\n",
      "Epoch 20, Batch 143/462, Loss: 0.7803599834442139\n",
      "Epoch 20, Batch 144/462, Loss: 0.8277059197425842\n",
      "Epoch 20, Batch 145/462, Loss: 0.79044508934021\n",
      "Epoch 20, Batch 146/462, Loss: 0.7548706531524658\n",
      "Epoch 20, Batch 147/462, Loss: 0.662199854850769\n",
      "Epoch 20, Batch 148/462, Loss: 0.728599488735199\n",
      "Epoch 20, Batch 149/462, Loss: 0.7961883544921875\n",
      "Epoch 20, Batch 150/462, Loss: 0.7384282946586609\n",
      "Epoch 20, Batch 151/462, Loss: 0.710318386554718\n",
      "Epoch 20, Batch 152/462, Loss: 0.853413999080658\n",
      "Epoch 20, Batch 153/462, Loss: 0.7638593912124634\n",
      "Epoch 20, Batch 154/462, Loss: 0.6152811646461487\n",
      "Epoch 20, Batch 155/462, Loss: 0.6605526804924011\n",
      "Epoch 20, Batch 156/462, Loss: 0.7044748067855835\n",
      "Epoch 20, Batch 157/462, Loss: 0.6983886957168579\n",
      "Epoch 20, Batch 158/462, Loss: 0.8497438430786133\n",
      "Epoch 20, Batch 159/462, Loss: 0.7430529594421387\n",
      "Epoch 20, Batch 160/462, Loss: 0.826134443283081\n",
      "Epoch 20, Batch 161/462, Loss: 0.7419895529747009\n",
      "Epoch 20, Batch 162/462, Loss: 0.5317255854606628\n",
      "Epoch 20, Batch 163/462, Loss: 0.6735253930091858\n",
      "Epoch 20, Batch 164/462, Loss: 0.6816220283508301\n",
      "Epoch 20, Batch 165/462, Loss: 0.8569051027297974\n",
      "Epoch 20, Batch 166/462, Loss: 0.6898676753044128\n",
      "Epoch 20, Batch 167/462, Loss: 0.790523111820221\n",
      "Epoch 20, Batch 168/462, Loss: 0.6842065453529358\n",
      "Epoch 20, Batch 169/462, Loss: 0.6327204704284668\n",
      "Epoch 20, Batch 170/462, Loss: 0.7944519519805908\n",
      "Epoch 20, Batch 171/462, Loss: 0.8352005481719971\n",
      "Epoch 20, Batch 172/462, Loss: 0.754571795463562\n",
      "Epoch 20, Batch 173/462, Loss: 0.6815544366836548\n",
      "Epoch 20, Batch 174/462, Loss: 0.8641040325164795\n",
      "Epoch 20, Batch 175/462, Loss: 0.595852255821228\n",
      "Epoch 20, Batch 176/462, Loss: 0.7391615509986877\n",
      "Epoch 20, Batch 177/462, Loss: 0.7245538830757141\n",
      "Epoch 20, Batch 178/462, Loss: 0.7102625370025635\n",
      "Epoch 20, Batch 179/462, Loss: 0.8385720252990723\n",
      "Epoch 20, Batch 180/462, Loss: 0.9243867993354797\n",
      "Epoch 20, Batch 181/462, Loss: 0.7352169752120972\n",
      "Epoch 20, Batch 182/462, Loss: 0.556443452835083\n",
      "Epoch 20, Batch 183/462, Loss: 0.7683989405632019\n",
      "Epoch 20, Batch 184/462, Loss: 0.7548152804374695\n",
      "Epoch 20, Batch 185/462, Loss: 0.6361064910888672\n",
      "Epoch 20, Batch 186/462, Loss: 0.9577435851097107\n",
      "Epoch 20, Batch 187/462, Loss: 0.775645911693573\n",
      "Epoch 20, Batch 188/462, Loss: 0.6363412141799927\n",
      "Epoch 20, Batch 189/462, Loss: 0.7361671924591064\n",
      "Epoch 20, Batch 190/462, Loss: 0.7603644132614136\n",
      "Epoch 20, Batch 191/462, Loss: 0.7478403449058533\n",
      "Epoch 20, Batch 192/462, Loss: 0.8276187181472778\n",
      "Epoch 20, Batch 193/462, Loss: 0.6647145748138428\n",
      "Epoch 20, Batch 194/462, Loss: 0.7108361124992371\n",
      "Epoch 20, Batch 195/462, Loss: 0.6409164071083069\n",
      "Epoch 20, Batch 196/462, Loss: 0.8113899230957031\n",
      "Epoch 20, Batch 197/462, Loss: 0.7282896637916565\n",
      "Epoch 20, Batch 198/462, Loss: 0.821081817150116\n",
      "Epoch 20, Batch 199/462, Loss: 0.8719460964202881\n",
      "Epoch 20, Batch 200/462, Loss: 0.7292010188102722\n",
      "Epoch 20, Batch 201/462, Loss: 0.7690759897232056\n",
      "Epoch 20, Batch 202/462, Loss: 0.6308015584945679\n",
      "Epoch 20, Batch 203/462, Loss: 0.7448087334632874\n",
      "Epoch 20, Batch 204/462, Loss: 0.6885135173797607\n",
      "Epoch 20, Batch 205/462, Loss: 0.8250909447669983\n",
      "Epoch 20, Batch 206/462, Loss: 0.7282880544662476\n",
      "Epoch 20, Batch 207/462, Loss: 0.7304089665412903\n",
      "Epoch 20, Batch 208/462, Loss: 0.7269468903541565\n",
      "Epoch 20, Batch 209/462, Loss: 0.7716973423957825\n",
      "Epoch 20, Batch 210/462, Loss: 0.6987805366516113\n",
      "Epoch 20, Batch 211/462, Loss: 0.7121585011482239\n",
      "Epoch 20, Batch 212/462, Loss: 0.6536363363265991\n",
      "Epoch 20, Batch 213/462, Loss: 0.8418222069740295\n",
      "Epoch 20, Batch 214/462, Loss: 0.6904249787330627\n",
      "Epoch 20, Batch 215/462, Loss: 0.590637743473053\n",
      "Epoch 20, Batch 216/462, Loss: 0.6338977813720703\n",
      "Epoch 20, Batch 217/462, Loss: 0.6303245425224304\n",
      "Epoch 20, Batch 218/462, Loss: 0.7912738919258118\n",
      "Epoch 20, Batch 219/462, Loss: 0.7583091259002686\n",
      "Epoch 20, Batch 220/462, Loss: 0.6556515097618103\n",
      "Epoch 20, Batch 221/462, Loss: 0.7170224785804749\n",
      "Epoch 20, Batch 222/462, Loss: 0.6375157833099365\n",
      "Epoch 20, Batch 223/462, Loss: 0.9840110540390015\n",
      "Epoch 20, Batch 224/462, Loss: 0.8299449682235718\n",
      "Epoch 20, Batch 225/462, Loss: 0.9341453313827515\n",
      "Epoch 20, Batch 226/462, Loss: 0.8327760696411133\n",
      "Epoch 20, Batch 227/462, Loss: 0.7409180998802185\n",
      "Epoch 20, Batch 228/462, Loss: 0.5942872166633606\n",
      "Epoch 20, Batch 229/462, Loss: 0.932231605052948\n",
      "Epoch 20, Batch 230/462, Loss: 0.7702604532241821\n",
      "Epoch 20, Batch 231/462, Loss: 0.804634690284729\n",
      "Epoch 20, Batch 232/462, Loss: 0.6973056793212891\n",
      "Epoch 20, Batch 233/462, Loss: 0.991328775882721\n",
      "Epoch 20, Batch 234/462, Loss: 0.5728986263275146\n",
      "Epoch 20, Batch 235/462, Loss: 0.7936245203018188\n",
      "Epoch 20, Batch 236/462, Loss: 0.8538706302642822\n",
      "Epoch 20, Batch 237/462, Loss: 0.8474621772766113\n",
      "Epoch 20, Batch 238/462, Loss: 0.7788740396499634\n",
      "Epoch 20, Batch 239/462, Loss: 0.6275933980941772\n",
      "Epoch 20, Batch 240/462, Loss: 0.7923409342765808\n",
      "Epoch 20, Batch 241/462, Loss: 0.7759882211685181\n",
      "Epoch 20, Batch 242/462, Loss: 0.720908522605896\n",
      "Epoch 20, Batch 243/462, Loss: 0.9490164518356323\n",
      "Epoch 20, Batch 244/462, Loss: 0.7651441693305969\n",
      "Epoch 20, Batch 245/462, Loss: 0.6281641125679016\n",
      "Epoch 20, Batch 246/462, Loss: 0.8042312860488892\n",
      "Epoch 20, Batch 247/462, Loss: 0.7247207164764404\n",
      "Epoch 20, Batch 248/462, Loss: 0.7832068204879761\n",
      "Epoch 20, Batch 249/462, Loss: 0.7296779155731201\n",
      "Epoch 20, Batch 250/462, Loss: 0.76187664270401\n",
      "Epoch 20, Batch 251/462, Loss: 0.8040335178375244\n",
      "Epoch 20, Batch 252/462, Loss: 0.7545603513717651\n",
      "Epoch 20, Batch 253/462, Loss: 0.8697357177734375\n",
      "Epoch 20, Batch 254/462, Loss: 0.7208923697471619\n",
      "Epoch 20, Batch 255/462, Loss: 0.8328778147697449\n",
      "Epoch 20, Batch 256/462, Loss: 0.7473601698875427\n",
      "Epoch 20, Batch 257/462, Loss: 0.811574399471283\n",
      "Epoch 20, Batch 258/462, Loss: 0.6741024851799011\n",
      "Epoch 20, Batch 259/462, Loss: 0.788009524345398\n",
      "Epoch 20, Batch 260/462, Loss: 0.7288395166397095\n",
      "Epoch 20, Batch 261/462, Loss: 0.662929356098175\n",
      "Epoch 20, Batch 262/462, Loss: 0.6771982312202454\n",
      "Epoch 20, Batch 263/462, Loss: 0.8089134693145752\n",
      "Epoch 20, Batch 264/462, Loss: 0.5633620619773865\n",
      "Epoch 20, Batch 265/462, Loss: 0.6707870364189148\n",
      "Epoch 20, Batch 266/462, Loss: 0.6991946697235107\n",
      "Epoch 20, Batch 267/462, Loss: 0.7672970294952393\n",
      "Epoch 20, Batch 268/462, Loss: 0.5876641869544983\n",
      "Epoch 20, Batch 269/462, Loss: 0.7739506959915161\n",
      "Epoch 20, Batch 270/462, Loss: 0.6172236800193787\n",
      "Epoch 20, Batch 271/462, Loss: 0.7637896537780762\n",
      "Epoch 20, Batch 272/462, Loss: 0.7114354968070984\n",
      "Epoch 20, Batch 273/462, Loss: 0.5982109308242798\n",
      "Epoch 20, Batch 274/462, Loss: 0.874519944190979\n",
      "Epoch 20, Batch 275/462, Loss: 0.7098637819290161\n",
      "Epoch 20, Batch 276/462, Loss: 0.6389898657798767\n",
      "Epoch 20, Batch 277/462, Loss: 0.7447783350944519\n",
      "Epoch 20, Batch 278/462, Loss: 0.8411543369293213\n",
      "Epoch 20, Batch 279/462, Loss: 0.6181042194366455\n",
      "Epoch 20, Batch 280/462, Loss: 0.730259120464325\n",
      "Epoch 20, Batch 281/462, Loss: 0.6681540012359619\n",
      "Epoch 20, Batch 282/462, Loss: 0.8730253577232361\n",
      "Epoch 20, Batch 283/462, Loss: 0.8174986243247986\n",
      "Epoch 20, Batch 284/462, Loss: 0.5887191295623779\n",
      "Epoch 20, Batch 285/462, Loss: 0.7716657519340515\n",
      "Epoch 20, Batch 286/462, Loss: 0.7499114871025085\n",
      "Epoch 20, Batch 287/462, Loss: 0.6788650751113892\n",
      "Epoch 20, Batch 288/462, Loss: 0.6770369410514832\n",
      "Epoch 20, Batch 289/462, Loss: 0.6230586171150208\n",
      "Epoch 20, Batch 290/462, Loss: 0.6547786593437195\n",
      "Epoch 20, Batch 291/462, Loss: 0.8929296135902405\n",
      "Epoch 20, Batch 292/462, Loss: 0.8713518381118774\n",
      "Epoch 20, Batch 293/462, Loss: 0.5297421216964722\n",
      "Epoch 20, Batch 294/462, Loss: 0.6236173510551453\n",
      "Epoch 20, Batch 295/462, Loss: 0.871092677116394\n",
      "Epoch 20, Batch 296/462, Loss: 0.6569830179214478\n",
      "Epoch 20, Batch 297/462, Loss: 0.6319782137870789\n",
      "Epoch 20, Batch 298/462, Loss: 0.780159056186676\n",
      "Epoch 20, Batch 299/462, Loss: 0.7392902970314026\n",
      "Epoch 20, Batch 300/462, Loss: 0.575817346572876\n",
      "Epoch 20, Batch 301/462, Loss: 0.7119572162628174\n",
      "Epoch 20, Batch 302/462, Loss: 0.8530887365341187\n",
      "Epoch 20, Batch 303/462, Loss: 0.7151401042938232\n",
      "Epoch 20, Batch 304/462, Loss: 0.7623901963233948\n",
      "Epoch 20, Batch 305/462, Loss: 0.7600451707839966\n",
      "Epoch 20, Batch 306/462, Loss: 0.8980852365493774\n",
      "Epoch 20, Batch 307/462, Loss: 0.8007030487060547\n",
      "Epoch 20, Batch 308/462, Loss: 0.9191999435424805\n",
      "Epoch 20, Batch 309/462, Loss: 0.7583930492401123\n",
      "Epoch 20, Batch 310/462, Loss: 0.734254002571106\n",
      "Epoch 20, Batch 311/462, Loss: 0.6889402866363525\n",
      "Epoch 20, Batch 312/462, Loss: 0.710581362247467\n",
      "Epoch 20, Batch 313/462, Loss: 1.0402544736862183\n",
      "Epoch 20, Batch 314/462, Loss: 0.8238677382469177\n",
      "Epoch 20, Batch 315/462, Loss: 0.6949939727783203\n",
      "Epoch 20, Batch 316/462, Loss: 0.7995410561561584\n",
      "Epoch 20, Batch 317/462, Loss: 0.7111029624938965\n",
      "Epoch 20, Batch 318/462, Loss: 0.8851935863494873\n",
      "Epoch 20, Batch 319/462, Loss: 0.7875401973724365\n",
      "Epoch 20, Batch 320/462, Loss: 0.7861067056655884\n",
      "Epoch 20, Batch 321/462, Loss: 0.7452759742736816\n",
      "Epoch 20, Batch 322/462, Loss: 0.810260534286499\n",
      "Epoch 20, Batch 323/462, Loss: 0.7301673293113708\n",
      "Epoch 20, Batch 324/462, Loss: 0.76633220911026\n",
      "Epoch 20, Batch 325/462, Loss: 0.8544738292694092\n",
      "Epoch 20, Batch 326/462, Loss: 0.7797819375991821\n",
      "Epoch 20, Batch 327/462, Loss: 0.7461501359939575\n",
      "Epoch 20, Batch 328/462, Loss: 0.7952183485031128\n",
      "Epoch 20, Batch 329/462, Loss: 0.8892059326171875\n",
      "Epoch 20, Batch 330/462, Loss: 0.7717897891998291\n",
      "Epoch 20, Batch 331/462, Loss: 0.6426471471786499\n",
      "Epoch 20, Batch 332/462, Loss: 0.7051136493682861\n",
      "Epoch 20, Batch 333/462, Loss: 0.6146594285964966\n",
      "Epoch 20, Batch 334/462, Loss: 0.5984134078025818\n",
      "Epoch 20, Batch 335/462, Loss: 0.7866476774215698\n",
      "Epoch 20, Batch 336/462, Loss: 0.6939504146575928\n",
      "Epoch 20, Batch 337/462, Loss: 0.7873827815055847\n",
      "Epoch 20, Batch 338/462, Loss: 0.8757091164588928\n",
      "Epoch 20, Batch 339/462, Loss: 0.8584479689598083\n",
      "Epoch 20, Batch 340/462, Loss: 0.6745575070381165\n",
      "Epoch 20, Batch 341/462, Loss: 0.8017069697380066\n",
      "Epoch 20, Batch 342/462, Loss: 0.6987476944923401\n",
      "Epoch 20, Batch 343/462, Loss: 0.9562829732894897\n",
      "Epoch 20, Batch 344/462, Loss: 0.7009943723678589\n",
      "Epoch 20, Batch 345/462, Loss: 0.5971344113349915\n",
      "Epoch 20, Batch 346/462, Loss: 0.630628764629364\n",
      "Epoch 20, Batch 347/462, Loss: 0.7225648760795593\n",
      "Epoch 20, Batch 348/462, Loss: 0.7987803816795349\n",
      "Epoch 20, Batch 349/462, Loss: 0.7099155187606812\n",
      "Epoch 20, Batch 350/462, Loss: 0.7813906073570251\n",
      "Epoch 20, Batch 351/462, Loss: 0.78102046251297\n",
      "Epoch 20, Batch 352/462, Loss: 0.718927800655365\n",
      "Epoch 20, Batch 353/462, Loss: 0.6972814202308655\n",
      "Epoch 20, Batch 354/462, Loss: 0.7033383846282959\n",
      "Epoch 20, Batch 355/462, Loss: 0.6746724843978882\n",
      "Epoch 20, Batch 356/462, Loss: 0.7884458303451538\n",
      "Epoch 20, Batch 357/462, Loss: 0.6568850874900818\n",
      "Epoch 20, Batch 358/462, Loss: 0.7542601227760315\n",
      "Epoch 20, Batch 359/462, Loss: 0.5547745823860168\n",
      "Epoch 20, Batch 360/462, Loss: 0.7991900444030762\n",
      "Epoch 20, Batch 361/462, Loss: 0.6910243034362793\n",
      "Epoch 20, Batch 362/462, Loss: 0.5611904859542847\n",
      "Epoch 20, Batch 363/462, Loss: 0.6850870251655579\n",
      "Epoch 20, Batch 364/462, Loss: 0.6555213332176208\n",
      "Epoch 20, Batch 365/462, Loss: 0.7608285546302795\n",
      "Epoch 20, Batch 366/462, Loss: 0.5664243698120117\n",
      "Epoch 20, Batch 367/462, Loss: 0.7550530433654785\n",
      "Epoch 20, Batch 368/462, Loss: 0.7704506516456604\n",
      "Epoch 20, Batch 369/462, Loss: 0.8419281244277954\n",
      "Epoch 20, Batch 370/462, Loss: 0.7347733974456787\n",
      "Epoch 20, Batch 371/462, Loss: 0.9150453805923462\n",
      "Epoch 20, Batch 372/462, Loss: 0.6706265807151794\n",
      "Epoch 20, Batch 373/462, Loss: 0.7167254090309143\n",
      "Epoch 20, Batch 374/462, Loss: 0.6537038087844849\n",
      "Epoch 20, Batch 375/462, Loss: 0.7237881422042847\n",
      "Epoch 20, Batch 376/462, Loss: 0.7771592140197754\n",
      "Epoch 20, Batch 377/462, Loss: 0.7197846174240112\n",
      "Epoch 20, Batch 378/462, Loss: 0.8206027746200562\n",
      "Epoch 20, Batch 379/462, Loss: 0.8792679905891418\n",
      "Epoch 20, Batch 380/462, Loss: 0.7609151005744934\n",
      "Epoch 20, Batch 381/462, Loss: 0.7660365104675293\n",
      "Epoch 20, Batch 382/462, Loss: 0.6259526610374451\n",
      "Epoch 20, Batch 383/462, Loss: 0.9123603701591492\n",
      "Epoch 20, Batch 384/462, Loss: 0.7998723387718201\n",
      "Epoch 20, Batch 385/462, Loss: 0.7145966291427612\n",
      "Epoch 20, Batch 386/462, Loss: 0.7427374720573425\n",
      "Epoch 20, Batch 387/462, Loss: 0.8850770592689514\n",
      "Epoch 20, Batch 388/462, Loss: 0.6782536506652832\n",
      "Epoch 20, Batch 389/462, Loss: 0.5829221606254578\n",
      "Epoch 20, Batch 390/462, Loss: 0.6617848873138428\n",
      "Epoch 20, Batch 391/462, Loss: 0.9610745310783386\n",
      "Epoch 20, Batch 392/462, Loss: 0.833403468132019\n",
      "Epoch 20, Batch 393/462, Loss: 0.7577566504478455\n",
      "Epoch 20, Batch 394/462, Loss: 0.6924740076065063\n",
      "Epoch 20, Batch 395/462, Loss: 0.8111739754676819\n",
      "Epoch 20, Batch 396/462, Loss: 0.7442166805267334\n",
      "Epoch 20, Batch 397/462, Loss: 0.8006232380867004\n",
      "Epoch 20, Batch 398/462, Loss: 0.8146584630012512\n",
      "Epoch 20, Batch 399/462, Loss: 0.7691761255264282\n",
      "Epoch 20, Batch 400/462, Loss: 0.6757247447967529\n",
      "Epoch 20, Batch 401/462, Loss: 0.7779467701911926\n",
      "Epoch 20, Batch 402/462, Loss: 0.8520624041557312\n",
      "Epoch 20, Batch 403/462, Loss: 0.7705475687980652\n",
      "Epoch 20, Batch 404/462, Loss: 0.9038981199264526\n",
      "Epoch 20, Batch 405/462, Loss: 0.782137393951416\n",
      "Epoch 20, Batch 406/462, Loss: 0.6624940633773804\n",
      "Epoch 20, Batch 407/462, Loss: 0.7084367871284485\n",
      "Epoch 20, Batch 408/462, Loss: 0.8039136528968811\n",
      "Epoch 20, Batch 409/462, Loss: 0.7272224426269531\n",
      "Epoch 20, Batch 410/462, Loss: 0.8179266452789307\n",
      "Epoch 20, Batch 411/462, Loss: 0.7093937397003174\n",
      "Epoch 20, Batch 412/462, Loss: 0.7196987867355347\n",
      "Epoch 20, Batch 413/462, Loss: 0.8326314091682434\n",
      "Epoch 20, Batch 414/462, Loss: 0.7583397030830383\n",
      "Epoch 20, Batch 415/462, Loss: 0.7155463695526123\n",
      "Epoch 20, Batch 416/462, Loss: 0.6909640431404114\n",
      "Epoch 20, Batch 417/462, Loss: 0.7839263677597046\n",
      "Epoch 20, Batch 418/462, Loss: 0.8576306104660034\n",
      "Epoch 20, Batch 419/462, Loss: 1.0109288692474365\n",
      "Epoch 20, Batch 420/462, Loss: 0.7571120858192444\n",
      "Epoch 20, Batch 421/462, Loss: 0.7338205575942993\n",
      "Epoch 20, Batch 422/462, Loss: 0.7987163662910461\n",
      "Epoch 20, Batch 423/462, Loss: 0.6920173764228821\n",
      "Epoch 20, Batch 424/462, Loss: 0.5972908735275269\n",
      "Epoch 20, Batch 425/462, Loss: 0.724086582660675\n",
      "Epoch 20, Batch 426/462, Loss: 0.8159242272377014\n",
      "Epoch 20, Batch 427/462, Loss: 0.7915999889373779\n",
      "Epoch 20, Batch 428/462, Loss: 0.7665907740592957\n",
      "Epoch 20, Batch 429/462, Loss: 0.853564977645874\n",
      "Epoch 20, Batch 430/462, Loss: 0.7180172801017761\n",
      "Epoch 20, Batch 431/462, Loss: 0.6644319295883179\n",
      "Epoch 20, Batch 432/462, Loss: 0.7668744325637817\n",
      "Epoch 20, Batch 433/462, Loss: 0.8253865242004395\n",
      "Epoch 20, Batch 434/462, Loss: 0.7613930106163025\n",
      "Epoch 20, Batch 435/462, Loss: 0.6982231736183167\n",
      "Epoch 20, Batch 436/462, Loss: 0.6398969292640686\n",
      "Epoch 20, Batch 437/462, Loss: 0.853787362575531\n",
      "Epoch 20, Batch 438/462, Loss: 0.7172590494155884\n",
      "Epoch 20, Batch 439/462, Loss: 0.6147171854972839\n",
      "Epoch 20, Batch 440/462, Loss: 0.7681611180305481\n",
      "Epoch 20, Batch 441/462, Loss: 0.8233044743537903\n",
      "Epoch 20, Batch 442/462, Loss: 0.7557438015937805\n",
      "Epoch 20, Batch 443/462, Loss: 0.594249427318573\n",
      "Epoch 20, Batch 444/462, Loss: 0.908306360244751\n",
      "Epoch 20, Batch 445/462, Loss: 0.685930073261261\n",
      "Epoch 20, Batch 446/462, Loss: 0.7648935317993164\n",
      "Epoch 20, Batch 447/462, Loss: 0.7480093240737915\n",
      "Epoch 20, Batch 448/462, Loss: 0.8570926785469055\n",
      "Epoch 20, Batch 449/462, Loss: 0.7396664619445801\n",
      "Epoch 20, Batch 450/462, Loss: 0.6731327176094055\n",
      "Epoch 20, Batch 451/462, Loss: 0.7696728706359863\n",
      "Epoch 20, Batch 452/462, Loss: 0.607302725315094\n",
      "Epoch 20, Batch 453/462, Loss: 0.6825072169303894\n",
      "Epoch 20, Batch 454/462, Loss: 0.6515522003173828\n",
      "Epoch 20, Batch 455/462, Loss: 0.7506698369979858\n",
      "Epoch 20, Batch 456/462, Loss: 0.6909232139587402\n",
      "Epoch 20, Batch 457/462, Loss: 0.6987211108207703\n",
      "Epoch 20, Batch 458/462, Loss: 0.8126291036605835\n",
      "Epoch 20, Batch 459/462, Loss: 0.7735550403594971\n",
      "Epoch 20, Batch 460/462, Loss: 0.7818275094032288\n",
      "Epoch 20, Batch 461/462, Loss: 0.9762216210365295\n",
      "Epoch 20, Batch 462/462, Loss: 0.5796918272972107\n",
      "Epoch 20, Loss: 346.2152324914932\n",
      "Epoch 21, Batch 1/462, Loss: 0.6382336020469666\n",
      "Epoch 21, Batch 2/462, Loss: 0.5895382165908813\n",
      "Epoch 21, Batch 3/462, Loss: 0.6196345090866089\n",
      "Epoch 21, Batch 4/462, Loss: 0.5994428396224976\n",
      "Epoch 21, Batch 5/462, Loss: 0.7436566352844238\n",
      "Epoch 21, Batch 6/462, Loss: 0.6616859436035156\n",
      "Epoch 21, Batch 7/462, Loss: 0.8188794851303101\n",
      "Epoch 21, Batch 8/462, Loss: 0.6332048177719116\n",
      "Epoch 21, Batch 9/462, Loss: 0.9649530053138733\n",
      "Epoch 21, Batch 10/462, Loss: 0.5789021253585815\n",
      "Epoch 21, Batch 11/462, Loss: 0.8144755363464355\n",
      "Epoch 21, Batch 12/462, Loss: 0.9657695293426514\n",
      "Epoch 21, Batch 13/462, Loss: 0.6769402027130127\n",
      "Epoch 21, Batch 14/462, Loss: 0.6273811459541321\n",
      "Epoch 21, Batch 15/462, Loss: 0.7312213778495789\n",
      "Epoch 21, Batch 16/462, Loss: 0.7931413650512695\n",
      "Epoch 21, Batch 17/462, Loss: 0.825083315372467\n",
      "Epoch 21, Batch 18/462, Loss: 0.6773238778114319\n",
      "Epoch 21, Batch 19/462, Loss: 1.0898476839065552\n",
      "Epoch 21, Batch 20/462, Loss: 0.7433711886405945\n",
      "Epoch 21, Batch 21/462, Loss: 0.677614688873291\n",
      "Epoch 21, Batch 22/462, Loss: 0.7069177031517029\n",
      "Epoch 21, Batch 23/462, Loss: 0.7336224913597107\n",
      "Epoch 21, Batch 24/462, Loss: 0.708629846572876\n",
      "Epoch 21, Batch 25/462, Loss: 0.8004166483879089\n",
      "Epoch 21, Batch 26/462, Loss: 0.6653090715408325\n",
      "Epoch 21, Batch 27/462, Loss: 0.5498713254928589\n",
      "Epoch 21, Batch 28/462, Loss: 0.6676453948020935\n",
      "Epoch 21, Batch 29/462, Loss: 0.7393642067909241\n",
      "Epoch 21, Batch 30/462, Loss: 0.8107506036758423\n",
      "Epoch 21, Batch 31/462, Loss: 0.6046798229217529\n",
      "Epoch 21, Batch 32/462, Loss: 0.83110111951828\n",
      "Epoch 21, Batch 33/462, Loss: 0.8182753324508667\n",
      "Epoch 21, Batch 34/462, Loss: 0.8988659381866455\n",
      "Epoch 21, Batch 35/462, Loss: 0.8092851042747498\n",
      "Epoch 21, Batch 36/462, Loss: 0.76804119348526\n",
      "Epoch 21, Batch 37/462, Loss: 0.6806224584579468\n",
      "Epoch 21, Batch 38/462, Loss: 0.6940636038780212\n",
      "Epoch 21, Batch 39/462, Loss: 1.082695484161377\n",
      "Epoch 21, Batch 40/462, Loss: 0.675769031047821\n",
      "Epoch 21, Batch 41/462, Loss: 0.6650676131248474\n",
      "Epoch 21, Batch 42/462, Loss: 0.5853502750396729\n",
      "Epoch 21, Batch 43/462, Loss: 0.7496658563613892\n",
      "Epoch 21, Batch 44/462, Loss: 0.7459526062011719\n",
      "Epoch 21, Batch 45/462, Loss: 0.7425588369369507\n",
      "Epoch 21, Batch 46/462, Loss: 0.6385119557380676\n",
      "Epoch 21, Batch 47/462, Loss: 0.6312924027442932\n",
      "Epoch 21, Batch 48/462, Loss: 0.6721981167793274\n",
      "Epoch 21, Batch 49/462, Loss: 0.7606297135353088\n",
      "Epoch 21, Batch 50/462, Loss: 0.6816735863685608\n",
      "Epoch 21, Batch 51/462, Loss: 0.793257474899292\n",
      "Epoch 21, Batch 52/462, Loss: 0.7786005735397339\n",
      "Epoch 21, Batch 53/462, Loss: 0.9328820109367371\n",
      "Epoch 21, Batch 54/462, Loss: 0.6111313104629517\n",
      "Epoch 21, Batch 55/462, Loss: 0.7678682208061218\n",
      "Epoch 21, Batch 56/462, Loss: 0.8147629499435425\n",
      "Epoch 21, Batch 57/462, Loss: 0.7389758229255676\n",
      "Epoch 21, Batch 58/462, Loss: 0.6923648715019226\n",
      "Epoch 21, Batch 59/462, Loss: 0.8363922238349915\n",
      "Epoch 21, Batch 60/462, Loss: 0.712735652923584\n",
      "Epoch 21, Batch 61/462, Loss: 0.6998671889305115\n",
      "Epoch 21, Batch 62/462, Loss: 0.6148681640625\n",
      "Epoch 21, Batch 63/462, Loss: 0.7913328409194946\n",
      "Epoch 21, Batch 64/462, Loss: 0.9586092233657837\n",
      "Epoch 21, Batch 65/462, Loss: 0.868139922618866\n",
      "Epoch 21, Batch 66/462, Loss: 0.7202388644218445\n",
      "Epoch 21, Batch 67/462, Loss: 0.8387909531593323\n",
      "Epoch 21, Batch 68/462, Loss: 0.5636071562767029\n",
      "Epoch 21, Batch 69/462, Loss: 0.6956083178520203\n",
      "Epoch 21, Batch 70/462, Loss: 0.846792995929718\n",
      "Epoch 21, Batch 71/462, Loss: 0.8770483136177063\n",
      "Epoch 21, Batch 72/462, Loss: 0.787277102470398\n",
      "Epoch 21, Batch 73/462, Loss: 0.9444804191589355\n",
      "Epoch 21, Batch 74/462, Loss: 0.7426342368125916\n",
      "Epoch 21, Batch 75/462, Loss: 0.9645633697509766\n",
      "Epoch 21, Batch 76/462, Loss: 1.028958797454834\n",
      "Epoch 21, Batch 77/462, Loss: 1.1637908220291138\n",
      "Epoch 21, Batch 78/462, Loss: 0.6550729870796204\n",
      "Epoch 21, Batch 79/462, Loss: 0.7997413277626038\n",
      "Epoch 21, Batch 80/462, Loss: 0.6734010577201843\n",
      "Epoch 21, Batch 81/462, Loss: 0.9094268083572388\n",
      "Epoch 21, Batch 82/462, Loss: 0.6658489108085632\n",
      "Epoch 21, Batch 83/462, Loss: 0.8058997988700867\n",
      "Epoch 21, Batch 84/462, Loss: 0.9091851711273193\n",
      "Epoch 21, Batch 85/462, Loss: 0.6977036595344543\n",
      "Epoch 21, Batch 86/462, Loss: 0.63029944896698\n",
      "Epoch 21, Batch 87/462, Loss: 0.7205792665481567\n",
      "Epoch 21, Batch 88/462, Loss: 0.7783350944519043\n",
      "Epoch 21, Batch 89/462, Loss: 0.6485934853553772\n",
      "Epoch 21, Batch 90/462, Loss: 0.6382470726966858\n",
      "Epoch 21, Batch 91/462, Loss: 0.6293099522590637\n",
      "Epoch 21, Batch 92/462, Loss: 0.6842105984687805\n",
      "Epoch 21, Batch 93/462, Loss: 0.9434182047843933\n",
      "Epoch 21, Batch 94/462, Loss: 0.7995844483375549\n",
      "Epoch 21, Batch 95/462, Loss: 0.7427113652229309\n",
      "Epoch 21, Batch 96/462, Loss: 0.6872583031654358\n",
      "Epoch 21, Batch 97/462, Loss: 0.6709334850311279\n",
      "Epoch 21, Batch 98/462, Loss: 0.6474359035491943\n",
      "Epoch 21, Batch 99/462, Loss: 0.7310518622398376\n",
      "Epoch 21, Batch 100/462, Loss: 0.8382160663604736\n",
      "Epoch 21, Batch 101/462, Loss: 0.7843614220619202\n",
      "Epoch 21, Batch 102/462, Loss: 0.8016813397407532\n",
      "Epoch 21, Batch 103/462, Loss: 0.5726932883262634\n",
      "Epoch 21, Batch 104/462, Loss: 0.9967373609542847\n",
      "Epoch 21, Batch 105/462, Loss: 0.8061719536781311\n",
      "Epoch 21, Batch 106/462, Loss: 0.7820133566856384\n",
      "Epoch 21, Batch 107/462, Loss: 0.7882254123687744\n",
      "Epoch 21, Batch 108/462, Loss: 0.9189049005508423\n",
      "Epoch 21, Batch 109/462, Loss: 1.0359995365142822\n",
      "Epoch 21, Batch 110/462, Loss: 0.8903700113296509\n",
      "Epoch 21, Batch 111/462, Loss: 0.7656940221786499\n",
      "Epoch 21, Batch 112/462, Loss: 0.7154504656791687\n",
      "Epoch 21, Batch 113/462, Loss: 0.7571147680282593\n",
      "Epoch 21, Batch 114/462, Loss: 0.740322470664978\n",
      "Epoch 21, Batch 115/462, Loss: 0.5869990587234497\n",
      "Epoch 21, Batch 116/462, Loss: 0.5962743163108826\n",
      "Epoch 21, Batch 117/462, Loss: 0.7664737701416016\n",
      "Epoch 21, Batch 118/462, Loss: 0.8474172353744507\n",
      "Epoch 21, Batch 119/462, Loss: 0.821486234664917\n",
      "Epoch 21, Batch 120/462, Loss: 0.679634153842926\n",
      "Epoch 21, Batch 121/462, Loss: 0.8529000282287598\n",
      "Epoch 21, Batch 122/462, Loss: 0.8663396239280701\n",
      "Epoch 21, Batch 123/462, Loss: 0.7466724514961243\n",
      "Epoch 21, Batch 124/462, Loss: 0.7204248309135437\n",
      "Epoch 21, Batch 125/462, Loss: 0.7956030368804932\n",
      "Epoch 21, Batch 126/462, Loss: 0.6471359133720398\n",
      "Epoch 21, Batch 127/462, Loss: 0.5977378487586975\n",
      "Epoch 21, Batch 128/462, Loss: 0.792942225933075\n",
      "Epoch 21, Batch 129/462, Loss: 0.9425324201583862\n",
      "Epoch 21, Batch 130/462, Loss: 0.8965775370597839\n",
      "Epoch 21, Batch 131/462, Loss: 0.8046756982803345\n",
      "Epoch 21, Batch 132/462, Loss: 0.7170113921165466\n",
      "Epoch 21, Batch 133/462, Loss: 0.9367637634277344\n",
      "Epoch 21, Batch 134/462, Loss: 0.7938188910484314\n",
      "Epoch 21, Batch 135/462, Loss: 0.6841151714324951\n",
      "Epoch 21, Batch 136/462, Loss: 0.7229362726211548\n",
      "Epoch 21, Batch 137/462, Loss: 0.9161719083786011\n",
      "Epoch 21, Batch 138/462, Loss: 0.7354532480239868\n",
      "Epoch 21, Batch 139/462, Loss: 0.7442895770072937\n",
      "Epoch 21, Batch 140/462, Loss: 0.7201620936393738\n",
      "Epoch 21, Batch 141/462, Loss: 0.7830101847648621\n",
      "Epoch 21, Batch 142/462, Loss: 0.8414122462272644\n",
      "Epoch 21, Batch 143/462, Loss: 0.7673282027244568\n",
      "Epoch 21, Batch 144/462, Loss: 0.7195404171943665\n",
      "Epoch 21, Batch 145/462, Loss: 0.7874999642372131\n",
      "Epoch 21, Batch 146/462, Loss: 0.9655131697654724\n",
      "Epoch 21, Batch 147/462, Loss: 0.7871748805046082\n",
      "Epoch 21, Batch 148/462, Loss: 0.7628946304321289\n",
      "Epoch 21, Batch 149/462, Loss: 0.8948981761932373\n",
      "Epoch 21, Batch 150/462, Loss: 0.6702495813369751\n",
      "Epoch 21, Batch 151/462, Loss: 0.6786994934082031\n",
      "Epoch 21, Batch 152/462, Loss: 0.7624494433403015\n",
      "Epoch 21, Batch 153/462, Loss: 0.742884635925293\n",
      "Epoch 21, Batch 154/462, Loss: 0.7996008396148682\n",
      "Epoch 21, Batch 155/462, Loss: 0.6054151654243469\n",
      "Epoch 21, Batch 156/462, Loss: 0.6879248023033142\n",
      "Epoch 21, Batch 157/462, Loss: 0.8677010536193848\n",
      "Epoch 21, Batch 158/462, Loss: 0.6418030858039856\n",
      "Epoch 21, Batch 159/462, Loss: 0.7634178400039673\n",
      "Epoch 21, Batch 160/462, Loss: 0.7653616666793823\n",
      "Epoch 21, Batch 161/462, Loss: 0.7794790863990784\n",
      "Epoch 21, Batch 162/462, Loss: 0.7485554814338684\n",
      "Epoch 21, Batch 163/462, Loss: 0.8271512985229492\n",
      "Epoch 21, Batch 164/462, Loss: 0.756833016872406\n",
      "Epoch 21, Batch 165/462, Loss: 0.7050870656967163\n",
      "Epoch 21, Batch 166/462, Loss: 0.7541666626930237\n",
      "Epoch 21, Batch 167/462, Loss: 0.7951485514640808\n",
      "Epoch 21, Batch 168/462, Loss: 0.8395947813987732\n",
      "Epoch 21, Batch 169/462, Loss: 0.8003284931182861\n",
      "Epoch 21, Batch 170/462, Loss: 0.695115864276886\n",
      "Epoch 21, Batch 171/462, Loss: 0.7427452206611633\n",
      "Epoch 21, Batch 172/462, Loss: 0.6646003127098083\n",
      "Epoch 21, Batch 173/462, Loss: 0.5326699614524841\n",
      "Epoch 21, Batch 174/462, Loss: 0.835141658782959\n",
      "Epoch 21, Batch 175/462, Loss: 0.9061727523803711\n",
      "Epoch 21, Batch 176/462, Loss: 0.6655404567718506\n",
      "Epoch 21, Batch 177/462, Loss: 0.7244247794151306\n",
      "Epoch 21, Batch 178/462, Loss: 0.7759414315223694\n",
      "Epoch 21, Batch 179/462, Loss: 0.7088009119033813\n",
      "Epoch 21, Batch 180/462, Loss: 0.7340914607048035\n",
      "Epoch 21, Batch 181/462, Loss: 0.7604959011077881\n",
      "Epoch 21, Batch 182/462, Loss: 0.5806729793548584\n",
      "Epoch 21, Batch 183/462, Loss: 0.6917502284049988\n",
      "Epoch 21, Batch 184/462, Loss: 0.7389733791351318\n",
      "Epoch 21, Batch 185/462, Loss: 0.588600754737854\n",
      "Epoch 21, Batch 186/462, Loss: 0.7668564915657043\n",
      "Epoch 21, Batch 187/462, Loss: 0.6689609289169312\n",
      "Epoch 21, Batch 188/462, Loss: 0.6937373876571655\n",
      "Epoch 21, Batch 189/462, Loss: 0.8575108051300049\n",
      "Epoch 21, Batch 190/462, Loss: 0.6541817784309387\n",
      "Epoch 21, Batch 191/462, Loss: 0.6271253824234009\n",
      "Epoch 21, Batch 192/462, Loss: 0.7094554901123047\n",
      "Epoch 21, Batch 193/462, Loss: 0.6630890369415283\n",
      "Epoch 21, Batch 194/462, Loss: 0.8202605247497559\n",
      "Epoch 21, Batch 195/462, Loss: 0.5853070616722107\n",
      "Epoch 21, Batch 196/462, Loss: 0.7312625050544739\n",
      "Epoch 21, Batch 197/462, Loss: 0.6949565410614014\n",
      "Epoch 21, Batch 198/462, Loss: 0.7908795475959778\n",
      "Epoch 21, Batch 199/462, Loss: 0.6147955060005188\n",
      "Epoch 21, Batch 200/462, Loss: 0.8143168091773987\n",
      "Epoch 21, Batch 201/462, Loss: 0.807198166847229\n",
      "Epoch 21, Batch 202/462, Loss: 0.7296091318130493\n",
      "Epoch 21, Batch 203/462, Loss: 0.706504225730896\n",
      "Epoch 21, Batch 204/462, Loss: 0.7322574853897095\n",
      "Epoch 21, Batch 205/462, Loss: 0.6333918571472168\n",
      "Epoch 21, Batch 206/462, Loss: 0.6542209982872009\n",
      "Epoch 21, Batch 207/462, Loss: 0.8440547585487366\n",
      "Epoch 21, Batch 208/462, Loss: 0.732445240020752\n",
      "Epoch 21, Batch 209/462, Loss: 0.7193568348884583\n",
      "Epoch 21, Batch 210/462, Loss: 0.667776346206665\n",
      "Epoch 21, Batch 211/462, Loss: 0.7826234698295593\n",
      "Epoch 21, Batch 212/462, Loss: 0.7824586033821106\n",
      "Epoch 21, Batch 213/462, Loss: 0.6633862853050232\n",
      "Epoch 21, Batch 214/462, Loss: 0.8498857021331787\n",
      "Epoch 21, Batch 215/462, Loss: 0.6581640243530273\n",
      "Epoch 21, Batch 216/462, Loss: 0.8564023375511169\n",
      "Epoch 21, Batch 217/462, Loss: 0.8196238279342651\n",
      "Epoch 21, Batch 218/462, Loss: 0.8190944194793701\n",
      "Epoch 21, Batch 219/462, Loss: 0.7185266017913818\n",
      "Epoch 21, Batch 220/462, Loss: 0.6814897060394287\n",
      "Epoch 21, Batch 221/462, Loss: 0.6617782711982727\n",
      "Epoch 21, Batch 222/462, Loss: 0.6959866285324097\n",
      "Epoch 21, Batch 223/462, Loss: 0.6093304753303528\n",
      "Epoch 21, Batch 224/462, Loss: 0.8793138265609741\n",
      "Epoch 21, Batch 225/462, Loss: 0.6811872720718384\n",
      "Epoch 21, Batch 226/462, Loss: 0.8462464809417725\n",
      "Epoch 21, Batch 227/462, Loss: 0.736530065536499\n",
      "Epoch 21, Batch 228/462, Loss: 0.876267671585083\n",
      "Epoch 21, Batch 229/462, Loss: 0.8940780162811279\n",
      "Epoch 21, Batch 230/462, Loss: 0.6060639023780823\n",
      "Epoch 21, Batch 231/462, Loss: 0.8563280701637268\n",
      "Epoch 21, Batch 232/462, Loss: 0.7846353054046631\n",
      "Epoch 21, Batch 233/462, Loss: 0.8551903963088989\n",
      "Epoch 21, Batch 234/462, Loss: 0.7025539875030518\n",
      "Epoch 21, Batch 235/462, Loss: 0.6486265659332275\n",
      "Epoch 21, Batch 236/462, Loss: 0.7397485971450806\n",
      "Epoch 21, Batch 237/462, Loss: 0.856475830078125\n",
      "Epoch 21, Batch 238/462, Loss: 0.8174348473548889\n",
      "Epoch 21, Batch 239/462, Loss: 0.8144853711128235\n",
      "Epoch 21, Batch 240/462, Loss: 0.8408525586128235\n",
      "Epoch 21, Batch 241/462, Loss: 0.652111828327179\n",
      "Epoch 21, Batch 242/462, Loss: 0.7690305709838867\n",
      "Epoch 21, Batch 243/462, Loss: 0.763825535774231\n",
      "Epoch 21, Batch 244/462, Loss: 0.7695995569229126\n",
      "Epoch 21, Batch 245/462, Loss: 0.6814326643943787\n",
      "Epoch 21, Batch 246/462, Loss: 0.8072333931922913\n",
      "Epoch 21, Batch 247/462, Loss: 0.6846513152122498\n",
      "Epoch 21, Batch 248/462, Loss: 0.6834614276885986\n",
      "Epoch 21, Batch 249/462, Loss: 0.7488365173339844\n",
      "Epoch 21, Batch 250/462, Loss: 0.848898708820343\n",
      "Epoch 21, Batch 251/462, Loss: 0.8681126832962036\n",
      "Epoch 21, Batch 252/462, Loss: 0.7108107805252075\n",
      "Epoch 21, Batch 253/462, Loss: 0.8795698285102844\n",
      "Epoch 21, Batch 254/462, Loss: 0.6984623074531555\n",
      "Epoch 21, Batch 255/462, Loss: 0.8877156376838684\n",
      "Epoch 21, Batch 256/462, Loss: 0.7024280428886414\n",
      "Epoch 21, Batch 257/462, Loss: 0.7914025783538818\n",
      "Epoch 21, Batch 258/462, Loss: 0.8156286478042603\n",
      "Epoch 21, Batch 259/462, Loss: 0.7933294773101807\n",
      "Epoch 21, Batch 260/462, Loss: 0.8852788805961609\n",
      "Epoch 21, Batch 261/462, Loss: 0.6388903260231018\n",
      "Epoch 21, Batch 262/462, Loss: 0.8646891117095947\n",
      "Epoch 21, Batch 263/462, Loss: 0.7413701415061951\n",
      "Epoch 21, Batch 264/462, Loss: 0.7272810935974121\n",
      "Epoch 21, Batch 265/462, Loss: 0.8569579720497131\n",
      "Epoch 21, Batch 266/462, Loss: 0.7274916768074036\n",
      "Epoch 21, Batch 267/462, Loss: 0.8876826763153076\n",
      "Epoch 21, Batch 268/462, Loss: 0.860501766204834\n",
      "Epoch 21, Batch 269/462, Loss: 0.8432206511497498\n",
      "Epoch 21, Batch 270/462, Loss: 0.6273939609527588\n",
      "Epoch 21, Batch 271/462, Loss: 0.8572618365287781\n",
      "Epoch 21, Batch 272/462, Loss: 0.9312026500701904\n",
      "Epoch 21, Batch 273/462, Loss: 0.7610882520675659\n",
      "Epoch 21, Batch 274/462, Loss: 0.8069689273834229\n",
      "Epoch 21, Batch 275/462, Loss: 0.6205641031265259\n",
      "Epoch 21, Batch 276/462, Loss: 0.77527916431427\n",
      "Epoch 21, Batch 277/462, Loss: 0.6895419359207153\n",
      "Epoch 21, Batch 278/462, Loss: 0.6201432943344116\n",
      "Epoch 21, Batch 279/462, Loss: 0.7498977780342102\n",
      "Epoch 21, Batch 280/462, Loss: 0.6919748783111572\n",
      "Epoch 21, Batch 281/462, Loss: 0.8902843594551086\n",
      "Epoch 21, Batch 282/462, Loss: 0.651636004447937\n",
      "Epoch 21, Batch 283/462, Loss: 0.8764891028404236\n",
      "Epoch 21, Batch 284/462, Loss: 0.64422607421875\n",
      "Epoch 21, Batch 285/462, Loss: 0.8223857283592224\n",
      "Epoch 21, Batch 286/462, Loss: 0.7064716815948486\n",
      "Epoch 21, Batch 287/462, Loss: 0.7327229380607605\n",
      "Epoch 21, Batch 288/462, Loss: 0.8404842019081116\n",
      "Epoch 21, Batch 289/462, Loss: 0.897401750087738\n",
      "Epoch 21, Batch 290/462, Loss: 0.7242634296417236\n",
      "Epoch 21, Batch 291/462, Loss: 0.5728884935379028\n",
      "Epoch 21, Batch 292/462, Loss: 0.6673378944396973\n",
      "Epoch 21, Batch 293/462, Loss: 0.8972024321556091\n",
      "Epoch 21, Batch 294/462, Loss: 0.7182142734527588\n",
      "Epoch 21, Batch 295/462, Loss: 0.6971454620361328\n",
      "Epoch 21, Batch 296/462, Loss: 0.6811162829399109\n",
      "Epoch 21, Batch 297/462, Loss: 0.7586181163787842\n",
      "Epoch 21, Batch 298/462, Loss: 0.8315730094909668\n",
      "Epoch 21, Batch 299/462, Loss: 0.733052134513855\n",
      "Epoch 21, Batch 300/462, Loss: 0.6204810738563538\n",
      "Epoch 21, Batch 301/462, Loss: 0.7033199667930603\n",
      "Epoch 21, Batch 302/462, Loss: 0.6808704733848572\n",
      "Epoch 21, Batch 303/462, Loss: 0.6961665153503418\n",
      "Epoch 21, Batch 304/462, Loss: 0.6621448397636414\n",
      "Epoch 21, Batch 305/462, Loss: 0.7325220108032227\n",
      "Epoch 21, Batch 306/462, Loss: 0.8921055793762207\n",
      "Epoch 21, Batch 307/462, Loss: 0.6855300068855286\n",
      "Epoch 21, Batch 308/462, Loss: 0.7488062977790833\n",
      "Epoch 21, Batch 309/462, Loss: 0.7584786415100098\n",
      "Epoch 21, Batch 310/462, Loss: 0.6930918097496033\n",
      "Epoch 21, Batch 311/462, Loss: 0.779852569103241\n",
      "Epoch 21, Batch 312/462, Loss: 0.581829309463501\n",
      "Epoch 21, Batch 313/462, Loss: 0.7778996229171753\n",
      "Epoch 21, Batch 314/462, Loss: 0.783880889415741\n",
      "Epoch 21, Batch 315/462, Loss: 0.8217204213142395\n",
      "Epoch 21, Batch 316/462, Loss: 0.6453621983528137\n",
      "Epoch 21, Batch 317/462, Loss: 0.654050350189209\n",
      "Epoch 21, Batch 318/462, Loss: 0.6743693351745605\n",
      "Epoch 21, Batch 319/462, Loss: 0.696929931640625\n",
      "Epoch 21, Batch 320/462, Loss: 0.6451708674430847\n",
      "Epoch 21, Batch 321/462, Loss: 0.7292832136154175\n",
      "Epoch 21, Batch 322/462, Loss: 0.8696793913841248\n",
      "Epoch 21, Batch 323/462, Loss: 0.793057382106781\n",
      "Epoch 21, Batch 324/462, Loss: 0.6449795365333557\n",
      "Epoch 21, Batch 325/462, Loss: 0.6901776194572449\n",
      "Epoch 21, Batch 326/462, Loss: 0.6622863411903381\n",
      "Epoch 21, Batch 327/462, Loss: 0.6963338851928711\n",
      "Epoch 21, Batch 328/462, Loss: 0.7598934173583984\n",
      "Epoch 21, Batch 329/462, Loss: 0.6299765110015869\n",
      "Epoch 21, Batch 330/462, Loss: 0.7075304985046387\n",
      "Epoch 21, Batch 331/462, Loss: 0.5555022358894348\n",
      "Epoch 21, Batch 332/462, Loss: 0.9135231971740723\n",
      "Epoch 21, Batch 333/462, Loss: 0.7914709448814392\n",
      "Epoch 21, Batch 334/462, Loss: 0.7588415145874023\n",
      "Epoch 21, Batch 335/462, Loss: 0.6691968441009521\n",
      "Epoch 21, Batch 336/462, Loss: 0.7855143547058105\n",
      "Epoch 21, Batch 337/462, Loss: 0.8448547720909119\n",
      "Epoch 21, Batch 338/462, Loss: 0.7091023921966553\n",
      "Epoch 21, Batch 339/462, Loss: 0.8095170259475708\n",
      "Epoch 21, Batch 340/462, Loss: 0.8107720613479614\n",
      "Epoch 21, Batch 341/462, Loss: 0.7506787180900574\n",
      "Epoch 21, Batch 342/462, Loss: 0.564889669418335\n",
      "Epoch 21, Batch 343/462, Loss: 0.8826425075531006\n",
      "Epoch 21, Batch 344/462, Loss: 0.703238844871521\n",
      "Epoch 21, Batch 345/462, Loss: 0.8841062784194946\n",
      "Epoch 21, Batch 346/462, Loss: 0.7253727912902832\n",
      "Epoch 21, Batch 347/462, Loss: 0.6043378710746765\n",
      "Epoch 21, Batch 348/462, Loss: 0.6413939595222473\n",
      "Epoch 21, Batch 349/462, Loss: 0.8276013135910034\n",
      "Epoch 21, Batch 350/462, Loss: 0.7779988050460815\n",
      "Epoch 21, Batch 351/462, Loss: 0.8257356286048889\n",
      "Epoch 21, Batch 352/462, Loss: 0.8873089551925659\n",
      "Epoch 21, Batch 353/462, Loss: 0.8726431131362915\n",
      "Epoch 21, Batch 354/462, Loss: 0.7286885380744934\n",
      "Epoch 21, Batch 355/462, Loss: 0.7502155900001526\n",
      "Epoch 21, Batch 356/462, Loss: 0.6199126243591309\n",
      "Epoch 21, Batch 357/462, Loss: 0.6048662066459656\n",
      "Epoch 21, Batch 358/462, Loss: 0.8997170329093933\n",
      "Epoch 21, Batch 359/462, Loss: 0.8166633248329163\n",
      "Epoch 21, Batch 360/462, Loss: 0.8663046360015869\n",
      "Epoch 21, Batch 361/462, Loss: 0.6862936019897461\n",
      "Epoch 21, Batch 362/462, Loss: 0.6856569647789001\n",
      "Epoch 21, Batch 363/462, Loss: 0.747539758682251\n",
      "Epoch 21, Batch 364/462, Loss: 0.7087271809577942\n",
      "Epoch 21, Batch 365/462, Loss: 0.7240673899650574\n",
      "Epoch 21, Batch 366/462, Loss: 0.6934881806373596\n",
      "Epoch 21, Batch 367/462, Loss: 0.7083684802055359\n",
      "Epoch 21, Batch 368/462, Loss: 0.710915207862854\n",
      "Epoch 21, Batch 369/462, Loss: 0.7177764773368835\n",
      "Epoch 21, Batch 370/462, Loss: 0.6944929957389832\n",
      "Epoch 21, Batch 371/462, Loss: 0.6678017377853394\n",
      "Epoch 21, Batch 372/462, Loss: 0.8214155435562134\n",
      "Epoch 21, Batch 373/462, Loss: 0.7386795878410339\n",
      "Epoch 21, Batch 374/462, Loss: 0.6527556777000427\n",
      "Epoch 21, Batch 375/462, Loss: 0.7376264929771423\n",
      "Epoch 21, Batch 376/462, Loss: 0.7606866359710693\n",
      "Epoch 21, Batch 377/462, Loss: 0.80110764503479\n",
      "Epoch 21, Batch 378/462, Loss: 0.6926649212837219\n",
      "Epoch 21, Batch 379/462, Loss: 0.7908714413642883\n",
      "Epoch 21, Batch 380/462, Loss: 0.8778376579284668\n",
      "Epoch 21, Batch 381/462, Loss: 0.7098239064216614\n",
      "Epoch 21, Batch 382/462, Loss: 0.7122468948364258\n",
      "Epoch 21, Batch 383/462, Loss: 0.7260966897010803\n",
      "Epoch 21, Batch 384/462, Loss: 0.6791775822639465\n",
      "Epoch 21, Batch 385/462, Loss: 0.8439939022064209\n",
      "Epoch 21, Batch 386/462, Loss: 0.8215776085853577\n",
      "Epoch 21, Batch 387/462, Loss: 0.5432564616203308\n",
      "Epoch 21, Batch 388/462, Loss: 0.6990395784378052\n",
      "Epoch 21, Batch 389/462, Loss: 0.7791747450828552\n",
      "Epoch 21, Batch 390/462, Loss: 0.8163529634475708\n",
      "Epoch 21, Batch 391/462, Loss: 0.8047000765800476\n",
      "Epoch 21, Batch 392/462, Loss: 0.8005366921424866\n",
      "Epoch 21, Batch 393/462, Loss: 0.6001520752906799\n",
      "Epoch 21, Batch 394/462, Loss: 0.6040797233581543\n",
      "Epoch 21, Batch 395/462, Loss: 0.7124759554862976\n",
      "Epoch 21, Batch 396/462, Loss: 0.9067503213882446\n",
      "Epoch 21, Batch 397/462, Loss: 0.6600116491317749\n",
      "Epoch 21, Batch 398/462, Loss: 0.8387042284011841\n",
      "Epoch 21, Batch 399/462, Loss: 0.8485254049301147\n",
      "Epoch 21, Batch 400/462, Loss: 0.6891317367553711\n",
      "Epoch 21, Batch 401/462, Loss: 0.7567126750946045\n",
      "Epoch 21, Batch 402/462, Loss: 0.6227734088897705\n",
      "Epoch 21, Batch 403/462, Loss: 0.6441471576690674\n",
      "Epoch 21, Batch 404/462, Loss: 0.6594623327255249\n",
      "Epoch 21, Batch 405/462, Loss: 0.7383063435554504\n",
      "Epoch 21, Batch 406/462, Loss: 0.8675515651702881\n",
      "Epoch 21, Batch 407/462, Loss: 0.9016808271408081\n",
      "Epoch 21, Batch 408/462, Loss: 0.8152871131896973\n",
      "Epoch 21, Batch 409/462, Loss: 0.75713711977005\n",
      "Epoch 21, Batch 410/462, Loss: 0.8636679649353027\n",
      "Epoch 21, Batch 411/462, Loss: 0.81946861743927\n",
      "Epoch 21, Batch 412/462, Loss: 0.8261830806732178\n",
      "Epoch 21, Batch 413/462, Loss: 0.6550142168998718\n",
      "Epoch 21, Batch 414/462, Loss: 0.7815064191818237\n",
      "Epoch 21, Batch 415/462, Loss: 0.5435164570808411\n",
      "Epoch 21, Batch 416/462, Loss: 0.6483070850372314\n",
      "Epoch 21, Batch 417/462, Loss: 0.6976803541183472\n",
      "Epoch 21, Batch 418/462, Loss: 0.9308590888977051\n",
      "Epoch 21, Batch 419/462, Loss: 0.8814108371734619\n",
      "Epoch 21, Batch 420/462, Loss: 0.7732335925102234\n",
      "Epoch 21, Batch 421/462, Loss: 0.6195144653320312\n",
      "Epoch 21, Batch 422/462, Loss: 0.8253241181373596\n",
      "Epoch 21, Batch 423/462, Loss: 0.798475444316864\n",
      "Epoch 21, Batch 424/462, Loss: 0.6889112591743469\n",
      "Epoch 21, Batch 425/462, Loss: 0.7026984691619873\n",
      "Epoch 21, Batch 426/462, Loss: 0.7257499694824219\n",
      "Epoch 21, Batch 427/462, Loss: 0.50530606508255\n",
      "Epoch 21, Batch 428/462, Loss: 0.6852893829345703\n",
      "Epoch 21, Batch 429/462, Loss: 1.0066440105438232\n",
      "Epoch 21, Batch 430/462, Loss: 0.7399293184280396\n",
      "Epoch 21, Batch 431/462, Loss: 0.8468806147575378\n",
      "Epoch 21, Batch 432/462, Loss: 0.5829776525497437\n",
      "Epoch 21, Batch 433/462, Loss: 0.712014377117157\n",
      "Epoch 21, Batch 434/462, Loss: 0.7243847846984863\n",
      "Epoch 21, Batch 435/462, Loss: 0.6869214177131653\n",
      "Epoch 21, Batch 436/462, Loss: 0.6445350646972656\n",
      "Epoch 21, Batch 437/462, Loss: 0.8009049892425537\n",
      "Epoch 21, Batch 438/462, Loss: 0.628312349319458\n",
      "Epoch 21, Batch 439/462, Loss: 0.6372132301330566\n",
      "Epoch 21, Batch 440/462, Loss: 0.756669819355011\n",
      "Epoch 21, Batch 441/462, Loss: 0.7620641589164734\n",
      "Epoch 21, Batch 442/462, Loss: 0.7771936058998108\n",
      "Epoch 21, Batch 443/462, Loss: 0.8131729960441589\n",
      "Epoch 21, Batch 444/462, Loss: 0.8047106862068176\n",
      "Epoch 21, Batch 445/462, Loss: 0.7963482141494751\n",
      "Epoch 21, Batch 446/462, Loss: 0.5929391384124756\n",
      "Epoch 21, Batch 447/462, Loss: 0.6014655232429504\n",
      "Epoch 21, Batch 448/462, Loss: 0.6581876873970032\n",
      "Epoch 21, Batch 449/462, Loss: 0.8833855390548706\n",
      "Epoch 21, Batch 450/462, Loss: 0.8563022613525391\n",
      "Epoch 21, Batch 451/462, Loss: 0.7557400465011597\n",
      "Epoch 21, Batch 452/462, Loss: 0.8756266832351685\n",
      "Epoch 21, Batch 453/462, Loss: 0.5419790744781494\n",
      "Epoch 21, Batch 454/462, Loss: 0.8284180164337158\n",
      "Epoch 21, Batch 455/462, Loss: 0.70814049243927\n",
      "Epoch 21, Batch 456/462, Loss: 0.9086440205574036\n",
      "Epoch 21, Batch 457/462, Loss: 0.8057137131690979\n",
      "Epoch 21, Batch 458/462, Loss: 0.5725311636924744\n",
      "Epoch 21, Batch 459/462, Loss: 0.736433744430542\n",
      "Epoch 21, Batch 460/462, Loss: 0.7876551151275635\n",
      "Epoch 21, Batch 461/462, Loss: 0.833769679069519\n",
      "Epoch 21, Batch 462/462, Loss: 0.7752622365951538\n",
      "Epoch 21, Loss: 346.8285602927208\n",
      "Epoch 22, Batch 1/462, Loss: 0.8071253299713135\n",
      "Epoch 22, Batch 2/462, Loss: 0.6386414170265198\n",
      "Epoch 22, Batch 3/462, Loss: 0.7862256765365601\n",
      "Epoch 22, Batch 4/462, Loss: 0.7044768929481506\n",
      "Epoch 22, Batch 5/462, Loss: 0.8699715733528137\n",
      "Epoch 22, Batch 6/462, Loss: 0.8220615386962891\n",
      "Epoch 22, Batch 7/462, Loss: 0.88966965675354\n",
      "Epoch 22, Batch 8/462, Loss: 0.7872391939163208\n",
      "Epoch 22, Batch 9/462, Loss: 0.7570406198501587\n",
      "Epoch 22, Batch 10/462, Loss: 0.7402030229568481\n",
      "Epoch 22, Batch 11/462, Loss: 0.6025034785270691\n",
      "Epoch 22, Batch 12/462, Loss: 0.7818991541862488\n",
      "Epoch 22, Batch 13/462, Loss: 0.9102560877799988\n",
      "Epoch 22, Batch 14/462, Loss: 0.7774562835693359\n",
      "Epoch 22, Batch 15/462, Loss: 0.8688711524009705\n",
      "Epoch 22, Batch 16/462, Loss: 0.7376193404197693\n",
      "Epoch 22, Batch 17/462, Loss: 0.718275249004364\n",
      "Epoch 22, Batch 18/462, Loss: 0.7150227427482605\n",
      "Epoch 22, Batch 19/462, Loss: 0.7658436298370361\n",
      "Epoch 22, Batch 20/462, Loss: 0.8085696697235107\n",
      "Epoch 22, Batch 21/462, Loss: 0.7718193531036377\n",
      "Epoch 22, Batch 22/462, Loss: 0.7522158026695251\n",
      "Epoch 22, Batch 23/462, Loss: 0.6459376811981201\n",
      "Epoch 22, Batch 24/462, Loss: 0.6079714298248291\n",
      "Epoch 22, Batch 25/462, Loss: 0.7624222636222839\n",
      "Epoch 22, Batch 26/462, Loss: 0.7636041045188904\n",
      "Epoch 22, Batch 27/462, Loss: 0.7890647649765015\n",
      "Epoch 22, Batch 28/462, Loss: 0.7396295666694641\n",
      "Epoch 22, Batch 29/462, Loss: 0.6879660487174988\n",
      "Epoch 22, Batch 30/462, Loss: 0.7947899699211121\n",
      "Epoch 22, Batch 31/462, Loss: 0.7716172933578491\n",
      "Epoch 22, Batch 32/462, Loss: 0.6535546183586121\n",
      "Epoch 22, Batch 33/462, Loss: 0.730263888835907\n",
      "Epoch 22, Batch 34/462, Loss: 0.6986859440803528\n",
      "Epoch 22, Batch 35/462, Loss: 0.7746009230613708\n",
      "Epoch 22, Batch 36/462, Loss: 0.7056439518928528\n",
      "Epoch 22, Batch 37/462, Loss: 0.775278627872467\n",
      "Epoch 22, Batch 38/462, Loss: 0.6809960603713989\n",
      "Epoch 22, Batch 39/462, Loss: 0.8913751244544983\n",
      "Epoch 22, Batch 40/462, Loss: 0.7791305184364319\n",
      "Epoch 22, Batch 41/462, Loss: 0.8607325553894043\n",
      "Epoch 22, Batch 42/462, Loss: 0.6689653992652893\n",
      "Epoch 22, Batch 43/462, Loss: 0.6723886728286743\n",
      "Epoch 22, Batch 44/462, Loss: 0.6826167106628418\n",
      "Epoch 22, Batch 45/462, Loss: 0.7105534076690674\n",
      "Epoch 22, Batch 46/462, Loss: 0.8419497013092041\n",
      "Epoch 22, Batch 47/462, Loss: 0.9691351652145386\n",
      "Epoch 22, Batch 48/462, Loss: 0.5995421409606934\n",
      "Epoch 22, Batch 49/462, Loss: 0.6345813870429993\n",
      "Epoch 22, Batch 50/462, Loss: 0.7293265461921692\n",
      "Epoch 22, Batch 51/462, Loss: 0.5603963136672974\n",
      "Epoch 22, Batch 52/462, Loss: 0.8287698030471802\n",
      "Epoch 22, Batch 53/462, Loss: 0.7799391150474548\n",
      "Epoch 22, Batch 54/462, Loss: 0.829334557056427\n",
      "Epoch 22, Batch 55/462, Loss: 0.8627970218658447\n",
      "Epoch 22, Batch 56/462, Loss: 0.7056856155395508\n",
      "Epoch 22, Batch 57/462, Loss: 0.582099437713623\n",
      "Epoch 22, Batch 58/462, Loss: 0.6251646876335144\n",
      "Epoch 22, Batch 59/462, Loss: 0.8308352828025818\n",
      "Epoch 22, Batch 60/462, Loss: 0.8153345584869385\n",
      "Epoch 22, Batch 61/462, Loss: 0.6270233988761902\n",
      "Epoch 22, Batch 62/462, Loss: 0.5382410883903503\n",
      "Epoch 22, Batch 63/462, Loss: 0.8551484942436218\n",
      "Epoch 22, Batch 64/462, Loss: 0.6723361611366272\n",
      "Epoch 22, Batch 65/462, Loss: 0.7178167700767517\n",
      "Epoch 22, Batch 66/462, Loss: 0.7059730291366577\n",
      "Epoch 22, Batch 67/462, Loss: 0.6808566451072693\n",
      "Epoch 22, Batch 68/462, Loss: 0.5892390012741089\n",
      "Epoch 22, Batch 69/462, Loss: 0.6666600108146667\n",
      "Epoch 22, Batch 70/462, Loss: 0.7486830949783325\n",
      "Epoch 22, Batch 71/462, Loss: 0.7401135563850403\n",
      "Epoch 22, Batch 72/462, Loss: 0.4975641369819641\n",
      "Epoch 22, Batch 73/462, Loss: 0.7900729775428772\n",
      "Epoch 22, Batch 74/462, Loss: 0.6815546751022339\n",
      "Epoch 22, Batch 75/462, Loss: 0.7317407727241516\n",
      "Epoch 22, Batch 76/462, Loss: 0.670909583568573\n",
      "Epoch 22, Batch 77/462, Loss: 0.7258327007293701\n",
      "Epoch 22, Batch 78/462, Loss: 0.7074465155601501\n",
      "Epoch 22, Batch 79/462, Loss: 0.7066375017166138\n",
      "Epoch 22, Batch 80/462, Loss: 0.7839421033859253\n",
      "Epoch 22, Batch 81/462, Loss: 0.7714526057243347\n",
      "Epoch 22, Batch 82/462, Loss: 0.7722988724708557\n",
      "Epoch 22, Batch 83/462, Loss: 0.7177424430847168\n",
      "Epoch 22, Batch 84/462, Loss: 0.8227658271789551\n",
      "Epoch 22, Batch 85/462, Loss: 0.670782744884491\n",
      "Epoch 22, Batch 86/462, Loss: 0.7626990079879761\n",
      "Epoch 22, Batch 87/462, Loss: 0.802049458026886\n",
      "Epoch 22, Batch 88/462, Loss: 0.7401993870735168\n",
      "Epoch 22, Batch 89/462, Loss: 0.754817545413971\n",
      "Epoch 22, Batch 90/462, Loss: 0.7164204120635986\n",
      "Epoch 22, Batch 91/462, Loss: 0.6714683771133423\n",
      "Epoch 22, Batch 92/462, Loss: 0.6602151989936829\n",
      "Epoch 22, Batch 93/462, Loss: 0.8342404365539551\n",
      "Epoch 22, Batch 94/462, Loss: 0.8933426141738892\n",
      "Epoch 22, Batch 95/462, Loss: 0.7916284799575806\n",
      "Epoch 22, Batch 96/462, Loss: 0.7913367748260498\n",
      "Epoch 22, Batch 97/462, Loss: 0.7834751605987549\n",
      "Epoch 22, Batch 98/462, Loss: 0.6905524134635925\n",
      "Epoch 22, Batch 99/462, Loss: 0.7551079988479614\n",
      "Epoch 22, Batch 100/462, Loss: 0.7653341293334961\n",
      "Epoch 22, Batch 101/462, Loss: 0.788901686668396\n",
      "Epoch 22, Batch 102/462, Loss: 0.6196334362030029\n",
      "Epoch 22, Batch 103/462, Loss: 0.7930235862731934\n",
      "Epoch 22, Batch 104/462, Loss: 0.7867355346679688\n",
      "Epoch 22, Batch 105/462, Loss: 0.8092323541641235\n",
      "Epoch 22, Batch 106/462, Loss: 0.865390956401825\n",
      "Epoch 22, Batch 107/462, Loss: 0.6177973747253418\n",
      "Epoch 22, Batch 108/462, Loss: 0.7726449966430664\n",
      "Epoch 22, Batch 109/462, Loss: 0.6938642859458923\n",
      "Epoch 22, Batch 110/462, Loss: 0.6262524724006653\n",
      "Epoch 22, Batch 111/462, Loss: 0.8768649101257324\n",
      "Epoch 22, Batch 112/462, Loss: 0.8661113977432251\n",
      "Epoch 22, Batch 113/462, Loss: 0.7737013697624207\n",
      "Epoch 22, Batch 114/462, Loss: 0.7994271516799927\n",
      "Epoch 22, Batch 115/462, Loss: 0.6920269727706909\n",
      "Epoch 22, Batch 116/462, Loss: 0.7212985754013062\n",
      "Epoch 22, Batch 117/462, Loss: 0.8962405920028687\n",
      "Epoch 22, Batch 118/462, Loss: 0.6532058715820312\n",
      "Epoch 22, Batch 119/462, Loss: 0.9898059368133545\n",
      "Epoch 22, Batch 120/462, Loss: 0.7525084018707275\n",
      "Epoch 22, Batch 121/462, Loss: 0.7736108303070068\n",
      "Epoch 22, Batch 122/462, Loss: 0.7721980810165405\n",
      "Epoch 22, Batch 123/462, Loss: 0.8839335441589355\n",
      "Epoch 22, Batch 124/462, Loss: 0.827002227306366\n",
      "Epoch 22, Batch 125/462, Loss: 0.5769582390785217\n",
      "Epoch 22, Batch 126/462, Loss: 0.6154460310935974\n",
      "Epoch 22, Batch 127/462, Loss: 0.5492904782295227\n",
      "Epoch 22, Batch 128/462, Loss: 0.7775687575340271\n",
      "Epoch 22, Batch 129/462, Loss: 0.6268007159233093\n",
      "Epoch 22, Batch 130/462, Loss: 0.8880407214164734\n",
      "Epoch 22, Batch 131/462, Loss: 0.8309403657913208\n",
      "Epoch 22, Batch 132/462, Loss: 0.7342712879180908\n",
      "Epoch 22, Batch 133/462, Loss: 0.5377441048622131\n",
      "Epoch 22, Batch 134/462, Loss: 0.6803985834121704\n",
      "Epoch 22, Batch 135/462, Loss: 0.5191819071769714\n",
      "Epoch 22, Batch 136/462, Loss: 0.6758912205696106\n",
      "Epoch 22, Batch 137/462, Loss: 0.9836748242378235\n",
      "Epoch 22, Batch 138/462, Loss: 0.7503933310508728\n",
      "Epoch 22, Batch 139/462, Loss: 0.7055856585502625\n",
      "Epoch 22, Batch 140/462, Loss: 0.6752105951309204\n",
      "Epoch 22, Batch 141/462, Loss: 0.6263958215713501\n",
      "Epoch 22, Batch 142/462, Loss: 0.6679643392562866\n",
      "Epoch 22, Batch 143/462, Loss: 0.6912945508956909\n",
      "Epoch 22, Batch 144/462, Loss: 0.8601086735725403\n",
      "Epoch 22, Batch 145/462, Loss: 0.8777141571044922\n",
      "Epoch 22, Batch 146/462, Loss: 0.6391406059265137\n",
      "Epoch 22, Batch 147/462, Loss: 0.749396562576294\n",
      "Epoch 22, Batch 148/462, Loss: 0.986835241317749\n",
      "Epoch 22, Batch 149/462, Loss: 0.8702940344810486\n",
      "Epoch 22, Batch 150/462, Loss: 0.7306895852088928\n",
      "Epoch 22, Batch 151/462, Loss: 0.8254247307777405\n",
      "Epoch 22, Batch 152/462, Loss: 0.8775609135627747\n",
      "Epoch 22, Batch 153/462, Loss: 0.6285490989685059\n",
      "Epoch 22, Batch 154/462, Loss: 0.703511655330658\n",
      "Epoch 22, Batch 155/462, Loss: 0.8001606464385986\n",
      "Epoch 22, Batch 156/462, Loss: 0.8869128227233887\n",
      "Epoch 22, Batch 157/462, Loss: 0.7196820378303528\n",
      "Epoch 22, Batch 158/462, Loss: 0.7190797924995422\n",
      "Epoch 22, Batch 159/462, Loss: 0.851946234703064\n",
      "Epoch 22, Batch 160/462, Loss: 0.7839527130126953\n",
      "Epoch 22, Batch 161/462, Loss: 0.6729920506477356\n",
      "Epoch 22, Batch 162/462, Loss: 0.7462582588195801\n",
      "Epoch 22, Batch 163/462, Loss: 0.8654994964599609\n",
      "Epoch 22, Batch 164/462, Loss: 0.8045902848243713\n",
      "Epoch 22, Batch 165/462, Loss: 0.770108699798584\n",
      "Epoch 22, Batch 166/462, Loss: 0.6704777479171753\n",
      "Epoch 22, Batch 167/462, Loss: 0.7934417128562927\n",
      "Epoch 22, Batch 168/462, Loss: 0.8307399749755859\n",
      "Epoch 22, Batch 169/462, Loss: 0.7494391202926636\n",
      "Epoch 22, Batch 170/462, Loss: 0.6454278826713562\n",
      "Epoch 22, Batch 171/462, Loss: 0.765291690826416\n",
      "Epoch 22, Batch 172/462, Loss: 0.786151647567749\n",
      "Epoch 22, Batch 173/462, Loss: 0.9253788590431213\n",
      "Epoch 22, Batch 174/462, Loss: 0.8282967209815979\n",
      "Epoch 22, Batch 175/462, Loss: 0.8470640182495117\n",
      "Epoch 22, Batch 176/462, Loss: 0.7557769417762756\n",
      "Epoch 22, Batch 177/462, Loss: 0.7251811027526855\n",
      "Epoch 22, Batch 178/462, Loss: 0.6888485550880432\n",
      "Epoch 22, Batch 179/462, Loss: 0.7454092502593994\n",
      "Epoch 22, Batch 180/462, Loss: 0.637363612651825\n",
      "Epoch 22, Batch 181/462, Loss: 0.7059045433998108\n",
      "Epoch 22, Batch 182/462, Loss: 0.7671336531639099\n",
      "Epoch 22, Batch 183/462, Loss: 0.5823951363563538\n",
      "Epoch 22, Batch 184/462, Loss: 0.6239408254623413\n",
      "Epoch 22, Batch 185/462, Loss: 0.8769538402557373\n",
      "Epoch 22, Batch 186/462, Loss: 0.8719531893730164\n",
      "Epoch 22, Batch 187/462, Loss: 0.7820073962211609\n",
      "Epoch 22, Batch 188/462, Loss: 0.6368759274482727\n",
      "Epoch 22, Batch 189/462, Loss: 0.7576608657836914\n",
      "Epoch 22, Batch 190/462, Loss: 0.7646079659461975\n",
      "Epoch 22, Batch 191/462, Loss: 0.6893917918205261\n",
      "Epoch 22, Batch 192/462, Loss: 0.699387788772583\n",
      "Epoch 22, Batch 193/462, Loss: 0.8869909644126892\n",
      "Epoch 22, Batch 194/462, Loss: 0.8640405535697937\n",
      "Epoch 22, Batch 195/462, Loss: 0.7167497873306274\n",
      "Epoch 22, Batch 196/462, Loss: 0.7128827571868896\n",
      "Epoch 22, Batch 197/462, Loss: 0.8636798858642578\n",
      "Epoch 22, Batch 198/462, Loss: 0.7927080392837524\n",
      "Epoch 22, Batch 199/462, Loss: 0.8183398842811584\n",
      "Epoch 22, Batch 200/462, Loss: 0.9442867636680603\n",
      "Epoch 22, Batch 201/462, Loss: 0.7900455594062805\n",
      "Epoch 22, Batch 202/462, Loss: 0.746792733669281\n",
      "Epoch 22, Batch 203/462, Loss: 0.7358930706977844\n",
      "Epoch 22, Batch 204/462, Loss: 0.8971166610717773\n",
      "Epoch 22, Batch 205/462, Loss: 0.8286700248718262\n",
      "Epoch 22, Batch 206/462, Loss: 0.6999017000198364\n",
      "Epoch 22, Batch 207/462, Loss: 0.7175216674804688\n",
      "Epoch 22, Batch 208/462, Loss: 0.6463499665260315\n",
      "Epoch 22, Batch 209/462, Loss: 0.7711261510848999\n",
      "Epoch 22, Batch 210/462, Loss: 0.6332303881645203\n",
      "Epoch 22, Batch 211/462, Loss: 0.8559945225715637\n",
      "Epoch 22, Batch 212/462, Loss: 0.7703011631965637\n",
      "Epoch 22, Batch 213/462, Loss: 0.6987904906272888\n",
      "Epoch 22, Batch 214/462, Loss: 0.7414524555206299\n",
      "Epoch 22, Batch 215/462, Loss: 0.6260770559310913\n",
      "Epoch 22, Batch 216/462, Loss: 0.7441895604133606\n",
      "Epoch 22, Batch 217/462, Loss: 0.6049816012382507\n",
      "Epoch 22, Batch 218/462, Loss: 0.7166018486022949\n",
      "Epoch 22, Batch 219/462, Loss: 0.6324481964111328\n",
      "Epoch 22, Batch 220/462, Loss: 0.9465446472167969\n",
      "Epoch 22, Batch 221/462, Loss: 0.793283224105835\n",
      "Epoch 22, Batch 222/462, Loss: 0.5293934345245361\n",
      "Epoch 22, Batch 223/462, Loss: 0.8667454123497009\n",
      "Epoch 22, Batch 224/462, Loss: 0.6959331035614014\n",
      "Epoch 22, Batch 225/462, Loss: 0.8191480040550232\n",
      "Epoch 22, Batch 226/462, Loss: 0.8011627197265625\n",
      "Epoch 22, Batch 227/462, Loss: 0.8314042091369629\n",
      "Epoch 22, Batch 228/462, Loss: 0.8732146620750427\n",
      "Epoch 22, Batch 229/462, Loss: 0.6773672103881836\n",
      "Epoch 22, Batch 230/462, Loss: 0.6987834572792053\n",
      "Epoch 22, Batch 231/462, Loss: 0.7146662473678589\n",
      "Epoch 22, Batch 232/462, Loss: 0.6446399092674255\n",
      "Epoch 22, Batch 233/462, Loss: 0.6514796018600464\n",
      "Epoch 22, Batch 234/462, Loss: 0.7245749831199646\n",
      "Epoch 22, Batch 235/462, Loss: 1.1137946844100952\n",
      "Epoch 22, Batch 236/462, Loss: 0.7888891100883484\n",
      "Epoch 22, Batch 237/462, Loss: 0.7196590304374695\n",
      "Epoch 22, Batch 238/462, Loss: 0.6916676759719849\n",
      "Epoch 22, Batch 239/462, Loss: 0.7489508986473083\n",
      "Epoch 22, Batch 240/462, Loss: 0.6265785694122314\n",
      "Epoch 22, Batch 241/462, Loss: 1.0041847229003906\n",
      "Epoch 22, Batch 242/462, Loss: 0.84178626537323\n",
      "Epoch 22, Batch 243/462, Loss: 0.727495551109314\n",
      "Epoch 22, Batch 244/462, Loss: 0.6341449022293091\n",
      "Epoch 22, Batch 245/462, Loss: 0.9185534119606018\n",
      "Epoch 22, Batch 246/462, Loss: 0.6177363991737366\n",
      "Epoch 22, Batch 247/462, Loss: 0.7721750736236572\n",
      "Epoch 22, Batch 248/462, Loss: 0.7413312196731567\n",
      "Epoch 22, Batch 249/462, Loss: 0.7985047698020935\n",
      "Epoch 22, Batch 250/462, Loss: 0.6465771198272705\n",
      "Epoch 22, Batch 251/462, Loss: 0.81876140832901\n",
      "Epoch 22, Batch 252/462, Loss: 0.5714316964149475\n",
      "Epoch 22, Batch 253/462, Loss: 0.6745688915252686\n",
      "Epoch 22, Batch 254/462, Loss: 0.5912966728210449\n",
      "Epoch 22, Batch 255/462, Loss: 0.7289158701896667\n",
      "Epoch 22, Batch 256/462, Loss: 0.745306670665741\n",
      "Epoch 22, Batch 257/462, Loss: 0.7290072441101074\n",
      "Epoch 22, Batch 258/462, Loss: 0.6992799043655396\n",
      "Epoch 22, Batch 259/462, Loss: 0.8847029209136963\n",
      "Epoch 22, Batch 260/462, Loss: 0.7949836850166321\n",
      "Epoch 22, Batch 261/462, Loss: 0.9064061641693115\n",
      "Epoch 22, Batch 262/462, Loss: 0.8454357385635376\n",
      "Epoch 22, Batch 263/462, Loss: 0.6112110018730164\n",
      "Epoch 22, Batch 264/462, Loss: 0.7441038489341736\n",
      "Epoch 22, Batch 265/462, Loss: 0.538915753364563\n",
      "Epoch 22, Batch 266/462, Loss: 0.7154553532600403\n",
      "Epoch 22, Batch 267/462, Loss: 0.6494285464286804\n",
      "Epoch 22, Batch 268/462, Loss: 0.8297398686408997\n",
      "Epoch 22, Batch 269/462, Loss: 0.7545472383499146\n",
      "Epoch 22, Batch 270/462, Loss: 0.7175406813621521\n",
      "Epoch 22, Batch 271/462, Loss: 0.8131790161132812\n",
      "Epoch 22, Batch 272/462, Loss: 0.837338387966156\n",
      "Epoch 22, Batch 273/462, Loss: 0.6976267695426941\n",
      "Epoch 22, Batch 274/462, Loss: 1.0167171955108643\n",
      "Epoch 22, Batch 275/462, Loss: 0.6898082494735718\n",
      "Epoch 22, Batch 276/462, Loss: 0.6620897650718689\n",
      "Epoch 22, Batch 277/462, Loss: 0.7492902874946594\n",
      "Epoch 22, Batch 278/462, Loss: 0.9004824161529541\n",
      "Epoch 22, Batch 279/462, Loss: 0.7917275428771973\n",
      "Epoch 22, Batch 280/462, Loss: 0.6527149677276611\n",
      "Epoch 22, Batch 281/462, Loss: 0.6759170293807983\n",
      "Epoch 22, Batch 282/462, Loss: 0.7474901676177979\n",
      "Epoch 22, Batch 283/462, Loss: 0.794751763343811\n",
      "Epoch 22, Batch 284/462, Loss: 0.7301691770553589\n",
      "Epoch 22, Batch 285/462, Loss: 0.7885204553604126\n",
      "Epoch 22, Batch 286/462, Loss: 0.7479121685028076\n",
      "Epoch 22, Batch 287/462, Loss: 0.8848991990089417\n",
      "Epoch 22, Batch 288/462, Loss: 0.7082503437995911\n",
      "Epoch 22, Batch 289/462, Loss: 0.7602392435073853\n",
      "Epoch 22, Batch 290/462, Loss: 0.6714244484901428\n",
      "Epoch 22, Batch 291/462, Loss: 0.6678518652915955\n",
      "Epoch 22, Batch 292/462, Loss: 0.7475492358207703\n",
      "Epoch 22, Batch 293/462, Loss: 0.9245954155921936\n",
      "Epoch 22, Batch 294/462, Loss: 0.6452429294586182\n",
      "Epoch 22, Batch 295/462, Loss: 0.8805390000343323\n",
      "Epoch 22, Batch 296/462, Loss: 0.7244166731834412\n",
      "Epoch 22, Batch 297/462, Loss: 0.6858139038085938\n",
      "Epoch 22, Batch 298/462, Loss: 0.7051071524620056\n",
      "Epoch 22, Batch 299/462, Loss: 0.5168293714523315\n",
      "Epoch 22, Batch 300/462, Loss: 0.7430887818336487\n",
      "Epoch 22, Batch 301/462, Loss: 0.8163402676582336\n",
      "Epoch 22, Batch 302/462, Loss: 0.5950514674186707\n",
      "Epoch 22, Batch 303/462, Loss: 0.830572247505188\n",
      "Epoch 22, Batch 304/462, Loss: 0.7907246351242065\n",
      "Epoch 22, Batch 305/462, Loss: 0.8950015902519226\n",
      "Epoch 22, Batch 306/462, Loss: 0.5901692509651184\n",
      "Epoch 22, Batch 307/462, Loss: 0.8619640469551086\n",
      "Epoch 22, Batch 308/462, Loss: 0.6103004217147827\n",
      "Epoch 22, Batch 309/462, Loss: 0.8167493343353271\n",
      "Epoch 22, Batch 310/462, Loss: 0.6773028373718262\n",
      "Epoch 22, Batch 311/462, Loss: 0.6286792755126953\n",
      "Epoch 22, Batch 312/462, Loss: 0.8705586791038513\n",
      "Epoch 22, Batch 313/462, Loss: 0.7391806840896606\n",
      "Epoch 22, Batch 314/462, Loss: 0.722365140914917\n",
      "Epoch 22, Batch 315/462, Loss: 0.8781506419181824\n",
      "Epoch 22, Batch 316/462, Loss: 0.5921399593353271\n",
      "Epoch 22, Batch 317/462, Loss: 0.7070889472961426\n",
      "Epoch 22, Batch 318/462, Loss: 0.5923139452934265\n",
      "Epoch 22, Batch 319/462, Loss: 0.7093068361282349\n",
      "Epoch 22, Batch 320/462, Loss: 0.8390367031097412\n",
      "Epoch 22, Batch 321/462, Loss: 0.7040405869483948\n",
      "Epoch 22, Batch 322/462, Loss: 0.8537238240242004\n",
      "Epoch 22, Batch 323/462, Loss: 0.6999610066413879\n",
      "Epoch 22, Batch 324/462, Loss: 0.7535684704780579\n",
      "Epoch 22, Batch 325/462, Loss: 0.773202121257782\n",
      "Epoch 22, Batch 326/462, Loss: 0.5906798839569092\n",
      "Epoch 22, Batch 327/462, Loss: 0.8015559911727905\n",
      "Epoch 22, Batch 328/462, Loss: 0.7536519765853882\n",
      "Epoch 22, Batch 329/462, Loss: 0.6341204643249512\n",
      "Epoch 22, Batch 330/462, Loss: 0.77632737159729\n",
      "Epoch 22, Batch 331/462, Loss: 0.6830698251724243\n",
      "Epoch 22, Batch 332/462, Loss: 0.6485928297042847\n",
      "Epoch 22, Batch 333/462, Loss: 0.64991295337677\n",
      "Epoch 22, Batch 334/462, Loss: 0.8857079148292542\n",
      "Epoch 22, Batch 335/462, Loss: 0.638074517250061\n",
      "Epoch 22, Batch 336/462, Loss: 0.8772773742675781\n",
      "Epoch 22, Batch 337/462, Loss: 0.7048898339271545\n",
      "Epoch 22, Batch 338/462, Loss: 0.7814282178878784\n",
      "Epoch 22, Batch 339/462, Loss: 0.6311619877815247\n",
      "Epoch 22, Batch 340/462, Loss: 0.7835226058959961\n",
      "Epoch 22, Batch 341/462, Loss: 0.6346012949943542\n",
      "Epoch 22, Batch 342/462, Loss: 1.0190272331237793\n",
      "Epoch 22, Batch 343/462, Loss: 0.765924870967865\n",
      "Epoch 22, Batch 344/462, Loss: 0.6925042867660522\n",
      "Epoch 22, Batch 345/462, Loss: 0.6809474229812622\n",
      "Epoch 22, Batch 346/462, Loss: 0.8825109004974365\n",
      "Epoch 22, Batch 347/462, Loss: 0.6965347528457642\n",
      "Epoch 22, Batch 348/462, Loss: 0.8070504069328308\n",
      "Epoch 22, Batch 349/462, Loss: 0.8750428557395935\n",
      "Epoch 22, Batch 350/462, Loss: 0.7154843807220459\n",
      "Epoch 22, Batch 351/462, Loss: 0.5943164825439453\n",
      "Epoch 22, Batch 352/462, Loss: 0.7524998784065247\n",
      "Epoch 22, Batch 353/462, Loss: 0.7941091060638428\n",
      "Epoch 22, Batch 354/462, Loss: 0.6905689239501953\n",
      "Epoch 22, Batch 355/462, Loss: 0.8160257935523987\n",
      "Epoch 22, Batch 356/462, Loss: 0.7378125190734863\n",
      "Epoch 22, Batch 357/462, Loss: 0.6416561007499695\n",
      "Epoch 22, Batch 358/462, Loss: 0.6237410306930542\n",
      "Epoch 22, Batch 359/462, Loss: 0.7805720567703247\n",
      "Epoch 22, Batch 360/462, Loss: 0.7294359803199768\n",
      "Epoch 22, Batch 361/462, Loss: 0.7257453203201294\n",
      "Epoch 22, Batch 362/462, Loss: 0.7572173476219177\n",
      "Epoch 22, Batch 363/462, Loss: 0.6620081663131714\n",
      "Epoch 22, Batch 364/462, Loss: 0.8683272004127502\n",
      "Epoch 22, Batch 365/462, Loss: 0.8923547863960266\n",
      "Epoch 22, Batch 366/462, Loss: 0.7176463603973389\n",
      "Epoch 22, Batch 367/462, Loss: 0.7647216320037842\n",
      "Epoch 22, Batch 368/462, Loss: 0.7485716938972473\n",
      "Epoch 22, Batch 369/462, Loss: 0.7373120784759521\n",
      "Epoch 22, Batch 370/462, Loss: 0.7428173422813416\n",
      "Epoch 22, Batch 371/462, Loss: 0.7469421625137329\n",
      "Epoch 22, Batch 372/462, Loss: 0.5039803981781006\n",
      "Epoch 22, Batch 373/462, Loss: 0.7897955179214478\n",
      "Epoch 22, Batch 374/462, Loss: 0.727144718170166\n",
      "Epoch 22, Batch 375/462, Loss: 0.7319434881210327\n",
      "Epoch 22, Batch 376/462, Loss: 0.6987144947052002\n",
      "Epoch 22, Batch 377/462, Loss: 1.0558823347091675\n",
      "Epoch 22, Batch 378/462, Loss: 0.8586602210998535\n",
      "Epoch 22, Batch 379/462, Loss: 0.7325890064239502\n",
      "Epoch 22, Batch 380/462, Loss: 0.8001461625099182\n",
      "Epoch 22, Batch 381/462, Loss: 0.6886404156684875\n",
      "Epoch 22, Batch 382/462, Loss: 0.7472416758537292\n",
      "Epoch 22, Batch 383/462, Loss: 0.7715895771980286\n",
      "Epoch 22, Batch 384/462, Loss: 0.5596786737442017\n",
      "Epoch 22, Batch 385/462, Loss: 0.8322499394416809\n",
      "Epoch 22, Batch 386/462, Loss: 0.7076269388198853\n",
      "Epoch 22, Batch 387/462, Loss: 0.7279455065727234\n",
      "Epoch 22, Batch 388/462, Loss: 0.6545155644416809\n",
      "Epoch 22, Batch 389/462, Loss: 0.819884181022644\n",
      "Epoch 22, Batch 390/462, Loss: 0.8614581227302551\n",
      "Epoch 22, Batch 391/462, Loss: 0.8112087249755859\n",
      "Epoch 22, Batch 392/462, Loss: 0.6691475510597229\n",
      "Epoch 22, Batch 393/462, Loss: 0.8800270557403564\n",
      "Epoch 22, Batch 394/462, Loss: 0.6715527772903442\n",
      "Epoch 22, Batch 395/462, Loss: 0.7556482553482056\n",
      "Epoch 22, Batch 396/462, Loss: 0.8460091352462769\n",
      "Epoch 22, Batch 397/462, Loss: 0.7031404376029968\n",
      "Epoch 22, Batch 398/462, Loss: 0.7120473980903625\n",
      "Epoch 22, Batch 399/462, Loss: 0.7847427129745483\n",
      "Epoch 22, Batch 400/462, Loss: 0.7647597789764404\n",
      "Epoch 22, Batch 401/462, Loss: 0.8836618065834045\n",
      "Epoch 22, Batch 402/462, Loss: 0.6605905890464783\n",
      "Epoch 22, Batch 403/462, Loss: 0.7723560333251953\n",
      "Epoch 22, Batch 404/462, Loss: 0.8005073070526123\n",
      "Epoch 22, Batch 405/462, Loss: 0.686819851398468\n",
      "Epoch 22, Batch 406/462, Loss: 0.6724437475204468\n",
      "Epoch 22, Batch 407/462, Loss: 0.8276655077934265\n",
      "Epoch 22, Batch 408/462, Loss: 0.7816932201385498\n",
      "Epoch 22, Batch 409/462, Loss: 0.617438793182373\n",
      "Epoch 22, Batch 410/462, Loss: 0.8305509686470032\n",
      "Epoch 22, Batch 411/462, Loss: 0.8061137795448303\n",
      "Epoch 22, Batch 412/462, Loss: 0.7679598927497864\n",
      "Epoch 22, Batch 413/462, Loss: 0.8368771076202393\n",
      "Epoch 22, Batch 414/462, Loss: 0.5234889388084412\n",
      "Epoch 22, Batch 415/462, Loss: 0.6567934155464172\n",
      "Epoch 22, Batch 416/462, Loss: 0.7930456399917603\n",
      "Epoch 22, Batch 417/462, Loss: 0.8715208768844604\n",
      "Epoch 22, Batch 418/462, Loss: 0.7931530475616455\n",
      "Epoch 22, Batch 419/462, Loss: 0.7308791279792786\n",
      "Epoch 22, Batch 420/462, Loss: 0.8631311655044556\n",
      "Epoch 22, Batch 421/462, Loss: 0.8268661499023438\n",
      "Epoch 22, Batch 422/462, Loss: 0.7452244758605957\n",
      "Epoch 22, Batch 423/462, Loss: 0.5711604952812195\n",
      "Epoch 22, Batch 424/462, Loss: 0.70476233959198\n",
      "Epoch 22, Batch 425/462, Loss: 0.7712042331695557\n",
      "Epoch 22, Batch 426/462, Loss: 0.7909194231033325\n",
      "Epoch 22, Batch 427/462, Loss: 0.7856770753860474\n",
      "Epoch 22, Batch 428/462, Loss: 0.6425574421882629\n",
      "Epoch 22, Batch 429/462, Loss: 0.7257869243621826\n",
      "Epoch 22, Batch 430/462, Loss: 0.7956717014312744\n",
      "Epoch 22, Batch 431/462, Loss: 0.6402382850646973\n",
      "Epoch 22, Batch 432/462, Loss: 0.7507951259613037\n",
      "Epoch 22, Batch 433/462, Loss: 0.9320462942123413\n",
      "Epoch 22, Batch 434/462, Loss: 0.7399715781211853\n",
      "Epoch 22, Batch 435/462, Loss: 0.5621286630630493\n",
      "Epoch 22, Batch 436/462, Loss: 0.763972282409668\n",
      "Epoch 22, Batch 437/462, Loss: 0.7220306396484375\n",
      "Epoch 22, Batch 438/462, Loss: 0.7080745697021484\n",
      "Epoch 22, Batch 439/462, Loss: 0.8316551446914673\n",
      "Epoch 22, Batch 440/462, Loss: 0.7678446173667908\n",
      "Epoch 22, Batch 441/462, Loss: 0.6643425226211548\n",
      "Epoch 22, Batch 442/462, Loss: 0.9008566737174988\n",
      "Epoch 22, Batch 443/462, Loss: 0.7421322464942932\n",
      "Epoch 22, Batch 444/462, Loss: 0.8680832386016846\n",
      "Epoch 22, Batch 445/462, Loss: 0.7712033987045288\n",
      "Epoch 22, Batch 446/462, Loss: 0.7391576170921326\n",
      "Epoch 22, Batch 447/462, Loss: 0.9036574959754944\n",
      "Epoch 22, Batch 448/462, Loss: 0.7999816536903381\n",
      "Epoch 22, Batch 449/462, Loss: 0.6646770238876343\n",
      "Epoch 22, Batch 450/462, Loss: 0.7328149080276489\n",
      "Epoch 22, Batch 451/462, Loss: 0.7680589556694031\n",
      "Epoch 22, Batch 452/462, Loss: 0.7833619117736816\n",
      "Epoch 22, Batch 453/462, Loss: 0.6525027751922607\n",
      "Epoch 22, Batch 454/462, Loss: 0.6715970635414124\n",
      "Epoch 22, Batch 455/462, Loss: 0.7985323667526245\n",
      "Epoch 22, Batch 456/462, Loss: 0.7589571475982666\n",
      "Epoch 22, Batch 457/462, Loss: 0.7017066478729248\n",
      "Epoch 22, Batch 458/462, Loss: 0.7239918112754822\n",
      "Epoch 22, Batch 459/462, Loss: 0.6782469749450684\n",
      "Epoch 22, Batch 460/462, Loss: 0.6133914589881897\n",
      "Epoch 22, Batch 461/462, Loss: 0.6819812655448914\n",
      "Epoch 22, Batch 462/462, Loss: 0.7620291709899902\n",
      "Epoch 22, Loss: 345.4649501442909\n",
      "Epoch 23, Batch 1/462, Loss: 0.9145513772964478\n",
      "Epoch 23, Batch 2/462, Loss: 0.5607485771179199\n",
      "Epoch 23, Batch 3/462, Loss: 0.7726883888244629\n",
      "Epoch 23, Batch 4/462, Loss: 0.646324098110199\n",
      "Epoch 23, Batch 5/462, Loss: 0.633094847202301\n",
      "Epoch 23, Batch 6/462, Loss: 0.712198793888092\n",
      "Epoch 23, Batch 7/462, Loss: 0.7938183546066284\n",
      "Epoch 23, Batch 8/462, Loss: 0.7484144568443298\n",
      "Epoch 23, Batch 9/462, Loss: 0.6985774040222168\n",
      "Epoch 23, Batch 10/462, Loss: 0.593379020690918\n",
      "Epoch 23, Batch 11/462, Loss: 0.72301185131073\n",
      "Epoch 23, Batch 12/462, Loss: 0.7005572319030762\n",
      "Epoch 23, Batch 13/462, Loss: 0.7528039216995239\n",
      "Epoch 23, Batch 14/462, Loss: 0.78719162940979\n",
      "Epoch 23, Batch 15/462, Loss: 0.7966095805168152\n",
      "Epoch 23, Batch 16/462, Loss: 0.8028117418289185\n",
      "Epoch 23, Batch 17/462, Loss: 0.8328539729118347\n",
      "Epoch 23, Batch 18/462, Loss: 0.6958040595054626\n",
      "Epoch 23, Batch 19/462, Loss: 0.7808462977409363\n",
      "Epoch 23, Batch 20/462, Loss: 0.6726837754249573\n",
      "Epoch 23, Batch 21/462, Loss: 0.632698655128479\n",
      "Epoch 23, Batch 22/462, Loss: 0.8924291133880615\n",
      "Epoch 23, Batch 23/462, Loss: 0.6557368040084839\n",
      "Epoch 23, Batch 24/462, Loss: 0.6133415102958679\n",
      "Epoch 23, Batch 25/462, Loss: 0.7011346817016602\n",
      "Epoch 23, Batch 26/462, Loss: 0.8832334280014038\n",
      "Epoch 23, Batch 27/462, Loss: 0.7342492938041687\n",
      "Epoch 23, Batch 28/462, Loss: 0.7620331048965454\n",
      "Epoch 23, Batch 29/462, Loss: 0.6830064654350281\n",
      "Epoch 23, Batch 30/462, Loss: 0.6629548072814941\n",
      "Epoch 23, Batch 31/462, Loss: 0.7644174695014954\n",
      "Epoch 23, Batch 32/462, Loss: 0.655219316482544\n",
      "Epoch 23, Batch 33/462, Loss: 0.9254275560379028\n",
      "Epoch 23, Batch 34/462, Loss: 0.8133148550987244\n",
      "Epoch 23, Batch 35/462, Loss: 0.9051460027694702\n",
      "Epoch 23, Batch 36/462, Loss: 0.785914421081543\n",
      "Epoch 23, Batch 37/462, Loss: 0.642430305480957\n",
      "Epoch 23, Batch 38/462, Loss: 0.6707108020782471\n",
      "Epoch 23, Batch 39/462, Loss: 0.8229604959487915\n",
      "Epoch 23, Batch 40/462, Loss: 0.6985012888908386\n",
      "Epoch 23, Batch 41/462, Loss: 1.0288041830062866\n",
      "Epoch 23, Batch 42/462, Loss: 0.5728296041488647\n",
      "Epoch 23, Batch 43/462, Loss: 0.6117105484008789\n",
      "Epoch 23, Batch 44/462, Loss: 0.8024573922157288\n",
      "Epoch 23, Batch 45/462, Loss: 0.8068404793739319\n",
      "Epoch 23, Batch 46/462, Loss: 0.7844650745391846\n",
      "Epoch 23, Batch 47/462, Loss: 0.592117190361023\n",
      "Epoch 23, Batch 48/462, Loss: 0.6454545855522156\n",
      "Epoch 23, Batch 49/462, Loss: 0.7731047868728638\n",
      "Epoch 23, Batch 50/462, Loss: 0.7603683471679688\n",
      "Epoch 23, Batch 51/462, Loss: 0.7405703663825989\n",
      "Epoch 23, Batch 52/462, Loss: 0.7784009575843811\n",
      "Epoch 23, Batch 53/462, Loss: 0.7478107213973999\n",
      "Epoch 23, Batch 54/462, Loss: 0.7654587626457214\n",
      "Epoch 23, Batch 55/462, Loss: 0.7033690214157104\n",
      "Epoch 23, Batch 56/462, Loss: 0.6280156970024109\n",
      "Epoch 23, Batch 57/462, Loss: 0.6868773102760315\n",
      "Epoch 23, Batch 58/462, Loss: 0.7295145988464355\n",
      "Epoch 23, Batch 59/462, Loss: 0.7911649346351624\n",
      "Epoch 23, Batch 60/462, Loss: 0.7479152083396912\n",
      "Epoch 23, Batch 61/462, Loss: 0.6542943716049194\n",
      "Epoch 23, Batch 62/462, Loss: 0.8464747667312622\n",
      "Epoch 23, Batch 63/462, Loss: 0.8270172476768494\n",
      "Epoch 23, Batch 64/462, Loss: 0.6649124622344971\n",
      "Epoch 23, Batch 65/462, Loss: 0.775554895401001\n",
      "Epoch 23, Batch 66/462, Loss: 0.635890781879425\n",
      "Epoch 23, Batch 67/462, Loss: 0.8609843254089355\n",
      "Epoch 23, Batch 68/462, Loss: 0.6479758620262146\n",
      "Epoch 23, Batch 69/462, Loss: 0.621601402759552\n",
      "Epoch 23, Batch 70/462, Loss: 0.5847477316856384\n",
      "Epoch 23, Batch 71/462, Loss: 0.7128400206565857\n",
      "Epoch 23, Batch 72/462, Loss: 0.8275543451309204\n",
      "Epoch 23, Batch 73/462, Loss: 0.6730228662490845\n",
      "Epoch 23, Batch 74/462, Loss: 0.7235775589942932\n",
      "Epoch 23, Batch 75/462, Loss: 0.8334059119224548\n",
      "Epoch 23, Batch 76/462, Loss: 0.6333056688308716\n",
      "Epoch 23, Batch 77/462, Loss: 0.8187662363052368\n",
      "Epoch 23, Batch 78/462, Loss: 0.6498780846595764\n",
      "Epoch 23, Batch 79/462, Loss: 0.7333833575248718\n",
      "Epoch 23, Batch 80/462, Loss: 0.6496609449386597\n",
      "Epoch 23, Batch 81/462, Loss: 0.6683944463729858\n",
      "Epoch 23, Batch 82/462, Loss: 0.57639080286026\n",
      "Epoch 23, Batch 83/462, Loss: 0.7838034629821777\n",
      "Epoch 23, Batch 84/462, Loss: 0.6783261299133301\n",
      "Epoch 23, Batch 85/462, Loss: 0.8763898015022278\n",
      "Epoch 23, Batch 86/462, Loss: 0.7596664428710938\n",
      "Epoch 23, Batch 87/462, Loss: 0.8784015774726868\n",
      "Epoch 23, Batch 88/462, Loss: 0.8268125057220459\n",
      "Epoch 23, Batch 89/462, Loss: 0.6295707821846008\n",
      "Epoch 23, Batch 90/462, Loss: 0.8580422401428223\n",
      "Epoch 23, Batch 91/462, Loss: 0.7586430907249451\n",
      "Epoch 23, Batch 92/462, Loss: 0.8716563582420349\n",
      "Epoch 23, Batch 93/462, Loss: 0.7348014116287231\n",
      "Epoch 23, Batch 94/462, Loss: 0.6995712518692017\n",
      "Epoch 23, Batch 95/462, Loss: 0.7022666335105896\n",
      "Epoch 23, Batch 96/462, Loss: 0.7110639214515686\n",
      "Epoch 23, Batch 97/462, Loss: 0.8194801807403564\n",
      "Epoch 23, Batch 98/462, Loss: 0.8352578282356262\n",
      "Epoch 23, Batch 99/462, Loss: 0.6819503903388977\n",
      "Epoch 23, Batch 100/462, Loss: 0.7896341681480408\n",
      "Epoch 23, Batch 101/462, Loss: 0.6911069750785828\n",
      "Epoch 23, Batch 102/462, Loss: 0.6176474690437317\n",
      "Epoch 23, Batch 103/462, Loss: 0.6595459580421448\n",
      "Epoch 23, Batch 104/462, Loss: 0.600418746471405\n",
      "Epoch 23, Batch 105/462, Loss: 0.7592763304710388\n",
      "Epoch 23, Batch 106/462, Loss: 0.6627984046936035\n",
      "Epoch 23, Batch 107/462, Loss: 0.7546640038490295\n",
      "Epoch 23, Batch 108/462, Loss: 0.7243397831916809\n",
      "Epoch 23, Batch 109/462, Loss: 0.5891236066818237\n",
      "Epoch 23, Batch 110/462, Loss: 0.8175390362739563\n",
      "Epoch 23, Batch 111/462, Loss: 0.9504883885383606\n",
      "Epoch 23, Batch 112/462, Loss: 0.6233147978782654\n",
      "Epoch 23, Batch 113/462, Loss: 0.49753013253211975\n",
      "Epoch 23, Batch 114/462, Loss: 0.8004888296127319\n",
      "Epoch 23, Batch 115/462, Loss: 0.6473936438560486\n",
      "Epoch 23, Batch 116/462, Loss: 0.6412104368209839\n",
      "Epoch 23, Batch 117/462, Loss: 0.7268010973930359\n",
      "Epoch 23, Batch 118/462, Loss: 0.5999597311019897\n",
      "Epoch 23, Batch 119/462, Loss: 0.8972999453544617\n",
      "Epoch 23, Batch 120/462, Loss: 0.8095247745513916\n",
      "Epoch 23, Batch 121/462, Loss: 0.7052649259567261\n",
      "Epoch 23, Batch 122/462, Loss: 0.851338267326355\n",
      "Epoch 23, Batch 123/462, Loss: 0.6396156549453735\n",
      "Epoch 23, Batch 124/462, Loss: 0.7544204592704773\n",
      "Epoch 23, Batch 125/462, Loss: 0.7915849089622498\n",
      "Epoch 23, Batch 126/462, Loss: 0.7214939594268799\n",
      "Epoch 23, Batch 127/462, Loss: 0.644049882888794\n",
      "Epoch 23, Batch 128/462, Loss: 0.7109416723251343\n",
      "Epoch 23, Batch 129/462, Loss: 0.9353053569793701\n",
      "Epoch 23, Batch 130/462, Loss: 0.7777937650680542\n",
      "Epoch 23, Batch 131/462, Loss: 0.6754024028778076\n",
      "Epoch 23, Batch 132/462, Loss: 0.8339011073112488\n",
      "Epoch 23, Batch 133/462, Loss: 0.7387036085128784\n",
      "Epoch 23, Batch 134/462, Loss: 0.7346087098121643\n",
      "Epoch 23, Batch 135/462, Loss: 0.8740546107292175\n",
      "Epoch 23, Batch 136/462, Loss: 0.708232581615448\n",
      "Epoch 23, Batch 137/462, Loss: 0.6184743046760559\n",
      "Epoch 23, Batch 138/462, Loss: 0.7347394227981567\n",
      "Epoch 23, Batch 139/462, Loss: 0.6976434588432312\n",
      "Epoch 23, Batch 140/462, Loss: 0.6270646452903748\n",
      "Epoch 23, Batch 141/462, Loss: 0.7986664175987244\n",
      "Epoch 23, Batch 142/462, Loss: 0.6938190460205078\n",
      "Epoch 23, Batch 143/462, Loss: 0.8167592883110046\n",
      "Epoch 23, Batch 144/462, Loss: 0.7228867411613464\n",
      "Epoch 23, Batch 145/462, Loss: 0.7407638430595398\n",
      "Epoch 23, Batch 146/462, Loss: 0.6806319355964661\n",
      "Epoch 23, Batch 147/462, Loss: 0.654253363609314\n",
      "Epoch 23, Batch 148/462, Loss: 0.6743861436843872\n",
      "Epoch 23, Batch 149/462, Loss: 0.7438928484916687\n",
      "Epoch 23, Batch 150/462, Loss: 0.8392959833145142\n",
      "Epoch 23, Batch 151/462, Loss: 0.7616989612579346\n",
      "Epoch 23, Batch 152/462, Loss: 0.8065676689147949\n",
      "Epoch 23, Batch 153/462, Loss: 0.7527637481689453\n",
      "Epoch 23, Batch 154/462, Loss: 0.7801061868667603\n",
      "Epoch 23, Batch 155/462, Loss: 0.7236977219581604\n",
      "Epoch 23, Batch 156/462, Loss: 0.701779305934906\n",
      "Epoch 23, Batch 157/462, Loss: 0.9082249999046326\n",
      "Epoch 23, Batch 158/462, Loss: 0.7946257591247559\n",
      "Epoch 23, Batch 159/462, Loss: 0.6784115433692932\n",
      "Epoch 23, Batch 160/462, Loss: 0.6741854548454285\n",
      "Epoch 23, Batch 161/462, Loss: 0.8568617105484009\n",
      "Epoch 23, Batch 162/462, Loss: 0.8834754228591919\n",
      "Epoch 23, Batch 163/462, Loss: 0.751897931098938\n",
      "Epoch 23, Batch 164/462, Loss: 0.5815836787223816\n",
      "Epoch 23, Batch 165/462, Loss: 0.7763006091117859\n",
      "Epoch 23, Batch 166/462, Loss: 0.9063948392868042\n",
      "Epoch 23, Batch 167/462, Loss: 0.7305079102516174\n",
      "Epoch 23, Batch 168/462, Loss: 0.6861869692802429\n",
      "Epoch 23, Batch 169/462, Loss: 0.7786096334457397\n",
      "Epoch 23, Batch 170/462, Loss: 0.7852609753608704\n",
      "Epoch 23, Batch 171/462, Loss: 0.8672389984130859\n",
      "Epoch 23, Batch 172/462, Loss: 0.9177263975143433\n",
      "Epoch 23, Batch 173/462, Loss: 0.7534711956977844\n",
      "Epoch 23, Batch 174/462, Loss: 0.6203928589820862\n",
      "Epoch 23, Batch 175/462, Loss: 0.6572628617286682\n",
      "Epoch 23, Batch 176/462, Loss: 0.652746319770813\n",
      "Epoch 23, Batch 177/462, Loss: 0.7111966013908386\n",
      "Epoch 23, Batch 178/462, Loss: 0.6696202754974365\n",
      "Epoch 23, Batch 179/462, Loss: 0.6576880216598511\n",
      "Epoch 23, Batch 180/462, Loss: 0.7116407155990601\n",
      "Epoch 23, Batch 181/462, Loss: 0.7272621393203735\n",
      "Epoch 23, Batch 182/462, Loss: 0.6222012042999268\n",
      "Epoch 23, Batch 183/462, Loss: 0.6994647979736328\n",
      "Epoch 23, Batch 184/462, Loss: 0.7484893202781677\n",
      "Epoch 23, Batch 185/462, Loss: 0.6965329051017761\n",
      "Epoch 23, Batch 186/462, Loss: 0.6414139270782471\n",
      "Epoch 23, Batch 187/462, Loss: 0.8543166518211365\n",
      "Epoch 23, Batch 188/462, Loss: 0.7509658336639404\n",
      "Epoch 23, Batch 189/462, Loss: 0.9154715538024902\n",
      "Epoch 23, Batch 190/462, Loss: 0.7823933362960815\n",
      "Epoch 23, Batch 191/462, Loss: 0.8736985325813293\n",
      "Epoch 23, Batch 192/462, Loss: 0.6785815358161926\n",
      "Epoch 23, Batch 193/462, Loss: 0.7672802805900574\n",
      "Epoch 23, Batch 194/462, Loss: 0.7214645743370056\n",
      "Epoch 23, Batch 195/462, Loss: 0.8792537450790405\n",
      "Epoch 23, Batch 196/462, Loss: 0.9548168778419495\n",
      "Epoch 23, Batch 197/462, Loss: 0.6357408165931702\n",
      "Epoch 23, Batch 198/462, Loss: 0.6244823336601257\n",
      "Epoch 23, Batch 199/462, Loss: 0.8300889730453491\n",
      "Epoch 23, Batch 200/462, Loss: 0.7747690677642822\n",
      "Epoch 23, Batch 201/462, Loss: 0.9447231888771057\n",
      "Epoch 23, Batch 202/462, Loss: 0.7109265327453613\n",
      "Epoch 23, Batch 203/462, Loss: 0.6645767688751221\n",
      "Epoch 23, Batch 204/462, Loss: 0.7281090617179871\n",
      "Epoch 23, Batch 205/462, Loss: 0.8762348294258118\n",
      "Epoch 23, Batch 206/462, Loss: 0.6638321876525879\n",
      "Epoch 23, Batch 207/462, Loss: 0.6264787912368774\n",
      "Epoch 23, Batch 208/462, Loss: 0.6774010062217712\n",
      "Epoch 23, Batch 209/462, Loss: 0.7997782230377197\n",
      "Epoch 23, Batch 210/462, Loss: 0.7051814794540405\n",
      "Epoch 23, Batch 211/462, Loss: 0.8573952317237854\n",
      "Epoch 23, Batch 212/462, Loss: 0.7038992047309875\n",
      "Epoch 23, Batch 213/462, Loss: 0.7528727650642395\n",
      "Epoch 23, Batch 214/462, Loss: 0.6590383052825928\n",
      "Epoch 23, Batch 215/462, Loss: 0.7408285140991211\n",
      "Epoch 23, Batch 216/462, Loss: 0.7146690487861633\n",
      "Epoch 23, Batch 217/462, Loss: 0.6791892051696777\n",
      "Epoch 23, Batch 218/462, Loss: 0.5878617167472839\n",
      "Epoch 23, Batch 219/462, Loss: 0.7562447190284729\n",
      "Epoch 23, Batch 220/462, Loss: 0.6244720816612244\n",
      "Epoch 23, Batch 221/462, Loss: 0.8717183470726013\n",
      "Epoch 23, Batch 222/462, Loss: 0.7907835245132446\n",
      "Epoch 23, Batch 223/462, Loss: 0.8730469346046448\n",
      "Epoch 23, Batch 224/462, Loss: 0.6834579110145569\n",
      "Epoch 23, Batch 225/462, Loss: 0.7314757704734802\n",
      "Epoch 23, Batch 226/462, Loss: 0.6894569396972656\n",
      "Epoch 23, Batch 227/462, Loss: 0.6309019923210144\n",
      "Epoch 23, Batch 228/462, Loss: 0.8876171708106995\n",
      "Epoch 23, Batch 229/462, Loss: 0.83669114112854\n",
      "Epoch 23, Batch 230/462, Loss: 0.7565315961837769\n",
      "Epoch 23, Batch 231/462, Loss: 0.8302627801895142\n",
      "Epoch 23, Batch 232/462, Loss: 0.5781034231185913\n",
      "Epoch 23, Batch 233/462, Loss: 0.9278647303581238\n",
      "Epoch 23, Batch 234/462, Loss: 0.6711065173149109\n",
      "Epoch 23, Batch 235/462, Loss: 0.7093841433525085\n",
      "Epoch 23, Batch 236/462, Loss: 0.6764460802078247\n",
      "Epoch 23, Batch 237/462, Loss: 0.7110470533370972\n",
      "Epoch 23, Batch 238/462, Loss: 0.6504389643669128\n",
      "Epoch 23, Batch 239/462, Loss: 0.627365231513977\n",
      "Epoch 23, Batch 240/462, Loss: 0.6512186527252197\n",
      "Epoch 23, Batch 241/462, Loss: 0.7522629499435425\n",
      "Epoch 23, Batch 242/462, Loss: 0.7888591289520264\n",
      "Epoch 23, Batch 243/462, Loss: 0.7647390961647034\n",
      "Epoch 23, Batch 244/462, Loss: 0.7660403847694397\n",
      "Epoch 23, Batch 245/462, Loss: 0.6852628588676453\n",
      "Epoch 23, Batch 246/462, Loss: 0.7053636908531189\n",
      "Epoch 23, Batch 247/462, Loss: 0.970323920249939\n",
      "Epoch 23, Batch 248/462, Loss: 0.8082411885261536\n",
      "Epoch 23, Batch 249/462, Loss: 0.6623210906982422\n",
      "Epoch 23, Batch 250/462, Loss: 0.765413224697113\n",
      "Epoch 23, Batch 251/462, Loss: 0.6374518275260925\n",
      "Epoch 23, Batch 252/462, Loss: 0.5097335577011108\n",
      "Epoch 23, Batch 253/462, Loss: 0.9794057607650757\n",
      "Epoch 23, Batch 254/462, Loss: 0.6626702547073364\n",
      "Epoch 23, Batch 255/462, Loss: 0.8704297542572021\n",
      "Epoch 23, Batch 256/462, Loss: 0.8471463918685913\n",
      "Epoch 23, Batch 257/462, Loss: 0.8109462261199951\n",
      "Epoch 23, Batch 258/462, Loss: 0.6137309670448303\n",
      "Epoch 23, Batch 259/462, Loss: 0.5834587216377258\n",
      "Epoch 23, Batch 260/462, Loss: 0.487283855676651\n",
      "Epoch 23, Batch 261/462, Loss: 0.762258768081665\n",
      "Epoch 23, Batch 262/462, Loss: 0.6917354464530945\n",
      "Epoch 23, Batch 263/462, Loss: 0.7357372045516968\n",
      "Epoch 23, Batch 264/462, Loss: 0.8057037591934204\n",
      "Epoch 23, Batch 265/462, Loss: 0.7852452993392944\n",
      "Epoch 23, Batch 266/462, Loss: 0.7533177733421326\n",
      "Epoch 23, Batch 267/462, Loss: 0.6145056486129761\n",
      "Epoch 23, Batch 268/462, Loss: 0.6791744232177734\n",
      "Epoch 23, Batch 269/462, Loss: 0.8396031260490417\n",
      "Epoch 23, Batch 270/462, Loss: 0.7407942414283752\n",
      "Epoch 23, Batch 271/462, Loss: 0.6786600947380066\n",
      "Epoch 23, Batch 272/462, Loss: 0.5966101288795471\n",
      "Epoch 23, Batch 273/462, Loss: 0.707758367061615\n",
      "Epoch 23, Batch 274/462, Loss: 0.6536846160888672\n",
      "Epoch 23, Batch 275/462, Loss: 0.7601373791694641\n",
      "Epoch 23, Batch 276/462, Loss: 0.6781343221664429\n",
      "Epoch 23, Batch 277/462, Loss: 0.7215070724487305\n",
      "Epoch 23, Batch 278/462, Loss: 0.9868454337120056\n",
      "Epoch 23, Batch 279/462, Loss: 0.7505452632904053\n",
      "Epoch 23, Batch 280/462, Loss: 0.7712013125419617\n",
      "Epoch 23, Batch 281/462, Loss: 0.811648964881897\n",
      "Epoch 23, Batch 282/462, Loss: 0.6753712892532349\n",
      "Epoch 23, Batch 283/462, Loss: 0.7475886940956116\n",
      "Epoch 23, Batch 284/462, Loss: 1.0209442377090454\n",
      "Epoch 23, Batch 285/462, Loss: 0.8942952156066895\n",
      "Epoch 23, Batch 286/462, Loss: 0.7409523129463196\n",
      "Epoch 23, Batch 287/462, Loss: 0.6622820496559143\n",
      "Epoch 23, Batch 288/462, Loss: 0.5468296408653259\n",
      "Epoch 23, Batch 289/462, Loss: 0.7460014224052429\n",
      "Epoch 23, Batch 290/462, Loss: 0.5839524865150452\n",
      "Epoch 23, Batch 291/462, Loss: 0.7736455798149109\n",
      "Epoch 23, Batch 292/462, Loss: 0.8454679250717163\n",
      "Epoch 23, Batch 293/462, Loss: 0.8871226906776428\n",
      "Epoch 23, Batch 294/462, Loss: 1.0454872846603394\n",
      "Epoch 23, Batch 295/462, Loss: 0.9245340824127197\n",
      "Epoch 23, Batch 296/462, Loss: 0.7199773192405701\n",
      "Epoch 23, Batch 297/462, Loss: 0.5842869281768799\n",
      "Epoch 23, Batch 298/462, Loss: 0.7342853546142578\n",
      "Epoch 23, Batch 299/462, Loss: 0.697382926940918\n",
      "Epoch 23, Batch 300/462, Loss: 0.6999567747116089\n",
      "Epoch 23, Batch 301/462, Loss: 0.783295214176178\n",
      "Epoch 23, Batch 302/462, Loss: 0.7046480774879456\n",
      "Epoch 23, Batch 303/462, Loss: 0.746465265750885\n",
      "Epoch 23, Batch 304/462, Loss: 0.7931578159332275\n",
      "Epoch 23, Batch 305/462, Loss: 0.7283576130867004\n",
      "Epoch 23, Batch 306/462, Loss: 0.7285100221633911\n",
      "Epoch 23, Batch 307/462, Loss: 0.7604067325592041\n",
      "Epoch 23, Batch 308/462, Loss: 0.7315866947174072\n",
      "Epoch 23, Batch 309/462, Loss: 0.7044438719749451\n",
      "Epoch 23, Batch 310/462, Loss: 0.7735816240310669\n",
      "Epoch 23, Batch 311/462, Loss: 0.7101107239723206\n",
      "Epoch 23, Batch 312/462, Loss: 0.8512070775032043\n",
      "Epoch 23, Batch 313/462, Loss: 0.7416114211082458\n",
      "Epoch 23, Batch 314/462, Loss: 0.7095190286636353\n",
      "Epoch 23, Batch 315/462, Loss: 0.7090444564819336\n",
      "Epoch 23, Batch 316/462, Loss: 0.7831082940101624\n",
      "Epoch 23, Batch 317/462, Loss: 0.6769507527351379\n",
      "Epoch 23, Batch 318/462, Loss: 0.6455345153808594\n",
      "Epoch 23, Batch 319/462, Loss: 0.8313652276992798\n",
      "Epoch 23, Batch 320/462, Loss: 0.6766043305397034\n",
      "Epoch 23, Batch 321/462, Loss: 0.6601793766021729\n",
      "Epoch 23, Batch 322/462, Loss: 0.7742577195167542\n",
      "Epoch 23, Batch 323/462, Loss: 0.873499870300293\n",
      "Epoch 23, Batch 324/462, Loss: 0.8814926147460938\n",
      "Epoch 23, Batch 325/462, Loss: 0.7183170914649963\n",
      "Epoch 23, Batch 326/462, Loss: 0.7486023902893066\n",
      "Epoch 23, Batch 327/462, Loss: 0.5341724753379822\n",
      "Epoch 23, Batch 328/462, Loss: 0.8092236518859863\n",
      "Epoch 23, Batch 329/462, Loss: 0.5482339859008789\n",
      "Epoch 23, Batch 330/462, Loss: 0.7227227687835693\n",
      "Epoch 23, Batch 331/462, Loss: 0.8676895499229431\n",
      "Epoch 23, Batch 332/462, Loss: 0.8374794721603394\n",
      "Epoch 23, Batch 333/462, Loss: 0.7283096313476562\n",
      "Epoch 23, Batch 334/462, Loss: 0.7592256665229797\n",
      "Epoch 23, Batch 335/462, Loss: 0.7828060388565063\n",
      "Epoch 23, Batch 336/462, Loss: 0.7256220579147339\n",
      "Epoch 23, Batch 337/462, Loss: 0.6616080403327942\n",
      "Epoch 23, Batch 338/462, Loss: 0.8092094659805298\n",
      "Epoch 23, Batch 339/462, Loss: 0.7312560081481934\n",
      "Epoch 23, Batch 340/462, Loss: 0.8957735300064087\n",
      "Epoch 23, Batch 341/462, Loss: 0.7183420062065125\n",
      "Epoch 23, Batch 342/462, Loss: 0.6957545280456543\n",
      "Epoch 23, Batch 343/462, Loss: 0.7893160581588745\n",
      "Epoch 23, Batch 344/462, Loss: 0.7208794355392456\n",
      "Epoch 23, Batch 345/462, Loss: 0.7772431373596191\n",
      "Epoch 23, Batch 346/462, Loss: 0.6015030741691589\n",
      "Epoch 23, Batch 347/462, Loss: 0.6970863342285156\n",
      "Epoch 23, Batch 348/462, Loss: 0.80941241979599\n",
      "Epoch 23, Batch 349/462, Loss: 0.7384885549545288\n",
      "Epoch 23, Batch 350/462, Loss: 0.7599213123321533\n",
      "Epoch 23, Batch 351/462, Loss: 0.8531656265258789\n",
      "Epoch 23, Batch 352/462, Loss: 0.7187432646751404\n",
      "Epoch 23, Batch 353/462, Loss: 0.7676842212677002\n",
      "Epoch 23, Batch 354/462, Loss: 0.8445144295692444\n",
      "Epoch 23, Batch 355/462, Loss: 0.7624645829200745\n",
      "Epoch 23, Batch 356/462, Loss: 0.6733417510986328\n",
      "Epoch 23, Batch 357/462, Loss: 0.8569058775901794\n",
      "Epoch 23, Batch 358/462, Loss: 0.7332493662834167\n",
      "Epoch 23, Batch 359/462, Loss: 0.8344981670379639\n",
      "Epoch 23, Batch 360/462, Loss: 0.7341261506080627\n",
      "Epoch 23, Batch 361/462, Loss: 0.7426362037658691\n",
      "Epoch 23, Batch 362/462, Loss: 0.6644532680511475\n",
      "Epoch 23, Batch 363/462, Loss: 0.7514137625694275\n",
      "Epoch 23, Batch 364/462, Loss: 0.7643378376960754\n",
      "Epoch 23, Batch 365/462, Loss: 0.675194501876831\n",
      "Epoch 23, Batch 366/462, Loss: 0.6687981486320496\n",
      "Epoch 23, Batch 367/462, Loss: 0.8191152811050415\n",
      "Epoch 23, Batch 368/462, Loss: 0.8935759663581848\n",
      "Epoch 23, Batch 369/462, Loss: 0.8377982974052429\n",
      "Epoch 23, Batch 370/462, Loss: 0.8190374970436096\n",
      "Epoch 23, Batch 371/462, Loss: 0.6945958137512207\n",
      "Epoch 23, Batch 372/462, Loss: 0.8099217414855957\n",
      "Epoch 23, Batch 373/462, Loss: 0.8218337297439575\n",
      "Epoch 23, Batch 374/462, Loss: 0.8151562213897705\n",
      "Epoch 23, Batch 375/462, Loss: 0.8676822185516357\n",
      "Epoch 23, Batch 376/462, Loss: 0.6655007600784302\n",
      "Epoch 23, Batch 377/462, Loss: 0.7243855595588684\n",
      "Epoch 23, Batch 378/462, Loss: 0.6736369729042053\n",
      "Epoch 23, Batch 379/462, Loss: 0.6893903613090515\n",
      "Epoch 23, Batch 380/462, Loss: 0.6898669600486755\n",
      "Epoch 23, Batch 381/462, Loss: 0.8521173596382141\n",
      "Epoch 23, Batch 382/462, Loss: 0.7658888101577759\n",
      "Epoch 23, Batch 383/462, Loss: 0.6985177397727966\n",
      "Epoch 23, Batch 384/462, Loss: 0.6677874326705933\n",
      "Epoch 23, Batch 385/462, Loss: 0.6312095522880554\n",
      "Epoch 23, Batch 386/462, Loss: 0.5963596701622009\n",
      "Epoch 23, Batch 387/462, Loss: 0.7071693539619446\n",
      "Epoch 23, Batch 388/462, Loss: 0.5469323396682739\n",
      "Epoch 23, Batch 389/462, Loss: 0.8099699020385742\n",
      "Epoch 23, Batch 390/462, Loss: 0.8345483541488647\n",
      "Epoch 23, Batch 391/462, Loss: 1.0067452192306519\n",
      "Epoch 23, Batch 392/462, Loss: 0.748428225517273\n",
      "Epoch 23, Batch 393/462, Loss: 0.641911506652832\n",
      "Epoch 23, Batch 394/462, Loss: 0.68796706199646\n",
      "Epoch 23, Batch 395/462, Loss: 0.8045797348022461\n",
      "Epoch 23, Batch 396/462, Loss: 0.7057756781578064\n",
      "Epoch 23, Batch 397/462, Loss: 0.610699474811554\n",
      "Epoch 23, Batch 398/462, Loss: 0.7565285563468933\n",
      "Epoch 23, Batch 399/462, Loss: 0.8143666982650757\n",
      "Epoch 23, Batch 400/462, Loss: 0.778296709060669\n",
      "Epoch 23, Batch 401/462, Loss: 0.551123321056366\n",
      "Epoch 23, Batch 402/462, Loss: 0.7579920291900635\n",
      "Epoch 23, Batch 403/462, Loss: 0.8049722909927368\n",
      "Epoch 23, Batch 404/462, Loss: 0.8199337720870972\n",
      "Epoch 23, Batch 405/462, Loss: 0.8484912514686584\n",
      "Epoch 23, Batch 406/462, Loss: 0.6052424907684326\n",
      "Epoch 23, Batch 407/462, Loss: 0.6043609976768494\n",
      "Epoch 23, Batch 408/462, Loss: 0.7525541186332703\n",
      "Epoch 23, Batch 409/462, Loss: 0.7346346378326416\n",
      "Epoch 23, Batch 410/462, Loss: 0.6243354678153992\n",
      "Epoch 23, Batch 411/462, Loss: 0.6892949938774109\n",
      "Epoch 23, Batch 412/462, Loss: 0.9570688605308533\n",
      "Epoch 23, Batch 413/462, Loss: 0.6822406053543091\n",
      "Epoch 23, Batch 414/462, Loss: 0.8092774748802185\n",
      "Epoch 23, Batch 415/462, Loss: 0.8495127558708191\n",
      "Epoch 23, Batch 416/462, Loss: 0.9343556761741638\n",
      "Epoch 23, Batch 417/462, Loss: 0.8437774181365967\n",
      "Epoch 23, Batch 418/462, Loss: 0.632349967956543\n",
      "Epoch 23, Batch 419/462, Loss: 0.6638240814208984\n",
      "Epoch 23, Batch 420/462, Loss: 0.6965504884719849\n",
      "Epoch 23, Batch 421/462, Loss: 0.5817468166351318\n",
      "Epoch 23, Batch 422/462, Loss: 0.8845695853233337\n",
      "Epoch 23, Batch 423/462, Loss: 0.714076578617096\n",
      "Epoch 23, Batch 424/462, Loss: 0.5980934500694275\n",
      "Epoch 23, Batch 425/462, Loss: 0.8135485649108887\n",
      "Epoch 23, Batch 426/462, Loss: 0.7698345184326172\n",
      "Epoch 23, Batch 427/462, Loss: 0.888934850692749\n",
      "Epoch 23, Batch 428/462, Loss: 0.6609054803848267\n",
      "Epoch 23, Batch 429/462, Loss: 0.7539856433868408\n",
      "Epoch 23, Batch 430/462, Loss: 0.7889594435691833\n",
      "Epoch 23, Batch 431/462, Loss: 0.7362504005432129\n",
      "Epoch 23, Batch 432/462, Loss: 0.7852471470832825\n",
      "Epoch 23, Batch 433/462, Loss: 0.8088460564613342\n",
      "Epoch 23, Batch 434/462, Loss: 0.8844783306121826\n",
      "Epoch 23, Batch 435/462, Loss: 0.8010947108268738\n",
      "Epoch 23, Batch 436/462, Loss: 0.8342711925506592\n",
      "Epoch 23, Batch 437/462, Loss: 0.6883630752563477\n",
      "Epoch 23, Batch 438/462, Loss: 0.6595644950866699\n",
      "Epoch 23, Batch 439/462, Loss: 0.7865274548530579\n",
      "Epoch 23, Batch 440/462, Loss: 0.9272281527519226\n",
      "Epoch 23, Batch 441/462, Loss: 0.8907352089881897\n",
      "Epoch 23, Batch 442/462, Loss: 0.8114368319511414\n",
      "Epoch 23, Batch 443/462, Loss: 0.8317230939865112\n",
      "Epoch 23, Batch 444/462, Loss: 0.9466438293457031\n",
      "Epoch 23, Batch 445/462, Loss: 0.9593128561973572\n",
      "Epoch 23, Batch 446/462, Loss: 0.7540626525878906\n",
      "Epoch 23, Batch 447/462, Loss: 0.6125820279121399\n",
      "Epoch 23, Batch 448/462, Loss: 0.6970394253730774\n",
      "Epoch 23, Batch 449/462, Loss: 0.6939699053764343\n",
      "Epoch 23, Batch 450/462, Loss: 0.6661581993103027\n",
      "Epoch 23, Batch 451/462, Loss: 0.6170523166656494\n",
      "Epoch 23, Batch 452/462, Loss: 0.5668781995773315\n",
      "Epoch 23, Batch 453/462, Loss: 0.9043542742729187\n",
      "Epoch 23, Batch 454/462, Loss: 0.6640939116477966\n",
      "Epoch 23, Batch 455/462, Loss: 0.6272261142730713\n",
      "Epoch 23, Batch 456/462, Loss: 0.8307088613510132\n",
      "Epoch 23, Batch 457/462, Loss: 0.7623017430305481\n",
      "Epoch 23, Batch 458/462, Loss: 0.6620314717292786\n",
      "Epoch 23, Batch 459/462, Loss: 0.7088530659675598\n",
      "Epoch 23, Batch 460/462, Loss: 0.7846997976303101\n",
      "Epoch 23, Batch 461/462, Loss: 0.8837763071060181\n",
      "Epoch 23, Batch 462/462, Loss: 0.9928039312362671\n",
      "Epoch 23, Loss: 343.4450399875641\n",
      "Epoch 24, Batch 1/462, Loss: 0.8465655446052551\n",
      "Epoch 24, Batch 2/462, Loss: 0.6950420141220093\n",
      "Epoch 24, Batch 3/462, Loss: 0.8580352067947388\n",
      "Epoch 24, Batch 4/462, Loss: 0.5837831497192383\n",
      "Epoch 24, Batch 5/462, Loss: 0.9358720779418945\n",
      "Epoch 24, Batch 6/462, Loss: 0.880641520023346\n",
      "Epoch 24, Batch 7/462, Loss: 0.784992516040802\n",
      "Epoch 24, Batch 8/462, Loss: 0.7335619330406189\n",
      "Epoch 24, Batch 9/462, Loss: 0.633674144744873\n",
      "Epoch 24, Batch 10/462, Loss: 0.6247009038925171\n",
      "Epoch 24, Batch 11/462, Loss: 0.6433002948760986\n",
      "Epoch 24, Batch 12/462, Loss: 0.9704354405403137\n",
      "Epoch 24, Batch 13/462, Loss: 0.6473168730735779\n",
      "Epoch 24, Batch 14/462, Loss: 0.6994350552558899\n",
      "Epoch 24, Batch 15/462, Loss: 0.8239493370056152\n",
      "Epoch 24, Batch 16/462, Loss: 0.6283090114593506\n",
      "Epoch 24, Batch 17/462, Loss: 0.6977580785751343\n",
      "Epoch 24, Batch 18/462, Loss: 0.9951526522636414\n",
      "Epoch 24, Batch 19/462, Loss: 0.6328328251838684\n",
      "Epoch 24, Batch 20/462, Loss: 0.8260653614997864\n",
      "Epoch 24, Batch 21/462, Loss: 0.7724500894546509\n",
      "Epoch 24, Batch 22/462, Loss: 0.6460118889808655\n",
      "Epoch 24, Batch 23/462, Loss: 0.902247965335846\n",
      "Epoch 24, Batch 24/462, Loss: 0.6456778049468994\n",
      "Epoch 24, Batch 25/462, Loss: 0.5919244885444641\n",
      "Epoch 24, Batch 26/462, Loss: 0.8246564269065857\n",
      "Epoch 24, Batch 27/462, Loss: 0.708341658115387\n",
      "Epoch 24, Batch 28/462, Loss: 0.7460341453552246\n",
      "Epoch 24, Batch 29/462, Loss: 0.6777821183204651\n",
      "Epoch 24, Batch 30/462, Loss: 0.7828103303909302\n",
      "Epoch 24, Batch 31/462, Loss: 0.6719728112220764\n",
      "Epoch 24, Batch 32/462, Loss: 0.8782303929328918\n",
      "Epoch 24, Batch 33/462, Loss: 0.7179778218269348\n",
      "Epoch 24, Batch 34/462, Loss: 0.8365530371665955\n",
      "Epoch 24, Batch 35/462, Loss: 0.8781436085700989\n",
      "Epoch 24, Batch 36/462, Loss: 0.6910437941551208\n",
      "Epoch 24, Batch 37/462, Loss: 0.812379002571106\n",
      "Epoch 24, Batch 38/462, Loss: 0.826800525188446\n",
      "Epoch 24, Batch 39/462, Loss: 0.7006377577781677\n",
      "Epoch 24, Batch 40/462, Loss: 0.864166796207428\n",
      "Epoch 24, Batch 41/462, Loss: 0.5171915888786316\n",
      "Epoch 24, Batch 42/462, Loss: 0.7025837898254395\n",
      "Epoch 24, Batch 43/462, Loss: 0.7348120808601379\n",
      "Epoch 24, Batch 44/462, Loss: 0.6291456818580627\n",
      "Epoch 24, Batch 45/462, Loss: 0.7122063636779785\n",
      "Epoch 24, Batch 46/462, Loss: 0.6098337173461914\n",
      "Epoch 24, Batch 47/462, Loss: 0.6688834428787231\n",
      "Epoch 24, Batch 48/462, Loss: 0.7376013398170471\n",
      "Epoch 24, Batch 49/462, Loss: 0.738469123840332\n",
      "Epoch 24, Batch 50/462, Loss: 0.7953386902809143\n",
      "Epoch 24, Batch 51/462, Loss: 0.7984245419502258\n",
      "Epoch 24, Batch 52/462, Loss: 0.7548388242721558\n",
      "Epoch 24, Batch 53/462, Loss: 0.8942052721977234\n",
      "Epoch 24, Batch 54/462, Loss: 0.6085507869720459\n",
      "Epoch 24, Batch 55/462, Loss: 0.7338264584541321\n",
      "Epoch 24, Batch 56/462, Loss: 0.6724302768707275\n",
      "Epoch 24, Batch 57/462, Loss: 0.6529251337051392\n",
      "Epoch 24, Batch 58/462, Loss: 0.6731866598129272\n",
      "Epoch 24, Batch 59/462, Loss: 0.6921124458312988\n",
      "Epoch 24, Batch 60/462, Loss: 0.785114586353302\n",
      "Epoch 24, Batch 61/462, Loss: 0.7933533787727356\n",
      "Epoch 24, Batch 62/462, Loss: 0.7238081097602844\n",
      "Epoch 24, Batch 63/462, Loss: 0.8193694353103638\n",
      "Epoch 24, Batch 64/462, Loss: 0.7564336657524109\n",
      "Epoch 24, Batch 65/462, Loss: 0.6816101670265198\n",
      "Epoch 24, Batch 66/462, Loss: 0.5999298691749573\n",
      "Epoch 24, Batch 67/462, Loss: 0.7588906288146973\n",
      "Epoch 24, Batch 68/462, Loss: 0.683056652545929\n",
      "Epoch 24, Batch 69/462, Loss: 0.6169075965881348\n",
      "Epoch 24, Batch 70/462, Loss: 0.736787736415863\n",
      "Epoch 24, Batch 71/462, Loss: 0.5757122039794922\n",
      "Epoch 24, Batch 72/462, Loss: 0.7810642719268799\n",
      "Epoch 24, Batch 73/462, Loss: 0.5667632818222046\n",
      "Epoch 24, Batch 74/462, Loss: 0.8345586657524109\n",
      "Epoch 24, Batch 75/462, Loss: 0.7355853319168091\n",
      "Epoch 24, Batch 76/462, Loss: 0.6157432794570923\n",
      "Epoch 24, Batch 77/462, Loss: 0.7982872128486633\n",
      "Epoch 24, Batch 78/462, Loss: 0.578774094581604\n",
      "Epoch 24, Batch 79/462, Loss: 0.7032520771026611\n",
      "Epoch 24, Batch 80/462, Loss: 0.5867608189582825\n",
      "Epoch 24, Batch 81/462, Loss: 0.885281503200531\n",
      "Epoch 24, Batch 82/462, Loss: 0.7695331573486328\n",
      "Epoch 24, Batch 83/462, Loss: 0.6889851689338684\n",
      "Epoch 24, Batch 84/462, Loss: 0.8002757430076599\n",
      "Epoch 24, Batch 85/462, Loss: 0.7900415658950806\n",
      "Epoch 24, Batch 86/462, Loss: 0.7413727045059204\n",
      "Epoch 24, Batch 87/462, Loss: 0.6905445456504822\n",
      "Epoch 24, Batch 88/462, Loss: 0.8143755197525024\n",
      "Epoch 24, Batch 89/462, Loss: 0.6940072178840637\n",
      "Epoch 24, Batch 90/462, Loss: 0.7899758815765381\n",
      "Epoch 24, Batch 91/462, Loss: 0.7957360744476318\n",
      "Epoch 24, Batch 92/462, Loss: 0.6838777661323547\n",
      "Epoch 24, Batch 93/462, Loss: 0.6558772921562195\n",
      "Epoch 24, Batch 94/462, Loss: 0.6993193626403809\n",
      "Epoch 24, Batch 95/462, Loss: 0.7426629066467285\n",
      "Epoch 24, Batch 96/462, Loss: 0.6504393219947815\n",
      "Epoch 24, Batch 97/462, Loss: 0.7790042161941528\n",
      "Epoch 24, Batch 98/462, Loss: 0.7354395985603333\n",
      "Epoch 24, Batch 99/462, Loss: 0.5363993048667908\n",
      "Epoch 24, Batch 100/462, Loss: 0.8267937302589417\n",
      "Epoch 24, Batch 101/462, Loss: 0.6360927224159241\n",
      "Epoch 24, Batch 102/462, Loss: 0.7796643376350403\n",
      "Epoch 24, Batch 103/462, Loss: 0.6966716647148132\n",
      "Epoch 24, Batch 104/462, Loss: 0.6184777021408081\n",
      "Epoch 24, Batch 105/462, Loss: 0.5379739999771118\n",
      "Epoch 24, Batch 106/462, Loss: 0.7588310241699219\n",
      "Epoch 24, Batch 107/462, Loss: 0.723092257976532\n",
      "Epoch 24, Batch 108/462, Loss: 0.7067880630493164\n",
      "Epoch 24, Batch 109/462, Loss: 0.7822836637496948\n",
      "Epoch 24, Batch 110/462, Loss: 0.7717896699905396\n",
      "Epoch 24, Batch 111/462, Loss: 0.7752978205680847\n",
      "Epoch 24, Batch 112/462, Loss: 0.720331609249115\n",
      "Epoch 24, Batch 113/462, Loss: 0.6669194102287292\n",
      "Epoch 24, Batch 114/462, Loss: 0.8134242296218872\n",
      "Epoch 24, Batch 115/462, Loss: 0.8018596172332764\n",
      "Epoch 24, Batch 116/462, Loss: 0.6081071496009827\n",
      "Epoch 24, Batch 117/462, Loss: 0.7518876194953918\n",
      "Epoch 24, Batch 118/462, Loss: 0.621503472328186\n",
      "Epoch 24, Batch 119/462, Loss: 0.6313259601593018\n",
      "Epoch 24, Batch 120/462, Loss: 0.6208671927452087\n",
      "Epoch 24, Batch 121/462, Loss: 0.7807556986808777\n",
      "Epoch 24, Batch 122/462, Loss: 0.8089380264282227\n",
      "Epoch 24, Batch 123/462, Loss: 0.7336398363113403\n",
      "Epoch 24, Batch 124/462, Loss: 0.6900222301483154\n",
      "Epoch 24, Batch 125/462, Loss: 0.7886298894882202\n",
      "Epoch 24, Batch 126/462, Loss: 0.7431318759918213\n",
      "Epoch 24, Batch 127/462, Loss: 0.8137538433074951\n",
      "Epoch 24, Batch 128/462, Loss: 0.8027338981628418\n",
      "Epoch 24, Batch 129/462, Loss: 0.7258339524269104\n",
      "Epoch 24, Batch 130/462, Loss: 0.9122250080108643\n",
      "Epoch 24, Batch 131/462, Loss: 0.6284326314926147\n",
      "Epoch 24, Batch 132/462, Loss: 0.6273550987243652\n",
      "Epoch 24, Batch 133/462, Loss: 0.6493619680404663\n",
      "Epoch 24, Batch 134/462, Loss: 0.6979093551635742\n",
      "Epoch 24, Batch 135/462, Loss: 0.7467017769813538\n",
      "Epoch 24, Batch 136/462, Loss: 0.7021527290344238\n",
      "Epoch 24, Batch 137/462, Loss: 0.7599737644195557\n",
      "Epoch 24, Batch 138/462, Loss: 0.8442338705062866\n",
      "Epoch 24, Batch 139/462, Loss: 0.8124776482582092\n",
      "Epoch 24, Batch 140/462, Loss: 0.714928925037384\n",
      "Epoch 24, Batch 141/462, Loss: 0.7243766784667969\n",
      "Epoch 24, Batch 142/462, Loss: 0.6903690695762634\n",
      "Epoch 24, Batch 143/462, Loss: 0.7694747447967529\n",
      "Epoch 24, Batch 144/462, Loss: 0.7582708597183228\n",
      "Epoch 24, Batch 145/462, Loss: 0.9339172840118408\n",
      "Epoch 24, Batch 146/462, Loss: 0.8545568585395813\n",
      "Epoch 24, Batch 147/462, Loss: 0.5784237384796143\n",
      "Epoch 24, Batch 148/462, Loss: 0.6740811467170715\n",
      "Epoch 24, Batch 149/462, Loss: 0.7835979461669922\n",
      "Epoch 24, Batch 150/462, Loss: 0.8176180124282837\n",
      "Epoch 24, Batch 151/462, Loss: 0.7928983569145203\n",
      "Epoch 24, Batch 152/462, Loss: 0.8194313645362854\n",
      "Epoch 24, Batch 153/462, Loss: 0.803881049156189\n",
      "Epoch 24, Batch 154/462, Loss: 0.6947420239448547\n",
      "Epoch 24, Batch 155/462, Loss: 0.6048921942710876\n",
      "Epoch 24, Batch 156/462, Loss: 0.9299353957176208\n",
      "Epoch 24, Batch 157/462, Loss: 0.6397741436958313\n",
      "Epoch 24, Batch 158/462, Loss: 0.6883746981620789\n",
      "Epoch 24, Batch 159/462, Loss: 0.7497918605804443\n",
      "Epoch 24, Batch 160/462, Loss: 0.8153170347213745\n",
      "Epoch 24, Batch 161/462, Loss: 0.6499661207199097\n",
      "Epoch 24, Batch 162/462, Loss: 0.6730462908744812\n",
      "Epoch 24, Batch 163/462, Loss: 0.7994797825813293\n",
      "Epoch 24, Batch 164/462, Loss: 0.7570282220840454\n",
      "Epoch 24, Batch 165/462, Loss: 0.8480486273765564\n",
      "Epoch 24, Batch 166/462, Loss: 0.68916255235672\n",
      "Epoch 24, Batch 167/462, Loss: 0.9063233733177185\n",
      "Epoch 24, Batch 168/462, Loss: 0.6481970548629761\n",
      "Epoch 24, Batch 169/462, Loss: 0.6409826278686523\n",
      "Epoch 24, Batch 170/462, Loss: 0.7764948010444641\n",
      "Epoch 24, Batch 171/462, Loss: 0.8215609192848206\n",
      "Epoch 24, Batch 172/462, Loss: 0.9279504418373108\n",
      "Epoch 24, Batch 173/462, Loss: 0.6960971355438232\n",
      "Epoch 24, Batch 174/462, Loss: 0.6413213014602661\n",
      "Epoch 24, Batch 175/462, Loss: 0.6611312627792358\n",
      "Epoch 24, Batch 176/462, Loss: 0.5313459634780884\n",
      "Epoch 24, Batch 177/462, Loss: 0.6550610661506653\n",
      "Epoch 24, Batch 178/462, Loss: 0.7124857306480408\n",
      "Epoch 24, Batch 179/462, Loss: 0.6960635185241699\n",
      "Epoch 24, Batch 180/462, Loss: 0.6172460317611694\n",
      "Epoch 24, Batch 181/462, Loss: 0.702117919921875\n",
      "Epoch 24, Batch 182/462, Loss: 0.7892250418663025\n",
      "Epoch 24, Batch 183/462, Loss: 0.6654819846153259\n",
      "Epoch 24, Batch 184/462, Loss: 0.8146230578422546\n",
      "Epoch 24, Batch 185/462, Loss: 1.018846035003662\n",
      "Epoch 24, Batch 186/462, Loss: 0.7833062410354614\n",
      "Epoch 24, Batch 187/462, Loss: 0.9110645651817322\n",
      "Epoch 24, Batch 188/462, Loss: 0.9114411473274231\n",
      "Epoch 24, Batch 189/462, Loss: 0.8671450018882751\n",
      "Epoch 24, Batch 190/462, Loss: 0.642441987991333\n",
      "Epoch 24, Batch 191/462, Loss: 0.7977433800697327\n",
      "Epoch 24, Batch 192/462, Loss: 0.9198564291000366\n",
      "Epoch 24, Batch 193/462, Loss: 0.7350633144378662\n",
      "Epoch 24, Batch 194/462, Loss: 0.8079296350479126\n",
      "Epoch 24, Batch 195/462, Loss: 0.9869430661201477\n",
      "Epoch 24, Batch 196/462, Loss: 0.7357286810874939\n",
      "Epoch 24, Batch 197/462, Loss: 0.6198992133140564\n",
      "Epoch 24, Batch 198/462, Loss: 0.7207096219062805\n",
      "Epoch 24, Batch 199/462, Loss: 0.7686449289321899\n",
      "Epoch 24, Batch 200/462, Loss: 0.9011366963386536\n",
      "Epoch 24, Batch 201/462, Loss: 0.6705100536346436\n",
      "Epoch 24, Batch 202/462, Loss: 0.9077550768852234\n",
      "Epoch 24, Batch 203/462, Loss: 0.852384626865387\n",
      "Epoch 24, Batch 204/462, Loss: 0.7230823040008545\n",
      "Epoch 24, Batch 205/462, Loss: 0.6131551861763\n",
      "Epoch 24, Batch 206/462, Loss: 0.5982083082199097\n",
      "Epoch 24, Batch 207/462, Loss: 0.7181124687194824\n",
      "Epoch 24, Batch 208/462, Loss: 0.677047610282898\n",
      "Epoch 24, Batch 209/462, Loss: 0.8736321330070496\n",
      "Epoch 24, Batch 210/462, Loss: 0.7606281042098999\n",
      "Epoch 24, Batch 211/462, Loss: 0.698200523853302\n",
      "Epoch 24, Batch 212/462, Loss: 0.6941937208175659\n",
      "Epoch 24, Batch 213/462, Loss: 0.7519574165344238\n",
      "Epoch 24, Batch 214/462, Loss: 0.6872862577438354\n",
      "Epoch 24, Batch 215/462, Loss: 0.834194004535675\n",
      "Epoch 24, Batch 216/462, Loss: 0.7811903357505798\n",
      "Epoch 24, Batch 217/462, Loss: 0.6703067421913147\n",
      "Epoch 24, Batch 218/462, Loss: 0.7564048767089844\n",
      "Epoch 24, Batch 219/462, Loss: 0.7035689353942871\n",
      "Epoch 24, Batch 220/462, Loss: 0.7952496409416199\n",
      "Epoch 24, Batch 221/462, Loss: 0.9961484670639038\n",
      "Epoch 24, Batch 222/462, Loss: 0.6750862002372742\n",
      "Epoch 24, Batch 223/462, Loss: 0.7581993341445923\n",
      "Epoch 24, Batch 224/462, Loss: 0.6423842906951904\n",
      "Epoch 24, Batch 225/462, Loss: 0.642787516117096\n",
      "Epoch 24, Batch 226/462, Loss: 0.7109826803207397\n",
      "Epoch 24, Batch 227/462, Loss: 0.7675122022628784\n",
      "Epoch 24, Batch 228/462, Loss: 0.8577490448951721\n",
      "Epoch 24, Batch 229/462, Loss: 0.7399897575378418\n",
      "Epoch 24, Batch 230/462, Loss: 0.6131221055984497\n",
      "Epoch 24, Batch 231/462, Loss: 0.6159890294075012\n",
      "Epoch 24, Batch 232/462, Loss: 0.8553529977798462\n",
      "Epoch 24, Batch 233/462, Loss: 0.7639355659484863\n",
      "Epoch 24, Batch 234/462, Loss: 0.6464998126029968\n",
      "Epoch 24, Batch 235/462, Loss: 0.7214791774749756\n",
      "Epoch 24, Batch 236/462, Loss: 0.8063017725944519\n",
      "Epoch 24, Batch 237/462, Loss: 0.5886329412460327\n",
      "Epoch 24, Batch 238/462, Loss: 0.752482533454895\n",
      "Epoch 24, Batch 239/462, Loss: 0.8120368719100952\n",
      "Epoch 24, Batch 240/462, Loss: 0.798811137676239\n",
      "Epoch 24, Batch 241/462, Loss: 0.8227801322937012\n",
      "Epoch 24, Batch 242/462, Loss: 0.7181344628334045\n",
      "Epoch 24, Batch 243/462, Loss: 0.7137648463249207\n",
      "Epoch 24, Batch 244/462, Loss: 0.8876309394836426\n",
      "Epoch 24, Batch 245/462, Loss: 0.7853884696960449\n",
      "Epoch 24, Batch 246/462, Loss: 0.6380470395088196\n",
      "Epoch 24, Batch 247/462, Loss: 0.6875484585762024\n",
      "Epoch 24, Batch 248/462, Loss: 0.6469565629959106\n",
      "Epoch 24, Batch 249/462, Loss: 0.7913315892219543\n",
      "Epoch 24, Batch 250/462, Loss: 0.6918883919715881\n",
      "Epoch 24, Batch 251/462, Loss: 0.560592770576477\n",
      "Epoch 24, Batch 252/462, Loss: 0.8573867082595825\n",
      "Epoch 24, Batch 253/462, Loss: 0.9668651819229126\n",
      "Epoch 24, Batch 254/462, Loss: 0.7645391225814819\n",
      "Epoch 24, Batch 255/462, Loss: 0.7099356055259705\n",
      "Epoch 24, Batch 256/462, Loss: 0.8061484694480896\n",
      "Epoch 24, Batch 257/462, Loss: 0.8546374440193176\n",
      "Epoch 24, Batch 258/462, Loss: 0.6886894106864929\n",
      "Epoch 24, Batch 259/462, Loss: 0.6834385991096497\n",
      "Epoch 24, Batch 260/462, Loss: 0.6779839396476746\n",
      "Epoch 24, Batch 261/462, Loss: 0.6400530934333801\n",
      "Epoch 24, Batch 262/462, Loss: 0.7620503902435303\n",
      "Epoch 24, Batch 263/462, Loss: 0.9027425646781921\n",
      "Epoch 24, Batch 264/462, Loss: 0.6942626237869263\n",
      "Epoch 24, Batch 265/462, Loss: 0.5984246134757996\n",
      "Epoch 24, Batch 266/462, Loss: 0.6715084910392761\n",
      "Epoch 24, Batch 267/462, Loss: 0.7786110043525696\n",
      "Epoch 24, Batch 268/462, Loss: 0.7791831493377686\n",
      "Epoch 24, Batch 269/462, Loss: 0.8047174215316772\n",
      "Epoch 24, Batch 270/462, Loss: 0.6307957172393799\n",
      "Epoch 24, Batch 271/462, Loss: 0.8387898206710815\n",
      "Epoch 24, Batch 272/462, Loss: 0.6028372049331665\n",
      "Epoch 24, Batch 273/462, Loss: 0.7007755637168884\n",
      "Epoch 24, Batch 274/462, Loss: 0.7621086835861206\n",
      "Epoch 24, Batch 275/462, Loss: 0.6606653928756714\n",
      "Epoch 24, Batch 276/462, Loss: 0.881278932094574\n",
      "Epoch 24, Batch 277/462, Loss: 0.6624019742012024\n",
      "Epoch 24, Batch 278/462, Loss: 0.7457862496376038\n",
      "Epoch 24, Batch 279/462, Loss: 0.6073766946792603\n",
      "Epoch 24, Batch 280/462, Loss: 0.6488180160522461\n",
      "Epoch 24, Batch 281/462, Loss: 0.7831768989562988\n",
      "Epoch 24, Batch 282/462, Loss: 0.812620222568512\n",
      "Epoch 24, Batch 283/462, Loss: 0.8120381832122803\n",
      "Epoch 24, Batch 284/462, Loss: 0.8616902232170105\n",
      "Epoch 24, Batch 285/462, Loss: 0.7117328643798828\n",
      "Epoch 24, Batch 286/462, Loss: 0.7582092881202698\n",
      "Epoch 24, Batch 287/462, Loss: 0.7557899355888367\n",
      "Epoch 24, Batch 288/462, Loss: 0.7989563941955566\n",
      "Epoch 24, Batch 289/462, Loss: 0.6982361674308777\n",
      "Epoch 24, Batch 290/462, Loss: 0.7836790680885315\n",
      "Epoch 24, Batch 291/462, Loss: 0.7591938376426697\n",
      "Epoch 24, Batch 292/462, Loss: 0.6727248430252075\n",
      "Epoch 24, Batch 293/462, Loss: 0.6724352836608887\n",
      "Epoch 24, Batch 294/462, Loss: 0.6788729429244995\n",
      "Epoch 24, Batch 295/462, Loss: 0.8258109092712402\n",
      "Epoch 24, Batch 296/462, Loss: 0.6585744619369507\n",
      "Epoch 24, Batch 297/462, Loss: 0.7267753481864929\n",
      "Epoch 24, Batch 298/462, Loss: 0.554950475692749\n",
      "Epoch 24, Batch 299/462, Loss: 0.948823869228363\n",
      "Epoch 24, Batch 300/462, Loss: 0.7150888442993164\n",
      "Epoch 24, Batch 301/462, Loss: 0.7107589840888977\n",
      "Epoch 24, Batch 302/462, Loss: 0.7266868352890015\n",
      "Epoch 24, Batch 303/462, Loss: 0.9515888690948486\n",
      "Epoch 24, Batch 304/462, Loss: 0.7253751158714294\n",
      "Epoch 24, Batch 305/462, Loss: 0.8209922909736633\n",
      "Epoch 24, Batch 306/462, Loss: 0.7397151589393616\n",
      "Epoch 24, Batch 307/462, Loss: 0.6452361345291138\n",
      "Epoch 24, Batch 308/462, Loss: 0.6237896680831909\n",
      "Epoch 24, Batch 309/462, Loss: 0.677524209022522\n",
      "Epoch 24, Batch 310/462, Loss: 0.8644241690635681\n",
      "Epoch 24, Batch 311/462, Loss: 0.8510057330131531\n",
      "Epoch 24, Batch 312/462, Loss: 0.7485709190368652\n",
      "Epoch 24, Batch 313/462, Loss: 0.6674865484237671\n",
      "Epoch 24, Batch 314/462, Loss: 0.8871082663536072\n",
      "Epoch 24, Batch 315/462, Loss: 0.7259701490402222\n",
      "Epoch 24, Batch 316/462, Loss: 0.7142714262008667\n",
      "Epoch 24, Batch 317/462, Loss: 0.6921094059944153\n",
      "Epoch 24, Batch 318/462, Loss: 0.6586992740631104\n",
      "Epoch 24, Batch 319/462, Loss: 0.6806958913803101\n",
      "Epoch 24, Batch 320/462, Loss: 0.8360715508460999\n",
      "Epoch 24, Batch 321/462, Loss: 0.692705512046814\n",
      "Epoch 24, Batch 322/462, Loss: 0.6970970034599304\n",
      "Epoch 24, Batch 323/462, Loss: 0.6035904288291931\n",
      "Epoch 24, Batch 324/462, Loss: 0.7894954681396484\n",
      "Epoch 24, Batch 325/462, Loss: 0.7535282373428345\n",
      "Epoch 24, Batch 326/462, Loss: 0.6826393604278564\n",
      "Epoch 24, Batch 327/462, Loss: 0.7136173248291016\n",
      "Epoch 24, Batch 328/462, Loss: 0.7711678147315979\n",
      "Epoch 24, Batch 329/462, Loss: 0.6474401354789734\n",
      "Epoch 24, Batch 330/462, Loss: 0.882185161113739\n",
      "Epoch 24, Batch 331/462, Loss: 0.7255252003669739\n",
      "Epoch 24, Batch 332/462, Loss: 0.8747285604476929\n",
      "Epoch 24, Batch 333/462, Loss: 0.682162880897522\n",
      "Epoch 24, Batch 334/462, Loss: 0.6690986156463623\n",
      "Epoch 24, Batch 335/462, Loss: 0.8357244729995728\n",
      "Epoch 24, Batch 336/462, Loss: 0.6896840929985046\n",
      "Epoch 24, Batch 337/462, Loss: 0.666090726852417\n",
      "Epoch 24, Batch 338/462, Loss: 0.7798689603805542\n",
      "Epoch 24, Batch 339/462, Loss: 0.7196479439735413\n",
      "Epoch 24, Batch 340/462, Loss: 0.8033815026283264\n",
      "Epoch 24, Batch 341/462, Loss: 0.6971116065979004\n",
      "Epoch 24, Batch 342/462, Loss: 0.7394929528236389\n",
      "Epoch 24, Batch 343/462, Loss: 0.8030194044113159\n",
      "Epoch 24, Batch 344/462, Loss: 0.7577458620071411\n",
      "Epoch 24, Batch 345/462, Loss: 0.7127583026885986\n",
      "Epoch 24, Batch 346/462, Loss: 0.6181137561798096\n",
      "Epoch 24, Batch 347/462, Loss: 0.8386218547821045\n",
      "Epoch 24, Batch 348/462, Loss: 0.6832901835441589\n",
      "Epoch 24, Batch 349/462, Loss: 0.6515979170799255\n",
      "Epoch 24, Batch 350/462, Loss: 0.7033166289329529\n",
      "Epoch 24, Batch 351/462, Loss: 0.7441579103469849\n",
      "Epoch 24, Batch 352/462, Loss: 0.7342897653579712\n",
      "Epoch 24, Batch 353/462, Loss: 0.7603891491889954\n",
      "Epoch 24, Batch 354/462, Loss: 0.7714051604270935\n",
      "Epoch 24, Batch 355/462, Loss: 0.7517408132553101\n",
      "Epoch 24, Batch 356/462, Loss: 0.7196310758590698\n",
      "Epoch 24, Batch 357/462, Loss: 0.7174027562141418\n",
      "Epoch 24, Batch 358/462, Loss: 0.887391209602356\n",
      "Epoch 24, Batch 359/462, Loss: 0.8885080218315125\n",
      "Epoch 24, Batch 360/462, Loss: 0.7852142453193665\n",
      "Epoch 24, Batch 361/462, Loss: 0.7846052050590515\n",
      "Epoch 24, Batch 362/462, Loss: 0.819781482219696\n",
      "Epoch 24, Batch 363/462, Loss: 0.8426631689071655\n",
      "Epoch 24, Batch 364/462, Loss: 0.7911144495010376\n",
      "Epoch 24, Batch 365/462, Loss: 0.677101194858551\n",
      "Epoch 24, Batch 366/462, Loss: 0.8786624073982239\n",
      "Epoch 24, Batch 367/462, Loss: 0.7938271760940552\n",
      "Epoch 24, Batch 368/462, Loss: 0.7837008833885193\n",
      "Epoch 24, Batch 369/462, Loss: 0.7168943881988525\n",
      "Epoch 24, Batch 370/462, Loss: 0.6032487154006958\n",
      "Epoch 24, Batch 371/462, Loss: 0.7955027222633362\n",
      "Epoch 24, Batch 372/462, Loss: 0.7812645435333252\n",
      "Epoch 24, Batch 373/462, Loss: 0.7221025228500366\n",
      "Epoch 24, Batch 374/462, Loss: 0.7181154489517212\n",
      "Epoch 24, Batch 375/462, Loss: 0.8266209363937378\n",
      "Epoch 24, Batch 376/462, Loss: 0.8317834138870239\n",
      "Epoch 24, Batch 377/462, Loss: 0.8784304261207581\n",
      "Epoch 24, Batch 378/462, Loss: 0.6257758140563965\n",
      "Epoch 24, Batch 379/462, Loss: 0.6280606985092163\n",
      "Epoch 24, Batch 380/462, Loss: 0.693614661693573\n",
      "Epoch 24, Batch 381/462, Loss: 0.5374835729598999\n",
      "Epoch 24, Batch 382/462, Loss: 0.7485747933387756\n",
      "Epoch 24, Batch 383/462, Loss: 0.5837062001228333\n",
      "Epoch 24, Batch 384/462, Loss: 0.8785138726234436\n",
      "Epoch 24, Batch 385/462, Loss: 0.6049439311027527\n",
      "Epoch 24, Batch 386/462, Loss: 0.9546687602996826\n",
      "Epoch 24, Batch 387/462, Loss: 0.7494827508926392\n",
      "Epoch 24, Batch 388/462, Loss: 0.7026970386505127\n",
      "Epoch 24, Batch 389/462, Loss: 0.575330376625061\n",
      "Epoch 24, Batch 390/462, Loss: 0.6908305883407593\n",
      "Epoch 24, Batch 391/462, Loss: 0.780264139175415\n",
      "Epoch 24, Batch 392/462, Loss: 0.8719806671142578\n",
      "Epoch 24, Batch 393/462, Loss: 0.8543015718460083\n",
      "Epoch 24, Batch 394/462, Loss: 0.8672224283218384\n",
      "Epoch 24, Batch 395/462, Loss: 0.8768473267555237\n",
      "Epoch 24, Batch 396/462, Loss: 0.6824336647987366\n",
      "Epoch 24, Batch 397/462, Loss: 0.7213349938392639\n",
      "Epoch 24, Batch 398/462, Loss: 0.9176142811775208\n",
      "Epoch 24, Batch 399/462, Loss: 0.8603013753890991\n",
      "Epoch 24, Batch 400/462, Loss: 0.8491070866584778\n",
      "Epoch 24, Batch 401/462, Loss: 0.7351260185241699\n",
      "Epoch 24, Batch 402/462, Loss: 0.9966500401496887\n",
      "Epoch 24, Batch 403/462, Loss: 0.825154721736908\n",
      "Epoch 24, Batch 404/462, Loss: 0.8765981793403625\n",
      "Epoch 24, Batch 405/462, Loss: 0.6702483892440796\n",
      "Epoch 24, Batch 406/462, Loss: 0.6191133260726929\n",
      "Epoch 24, Batch 407/462, Loss: 0.8993690013885498\n",
      "Epoch 24, Batch 408/462, Loss: 0.589647114276886\n",
      "Epoch 24, Batch 409/462, Loss: 0.6050409078598022\n",
      "Epoch 24, Batch 410/462, Loss: 0.893902599811554\n",
      "Epoch 24, Batch 411/462, Loss: 0.6121501922607422\n",
      "Epoch 24, Batch 412/462, Loss: 0.765313982963562\n",
      "Epoch 24, Batch 413/462, Loss: 0.884288489818573\n",
      "Epoch 24, Batch 414/462, Loss: 0.7112439870834351\n",
      "Epoch 24, Batch 415/462, Loss: 0.7897504568099976\n",
      "Epoch 24, Batch 416/462, Loss: 0.8318163752555847\n",
      "Epoch 24, Batch 417/462, Loss: 0.6543734669685364\n",
      "Epoch 24, Batch 418/462, Loss: 0.5933055877685547\n",
      "Epoch 24, Batch 419/462, Loss: 0.702235221862793\n",
      "Epoch 24, Batch 420/462, Loss: 0.6413767337799072\n",
      "Epoch 24, Batch 421/462, Loss: 0.7574005126953125\n",
      "Epoch 24, Batch 422/462, Loss: 0.8208670020103455\n",
      "Epoch 24, Batch 423/462, Loss: 0.6952792406082153\n",
      "Epoch 24, Batch 424/462, Loss: 0.6118459701538086\n",
      "Epoch 24, Batch 425/462, Loss: 0.8030211925506592\n",
      "Epoch 24, Batch 426/462, Loss: 0.7323833107948303\n",
      "Epoch 24, Batch 427/462, Loss: 0.6204119920730591\n",
      "Epoch 24, Batch 428/462, Loss: 0.807369589805603\n",
      "Epoch 24, Batch 429/462, Loss: 0.6902464032173157\n",
      "Epoch 24, Batch 430/462, Loss: 0.873374879360199\n",
      "Epoch 24, Batch 431/462, Loss: 0.7312281727790833\n",
      "Epoch 24, Batch 432/462, Loss: 0.7681527137756348\n",
      "Epoch 24, Batch 433/462, Loss: 0.7358548045158386\n",
      "Epoch 24, Batch 434/462, Loss: 0.7727486491203308\n",
      "Epoch 24, Batch 435/462, Loss: 0.7610961198806763\n",
      "Epoch 24, Batch 436/462, Loss: 0.7325830459594727\n",
      "Epoch 24, Batch 437/462, Loss: 0.642707884311676\n",
      "Epoch 24, Batch 438/462, Loss: 0.8065308928489685\n",
      "Epoch 24, Batch 439/462, Loss: 0.5548332929611206\n",
      "Epoch 24, Batch 440/462, Loss: 0.678213357925415\n",
      "Epoch 24, Batch 441/462, Loss: 0.5887247323989868\n",
      "Epoch 24, Batch 442/462, Loss: 0.6708182096481323\n",
      "Epoch 24, Batch 443/462, Loss: 0.8372101187705994\n",
      "Epoch 24, Batch 444/462, Loss: 0.712649405002594\n",
      "Epoch 24, Batch 445/462, Loss: 0.7641389966011047\n",
      "Epoch 24, Batch 446/462, Loss: 0.5298138856887817\n",
      "Epoch 24, Batch 447/462, Loss: 0.7314028143882751\n",
      "Epoch 24, Batch 448/462, Loss: 0.7248338460922241\n",
      "Epoch 24, Batch 449/462, Loss: 0.7330338358879089\n",
      "Epoch 24, Batch 450/462, Loss: 0.7187193036079407\n",
      "Epoch 24, Batch 451/462, Loss: 0.8130994439125061\n",
      "Epoch 24, Batch 452/462, Loss: 0.7706027030944824\n",
      "Epoch 24, Batch 453/462, Loss: 0.6751227378845215\n",
      "Epoch 24, Batch 454/462, Loss: 0.6123066544532776\n",
      "Epoch 24, Batch 455/462, Loss: 0.5891209244728088\n",
      "Epoch 24, Batch 456/462, Loss: 0.8043901324272156\n",
      "Epoch 24, Batch 457/462, Loss: 0.6721992492675781\n",
      "Epoch 24, Batch 458/462, Loss: 0.7451716661453247\n",
      "Epoch 24, Batch 459/462, Loss: 0.8370300531387329\n",
      "Epoch 24, Batch 460/462, Loss: 0.6833274364471436\n",
      "Epoch 24, Batch 461/462, Loss: 0.7353312969207764\n",
      "Epoch 24, Batch 462/462, Loss: 0.6916301846504211\n",
      "Epoch 24, Loss: 341.7994920015335\n",
      "Epoch 25, Batch 1/462, Loss: 0.7416850924491882\n",
      "Epoch 25, Batch 2/462, Loss: 0.6083303689956665\n",
      "Epoch 25, Batch 3/462, Loss: 0.6500607132911682\n",
      "Epoch 25, Batch 4/462, Loss: 0.7650747895240784\n",
      "Epoch 25, Batch 5/462, Loss: 0.7334014773368835\n",
      "Epoch 25, Batch 6/462, Loss: 0.6628636717796326\n",
      "Epoch 25, Batch 7/462, Loss: 0.7132858037948608\n",
      "Epoch 25, Batch 8/462, Loss: 0.7289759516716003\n",
      "Epoch 25, Batch 9/462, Loss: 0.7870378494262695\n",
      "Epoch 25, Batch 10/462, Loss: 0.6457407474517822\n",
      "Epoch 25, Batch 11/462, Loss: 0.8188676238059998\n",
      "Epoch 25, Batch 12/462, Loss: 0.7158467769622803\n",
      "Epoch 25, Batch 13/462, Loss: 0.761572539806366\n",
      "Epoch 25, Batch 14/462, Loss: 0.8244679570198059\n",
      "Epoch 25, Batch 15/462, Loss: 0.8379637002944946\n",
      "Epoch 25, Batch 16/462, Loss: 0.800653338432312\n",
      "Epoch 25, Batch 17/462, Loss: 0.6971781253814697\n",
      "Epoch 25, Batch 18/462, Loss: 0.8354662656784058\n",
      "Epoch 25, Batch 19/462, Loss: 0.7612838745117188\n",
      "Epoch 25, Batch 20/462, Loss: 0.7915729284286499\n",
      "Epoch 25, Batch 21/462, Loss: 0.7054096460342407\n",
      "Epoch 25, Batch 22/462, Loss: 0.7080021500587463\n",
      "Epoch 25, Batch 23/462, Loss: 0.8808674216270447\n",
      "Epoch 25, Batch 24/462, Loss: 0.7755764722824097\n",
      "Epoch 25, Batch 25/462, Loss: 0.8185958862304688\n",
      "Epoch 25, Batch 26/462, Loss: 1.002150297164917\n",
      "Epoch 25, Batch 27/462, Loss: 0.9012944102287292\n",
      "Epoch 25, Batch 28/462, Loss: 0.9951781630516052\n",
      "Epoch 25, Batch 29/462, Loss: 0.9431741237640381\n",
      "Epoch 25, Batch 30/462, Loss: 0.6170811653137207\n",
      "Epoch 25, Batch 31/462, Loss: 0.7283466458320618\n",
      "Epoch 25, Batch 32/462, Loss: 0.7428246140480042\n",
      "Epoch 25, Batch 33/462, Loss: 0.8836750984191895\n",
      "Epoch 25, Batch 34/462, Loss: 0.7677691578865051\n",
      "Epoch 25, Batch 35/462, Loss: 0.5639041066169739\n",
      "Epoch 25, Batch 36/462, Loss: 0.668479859828949\n",
      "Epoch 25, Batch 37/462, Loss: 0.7005692720413208\n",
      "Epoch 25, Batch 38/462, Loss: 0.8158204555511475\n",
      "Epoch 25, Batch 39/462, Loss: 0.8755035400390625\n",
      "Epoch 25, Batch 40/462, Loss: 0.7020701766014099\n",
      "Epoch 25, Batch 41/462, Loss: 0.7516897916793823\n",
      "Epoch 25, Batch 42/462, Loss: 0.6406126022338867\n",
      "Epoch 25, Batch 43/462, Loss: 0.7460665106773376\n",
      "Epoch 25, Batch 44/462, Loss: 0.8279524445533752\n",
      "Epoch 25, Batch 45/462, Loss: 0.7622458338737488\n",
      "Epoch 25, Batch 46/462, Loss: 0.6590415239334106\n",
      "Epoch 25, Batch 47/462, Loss: 0.5954198241233826\n",
      "Epoch 25, Batch 48/462, Loss: 0.7372720837593079\n",
      "Epoch 25, Batch 49/462, Loss: 0.7986157536506653\n",
      "Epoch 25, Batch 50/462, Loss: 0.8574270606040955\n",
      "Epoch 25, Batch 51/462, Loss: 0.8032193183898926\n",
      "Epoch 25, Batch 52/462, Loss: 0.7160612940788269\n",
      "Epoch 25, Batch 53/462, Loss: 0.5319986343383789\n",
      "Epoch 25, Batch 54/462, Loss: 0.7017393112182617\n",
      "Epoch 25, Batch 55/462, Loss: 0.7780149579048157\n",
      "Epoch 25, Batch 56/462, Loss: 0.6268817782402039\n",
      "Epoch 25, Batch 57/462, Loss: 0.826229989528656\n",
      "Epoch 25, Batch 58/462, Loss: 0.7272638082504272\n",
      "Epoch 25, Batch 59/462, Loss: 0.6820880174636841\n",
      "Epoch 25, Batch 60/462, Loss: 0.7563104629516602\n",
      "Epoch 25, Batch 61/462, Loss: 0.7841005921363831\n",
      "Epoch 25, Batch 62/462, Loss: 0.7777986526489258\n",
      "Epoch 25, Batch 63/462, Loss: 0.7312735915184021\n",
      "Epoch 25, Batch 64/462, Loss: 0.722500741481781\n",
      "Epoch 25, Batch 65/462, Loss: 0.827255129814148\n",
      "Epoch 25, Batch 66/462, Loss: 0.758223831653595\n",
      "Epoch 25, Batch 67/462, Loss: 0.7366242408752441\n",
      "Epoch 25, Batch 68/462, Loss: 0.8557290434837341\n",
      "Epoch 25, Batch 69/462, Loss: 0.5812915563583374\n",
      "Epoch 25, Batch 70/462, Loss: 0.7805966734886169\n",
      "Epoch 25, Batch 71/462, Loss: 0.8378568291664124\n",
      "Epoch 25, Batch 72/462, Loss: 0.6574530601501465\n",
      "Epoch 25, Batch 73/462, Loss: 0.6940128803253174\n",
      "Epoch 25, Batch 74/462, Loss: 0.8618497252464294\n",
      "Epoch 25, Batch 75/462, Loss: 0.8340405821800232\n",
      "Epoch 25, Batch 76/462, Loss: 0.8221430778503418\n",
      "Epoch 25, Batch 77/462, Loss: 0.6416326761245728\n",
      "Epoch 25, Batch 78/462, Loss: 0.8204611539840698\n",
      "Epoch 25, Batch 79/462, Loss: 0.589608371257782\n",
      "Epoch 25, Batch 80/462, Loss: 0.6355298161506653\n",
      "Epoch 25, Batch 81/462, Loss: 0.7313147783279419\n",
      "Epoch 25, Batch 82/462, Loss: 0.6678854823112488\n",
      "Epoch 25, Batch 83/462, Loss: 0.6945745348930359\n",
      "Epoch 25, Batch 84/462, Loss: 0.8479682207107544\n",
      "Epoch 25, Batch 85/462, Loss: 0.7961340546607971\n",
      "Epoch 25, Batch 86/462, Loss: 0.7991838455200195\n",
      "Epoch 25, Batch 87/462, Loss: 0.623638927936554\n",
      "Epoch 25, Batch 88/462, Loss: 0.830482006072998\n",
      "Epoch 25, Batch 89/462, Loss: 0.7079335451126099\n",
      "Epoch 25, Batch 90/462, Loss: 0.8203964829444885\n",
      "Epoch 25, Batch 91/462, Loss: 0.6949824094772339\n",
      "Epoch 25, Batch 92/462, Loss: 0.8751000165939331\n",
      "Epoch 25, Batch 93/462, Loss: 0.9446478486061096\n",
      "Epoch 25, Batch 94/462, Loss: 0.7695755958557129\n",
      "Epoch 25, Batch 95/462, Loss: 0.7677112221717834\n",
      "Epoch 25, Batch 96/462, Loss: 0.8172149658203125\n",
      "Epoch 25, Batch 97/462, Loss: 0.7776635885238647\n",
      "Epoch 25, Batch 98/462, Loss: 0.7798618674278259\n",
      "Epoch 25, Batch 99/462, Loss: 0.8301265239715576\n",
      "Epoch 25, Batch 100/462, Loss: 0.688441812992096\n",
      "Epoch 25, Batch 101/462, Loss: 0.7201879024505615\n",
      "Epoch 25, Batch 102/462, Loss: 0.8280691504478455\n",
      "Epoch 25, Batch 103/462, Loss: 0.7769665718078613\n",
      "Epoch 25, Batch 104/462, Loss: 0.6495811343193054\n",
      "Epoch 25, Batch 105/462, Loss: 0.7264646887779236\n",
      "Epoch 25, Batch 106/462, Loss: 0.6759636998176575\n",
      "Epoch 25, Batch 107/462, Loss: 0.8889263272285461\n",
      "Epoch 25, Batch 108/462, Loss: 0.6753910779953003\n",
      "Epoch 25, Batch 109/462, Loss: 0.7397411465644836\n",
      "Epoch 25, Batch 110/462, Loss: 0.6089038252830505\n",
      "Epoch 25, Batch 111/462, Loss: 0.673210084438324\n",
      "Epoch 25, Batch 112/462, Loss: 0.7031834721565247\n",
      "Epoch 25, Batch 113/462, Loss: 0.8824246525764465\n",
      "Epoch 25, Batch 114/462, Loss: 0.7275283336639404\n",
      "Epoch 25, Batch 115/462, Loss: 0.8490900993347168\n",
      "Epoch 25, Batch 116/462, Loss: 0.8409485220909119\n",
      "Epoch 25, Batch 117/462, Loss: 0.5351710915565491\n",
      "Epoch 25, Batch 118/462, Loss: 0.7490306496620178\n",
      "Epoch 25, Batch 119/462, Loss: 0.8733049035072327\n",
      "Epoch 25, Batch 120/462, Loss: 0.8608167767524719\n",
      "Epoch 25, Batch 121/462, Loss: 0.7353202700614929\n",
      "Epoch 25, Batch 122/462, Loss: 0.7909783124923706\n",
      "Epoch 25, Batch 123/462, Loss: 0.7074788212776184\n",
      "Epoch 25, Batch 124/462, Loss: 0.7290337681770325\n",
      "Epoch 25, Batch 125/462, Loss: 0.769558310508728\n",
      "Epoch 25, Batch 126/462, Loss: 0.6496033668518066\n",
      "Epoch 25, Batch 127/462, Loss: 0.636436939239502\n",
      "Epoch 25, Batch 128/462, Loss: 0.7524394392967224\n",
      "Epoch 25, Batch 129/462, Loss: 0.8138189315795898\n",
      "Epoch 25, Batch 130/462, Loss: 0.7877558469772339\n",
      "Epoch 25, Batch 131/462, Loss: 0.671463668346405\n",
      "Epoch 25, Batch 132/462, Loss: 0.7740772366523743\n",
      "Epoch 25, Batch 133/462, Loss: 0.9468656778335571\n",
      "Epoch 25, Batch 134/462, Loss: 0.773634135723114\n",
      "Epoch 25, Batch 135/462, Loss: 0.7977681159973145\n",
      "Epoch 25, Batch 136/462, Loss: 0.7595958113670349\n",
      "Epoch 25, Batch 137/462, Loss: 0.638177216053009\n",
      "Epoch 25, Batch 138/462, Loss: 0.6258872151374817\n",
      "Epoch 25, Batch 139/462, Loss: 0.7621415853500366\n",
      "Epoch 25, Batch 140/462, Loss: 0.6770631670951843\n",
      "Epoch 25, Batch 141/462, Loss: 0.6723635792732239\n",
      "Epoch 25, Batch 142/462, Loss: 0.6833080053329468\n",
      "Epoch 25, Batch 143/462, Loss: 0.6730791926383972\n",
      "Epoch 25, Batch 144/462, Loss: 0.6947208046913147\n",
      "Epoch 25, Batch 145/462, Loss: 0.5575636029243469\n",
      "Epoch 25, Batch 146/462, Loss: 0.7064245343208313\n",
      "Epoch 25, Batch 147/462, Loss: 0.7084481120109558\n",
      "Epoch 25, Batch 148/462, Loss: 0.7884200811386108\n",
      "Epoch 25, Batch 149/462, Loss: 0.7094278931617737\n",
      "Epoch 25, Batch 150/462, Loss: 0.7044939398765564\n",
      "Epoch 25, Batch 151/462, Loss: 0.8617826700210571\n",
      "Epoch 25, Batch 152/462, Loss: 0.6585846543312073\n",
      "Epoch 25, Batch 153/462, Loss: 0.7621046900749207\n",
      "Epoch 25, Batch 154/462, Loss: 0.6286035180091858\n",
      "Epoch 25, Batch 155/462, Loss: 0.6862576007843018\n",
      "Epoch 25, Batch 156/462, Loss: 0.7179127931594849\n",
      "Epoch 25, Batch 157/462, Loss: 0.7236062288284302\n",
      "Epoch 25, Batch 158/462, Loss: 0.6569991707801819\n",
      "Epoch 25, Batch 159/462, Loss: 0.7291325926780701\n",
      "Epoch 25, Batch 160/462, Loss: 0.729770302772522\n",
      "Epoch 25, Batch 161/462, Loss: 0.7458330392837524\n",
      "Epoch 25, Batch 162/462, Loss: 0.5959566831588745\n",
      "Epoch 25, Batch 163/462, Loss: 0.6175521016120911\n",
      "Epoch 25, Batch 164/462, Loss: 0.6642123460769653\n",
      "Epoch 25, Batch 165/462, Loss: 0.7062366604804993\n",
      "Epoch 25, Batch 166/462, Loss: 0.6716509461402893\n",
      "Epoch 25, Batch 167/462, Loss: 0.7339192628860474\n",
      "Epoch 25, Batch 168/462, Loss: 0.7408075928688049\n",
      "Epoch 25, Batch 169/462, Loss: 0.7402316331863403\n",
      "Epoch 25, Batch 170/462, Loss: 0.8441372513771057\n",
      "Epoch 25, Batch 171/462, Loss: 0.9455885887145996\n",
      "Epoch 25, Batch 172/462, Loss: 0.7948836088180542\n",
      "Epoch 25, Batch 173/462, Loss: 0.7176656723022461\n",
      "Epoch 25, Batch 174/462, Loss: 0.7312145829200745\n",
      "Epoch 25, Batch 175/462, Loss: 0.8187187910079956\n",
      "Epoch 25, Batch 176/462, Loss: 0.9129229187965393\n",
      "Epoch 25, Batch 177/462, Loss: 0.8593254089355469\n",
      "Epoch 25, Batch 178/462, Loss: 0.6700281500816345\n",
      "Epoch 25, Batch 179/462, Loss: 0.6405860781669617\n",
      "Epoch 25, Batch 180/462, Loss: 0.8919801712036133\n",
      "Epoch 25, Batch 181/462, Loss: 0.787631630897522\n",
      "Epoch 25, Batch 182/462, Loss: 0.7406283020973206\n",
      "Epoch 25, Batch 183/462, Loss: 0.5937471389770508\n",
      "Epoch 25, Batch 184/462, Loss: 0.6028760671615601\n",
      "Epoch 25, Batch 185/462, Loss: 0.788147509098053\n",
      "Epoch 25, Batch 186/462, Loss: 0.6159006357192993\n",
      "Epoch 25, Batch 187/462, Loss: 0.8877164125442505\n",
      "Epoch 25, Batch 188/462, Loss: 0.8030797243118286\n",
      "Epoch 25, Batch 189/462, Loss: 0.737277090549469\n",
      "Epoch 25, Batch 190/462, Loss: 0.773160457611084\n",
      "Epoch 25, Batch 191/462, Loss: 0.5274233818054199\n",
      "Epoch 25, Batch 192/462, Loss: 0.6999229788780212\n",
      "Epoch 25, Batch 193/462, Loss: 0.7623329162597656\n",
      "Epoch 25, Batch 194/462, Loss: 0.6991475820541382\n",
      "Epoch 25, Batch 195/462, Loss: 0.8625961542129517\n",
      "Epoch 25, Batch 196/462, Loss: 0.7281392812728882\n",
      "Epoch 25, Batch 197/462, Loss: 0.5542474985122681\n",
      "Epoch 25, Batch 198/462, Loss: 0.6389477849006653\n",
      "Epoch 25, Batch 199/462, Loss: 0.6742969155311584\n",
      "Epoch 25, Batch 200/462, Loss: 0.8959123492240906\n",
      "Epoch 25, Batch 201/462, Loss: 0.8445091843605042\n",
      "Epoch 25, Batch 202/462, Loss: 1.0264780521392822\n",
      "Epoch 25, Batch 203/462, Loss: 0.7873349189758301\n",
      "Epoch 25, Batch 204/462, Loss: 0.8033795952796936\n",
      "Epoch 25, Batch 205/462, Loss: 0.6467738151550293\n",
      "Epoch 25, Batch 206/462, Loss: 0.87570720911026\n",
      "Epoch 25, Batch 207/462, Loss: 0.966670036315918\n",
      "Epoch 25, Batch 208/462, Loss: 0.6979435086250305\n",
      "Epoch 25, Batch 209/462, Loss: 0.9831036925315857\n",
      "Epoch 25, Batch 210/462, Loss: 0.7834305763244629\n",
      "Epoch 25, Batch 211/462, Loss: 0.5940960049629211\n",
      "Epoch 25, Batch 212/462, Loss: 0.7800205945968628\n",
      "Epoch 25, Batch 213/462, Loss: 0.6556373238563538\n",
      "Epoch 25, Batch 214/462, Loss: 0.5808108448982239\n",
      "Epoch 25, Batch 215/462, Loss: 0.6209608316421509\n",
      "Epoch 25, Batch 216/462, Loss: 0.695113480091095\n",
      "Epoch 25, Batch 217/462, Loss: 0.5947891473770142\n",
      "Epoch 25, Batch 218/462, Loss: 0.7001204490661621\n",
      "Epoch 25, Batch 219/462, Loss: 0.7118291854858398\n",
      "Epoch 25, Batch 220/462, Loss: 0.6919013857841492\n",
      "Epoch 25, Batch 221/462, Loss: 0.8007717728614807\n",
      "Epoch 25, Batch 222/462, Loss: 0.6396204233169556\n",
      "Epoch 25, Batch 223/462, Loss: 0.747183084487915\n",
      "Epoch 25, Batch 224/462, Loss: 0.75468909740448\n",
      "Epoch 25, Batch 225/462, Loss: 0.7632017135620117\n",
      "Epoch 25, Batch 226/462, Loss: 0.6455763578414917\n",
      "Epoch 25, Batch 227/462, Loss: 0.6703106164932251\n",
      "Epoch 25, Batch 228/462, Loss: 0.7911580204963684\n",
      "Epoch 25, Batch 229/462, Loss: 0.804227888584137\n",
      "Epoch 25, Batch 230/462, Loss: 0.6655849814414978\n",
      "Epoch 25, Batch 231/462, Loss: 0.8368316292762756\n",
      "Epoch 25, Batch 232/462, Loss: 0.7374134659767151\n",
      "Epoch 25, Batch 233/462, Loss: 0.6452388167381287\n",
      "Epoch 25, Batch 234/462, Loss: 0.8400675058364868\n",
      "Epoch 25, Batch 235/462, Loss: 0.7391188144683838\n",
      "Epoch 25, Batch 236/462, Loss: 0.6609864830970764\n",
      "Epoch 25, Batch 237/462, Loss: 0.7859805226325989\n",
      "Epoch 25, Batch 238/462, Loss: 0.7836282849311829\n",
      "Epoch 25, Batch 239/462, Loss: 0.7549952864646912\n",
      "Epoch 25, Batch 240/462, Loss: 0.6827743649482727\n",
      "Epoch 25, Batch 241/462, Loss: 0.7239890694618225\n",
      "Epoch 25, Batch 242/462, Loss: 0.655139684677124\n",
      "Epoch 25, Batch 243/462, Loss: 0.7135936617851257\n",
      "Epoch 25, Batch 244/462, Loss: 0.7624977231025696\n",
      "Epoch 25, Batch 245/462, Loss: 0.6896485686302185\n",
      "Epoch 25, Batch 246/462, Loss: 0.6932324171066284\n",
      "Epoch 25, Batch 247/462, Loss: 0.7196100950241089\n",
      "Epoch 25, Batch 248/462, Loss: 0.8494039177894592\n",
      "Epoch 25, Batch 249/462, Loss: 0.8111849427223206\n",
      "Epoch 25, Batch 250/462, Loss: 0.6426403522491455\n",
      "Epoch 25, Batch 251/462, Loss: 0.7242759466171265\n",
      "Epoch 25, Batch 252/462, Loss: 0.7233614921569824\n",
      "Epoch 25, Batch 253/462, Loss: 0.7035645246505737\n",
      "Epoch 25, Batch 254/462, Loss: 0.9129884243011475\n",
      "Epoch 25, Batch 255/462, Loss: 0.8044229745864868\n",
      "Epoch 25, Batch 256/462, Loss: 0.7851471304893494\n",
      "Epoch 25, Batch 257/462, Loss: 0.6099151372909546\n",
      "Epoch 25, Batch 258/462, Loss: 0.7204864025115967\n",
      "Epoch 25, Batch 259/462, Loss: 0.7057108879089355\n",
      "Epoch 25, Batch 260/462, Loss: 0.8125349283218384\n",
      "Epoch 25, Batch 261/462, Loss: 0.7975720167160034\n",
      "Epoch 25, Batch 262/462, Loss: 0.7424085736274719\n",
      "Epoch 25, Batch 263/462, Loss: 0.6473488211631775\n",
      "Epoch 25, Batch 264/462, Loss: 0.877436637878418\n",
      "Epoch 25, Batch 265/462, Loss: 0.6527037024497986\n",
      "Epoch 25, Batch 266/462, Loss: 0.6409172415733337\n",
      "Epoch 25, Batch 267/462, Loss: 0.7175778746604919\n",
      "Epoch 25, Batch 268/462, Loss: 0.6344236731529236\n",
      "Epoch 25, Batch 269/462, Loss: 1.0010228157043457\n",
      "Epoch 25, Batch 270/462, Loss: 0.6951237320899963\n",
      "Epoch 25, Batch 271/462, Loss: 0.7330266833305359\n",
      "Epoch 25, Batch 272/462, Loss: 0.7840202450752258\n",
      "Epoch 25, Batch 273/462, Loss: 0.7430918216705322\n",
      "Epoch 25, Batch 274/462, Loss: 0.5686330199241638\n",
      "Epoch 25, Batch 275/462, Loss: 0.8140233755111694\n",
      "Epoch 25, Batch 276/462, Loss: 0.8851630091667175\n",
      "Epoch 25, Batch 277/462, Loss: 0.7277059555053711\n",
      "Epoch 25, Batch 278/462, Loss: 0.7525113224983215\n",
      "Epoch 25, Batch 279/462, Loss: 0.8229448199272156\n",
      "Epoch 25, Batch 280/462, Loss: 0.8000181317329407\n",
      "Epoch 25, Batch 281/462, Loss: 0.7038718461990356\n",
      "Epoch 25, Batch 282/462, Loss: 0.7397983074188232\n",
      "Epoch 25, Batch 283/462, Loss: 0.6369224786758423\n",
      "Epoch 25, Batch 284/462, Loss: 0.6673166751861572\n",
      "Epoch 25, Batch 285/462, Loss: 0.7685995697975159\n",
      "Epoch 25, Batch 286/462, Loss: 0.6000859141349792\n",
      "Epoch 25, Batch 287/462, Loss: 0.6478273272514343\n",
      "Epoch 25, Batch 288/462, Loss: 0.8023850917816162\n",
      "Epoch 25, Batch 289/462, Loss: 0.8373952507972717\n",
      "Epoch 25, Batch 290/462, Loss: 0.6653596758842468\n",
      "Epoch 25, Batch 291/462, Loss: 0.6272056102752686\n",
      "Epoch 25, Batch 292/462, Loss: 0.8775733709335327\n",
      "Epoch 25, Batch 293/462, Loss: 0.709495484828949\n",
      "Epoch 25, Batch 294/462, Loss: 0.792056143283844\n",
      "Epoch 25, Batch 295/462, Loss: 0.7239120006561279\n",
      "Epoch 25, Batch 296/462, Loss: 0.7817094922065735\n",
      "Epoch 25, Batch 297/462, Loss: 0.7562040686607361\n",
      "Epoch 25, Batch 298/462, Loss: 0.5759092569351196\n",
      "Epoch 25, Batch 299/462, Loss: 0.8794941306114197\n",
      "Epoch 25, Batch 300/462, Loss: 0.6870360374450684\n",
      "Epoch 25, Batch 301/462, Loss: 0.6185633540153503\n",
      "Epoch 25, Batch 302/462, Loss: 0.6025063991546631\n",
      "Epoch 25, Batch 303/462, Loss: 0.6603250503540039\n",
      "Epoch 25, Batch 304/462, Loss: 0.6968445777893066\n",
      "Epoch 25, Batch 305/462, Loss: 1.0576941967010498\n",
      "Epoch 25, Batch 306/462, Loss: 0.6313179731369019\n",
      "Epoch 25, Batch 307/462, Loss: 0.797192394733429\n",
      "Epoch 25, Batch 308/462, Loss: 0.7677850723266602\n",
      "Epoch 25, Batch 309/462, Loss: 0.7220574021339417\n",
      "Epoch 25, Batch 310/462, Loss: 0.8040763735771179\n",
      "Epoch 25, Batch 311/462, Loss: 0.6427890658378601\n",
      "Epoch 25, Batch 312/462, Loss: 0.7834001183509827\n",
      "Epoch 25, Batch 313/462, Loss: 0.6937313079833984\n",
      "Epoch 25, Batch 314/462, Loss: 0.6480864882469177\n",
      "Epoch 25, Batch 315/462, Loss: 0.7302939891815186\n",
      "Epoch 25, Batch 316/462, Loss: 0.7910560369491577\n",
      "Epoch 25, Batch 317/462, Loss: 0.8095014095306396\n",
      "Epoch 25, Batch 318/462, Loss: 0.5559026002883911\n",
      "Epoch 25, Batch 319/462, Loss: 0.7015633583068848\n",
      "Epoch 25, Batch 320/462, Loss: 0.6834554672241211\n",
      "Epoch 25, Batch 321/462, Loss: 0.825806200504303\n",
      "Epoch 25, Batch 322/462, Loss: 0.8105500936508179\n",
      "Epoch 25, Batch 323/462, Loss: 0.5481792092323303\n",
      "Epoch 25, Batch 324/462, Loss: 0.7190440893173218\n",
      "Epoch 25, Batch 325/462, Loss: 0.5521495938301086\n",
      "Epoch 25, Batch 326/462, Loss: 0.8475440740585327\n",
      "Epoch 25, Batch 327/462, Loss: 0.6721702218055725\n",
      "Epoch 25, Batch 328/462, Loss: 0.6599265933036804\n",
      "Epoch 25, Batch 329/462, Loss: 0.6103947162628174\n",
      "Epoch 25, Batch 330/462, Loss: 0.7116268277168274\n",
      "Epoch 25, Batch 331/462, Loss: 0.703116238117218\n",
      "Epoch 25, Batch 332/462, Loss: 0.6506218314170837\n",
      "Epoch 25, Batch 333/462, Loss: 0.5698700547218323\n",
      "Epoch 25, Batch 334/462, Loss: 0.7225868701934814\n",
      "Epoch 25, Batch 335/462, Loss: 0.795669674873352\n",
      "Epoch 25, Batch 336/462, Loss: 0.8771436214447021\n",
      "Epoch 25, Batch 337/462, Loss: 0.8531405925750732\n",
      "Epoch 25, Batch 338/462, Loss: 0.6343511939048767\n",
      "Epoch 25, Batch 339/462, Loss: 0.7101042866706848\n",
      "Epoch 25, Batch 340/462, Loss: 0.6987393498420715\n",
      "Epoch 25, Batch 341/462, Loss: 0.8599837422370911\n",
      "Epoch 25, Batch 342/462, Loss: 0.7830603718757629\n",
      "Epoch 25, Batch 343/462, Loss: 0.7466758489608765\n",
      "Epoch 25, Batch 344/462, Loss: 0.6313195824623108\n",
      "Epoch 25, Batch 345/462, Loss: 0.6296736598014832\n",
      "Epoch 25, Batch 346/462, Loss: 0.7352306842803955\n",
      "Epoch 25, Batch 347/462, Loss: 0.5578595399856567\n",
      "Epoch 25, Batch 348/462, Loss: 0.662575900554657\n",
      "Epoch 25, Batch 349/462, Loss: 0.8450610637664795\n",
      "Epoch 25, Batch 350/462, Loss: 0.9390747547149658\n",
      "Epoch 25, Batch 351/462, Loss: 0.6748834848403931\n",
      "Epoch 25, Batch 352/462, Loss: 0.756558358669281\n",
      "Epoch 25, Batch 353/462, Loss: 0.6991641521453857\n",
      "Epoch 25, Batch 354/462, Loss: 0.6938431859016418\n",
      "Epoch 25, Batch 355/462, Loss: 0.941261351108551\n",
      "Epoch 25, Batch 356/462, Loss: 0.7722665667533875\n",
      "Epoch 25, Batch 357/462, Loss: 0.6151363253593445\n",
      "Epoch 25, Batch 358/462, Loss: 0.8895400762557983\n",
      "Epoch 25, Batch 359/462, Loss: 0.6404626369476318\n",
      "Epoch 25, Batch 360/462, Loss: 0.6400676369667053\n",
      "Epoch 25, Batch 361/462, Loss: 0.8544113636016846\n",
      "Epoch 25, Batch 362/462, Loss: 0.6117336750030518\n",
      "Epoch 25, Batch 363/462, Loss: 0.8969745635986328\n",
      "Epoch 25, Batch 364/462, Loss: 0.8373122215270996\n",
      "Epoch 25, Batch 365/462, Loss: 0.6303472518920898\n",
      "Epoch 25, Batch 366/462, Loss: 0.9026397466659546\n",
      "Epoch 25, Batch 367/462, Loss: 0.8609180450439453\n",
      "Epoch 25, Batch 368/462, Loss: 0.7563648819923401\n",
      "Epoch 25, Batch 369/462, Loss: 0.7504270672798157\n",
      "Epoch 25, Batch 370/462, Loss: 0.5411887168884277\n",
      "Epoch 25, Batch 371/462, Loss: 0.8509418964385986\n",
      "Epoch 25, Batch 372/462, Loss: 0.8403071165084839\n",
      "Epoch 25, Batch 373/462, Loss: 0.9503681659698486\n",
      "Epoch 25, Batch 374/462, Loss: 0.7657759189605713\n",
      "Epoch 25, Batch 375/462, Loss: 0.7633894085884094\n",
      "Epoch 25, Batch 376/462, Loss: 0.8353165984153748\n",
      "Epoch 25, Batch 377/462, Loss: 0.6337565779685974\n",
      "Epoch 25, Batch 378/462, Loss: 0.6395235657691956\n",
      "Epoch 25, Batch 379/462, Loss: 0.5127469301223755\n",
      "Epoch 25, Batch 380/462, Loss: 0.7374433279037476\n",
      "Epoch 25, Batch 381/462, Loss: 0.561522364616394\n",
      "Epoch 25, Batch 382/462, Loss: 0.6356359124183655\n",
      "Epoch 25, Batch 383/462, Loss: 0.7261499762535095\n",
      "Epoch 25, Batch 384/462, Loss: 0.6574148535728455\n",
      "Epoch 25, Batch 385/462, Loss: 0.7404447197914124\n",
      "Epoch 25, Batch 386/462, Loss: 0.845851480960846\n",
      "Epoch 25, Batch 387/462, Loss: 0.5600810647010803\n",
      "Epoch 25, Batch 388/462, Loss: 0.592901885509491\n",
      "Epoch 25, Batch 389/462, Loss: 0.7926813960075378\n",
      "Epoch 25, Batch 390/462, Loss: 0.8100939989089966\n",
      "Epoch 25, Batch 391/462, Loss: 0.9291845560073853\n",
      "Epoch 25, Batch 392/462, Loss: 0.8184674978256226\n",
      "Epoch 25, Batch 393/462, Loss: 0.7233160138130188\n",
      "Epoch 25, Batch 394/462, Loss: 0.6720879077911377\n",
      "Epoch 25, Batch 395/462, Loss: 0.7095689177513123\n",
      "Epoch 25, Batch 396/462, Loss: 0.6938347220420837\n",
      "Epoch 25, Batch 397/462, Loss: 0.8443366885185242\n",
      "Epoch 25, Batch 398/462, Loss: 0.7601406574249268\n",
      "Epoch 25, Batch 399/462, Loss: 0.6922808885574341\n",
      "Epoch 25, Batch 400/462, Loss: 0.5546987056732178\n",
      "Epoch 25, Batch 401/462, Loss: 0.7845582365989685\n",
      "Epoch 25, Batch 402/462, Loss: 0.637522280216217\n",
      "Epoch 25, Batch 403/462, Loss: 0.7995901703834534\n",
      "Epoch 25, Batch 404/462, Loss: 0.6138277053833008\n",
      "Epoch 25, Batch 405/462, Loss: 0.7155963778495789\n",
      "Epoch 25, Batch 406/462, Loss: 0.7104886174201965\n",
      "Epoch 25, Batch 407/462, Loss: 0.6922147274017334\n",
      "Epoch 25, Batch 408/462, Loss: 0.8624181151390076\n",
      "Epoch 25, Batch 409/462, Loss: 0.7994034290313721\n",
      "Epoch 25, Batch 410/462, Loss: 0.8110584020614624\n",
      "Epoch 25, Batch 411/462, Loss: 0.6918293237686157\n",
      "Epoch 25, Batch 412/462, Loss: 0.6317541003227234\n",
      "Epoch 25, Batch 413/462, Loss: 0.630083441734314\n",
      "Epoch 25, Batch 414/462, Loss: 0.8972064256668091\n",
      "Epoch 25, Batch 415/462, Loss: 0.6212552189826965\n",
      "Epoch 25, Batch 416/462, Loss: 0.9269717931747437\n",
      "Epoch 25, Batch 417/462, Loss: 0.68770432472229\n",
      "Epoch 25, Batch 418/462, Loss: 0.6770845651626587\n",
      "Epoch 25, Batch 419/462, Loss: 0.7797699570655823\n",
      "Epoch 25, Batch 420/462, Loss: 0.6893556714057922\n",
      "Epoch 25, Batch 421/462, Loss: 0.6849949955940247\n",
      "Epoch 25, Batch 422/462, Loss: 0.8392554521560669\n",
      "Epoch 25, Batch 423/462, Loss: 0.7111157178878784\n",
      "Epoch 25, Batch 424/462, Loss: 0.911539614200592\n",
      "Epoch 25, Batch 425/462, Loss: 0.691702663898468\n",
      "Epoch 25, Batch 426/462, Loss: 0.8454667925834656\n",
      "Epoch 25, Batch 427/462, Loss: 0.7061173319816589\n",
      "Epoch 25, Batch 428/462, Loss: 0.6777002215385437\n",
      "Epoch 25, Batch 429/462, Loss: 0.7120456695556641\n",
      "Epoch 25, Batch 430/462, Loss: 0.6085115075111389\n",
      "Epoch 25, Batch 431/462, Loss: 0.7238406538963318\n",
      "Epoch 25, Batch 432/462, Loss: 0.7998819947242737\n",
      "Epoch 25, Batch 433/462, Loss: 0.5445035696029663\n",
      "Epoch 25, Batch 434/462, Loss: 0.6557779312133789\n",
      "Epoch 25, Batch 435/462, Loss: 0.934303343296051\n",
      "Epoch 25, Batch 436/462, Loss: 0.6177679896354675\n",
      "Epoch 25, Batch 437/462, Loss: 0.5306050777435303\n",
      "Epoch 25, Batch 438/462, Loss: 0.8622994422912598\n",
      "Epoch 25, Batch 439/462, Loss: 0.7697493433952332\n",
      "Epoch 25, Batch 440/462, Loss: 0.7067534923553467\n",
      "Epoch 25, Batch 441/462, Loss: 0.7635763883590698\n",
      "Epoch 25, Batch 442/462, Loss: 0.5831714868545532\n",
      "Epoch 25, Batch 443/462, Loss: 0.7528195381164551\n",
      "Epoch 25, Batch 444/462, Loss: 0.6967516541481018\n",
      "Epoch 25, Batch 445/462, Loss: 0.6118757724761963\n",
      "Epoch 25, Batch 446/462, Loss: 0.710303783416748\n",
      "Epoch 25, Batch 447/462, Loss: 0.7504390478134155\n",
      "Epoch 25, Batch 448/462, Loss: 0.6592799425125122\n",
      "Epoch 25, Batch 449/462, Loss: 0.8579798340797424\n",
      "Epoch 25, Batch 450/462, Loss: 0.6869024634361267\n",
      "Epoch 25, Batch 451/462, Loss: 0.8929469585418701\n",
      "Epoch 25, Batch 452/462, Loss: 0.7613399028778076\n",
      "Epoch 25, Batch 453/462, Loss: 0.6064779758453369\n",
      "Epoch 25, Batch 454/462, Loss: 0.6427892446517944\n",
      "Epoch 25, Batch 455/462, Loss: 0.6636867523193359\n",
      "Epoch 25, Batch 456/462, Loss: 0.6326313614845276\n",
      "Epoch 25, Batch 457/462, Loss: 0.8611970543861389\n",
      "Epoch 25, Batch 458/462, Loss: 0.8137792348861694\n",
      "Epoch 25, Batch 459/462, Loss: 0.7805690169334412\n",
      "Epoch 25, Batch 460/462, Loss: 0.7050168514251709\n",
      "Epoch 25, Batch 461/462, Loss: 0.8384469747543335\n",
      "Epoch 25, Batch 462/462, Loss: 0.7801083922386169\n",
      "Epoch 25, Loss: 340.9662043452263\n",
      "Epoch 26, Batch 1/462, Loss: 0.7056706547737122\n",
      "Epoch 26, Batch 2/462, Loss: 0.6715074777603149\n",
      "Epoch 26, Batch 3/462, Loss: 0.6585221290588379\n",
      "Epoch 26, Batch 4/462, Loss: 0.6748760342597961\n",
      "Epoch 26, Batch 5/462, Loss: 0.6798043847084045\n",
      "Epoch 26, Batch 6/462, Loss: 0.7134815454483032\n",
      "Epoch 26, Batch 7/462, Loss: 0.7805540561676025\n",
      "Epoch 26, Batch 8/462, Loss: 0.7723087072372437\n",
      "Epoch 26, Batch 9/462, Loss: 1.0852372646331787\n",
      "Epoch 26, Batch 10/462, Loss: 0.6881563067436218\n",
      "Epoch 26, Batch 11/462, Loss: 0.5985565185546875\n",
      "Epoch 26, Batch 12/462, Loss: 0.884902834892273\n",
      "Epoch 26, Batch 13/462, Loss: 0.7122549414634705\n",
      "Epoch 26, Batch 14/462, Loss: 0.7729765176773071\n",
      "Epoch 26, Batch 15/462, Loss: 0.7041537165641785\n",
      "Epoch 26, Batch 16/462, Loss: 0.7917582392692566\n",
      "Epoch 26, Batch 17/462, Loss: 0.6731840968132019\n",
      "Epoch 26, Batch 18/462, Loss: 0.8009512424468994\n",
      "Epoch 26, Batch 19/462, Loss: 0.7785294055938721\n",
      "Epoch 26, Batch 20/462, Loss: 0.5769686698913574\n",
      "Epoch 26, Batch 21/462, Loss: 0.7660267949104309\n",
      "Epoch 26, Batch 22/462, Loss: 0.7754106521606445\n",
      "Epoch 26, Batch 23/462, Loss: 0.7048152685165405\n",
      "Epoch 26, Batch 24/462, Loss: 0.6527141332626343\n",
      "Epoch 26, Batch 25/462, Loss: 0.8050276041030884\n",
      "Epoch 26, Batch 26/462, Loss: 0.7273306846618652\n",
      "Epoch 26, Batch 27/462, Loss: 0.7274156808853149\n",
      "Epoch 26, Batch 28/462, Loss: 0.845488429069519\n",
      "Epoch 26, Batch 29/462, Loss: 0.6710867285728455\n",
      "Epoch 26, Batch 30/462, Loss: 0.6373288631439209\n",
      "Epoch 26, Batch 31/462, Loss: 0.8104730248451233\n",
      "Epoch 26, Batch 32/462, Loss: 0.7790651917457581\n",
      "Epoch 26, Batch 33/462, Loss: 0.7945065498352051\n",
      "Epoch 26, Batch 34/462, Loss: 0.895790696144104\n",
      "Epoch 26, Batch 35/462, Loss: 0.6736635565757751\n",
      "Epoch 26, Batch 36/462, Loss: 0.7411303520202637\n",
      "Epoch 26, Batch 37/462, Loss: 0.8844498991966248\n",
      "Epoch 26, Batch 38/462, Loss: 0.6917703747749329\n",
      "Epoch 26, Batch 39/462, Loss: 0.7881850004196167\n",
      "Epoch 26, Batch 40/462, Loss: 0.798856794834137\n",
      "Epoch 26, Batch 41/462, Loss: 0.7266188859939575\n",
      "Epoch 26, Batch 42/462, Loss: 0.7000579237937927\n",
      "Epoch 26, Batch 43/462, Loss: 0.6851521134376526\n",
      "Epoch 26, Batch 44/462, Loss: 0.6523870229721069\n",
      "Epoch 26, Batch 45/462, Loss: 0.7634310126304626\n",
      "Epoch 26, Batch 46/462, Loss: 0.6537619233131409\n",
      "Epoch 26, Batch 47/462, Loss: 0.7110875844955444\n",
      "Epoch 26, Batch 48/462, Loss: 0.6902838945388794\n",
      "Epoch 26, Batch 49/462, Loss: 0.6986114978790283\n",
      "Epoch 26, Batch 50/462, Loss: 0.7350400686264038\n",
      "Epoch 26, Batch 51/462, Loss: 0.6020247936248779\n",
      "Epoch 26, Batch 52/462, Loss: 0.7658289074897766\n",
      "Epoch 26, Batch 53/462, Loss: 0.925122082233429\n",
      "Epoch 26, Batch 54/462, Loss: 0.7157443761825562\n",
      "Epoch 26, Batch 55/462, Loss: 0.8308162689208984\n",
      "Epoch 26, Batch 56/462, Loss: 0.7901285886764526\n",
      "Epoch 26, Batch 57/462, Loss: 0.7198764085769653\n",
      "Epoch 26, Batch 58/462, Loss: 0.9190521240234375\n",
      "Epoch 26, Batch 59/462, Loss: 0.7878004908561707\n",
      "Epoch 26, Batch 60/462, Loss: 0.7114885449409485\n",
      "Epoch 26, Batch 61/462, Loss: 0.8386949300765991\n",
      "Epoch 26, Batch 62/462, Loss: 0.6718425750732422\n",
      "Epoch 26, Batch 63/462, Loss: 0.7946342825889587\n",
      "Epoch 26, Batch 64/462, Loss: 0.7876940369606018\n",
      "Epoch 26, Batch 65/462, Loss: 0.6061018705368042\n",
      "Epoch 26, Batch 66/462, Loss: 0.8915087580680847\n",
      "Epoch 26, Batch 67/462, Loss: 0.716316819190979\n",
      "Epoch 26, Batch 68/462, Loss: 0.7104925513267517\n",
      "Epoch 26, Batch 69/462, Loss: 0.7206218242645264\n",
      "Epoch 26, Batch 70/462, Loss: 0.8707932829856873\n",
      "Epoch 26, Batch 71/462, Loss: 0.8758381009101868\n",
      "Epoch 26, Batch 72/462, Loss: 0.7453504800796509\n",
      "Epoch 26, Batch 73/462, Loss: 1.0668257474899292\n",
      "Epoch 26, Batch 74/462, Loss: 0.800421953201294\n",
      "Epoch 26, Batch 75/462, Loss: 0.7697740197181702\n",
      "Epoch 26, Batch 76/462, Loss: 0.6789315938949585\n",
      "Epoch 26, Batch 77/462, Loss: 0.8024793267250061\n",
      "Epoch 26, Batch 78/462, Loss: 0.7278498411178589\n",
      "Epoch 26, Batch 79/462, Loss: 0.6047821044921875\n",
      "Epoch 26, Batch 80/462, Loss: 0.7087192535400391\n",
      "Epoch 26, Batch 81/462, Loss: 0.6713386178016663\n",
      "Epoch 26, Batch 82/462, Loss: 0.745810866355896\n",
      "Epoch 26, Batch 83/462, Loss: 0.7524922490119934\n",
      "Epoch 26, Batch 84/462, Loss: 0.8217492699623108\n",
      "Epoch 26, Batch 85/462, Loss: 0.7388448119163513\n",
      "Epoch 26, Batch 86/462, Loss: 0.7646186947822571\n",
      "Epoch 26, Batch 87/462, Loss: 0.6179062128067017\n",
      "Epoch 26, Batch 88/462, Loss: 0.7493818402290344\n",
      "Epoch 26, Batch 89/462, Loss: 0.7003177404403687\n",
      "Epoch 26, Batch 90/462, Loss: 0.675324022769928\n",
      "Epoch 26, Batch 91/462, Loss: 0.8823685050010681\n",
      "Epoch 26, Batch 92/462, Loss: 0.8296288251876831\n",
      "Epoch 26, Batch 93/462, Loss: 0.7056038975715637\n",
      "Epoch 26, Batch 94/462, Loss: 0.688487708568573\n",
      "Epoch 26, Batch 95/462, Loss: 0.7295408844947815\n",
      "Epoch 26, Batch 96/462, Loss: 0.562514066696167\n",
      "Epoch 26, Batch 97/462, Loss: 0.8039997816085815\n",
      "Epoch 26, Batch 98/462, Loss: 0.7075932025909424\n",
      "Epoch 26, Batch 99/462, Loss: 0.7291367053985596\n",
      "Epoch 26, Batch 100/462, Loss: 0.6772470474243164\n",
      "Epoch 26, Batch 101/462, Loss: 0.7994414567947388\n",
      "Epoch 26, Batch 102/462, Loss: 0.7073785066604614\n",
      "Epoch 26, Batch 103/462, Loss: 0.7940374612808228\n",
      "Epoch 26, Batch 104/462, Loss: 0.6523251533508301\n",
      "Epoch 26, Batch 105/462, Loss: 0.7146700620651245\n",
      "Epoch 26, Batch 106/462, Loss: 0.6680532693862915\n",
      "Epoch 26, Batch 107/462, Loss: 0.7437270879745483\n",
      "Epoch 26, Batch 108/462, Loss: 0.9219508171081543\n",
      "Epoch 26, Batch 109/462, Loss: 0.8324945569038391\n",
      "Epoch 26, Batch 110/462, Loss: 0.870621383190155\n",
      "Epoch 26, Batch 111/462, Loss: 0.7535457611083984\n",
      "Epoch 26, Batch 112/462, Loss: 0.7412155866622925\n",
      "Epoch 26, Batch 113/462, Loss: 0.8229261040687561\n",
      "Epoch 26, Batch 114/462, Loss: 0.9189824461936951\n",
      "Epoch 26, Batch 115/462, Loss: 0.8305382132530212\n",
      "Epoch 26, Batch 116/462, Loss: 0.6917721629142761\n",
      "Epoch 26, Batch 117/462, Loss: 0.6152008771896362\n",
      "Epoch 26, Batch 118/462, Loss: 0.6951615810394287\n",
      "Epoch 26, Batch 119/462, Loss: 0.7323166131973267\n",
      "Epoch 26, Batch 120/462, Loss: 0.7527536749839783\n",
      "Epoch 26, Batch 121/462, Loss: 0.7620461583137512\n",
      "Epoch 26, Batch 122/462, Loss: 0.6920389533042908\n",
      "Epoch 26, Batch 123/462, Loss: 0.9308640360832214\n",
      "Epoch 26, Batch 124/462, Loss: 0.9112083315849304\n",
      "Epoch 26, Batch 125/462, Loss: 0.5929227471351624\n",
      "Epoch 26, Batch 126/462, Loss: 0.7792645692825317\n",
      "Epoch 26, Batch 127/462, Loss: 0.9078205227851868\n",
      "Epoch 26, Batch 128/462, Loss: 0.7878644466400146\n",
      "Epoch 26, Batch 129/462, Loss: 0.7050777673721313\n",
      "Epoch 26, Batch 130/462, Loss: 0.7430689930915833\n",
      "Epoch 26, Batch 131/462, Loss: 1.1143814325332642\n",
      "Epoch 26, Batch 132/462, Loss: 0.7148493528366089\n",
      "Epoch 26, Batch 133/462, Loss: 0.7434964776039124\n",
      "Epoch 26, Batch 134/462, Loss: 0.7803886532783508\n",
      "Epoch 26, Batch 135/462, Loss: 0.8779062628746033\n",
      "Epoch 26, Batch 136/462, Loss: 0.602379560470581\n",
      "Epoch 26, Batch 137/462, Loss: 0.8230924010276794\n",
      "Epoch 26, Batch 138/462, Loss: 0.8425697088241577\n",
      "Epoch 26, Batch 139/462, Loss: 0.6269185543060303\n",
      "Epoch 26, Batch 140/462, Loss: 0.8290800452232361\n",
      "Epoch 26, Batch 141/462, Loss: 0.8058952689170837\n",
      "Epoch 26, Batch 142/462, Loss: 0.6624855995178223\n",
      "Epoch 26, Batch 143/462, Loss: 0.6171544790267944\n",
      "Epoch 26, Batch 144/462, Loss: 0.675864040851593\n",
      "Epoch 26, Batch 145/462, Loss: 0.6634541749954224\n",
      "Epoch 26, Batch 146/462, Loss: 0.8223596811294556\n",
      "Epoch 26, Batch 147/462, Loss: 0.6515886783599854\n",
      "Epoch 26, Batch 148/462, Loss: 0.6401129364967346\n",
      "Epoch 26, Batch 149/462, Loss: 0.7837203145027161\n",
      "Epoch 26, Batch 150/462, Loss: 0.7681503295898438\n",
      "Epoch 26, Batch 151/462, Loss: 0.781312108039856\n",
      "Epoch 26, Batch 152/462, Loss: 0.6440060138702393\n",
      "Epoch 26, Batch 153/462, Loss: 0.7338372468948364\n",
      "Epoch 26, Batch 154/462, Loss: 0.6762447953224182\n",
      "Epoch 26, Batch 155/462, Loss: 0.7481269836425781\n",
      "Epoch 26, Batch 156/462, Loss: 0.8058962225914001\n",
      "Epoch 26, Batch 157/462, Loss: 0.6698177456855774\n",
      "Epoch 26, Batch 158/462, Loss: 0.6586337089538574\n",
      "Epoch 26, Batch 159/462, Loss: 0.746071994304657\n",
      "Epoch 26, Batch 160/462, Loss: 0.7827882766723633\n",
      "Epoch 26, Batch 161/462, Loss: 0.6783263087272644\n",
      "Epoch 26, Batch 162/462, Loss: 0.7352297306060791\n",
      "Epoch 26, Batch 163/462, Loss: 0.7284886837005615\n",
      "Epoch 26, Batch 164/462, Loss: 0.8141493797302246\n",
      "Epoch 26, Batch 165/462, Loss: 0.6203038096427917\n",
      "Epoch 26, Batch 166/462, Loss: 0.6614577174186707\n",
      "Epoch 26, Batch 167/462, Loss: 0.8136157989501953\n",
      "Epoch 26, Batch 168/462, Loss: 0.723231315612793\n",
      "Epoch 26, Batch 169/462, Loss: 0.7965186834335327\n",
      "Epoch 26, Batch 170/462, Loss: 0.7383391261100769\n",
      "Epoch 26, Batch 171/462, Loss: 0.7618371248245239\n",
      "Epoch 26, Batch 172/462, Loss: 0.6809508204460144\n",
      "Epoch 26, Batch 173/462, Loss: 0.7580713033676147\n",
      "Epoch 26, Batch 174/462, Loss: 0.669752836227417\n",
      "Epoch 26, Batch 175/462, Loss: 0.6673212051391602\n",
      "Epoch 26, Batch 176/462, Loss: 0.8481110334396362\n",
      "Epoch 26, Batch 177/462, Loss: 0.6412792205810547\n",
      "Epoch 26, Batch 178/462, Loss: 0.5611939430236816\n",
      "Epoch 26, Batch 179/462, Loss: 0.6743851900100708\n",
      "Epoch 26, Batch 180/462, Loss: 0.6354514956474304\n",
      "Epoch 26, Batch 181/462, Loss: 0.7658936381340027\n",
      "Epoch 26, Batch 182/462, Loss: 0.8373520970344543\n",
      "Epoch 26, Batch 183/462, Loss: 0.71683669090271\n",
      "Epoch 26, Batch 184/462, Loss: 0.6523846983909607\n",
      "Epoch 26, Batch 185/462, Loss: 0.8303838968276978\n",
      "Epoch 26, Batch 186/462, Loss: 0.9573929309844971\n",
      "Epoch 26, Batch 187/462, Loss: 0.7754087448120117\n",
      "Epoch 26, Batch 188/462, Loss: 0.6670413017272949\n",
      "Epoch 26, Batch 189/462, Loss: 0.6603369116783142\n",
      "Epoch 26, Batch 190/462, Loss: 0.7615317106246948\n",
      "Epoch 26, Batch 191/462, Loss: 0.6833910942077637\n",
      "Epoch 26, Batch 192/462, Loss: 0.8366794586181641\n",
      "Epoch 26, Batch 193/462, Loss: 0.7736265659332275\n",
      "Epoch 26, Batch 194/462, Loss: 0.6585586667060852\n",
      "Epoch 26, Batch 195/462, Loss: 0.546394944190979\n",
      "Epoch 26, Batch 196/462, Loss: 0.55758136510849\n",
      "Epoch 26, Batch 197/462, Loss: 0.6892300844192505\n",
      "Epoch 26, Batch 198/462, Loss: 0.7644994854927063\n",
      "Epoch 26, Batch 199/462, Loss: 0.6453121900558472\n",
      "Epoch 26, Batch 200/462, Loss: 0.7405819892883301\n",
      "Epoch 26, Batch 201/462, Loss: 0.9231038689613342\n",
      "Epoch 26, Batch 202/462, Loss: 0.6553885340690613\n",
      "Epoch 26, Batch 203/462, Loss: 0.669394314289093\n",
      "Epoch 26, Batch 204/462, Loss: 0.7028855085372925\n",
      "Epoch 26, Batch 205/462, Loss: 0.754778265953064\n",
      "Epoch 26, Batch 206/462, Loss: 0.769599199295044\n",
      "Epoch 26, Batch 207/462, Loss: 0.7483139038085938\n",
      "Epoch 26, Batch 208/462, Loss: 0.8228890895843506\n",
      "Epoch 26, Batch 209/462, Loss: 0.7061184048652649\n",
      "Epoch 26, Batch 210/462, Loss: 0.7274425625801086\n",
      "Epoch 26, Batch 211/462, Loss: 0.5908750891685486\n",
      "Epoch 26, Batch 212/462, Loss: 0.928497314453125\n",
      "Epoch 26, Batch 213/462, Loss: 0.8254896998405457\n",
      "Epoch 26, Batch 214/462, Loss: 0.8042991161346436\n",
      "Epoch 26, Batch 215/462, Loss: 0.7906605005264282\n",
      "Epoch 26, Batch 216/462, Loss: 0.7340178489685059\n",
      "Epoch 26, Batch 217/462, Loss: 0.7491618990898132\n",
      "Epoch 26, Batch 218/462, Loss: 0.5613672733306885\n",
      "Epoch 26, Batch 219/462, Loss: 0.663765013217926\n",
      "Epoch 26, Batch 220/462, Loss: 0.6931161880493164\n",
      "Epoch 26, Batch 221/462, Loss: 0.7486107349395752\n",
      "Epoch 26, Batch 222/462, Loss: 0.6240525245666504\n",
      "Epoch 26, Batch 223/462, Loss: 0.7698105573654175\n",
      "Epoch 26, Batch 224/462, Loss: 0.6900300979614258\n",
      "Epoch 26, Batch 225/462, Loss: 0.7230756878852844\n",
      "Epoch 26, Batch 226/462, Loss: 0.9007000923156738\n",
      "Epoch 26, Batch 227/462, Loss: 0.7679623365402222\n",
      "Epoch 26, Batch 228/462, Loss: 0.8095923662185669\n",
      "Epoch 26, Batch 229/462, Loss: 0.6683085560798645\n",
      "Epoch 26, Batch 230/462, Loss: 0.8342154622077942\n",
      "Epoch 26, Batch 231/462, Loss: 0.8020802736282349\n",
      "Epoch 26, Batch 232/462, Loss: 0.6954400539398193\n",
      "Epoch 26, Batch 233/462, Loss: 0.7595645785331726\n",
      "Epoch 26, Batch 234/462, Loss: 0.6929530501365662\n",
      "Epoch 26, Batch 235/462, Loss: 0.6130448579788208\n",
      "Epoch 26, Batch 236/462, Loss: 0.6837786436080933\n",
      "Epoch 26, Batch 237/462, Loss: 0.622262716293335\n",
      "Epoch 26, Batch 238/462, Loss: 0.9030308723449707\n",
      "Epoch 26, Batch 239/462, Loss: 0.7220577001571655\n",
      "Epoch 26, Batch 240/462, Loss: 0.619066596031189\n",
      "Epoch 26, Batch 241/462, Loss: 0.6855779886245728\n",
      "Epoch 26, Batch 242/462, Loss: 0.7399764060974121\n",
      "Epoch 26, Batch 243/462, Loss: 0.855292022228241\n",
      "Epoch 26, Batch 244/462, Loss: 0.6981165409088135\n",
      "Epoch 26, Batch 245/462, Loss: 0.7330596446990967\n",
      "Epoch 26, Batch 246/462, Loss: 0.8177878260612488\n",
      "Epoch 26, Batch 247/462, Loss: 0.7921222448348999\n",
      "Epoch 26, Batch 248/462, Loss: 0.6797980070114136\n",
      "Epoch 26, Batch 249/462, Loss: 0.9726395010948181\n",
      "Epoch 26, Batch 250/462, Loss: 0.7887020111083984\n",
      "Epoch 26, Batch 251/462, Loss: 0.8709298372268677\n",
      "Epoch 26, Batch 252/462, Loss: 0.6182229518890381\n",
      "Epoch 26, Batch 253/462, Loss: 0.7106448411941528\n",
      "Epoch 26, Batch 254/462, Loss: 0.8885835409164429\n",
      "Epoch 26, Batch 255/462, Loss: 0.6675838828086853\n",
      "Epoch 26, Batch 256/462, Loss: 0.704378604888916\n",
      "Epoch 26, Batch 257/462, Loss: 0.7170383334159851\n",
      "Epoch 26, Batch 258/462, Loss: 0.731884241104126\n",
      "Epoch 26, Batch 259/462, Loss: 0.7083542346954346\n",
      "Epoch 26, Batch 260/462, Loss: 0.7459385991096497\n",
      "Epoch 26, Batch 261/462, Loss: 0.565456748008728\n",
      "Epoch 26, Batch 262/462, Loss: 0.8765484094619751\n",
      "Epoch 26, Batch 263/462, Loss: 0.6188902258872986\n",
      "Epoch 26, Batch 264/462, Loss: 0.7291901707649231\n",
      "Epoch 26, Batch 265/462, Loss: 0.7402142286300659\n",
      "Epoch 26, Batch 266/462, Loss: 0.8844117522239685\n",
      "Epoch 26, Batch 267/462, Loss: 0.6916685104370117\n",
      "Epoch 26, Batch 268/462, Loss: 0.769676923751831\n",
      "Epoch 26, Batch 269/462, Loss: 0.6220055222511292\n",
      "Epoch 26, Batch 270/462, Loss: 0.7392828464508057\n",
      "Epoch 26, Batch 271/462, Loss: 0.7050447463989258\n",
      "Epoch 26, Batch 272/462, Loss: 0.7911492586135864\n",
      "Epoch 26, Batch 273/462, Loss: 0.6532835960388184\n",
      "Epoch 26, Batch 274/462, Loss: 0.6114882826805115\n",
      "Epoch 26, Batch 275/462, Loss: 0.793064296245575\n",
      "Epoch 26, Batch 276/462, Loss: 0.6515176892280579\n",
      "Epoch 26, Batch 277/462, Loss: 0.7574030756950378\n",
      "Epoch 26, Batch 278/462, Loss: 0.6850032210350037\n",
      "Epoch 26, Batch 279/462, Loss: 0.5925728678703308\n",
      "Epoch 26, Batch 280/462, Loss: 0.6097736954689026\n",
      "Epoch 26, Batch 281/462, Loss: 0.6070810556411743\n",
      "Epoch 26, Batch 282/462, Loss: 0.7483803033828735\n",
      "Epoch 26, Batch 283/462, Loss: 0.7162780165672302\n",
      "Epoch 26, Batch 284/462, Loss: 0.5784062147140503\n",
      "Epoch 26, Batch 285/462, Loss: 0.7192044258117676\n",
      "Epoch 26, Batch 286/462, Loss: 0.6702994108200073\n",
      "Epoch 26, Batch 287/462, Loss: 0.620939314365387\n",
      "Epoch 26, Batch 288/462, Loss: 0.668300211429596\n",
      "Epoch 26, Batch 289/462, Loss: 0.820865273475647\n",
      "Epoch 26, Batch 290/462, Loss: 0.7706467509269714\n",
      "Epoch 26, Batch 291/462, Loss: 0.8119017481803894\n",
      "Epoch 26, Batch 292/462, Loss: 0.8172727227210999\n",
      "Epoch 26, Batch 293/462, Loss: 0.5962129831314087\n",
      "Epoch 26, Batch 294/462, Loss: 0.845312774181366\n",
      "Epoch 26, Batch 295/462, Loss: 0.774245023727417\n",
      "Epoch 26, Batch 296/462, Loss: 0.6926071047782898\n",
      "Epoch 26, Batch 297/462, Loss: 0.8281495571136475\n",
      "Epoch 26, Batch 298/462, Loss: 0.9499255418777466\n",
      "Epoch 26, Batch 299/462, Loss: 0.6816354393959045\n",
      "Epoch 26, Batch 300/462, Loss: 0.7769356966018677\n",
      "Epoch 26, Batch 301/462, Loss: 0.9677260518074036\n",
      "Epoch 26, Batch 302/462, Loss: 0.7515703439712524\n",
      "Epoch 26, Batch 303/462, Loss: 0.7515495419502258\n",
      "Epoch 26, Batch 304/462, Loss: 0.6996190547943115\n",
      "Epoch 26, Batch 305/462, Loss: 0.8373062610626221\n",
      "Epoch 26, Batch 306/462, Loss: 0.7074518799781799\n",
      "Epoch 26, Batch 307/462, Loss: 0.7143124938011169\n",
      "Epoch 26, Batch 308/462, Loss: 0.6401044130325317\n",
      "Epoch 26, Batch 309/462, Loss: 0.602469801902771\n",
      "Epoch 26, Batch 310/462, Loss: 0.8387129902839661\n",
      "Epoch 26, Batch 311/462, Loss: 0.686987042427063\n",
      "Epoch 26, Batch 312/462, Loss: 0.7256292104721069\n",
      "Epoch 26, Batch 313/462, Loss: 0.6393435001373291\n",
      "Epoch 26, Batch 314/462, Loss: 0.6687918901443481\n",
      "Epoch 26, Batch 315/462, Loss: 0.7579156756401062\n",
      "Epoch 26, Batch 316/462, Loss: 0.7787226438522339\n",
      "Epoch 26, Batch 317/462, Loss: 0.8515651822090149\n",
      "Epoch 26, Batch 318/462, Loss: 0.8245216608047485\n",
      "Epoch 26, Batch 319/462, Loss: 0.8642129302024841\n",
      "Epoch 26, Batch 320/462, Loss: 0.64100182056427\n",
      "Epoch 26, Batch 321/462, Loss: 0.6256974935531616\n",
      "Epoch 26, Batch 322/462, Loss: 0.6872308850288391\n",
      "Epoch 26, Batch 323/462, Loss: 0.6932497024536133\n",
      "Epoch 26, Batch 324/462, Loss: 0.6140444278717041\n",
      "Epoch 26, Batch 325/462, Loss: 0.7840591073036194\n",
      "Epoch 26, Batch 326/462, Loss: 0.6484637260437012\n",
      "Epoch 26, Batch 327/462, Loss: 0.7822076678276062\n",
      "Epoch 26, Batch 328/462, Loss: 0.782626748085022\n",
      "Epoch 26, Batch 329/462, Loss: 0.7178255915641785\n",
      "Epoch 26, Batch 330/462, Loss: 0.5868347883224487\n",
      "Epoch 26, Batch 331/462, Loss: 0.8258280754089355\n",
      "Epoch 26, Batch 332/462, Loss: 0.7091577053070068\n",
      "Epoch 26, Batch 333/462, Loss: 0.6626875400543213\n",
      "Epoch 26, Batch 334/462, Loss: 0.8136405944824219\n",
      "Epoch 26, Batch 335/462, Loss: 0.7137008905410767\n",
      "Epoch 26, Batch 336/462, Loss: 0.6658738255500793\n",
      "Epoch 26, Batch 337/462, Loss: 0.8094600439071655\n",
      "Epoch 26, Batch 338/462, Loss: 0.889227032661438\n",
      "Epoch 26, Batch 339/462, Loss: 0.7681926488876343\n",
      "Epoch 26, Batch 340/462, Loss: 0.8504865765571594\n",
      "Epoch 26, Batch 341/462, Loss: 0.7826303243637085\n",
      "Epoch 26, Batch 342/462, Loss: 0.5828018188476562\n",
      "Epoch 26, Batch 343/462, Loss: 0.7846721410751343\n",
      "Epoch 26, Batch 344/462, Loss: 0.7003962993621826\n",
      "Epoch 26, Batch 345/462, Loss: 0.6124926805496216\n",
      "Epoch 26, Batch 346/462, Loss: 0.7080826163291931\n",
      "Epoch 26, Batch 347/462, Loss: 0.8412010669708252\n",
      "Epoch 26, Batch 348/462, Loss: 0.6227554678916931\n",
      "Epoch 26, Batch 349/462, Loss: 0.7724534869194031\n",
      "Epoch 26, Batch 350/462, Loss: 0.7127227783203125\n",
      "Epoch 26, Batch 351/462, Loss: 0.8090007901191711\n",
      "Epoch 26, Batch 352/462, Loss: 0.682241678237915\n",
      "Epoch 26, Batch 353/462, Loss: 0.6320141553878784\n",
      "Epoch 26, Batch 354/462, Loss: 0.856879711151123\n",
      "Epoch 26, Batch 355/462, Loss: 0.7664010524749756\n",
      "Epoch 26, Batch 356/462, Loss: 0.7827054858207703\n",
      "Epoch 26, Batch 357/462, Loss: 0.7608919739723206\n",
      "Epoch 26, Batch 358/462, Loss: 0.6393194198608398\n",
      "Epoch 26, Batch 359/462, Loss: 0.8035004138946533\n",
      "Epoch 26, Batch 360/462, Loss: 0.7403827905654907\n",
      "Epoch 26, Batch 361/462, Loss: 0.7106336951255798\n",
      "Epoch 26, Batch 362/462, Loss: 0.6698353290557861\n",
      "Epoch 26, Batch 363/462, Loss: 0.6250588893890381\n",
      "Epoch 26, Batch 364/462, Loss: 0.7169798612594604\n",
      "Epoch 26, Batch 365/462, Loss: 0.8128390312194824\n",
      "Epoch 26, Batch 366/462, Loss: 0.8173649907112122\n",
      "Epoch 26, Batch 367/462, Loss: 0.8878465890884399\n",
      "Epoch 26, Batch 368/462, Loss: 0.7850503921508789\n",
      "Epoch 26, Batch 369/462, Loss: 0.7351689338684082\n",
      "Epoch 26, Batch 370/462, Loss: 0.9486474394798279\n",
      "Epoch 26, Batch 371/462, Loss: 0.49868279695510864\n",
      "Epoch 26, Batch 372/462, Loss: 0.6632655262947083\n",
      "Epoch 26, Batch 373/462, Loss: 0.7583550214767456\n",
      "Epoch 26, Batch 374/462, Loss: 0.5810050964355469\n",
      "Epoch 26, Batch 375/462, Loss: 0.7368461489677429\n",
      "Epoch 26, Batch 376/462, Loss: 0.5924615859985352\n",
      "Epoch 26, Batch 377/462, Loss: 0.6270603537559509\n",
      "Epoch 26, Batch 378/462, Loss: 0.8658106327056885\n",
      "Epoch 26, Batch 379/462, Loss: 0.71099853515625\n",
      "Epoch 26, Batch 380/462, Loss: 0.7318345308303833\n",
      "Epoch 26, Batch 381/462, Loss: 0.7955803871154785\n",
      "Epoch 26, Batch 382/462, Loss: 0.5617504715919495\n",
      "Epoch 26, Batch 383/462, Loss: 0.7157103419303894\n",
      "Epoch 26, Batch 384/462, Loss: 0.6899677515029907\n",
      "Epoch 26, Batch 385/462, Loss: 0.6446519494056702\n",
      "Epoch 26, Batch 386/462, Loss: 0.5991889834403992\n",
      "Epoch 26, Batch 387/462, Loss: 0.8116917014122009\n",
      "Epoch 26, Batch 388/462, Loss: 0.6411933898925781\n",
      "Epoch 26, Batch 389/462, Loss: 0.6989685893058777\n",
      "Epoch 26, Batch 390/462, Loss: 0.6430250406265259\n",
      "Epoch 26, Batch 391/462, Loss: 0.8816499710083008\n",
      "Epoch 26, Batch 392/462, Loss: 0.7559512257575989\n",
      "Epoch 26, Batch 393/462, Loss: 0.7588385939598083\n",
      "Epoch 26, Batch 394/462, Loss: 0.6708864569664001\n",
      "Epoch 26, Batch 395/462, Loss: 0.7326343655586243\n",
      "Epoch 26, Batch 396/462, Loss: 0.6566164493560791\n",
      "Epoch 26, Batch 397/462, Loss: 0.5613676309585571\n",
      "Epoch 26, Batch 398/462, Loss: 0.7481415271759033\n",
      "Epoch 26, Batch 399/462, Loss: 0.717592716217041\n",
      "Epoch 26, Batch 400/462, Loss: 0.7610072493553162\n",
      "Epoch 26, Batch 401/462, Loss: 0.983069121837616\n",
      "Epoch 26, Batch 402/462, Loss: 0.8186330199241638\n",
      "Epoch 26, Batch 403/462, Loss: 0.7189253568649292\n",
      "Epoch 26, Batch 404/462, Loss: 0.711105465888977\n",
      "Epoch 26, Batch 405/462, Loss: 0.7923664450645447\n",
      "Epoch 26, Batch 406/462, Loss: 0.8071212768554688\n",
      "Epoch 26, Batch 407/462, Loss: 0.598368763923645\n",
      "Epoch 26, Batch 408/462, Loss: 0.7888597249984741\n",
      "Epoch 26, Batch 409/462, Loss: 0.5912555456161499\n",
      "Epoch 26, Batch 410/462, Loss: 0.8207604289054871\n",
      "Epoch 26, Batch 411/462, Loss: 0.6624545454978943\n",
      "Epoch 26, Batch 412/462, Loss: 0.6777578592300415\n",
      "Epoch 26, Batch 413/462, Loss: 0.6677744388580322\n",
      "Epoch 26, Batch 414/462, Loss: 0.6388258337974548\n",
      "Epoch 26, Batch 415/462, Loss: 0.5998471975326538\n",
      "Epoch 26, Batch 416/462, Loss: 0.6295602917671204\n",
      "Epoch 26, Batch 417/462, Loss: 0.7897101044654846\n",
      "Epoch 26, Batch 418/462, Loss: 0.6486002802848816\n",
      "Epoch 26, Batch 419/462, Loss: 0.7717200517654419\n",
      "Epoch 26, Batch 420/462, Loss: 0.8091132640838623\n",
      "Epoch 26, Batch 421/462, Loss: 0.7574043869972229\n",
      "Epoch 26, Batch 422/462, Loss: 0.7929044365882874\n",
      "Epoch 26, Batch 423/462, Loss: 0.6170612573623657\n",
      "Epoch 26, Batch 424/462, Loss: 0.7175227999687195\n",
      "Epoch 26, Batch 425/462, Loss: 0.7065809369087219\n",
      "Epoch 26, Batch 426/462, Loss: 0.7273097038269043\n",
      "Epoch 26, Batch 427/462, Loss: 0.7262936234474182\n",
      "Epoch 26, Batch 428/462, Loss: 0.77431321144104\n",
      "Epoch 26, Batch 429/462, Loss: 0.6896918416023254\n",
      "Epoch 26, Batch 430/462, Loss: 0.6567180156707764\n",
      "Epoch 26, Batch 431/462, Loss: 0.8289108872413635\n",
      "Epoch 26, Batch 432/462, Loss: 0.836948573589325\n",
      "Epoch 26, Batch 433/462, Loss: 0.4701097905635834\n",
      "Epoch 26, Batch 434/462, Loss: 0.8113464713096619\n",
      "Epoch 26, Batch 435/462, Loss: 0.628896951675415\n",
      "Epoch 26, Batch 436/462, Loss: 0.7579751014709473\n",
      "Epoch 26, Batch 437/462, Loss: 0.7201868295669556\n",
      "Epoch 26, Batch 438/462, Loss: 0.6436877846717834\n",
      "Epoch 26, Batch 439/462, Loss: 0.8996194005012512\n",
      "Epoch 26, Batch 440/462, Loss: 0.7136078476905823\n",
      "Epoch 26, Batch 441/462, Loss: 0.7660401463508606\n",
      "Epoch 26, Batch 442/462, Loss: 0.7142464518547058\n",
      "Epoch 26, Batch 443/462, Loss: 0.6742377281188965\n",
      "Epoch 26, Batch 444/462, Loss: 0.7702370285987854\n",
      "Epoch 26, Batch 445/462, Loss: 0.7016454935073853\n",
      "Epoch 26, Batch 446/462, Loss: 0.7326444983482361\n",
      "Epoch 26, Batch 447/462, Loss: 0.9213181138038635\n",
      "Epoch 26, Batch 448/462, Loss: 0.9324623942375183\n",
      "Epoch 26, Batch 449/462, Loss: 0.7226383090019226\n",
      "Epoch 26, Batch 450/462, Loss: 0.7644230127334595\n",
      "Epoch 26, Batch 451/462, Loss: 0.5610957145690918\n",
      "Epoch 26, Batch 452/462, Loss: 0.7295099496841431\n",
      "Epoch 26, Batch 453/462, Loss: 0.9611921310424805\n",
      "Epoch 26, Batch 454/462, Loss: 0.6131072640419006\n",
      "Epoch 26, Batch 455/462, Loss: 0.6089960336685181\n",
      "Epoch 26, Batch 456/462, Loss: 0.6639696955680847\n",
      "Epoch 26, Batch 457/462, Loss: 0.6100852489471436\n",
      "Epoch 26, Batch 458/462, Loss: 0.7257302403450012\n",
      "Epoch 26, Batch 459/462, Loss: 0.7252717018127441\n",
      "Epoch 26, Batch 460/462, Loss: 0.7118810415267944\n",
      "Epoch 26, Batch 461/462, Loss: 0.7037692666053772\n",
      "Epoch 26, Batch 462/462, Loss: 0.7081663012504578\n",
      "Epoch 26, Loss: 339.81607005000114\n",
      "Epoch 27, Batch 1/462, Loss: 0.7634682655334473\n",
      "Epoch 27, Batch 2/462, Loss: 0.7690132260322571\n",
      "Epoch 27, Batch 3/462, Loss: 0.8090023398399353\n",
      "Epoch 27, Batch 4/462, Loss: 0.8505853414535522\n",
      "Epoch 27, Batch 5/462, Loss: 0.7177664637565613\n",
      "Epoch 27, Batch 6/462, Loss: 0.7897597551345825\n",
      "Epoch 27, Batch 7/462, Loss: 0.8017266392707825\n",
      "Epoch 27, Batch 8/462, Loss: 0.6374601125717163\n",
      "Epoch 27, Batch 9/462, Loss: 0.7022966146469116\n",
      "Epoch 27, Batch 10/462, Loss: 0.7102968096733093\n",
      "Epoch 27, Batch 11/462, Loss: 0.8636174201965332\n",
      "Epoch 27, Batch 12/462, Loss: 0.7407599687576294\n",
      "Epoch 27, Batch 13/462, Loss: 0.7850548028945923\n",
      "Epoch 27, Batch 14/462, Loss: 0.6930917501449585\n",
      "Epoch 27, Batch 15/462, Loss: 0.7261342406272888\n",
      "Epoch 27, Batch 16/462, Loss: 0.7640172839164734\n",
      "Epoch 27, Batch 17/462, Loss: 0.6413610577583313\n",
      "Epoch 27, Batch 18/462, Loss: 0.7063992619514465\n",
      "Epoch 27, Batch 19/462, Loss: 0.7846812605857849\n",
      "Epoch 27, Batch 20/462, Loss: 0.5072710514068604\n",
      "Epoch 27, Batch 21/462, Loss: 0.7823994159698486\n",
      "Epoch 27, Batch 22/462, Loss: 0.5327287316322327\n",
      "Epoch 27, Batch 23/462, Loss: 0.7683932185173035\n",
      "Epoch 27, Batch 24/462, Loss: 0.7945601940155029\n",
      "Epoch 27, Batch 25/462, Loss: 0.7403964996337891\n",
      "Epoch 27, Batch 26/462, Loss: 0.8480516672134399\n",
      "Epoch 27, Batch 27/462, Loss: 0.6181139945983887\n",
      "Epoch 27, Batch 28/462, Loss: 0.7546007037162781\n",
      "Epoch 27, Batch 29/462, Loss: 0.7416188716888428\n",
      "Epoch 27, Batch 30/462, Loss: 0.6115682721138\n",
      "Epoch 27, Batch 31/462, Loss: 0.8929216861724854\n",
      "Epoch 27, Batch 32/462, Loss: 0.6477231383323669\n",
      "Epoch 27, Batch 33/462, Loss: 0.8146126866340637\n",
      "Epoch 27, Batch 34/462, Loss: 0.721931517124176\n",
      "Epoch 27, Batch 35/462, Loss: 0.7913634777069092\n",
      "Epoch 27, Batch 36/462, Loss: 0.7642344236373901\n",
      "Epoch 27, Batch 37/462, Loss: 0.728364109992981\n",
      "Epoch 27, Batch 38/462, Loss: 0.5904555916786194\n",
      "Epoch 27, Batch 39/462, Loss: 0.6820794343948364\n",
      "Epoch 27, Batch 40/462, Loss: 0.5988013744354248\n",
      "Epoch 27, Batch 41/462, Loss: 0.6217094659805298\n",
      "Epoch 27, Batch 42/462, Loss: 0.7528233528137207\n",
      "Epoch 27, Batch 43/462, Loss: 0.8619319796562195\n",
      "Epoch 27, Batch 44/462, Loss: 0.7462341785430908\n",
      "Epoch 27, Batch 45/462, Loss: 0.8459071516990662\n",
      "Epoch 27, Batch 46/462, Loss: 0.8375114798545837\n",
      "Epoch 27, Batch 47/462, Loss: 0.48752516508102417\n",
      "Epoch 27, Batch 48/462, Loss: 0.6908359527587891\n",
      "Epoch 27, Batch 49/462, Loss: 0.7995114922523499\n",
      "Epoch 27, Batch 50/462, Loss: 0.8132535815238953\n",
      "Epoch 27, Batch 51/462, Loss: 0.6822571754455566\n",
      "Epoch 27, Batch 52/462, Loss: 0.6484346985816956\n",
      "Epoch 27, Batch 53/462, Loss: 0.8367636799812317\n",
      "Epoch 27, Batch 54/462, Loss: 0.9155373573303223\n",
      "Epoch 27, Batch 55/462, Loss: 0.7713037133216858\n",
      "Epoch 27, Batch 56/462, Loss: 0.6797990202903748\n",
      "Epoch 27, Batch 57/462, Loss: 0.7593387365341187\n",
      "Epoch 27, Batch 58/462, Loss: 0.7366725206375122\n",
      "Epoch 27, Batch 59/462, Loss: 0.6094894409179688\n",
      "Epoch 27, Batch 60/462, Loss: 0.7292572855949402\n",
      "Epoch 27, Batch 61/462, Loss: 0.7130133509635925\n",
      "Epoch 27, Batch 62/462, Loss: 0.8949869871139526\n",
      "Epoch 27, Batch 63/462, Loss: 0.7754554748535156\n",
      "Epoch 27, Batch 64/462, Loss: 0.7345277070999146\n",
      "Epoch 27, Batch 65/462, Loss: 0.7174777388572693\n",
      "Epoch 27, Batch 66/462, Loss: 0.6357897520065308\n",
      "Epoch 27, Batch 67/462, Loss: 0.6784617900848389\n",
      "Epoch 27, Batch 68/462, Loss: 0.6889231204986572\n",
      "Epoch 27, Batch 69/462, Loss: 0.7205085754394531\n",
      "Epoch 27, Batch 70/462, Loss: 0.7480589151382446\n",
      "Epoch 27, Batch 71/462, Loss: 0.6448416113853455\n",
      "Epoch 27, Batch 72/462, Loss: 0.769572377204895\n",
      "Epoch 27, Batch 73/462, Loss: 0.6125525236129761\n",
      "Epoch 27, Batch 74/462, Loss: 0.788811981678009\n",
      "Epoch 27, Batch 75/462, Loss: 0.7639827132225037\n",
      "Epoch 27, Batch 76/462, Loss: 0.7590534090995789\n",
      "Epoch 27, Batch 77/462, Loss: 0.9355811476707458\n",
      "Epoch 27, Batch 78/462, Loss: 0.68409264087677\n",
      "Epoch 27, Batch 79/462, Loss: 0.7545393705368042\n",
      "Epoch 27, Batch 80/462, Loss: 0.7118226289749146\n",
      "Epoch 27, Batch 81/462, Loss: 0.8478950262069702\n",
      "Epoch 27, Batch 82/462, Loss: 0.8508638143539429\n",
      "Epoch 27, Batch 83/462, Loss: 0.7077510356903076\n",
      "Epoch 27, Batch 84/462, Loss: 0.7912992835044861\n",
      "Epoch 27, Batch 85/462, Loss: 0.669775664806366\n",
      "Epoch 27, Batch 86/462, Loss: 0.7947899103164673\n",
      "Epoch 27, Batch 87/462, Loss: 0.6712839007377625\n",
      "Epoch 27, Batch 88/462, Loss: 0.7982690334320068\n",
      "Epoch 27, Batch 89/462, Loss: 0.8176204562187195\n",
      "Epoch 27, Batch 90/462, Loss: 0.8392667174339294\n",
      "Epoch 27, Batch 91/462, Loss: 0.7340692281723022\n",
      "Epoch 27, Batch 92/462, Loss: 0.7184916138648987\n",
      "Epoch 27, Batch 93/462, Loss: 0.7746908068656921\n",
      "Epoch 27, Batch 94/462, Loss: 0.668289065361023\n",
      "Epoch 27, Batch 95/462, Loss: 0.7982132434844971\n",
      "Epoch 27, Batch 96/462, Loss: 0.8165817260742188\n",
      "Epoch 27, Batch 97/462, Loss: 0.7580305337905884\n",
      "Epoch 27, Batch 98/462, Loss: 0.7717965841293335\n",
      "Epoch 27, Batch 99/462, Loss: 0.7870174050331116\n",
      "Epoch 27, Batch 100/462, Loss: 0.8210536241531372\n",
      "Epoch 27, Batch 101/462, Loss: 0.6875094175338745\n",
      "Epoch 27, Batch 102/462, Loss: 0.7528690695762634\n",
      "Epoch 27, Batch 103/462, Loss: 0.737673282623291\n",
      "Epoch 27, Batch 104/462, Loss: 0.7698695659637451\n",
      "Epoch 27, Batch 105/462, Loss: 0.8224872350692749\n",
      "Epoch 27, Batch 106/462, Loss: 0.7220026254653931\n",
      "Epoch 27, Batch 107/462, Loss: 0.7008071541786194\n",
      "Epoch 27, Batch 108/462, Loss: 0.6116978526115417\n",
      "Epoch 27, Batch 109/462, Loss: 0.6427329182624817\n",
      "Epoch 27, Batch 110/462, Loss: 0.6528177857398987\n",
      "Epoch 27, Batch 111/462, Loss: 0.7970508933067322\n",
      "Epoch 27, Batch 112/462, Loss: 0.9208955764770508\n",
      "Epoch 27, Batch 113/462, Loss: 0.7344371676445007\n",
      "Epoch 27, Batch 114/462, Loss: 0.5744414329528809\n",
      "Epoch 27, Batch 115/462, Loss: 0.8582137823104858\n",
      "Epoch 27, Batch 116/462, Loss: 0.8623742461204529\n",
      "Epoch 27, Batch 117/462, Loss: 0.6851922273635864\n",
      "Epoch 27, Batch 118/462, Loss: 0.7188138961791992\n",
      "Epoch 27, Batch 119/462, Loss: 0.8807506561279297\n",
      "Epoch 27, Batch 120/462, Loss: 0.592861533164978\n",
      "Epoch 27, Batch 121/462, Loss: 0.8290931582450867\n",
      "Epoch 27, Batch 122/462, Loss: 0.7110397815704346\n",
      "Epoch 27, Batch 123/462, Loss: 0.6196330189704895\n",
      "Epoch 27, Batch 124/462, Loss: 0.8104854226112366\n",
      "Epoch 27, Batch 125/462, Loss: 0.6825802326202393\n",
      "Epoch 27, Batch 126/462, Loss: 0.600393533706665\n",
      "Epoch 27, Batch 127/462, Loss: 0.6670877933502197\n",
      "Epoch 27, Batch 128/462, Loss: 0.9129189252853394\n",
      "Epoch 27, Batch 129/462, Loss: 0.7952135801315308\n",
      "Epoch 27, Batch 130/462, Loss: 0.6864305138587952\n",
      "Epoch 27, Batch 131/462, Loss: 0.7889444828033447\n",
      "Epoch 27, Batch 132/462, Loss: 0.6217272877693176\n",
      "Epoch 27, Batch 133/462, Loss: 0.8583921194076538\n",
      "Epoch 27, Batch 134/462, Loss: 0.6935730576515198\n",
      "Epoch 27, Batch 135/462, Loss: 0.7045008540153503\n",
      "Epoch 27, Batch 136/462, Loss: 0.6469650864601135\n",
      "Epoch 27, Batch 137/462, Loss: 0.6055142283439636\n",
      "Epoch 27, Batch 138/462, Loss: 0.8105583786964417\n",
      "Epoch 27, Batch 139/462, Loss: 0.7962585687637329\n",
      "Epoch 27, Batch 140/462, Loss: 0.743283212184906\n",
      "Epoch 27, Batch 141/462, Loss: 0.6767585873603821\n",
      "Epoch 27, Batch 142/462, Loss: 0.6747150421142578\n",
      "Epoch 27, Batch 143/462, Loss: 0.6700076460838318\n",
      "Epoch 27, Batch 144/462, Loss: 0.9776589274406433\n",
      "Epoch 27, Batch 145/462, Loss: 0.7363731861114502\n",
      "Epoch 27, Batch 146/462, Loss: 0.7642792463302612\n",
      "Epoch 27, Batch 147/462, Loss: 0.5959737300872803\n",
      "Epoch 27, Batch 148/462, Loss: 0.6351920366287231\n",
      "Epoch 27, Batch 149/462, Loss: 0.6427958607673645\n",
      "Epoch 27, Batch 150/462, Loss: 0.6663504838943481\n",
      "Epoch 27, Batch 151/462, Loss: 0.6253832578659058\n",
      "Epoch 27, Batch 152/462, Loss: 0.8974210619926453\n",
      "Epoch 27, Batch 153/462, Loss: 0.685312032699585\n",
      "Epoch 27, Batch 154/462, Loss: 0.6869135499000549\n",
      "Epoch 27, Batch 155/462, Loss: 0.6498941779136658\n",
      "Epoch 27, Batch 156/462, Loss: 0.6781666278839111\n",
      "Epoch 27, Batch 157/462, Loss: 0.7351981997489929\n",
      "Epoch 27, Batch 158/462, Loss: 0.8061190843582153\n",
      "Epoch 27, Batch 159/462, Loss: 0.7624009847640991\n",
      "Epoch 27, Batch 160/462, Loss: 0.8343029618263245\n",
      "Epoch 27, Batch 161/462, Loss: 0.7899394631385803\n",
      "Epoch 27, Batch 162/462, Loss: 0.7446505427360535\n",
      "Epoch 27, Batch 163/462, Loss: 0.6053617596626282\n",
      "Epoch 27, Batch 164/462, Loss: 0.5740327835083008\n",
      "Epoch 27, Batch 165/462, Loss: 0.638602614402771\n",
      "Epoch 27, Batch 166/462, Loss: 0.7998325228691101\n",
      "Epoch 27, Batch 167/462, Loss: 0.7881807088851929\n",
      "Epoch 27, Batch 168/462, Loss: 0.8508532643318176\n",
      "Epoch 27, Batch 169/462, Loss: 0.5649508833885193\n",
      "Epoch 27, Batch 170/462, Loss: 0.8417230248451233\n",
      "Epoch 27, Batch 171/462, Loss: 0.773764431476593\n",
      "Epoch 27, Batch 172/462, Loss: 0.633450448513031\n",
      "Epoch 27, Batch 173/462, Loss: 0.6950473785400391\n",
      "Epoch 27, Batch 174/462, Loss: 0.8532014489173889\n",
      "Epoch 27, Batch 175/462, Loss: 0.5112355947494507\n",
      "Epoch 27, Batch 176/462, Loss: 0.73873370885849\n",
      "Epoch 27, Batch 177/462, Loss: 0.9837527871131897\n",
      "Epoch 27, Batch 178/462, Loss: 0.8261029124259949\n",
      "Epoch 27, Batch 179/462, Loss: 0.6307916641235352\n",
      "Epoch 27, Batch 180/462, Loss: 0.6948230266571045\n",
      "Epoch 27, Batch 181/462, Loss: 0.6778366565704346\n",
      "Epoch 27, Batch 182/462, Loss: 0.8261579275131226\n",
      "Epoch 27, Batch 183/462, Loss: 0.6732717156410217\n",
      "Epoch 27, Batch 184/462, Loss: 0.9757170677185059\n",
      "Epoch 27, Batch 185/462, Loss: 0.7472136616706848\n",
      "Epoch 27, Batch 186/462, Loss: 0.8112679719924927\n",
      "Epoch 27, Batch 187/462, Loss: 0.6615021228790283\n",
      "Epoch 27, Batch 188/462, Loss: 0.568463921546936\n",
      "Epoch 27, Batch 189/462, Loss: 0.6162123084068298\n",
      "Epoch 27, Batch 190/462, Loss: 0.8911737203598022\n",
      "Epoch 27, Batch 191/462, Loss: 0.8378470540046692\n",
      "Epoch 27, Batch 192/462, Loss: 0.6701145768165588\n",
      "Epoch 27, Batch 193/462, Loss: 0.7155258059501648\n",
      "Epoch 27, Batch 194/462, Loss: 0.8275341987609863\n",
      "Epoch 27, Batch 195/462, Loss: 0.7046094536781311\n",
      "Epoch 27, Batch 196/462, Loss: 0.8069542050361633\n",
      "Epoch 27, Batch 197/462, Loss: 0.7869122624397278\n",
      "Epoch 27, Batch 198/462, Loss: 0.7485786080360413\n",
      "Epoch 27, Batch 199/462, Loss: 0.8071970343589783\n",
      "Epoch 27, Batch 200/462, Loss: 0.6045784950256348\n",
      "Epoch 27, Batch 201/462, Loss: 0.7494944334030151\n",
      "Epoch 27, Batch 202/462, Loss: 1.025170087814331\n",
      "Epoch 27, Batch 203/462, Loss: 0.646751880645752\n",
      "Epoch 27, Batch 204/462, Loss: 0.8075141310691833\n",
      "Epoch 27, Batch 205/462, Loss: 0.8934497237205505\n",
      "Epoch 27, Batch 206/462, Loss: 0.7258886694908142\n",
      "Epoch 27, Batch 207/462, Loss: 0.6086495518684387\n",
      "Epoch 27, Batch 208/462, Loss: 0.6238135099411011\n",
      "Epoch 27, Batch 209/462, Loss: 0.7668101787567139\n",
      "Epoch 27, Batch 210/462, Loss: 0.8111516237258911\n",
      "Epoch 27, Batch 211/462, Loss: 0.6745299100875854\n",
      "Epoch 27, Batch 212/462, Loss: 0.8628208041191101\n",
      "Epoch 27, Batch 213/462, Loss: 0.8483612537384033\n",
      "Epoch 27, Batch 214/462, Loss: 0.6063222885131836\n",
      "Epoch 27, Batch 215/462, Loss: 0.7182083129882812\n",
      "Epoch 27, Batch 216/462, Loss: 0.5730971097946167\n",
      "Epoch 27, Batch 217/462, Loss: 0.8156270980834961\n",
      "Epoch 27, Batch 218/462, Loss: 0.4948199391365051\n",
      "Epoch 27, Batch 219/462, Loss: 0.5721644163131714\n",
      "Epoch 27, Batch 220/462, Loss: 0.8523852825164795\n",
      "Epoch 27, Batch 221/462, Loss: 0.9213000535964966\n",
      "Epoch 27, Batch 222/462, Loss: 0.6345863342285156\n",
      "Epoch 27, Batch 223/462, Loss: 0.9073660373687744\n",
      "Epoch 27, Batch 224/462, Loss: 0.828724205493927\n",
      "Epoch 27, Batch 225/462, Loss: 0.7818203568458557\n",
      "Epoch 27, Batch 226/462, Loss: 0.6699368953704834\n",
      "Epoch 27, Batch 227/462, Loss: 0.9279976487159729\n",
      "Epoch 27, Batch 228/462, Loss: 0.5416662096977234\n",
      "Epoch 27, Batch 229/462, Loss: 0.7376759648323059\n",
      "Epoch 27, Batch 230/462, Loss: 0.7153383493423462\n",
      "Epoch 27, Batch 231/462, Loss: 0.5861429572105408\n",
      "Epoch 27, Batch 232/462, Loss: 0.7912214994430542\n",
      "Epoch 27, Batch 233/462, Loss: 0.7084983587265015\n",
      "Epoch 27, Batch 234/462, Loss: 0.8953021168708801\n",
      "Epoch 27, Batch 235/462, Loss: 0.6624385714530945\n",
      "Epoch 27, Batch 236/462, Loss: 0.7950711250305176\n",
      "Epoch 27, Batch 237/462, Loss: 0.669743537902832\n",
      "Epoch 27, Batch 238/462, Loss: 0.7424429655075073\n",
      "Epoch 27, Batch 239/462, Loss: 0.6602412462234497\n",
      "Epoch 27, Batch 240/462, Loss: 0.9685556292533875\n",
      "Epoch 27, Batch 241/462, Loss: 0.6929172277450562\n",
      "Epoch 27, Batch 242/462, Loss: 0.7280821204185486\n",
      "Epoch 27, Batch 243/462, Loss: 0.6344334483146667\n",
      "Epoch 27, Batch 244/462, Loss: 0.6214508414268494\n",
      "Epoch 27, Batch 245/462, Loss: 0.6988838315010071\n",
      "Epoch 27, Batch 246/462, Loss: 0.7210965156555176\n",
      "Epoch 27, Batch 247/462, Loss: 0.8357876539230347\n",
      "Epoch 27, Batch 248/462, Loss: 0.7418922781944275\n",
      "Epoch 27, Batch 249/462, Loss: 0.8041657209396362\n",
      "Epoch 27, Batch 250/462, Loss: 0.6749981045722961\n",
      "Epoch 27, Batch 251/462, Loss: 0.7824810147285461\n",
      "Epoch 27, Batch 252/462, Loss: 0.7613459229469299\n",
      "Epoch 27, Batch 253/462, Loss: 0.7205793261528015\n",
      "Epoch 27, Batch 254/462, Loss: 0.8152867555618286\n",
      "Epoch 27, Batch 255/462, Loss: 0.7081934213638306\n",
      "Epoch 27, Batch 256/462, Loss: 0.8521895408630371\n",
      "Epoch 27, Batch 257/462, Loss: 0.7579708099365234\n",
      "Epoch 27, Batch 258/462, Loss: 0.7792943716049194\n",
      "Epoch 27, Batch 259/462, Loss: 0.8105435371398926\n",
      "Epoch 27, Batch 260/462, Loss: 0.8051444888114929\n",
      "Epoch 27, Batch 261/462, Loss: 0.8058611750602722\n",
      "Epoch 27, Batch 262/462, Loss: 0.7620304226875305\n",
      "Epoch 27, Batch 263/462, Loss: 0.719377875328064\n",
      "Epoch 27, Batch 264/462, Loss: 0.658296525478363\n",
      "Epoch 27, Batch 265/462, Loss: 0.7088727355003357\n",
      "Epoch 27, Batch 266/462, Loss: 0.8093236684799194\n",
      "Epoch 27, Batch 267/462, Loss: 0.6926653385162354\n",
      "Epoch 27, Batch 268/462, Loss: 0.635542631149292\n",
      "Epoch 27, Batch 269/462, Loss: 0.7864400148391724\n",
      "Epoch 27, Batch 270/462, Loss: 0.6991589665412903\n",
      "Epoch 27, Batch 271/462, Loss: 0.7839437127113342\n",
      "Epoch 27, Batch 272/462, Loss: 0.6476379632949829\n",
      "Epoch 27, Batch 273/462, Loss: 0.7540425062179565\n",
      "Epoch 27, Batch 274/462, Loss: 0.7147254347801208\n",
      "Epoch 27, Batch 275/462, Loss: 0.7744636535644531\n",
      "Epoch 27, Batch 276/462, Loss: 0.8171427845954895\n",
      "Epoch 27, Batch 277/462, Loss: 0.8600783348083496\n",
      "Epoch 27, Batch 278/462, Loss: 0.6982976794242859\n",
      "Epoch 27, Batch 279/462, Loss: 0.7738757729530334\n",
      "Epoch 27, Batch 280/462, Loss: 0.7223258018493652\n",
      "Epoch 27, Batch 281/462, Loss: 0.7801442742347717\n",
      "Epoch 27, Batch 282/462, Loss: 0.6880401372909546\n",
      "Epoch 27, Batch 283/462, Loss: 0.8116213083267212\n",
      "Epoch 27, Batch 284/462, Loss: 0.6708608865737915\n",
      "Epoch 27, Batch 285/462, Loss: 0.6298053860664368\n",
      "Epoch 27, Batch 286/462, Loss: 0.7356879115104675\n",
      "Epoch 27, Batch 287/462, Loss: 0.7058587074279785\n",
      "Epoch 27, Batch 288/462, Loss: 0.668130099773407\n",
      "Epoch 27, Batch 289/462, Loss: 0.8040733337402344\n",
      "Epoch 27, Batch 290/462, Loss: 0.8361425399780273\n",
      "Epoch 27, Batch 291/462, Loss: 0.7241436243057251\n",
      "Epoch 27, Batch 292/462, Loss: 0.5756574869155884\n",
      "Epoch 27, Batch 293/462, Loss: 0.7563868761062622\n",
      "Epoch 27, Batch 294/462, Loss: 0.9950118064880371\n",
      "Epoch 27, Batch 295/462, Loss: 0.7412281632423401\n",
      "Epoch 27, Batch 296/462, Loss: 0.7397544980049133\n",
      "Epoch 27, Batch 297/462, Loss: 0.7760735154151917\n",
      "Epoch 27, Batch 298/462, Loss: 0.6938982605934143\n",
      "Epoch 27, Batch 299/462, Loss: 0.6417847871780396\n",
      "Epoch 27, Batch 300/462, Loss: 0.6707977652549744\n",
      "Epoch 27, Batch 301/462, Loss: 0.5887607336044312\n",
      "Epoch 27, Batch 302/462, Loss: 0.6229556798934937\n",
      "Epoch 27, Batch 303/462, Loss: 0.5654175877571106\n",
      "Epoch 27, Batch 304/462, Loss: 0.7417744398117065\n",
      "Epoch 27, Batch 305/462, Loss: 0.7071660757064819\n",
      "Epoch 27, Batch 306/462, Loss: 0.6253350973129272\n",
      "Epoch 27, Batch 307/462, Loss: 0.6960026621818542\n",
      "Epoch 27, Batch 308/462, Loss: 0.681666374206543\n",
      "Epoch 27, Batch 309/462, Loss: 0.7506893873214722\n",
      "Epoch 27, Batch 310/462, Loss: 0.8131712675094604\n",
      "Epoch 27, Batch 311/462, Loss: 0.7034568786621094\n",
      "Epoch 27, Batch 312/462, Loss: 0.7727857232093811\n",
      "Epoch 27, Batch 313/462, Loss: 0.7937615513801575\n",
      "Epoch 27, Batch 314/462, Loss: 0.7433382868766785\n",
      "Epoch 27, Batch 315/462, Loss: 0.691958487033844\n",
      "Epoch 27, Batch 316/462, Loss: 0.6123000979423523\n",
      "Epoch 27, Batch 317/462, Loss: 0.732138991355896\n",
      "Epoch 27, Batch 318/462, Loss: 0.8546098470687866\n",
      "Epoch 27, Batch 319/462, Loss: 0.8613303303718567\n",
      "Epoch 27, Batch 320/462, Loss: 0.7405871152877808\n",
      "Epoch 27, Batch 321/462, Loss: 0.8334307670593262\n",
      "Epoch 27, Batch 322/462, Loss: 0.7991239428520203\n",
      "Epoch 27, Batch 323/462, Loss: 0.6385853290557861\n",
      "Epoch 27, Batch 324/462, Loss: 0.7647117972373962\n",
      "Epoch 27, Batch 325/462, Loss: 0.6095327138900757\n",
      "Epoch 27, Batch 326/462, Loss: 0.7770695686340332\n",
      "Epoch 27, Batch 327/462, Loss: 0.7958899140357971\n",
      "Epoch 27, Batch 328/462, Loss: 0.7042725086212158\n",
      "Epoch 27, Batch 329/462, Loss: 0.8762215971946716\n",
      "Epoch 27, Batch 330/462, Loss: 0.7250539064407349\n",
      "Epoch 27, Batch 331/462, Loss: 0.9034363627433777\n",
      "Epoch 27, Batch 332/462, Loss: 0.5907377004623413\n",
      "Epoch 27, Batch 333/462, Loss: 0.7124002575874329\n",
      "Epoch 27, Batch 334/462, Loss: 0.8208877444267273\n",
      "Epoch 27, Batch 335/462, Loss: 0.6642882227897644\n",
      "Epoch 27, Batch 336/462, Loss: 0.7345000505447388\n",
      "Epoch 27, Batch 337/462, Loss: 0.8475951552391052\n",
      "Epoch 27, Batch 338/462, Loss: 0.823530912399292\n",
      "Epoch 27, Batch 339/462, Loss: 0.6800756454467773\n",
      "Epoch 27, Batch 340/462, Loss: 0.6349877715110779\n",
      "Epoch 27, Batch 341/462, Loss: 0.7893108129501343\n",
      "Epoch 27, Batch 342/462, Loss: 0.6938179731369019\n",
      "Epoch 27, Batch 343/462, Loss: 0.6880819797515869\n",
      "Epoch 27, Batch 344/462, Loss: 0.8140408396720886\n",
      "Epoch 27, Batch 345/462, Loss: 0.8811519742012024\n",
      "Epoch 27, Batch 346/462, Loss: 0.6342616677284241\n",
      "Epoch 27, Batch 347/462, Loss: 0.8580430746078491\n",
      "Epoch 27, Batch 348/462, Loss: 0.6347652673721313\n",
      "Epoch 27, Batch 349/462, Loss: 0.6445950269699097\n",
      "Epoch 27, Batch 350/462, Loss: 0.7074894309043884\n",
      "Epoch 27, Batch 351/462, Loss: 0.7215395569801331\n",
      "Epoch 27, Batch 352/462, Loss: 0.9615477323532104\n",
      "Epoch 27, Batch 353/462, Loss: 0.8530260324478149\n",
      "Epoch 27, Batch 354/462, Loss: 0.7496023774147034\n",
      "Epoch 27, Batch 355/462, Loss: 0.7541052103042603\n",
      "Epoch 27, Batch 356/462, Loss: 0.7833420634269714\n",
      "Epoch 27, Batch 357/462, Loss: 0.848987340927124\n",
      "Epoch 27, Batch 358/462, Loss: 0.6679461002349854\n",
      "Epoch 27, Batch 359/462, Loss: 0.6708479523658752\n",
      "Epoch 27, Batch 360/462, Loss: 0.6697002053260803\n",
      "Epoch 27, Batch 361/462, Loss: 0.8862613439559937\n",
      "Epoch 27, Batch 362/462, Loss: 0.63396817445755\n",
      "Epoch 27, Batch 363/462, Loss: 0.6119000315666199\n",
      "Epoch 27, Batch 364/462, Loss: 0.6921460032463074\n",
      "Epoch 27, Batch 365/462, Loss: 0.8750386834144592\n",
      "Epoch 27, Batch 366/462, Loss: 0.604995846748352\n",
      "Epoch 27, Batch 367/462, Loss: 0.8068261742591858\n",
      "Epoch 27, Batch 368/462, Loss: 0.7931088805198669\n",
      "Epoch 27, Batch 369/462, Loss: 0.7040208578109741\n",
      "Epoch 27, Batch 370/462, Loss: 0.9350185990333557\n",
      "Epoch 27, Batch 371/462, Loss: 0.8222026824951172\n",
      "Epoch 27, Batch 372/462, Loss: 0.895331621170044\n",
      "Epoch 27, Batch 373/462, Loss: 0.6995905041694641\n",
      "Epoch 27, Batch 374/462, Loss: 0.6936357617378235\n",
      "Epoch 27, Batch 375/462, Loss: 0.6205680966377258\n",
      "Epoch 27, Batch 376/462, Loss: 0.9473364949226379\n",
      "Epoch 27, Batch 377/462, Loss: 0.8705002069473267\n",
      "Epoch 27, Batch 378/462, Loss: 0.8316634893417358\n",
      "Epoch 27, Batch 379/462, Loss: 0.7672955393791199\n",
      "Epoch 27, Batch 380/462, Loss: 0.5866456031799316\n",
      "Epoch 27, Batch 381/462, Loss: 0.6622741222381592\n",
      "Epoch 27, Batch 382/462, Loss: 0.8585358262062073\n",
      "Epoch 27, Batch 383/462, Loss: 0.6824320554733276\n",
      "Epoch 27, Batch 384/462, Loss: 0.7620207667350769\n",
      "Epoch 27, Batch 385/462, Loss: 0.680860698223114\n",
      "Epoch 27, Batch 386/462, Loss: 0.8001192212104797\n",
      "Epoch 27, Batch 387/462, Loss: 0.7563250064849854\n",
      "Epoch 27, Batch 388/462, Loss: 0.6458351612091064\n",
      "Epoch 27, Batch 389/462, Loss: 0.6822336912155151\n",
      "Epoch 27, Batch 390/462, Loss: 0.6700243353843689\n",
      "Epoch 27, Batch 391/462, Loss: 0.7095204591751099\n",
      "Epoch 27, Batch 392/462, Loss: 0.671770453453064\n",
      "Epoch 27, Batch 393/462, Loss: 0.7608824968338013\n",
      "Epoch 27, Batch 394/462, Loss: 0.6892397403717041\n",
      "Epoch 27, Batch 395/462, Loss: 0.7442010641098022\n",
      "Epoch 27, Batch 396/462, Loss: 0.7530235648155212\n",
      "Epoch 27, Batch 397/462, Loss: 0.8377826809883118\n",
      "Epoch 27, Batch 398/462, Loss: 0.7207298278808594\n",
      "Epoch 27, Batch 399/462, Loss: 0.6878110766410828\n",
      "Epoch 27, Batch 400/462, Loss: 0.8516858816146851\n",
      "Epoch 27, Batch 401/462, Loss: 0.7569637894630432\n",
      "Epoch 27, Batch 402/462, Loss: 0.8211113810539246\n",
      "Epoch 27, Batch 403/462, Loss: 0.9248718619346619\n",
      "Epoch 27, Batch 404/462, Loss: 0.7545148134231567\n",
      "Epoch 27, Batch 405/462, Loss: 0.8210036158561707\n",
      "Epoch 27, Batch 406/462, Loss: 0.7616186738014221\n",
      "Epoch 27, Batch 407/462, Loss: 0.6524803638458252\n",
      "Epoch 27, Batch 408/462, Loss: 0.6973297595977783\n",
      "Epoch 27, Batch 409/462, Loss: 0.8258942365646362\n",
      "Epoch 27, Batch 410/462, Loss: 0.6128723621368408\n",
      "Epoch 27, Batch 411/462, Loss: 0.6370936036109924\n",
      "Epoch 27, Batch 412/462, Loss: 0.817184329032898\n",
      "Epoch 27, Batch 413/462, Loss: 0.6492173671722412\n",
      "Epoch 27, Batch 414/462, Loss: 0.7975931167602539\n",
      "Epoch 27, Batch 415/462, Loss: 0.7151615619659424\n",
      "Epoch 27, Batch 416/462, Loss: 0.5544747710227966\n",
      "Epoch 27, Batch 417/462, Loss: 0.7935551404953003\n",
      "Epoch 27, Batch 418/462, Loss: 0.7375198602676392\n",
      "Epoch 27, Batch 419/462, Loss: 0.7248327136039734\n",
      "Epoch 27, Batch 420/462, Loss: 0.7984098792076111\n",
      "Epoch 27, Batch 421/462, Loss: 0.8200245499610901\n",
      "Epoch 27, Batch 422/462, Loss: 0.7989150881767273\n",
      "Epoch 27, Batch 423/462, Loss: 0.7631212472915649\n",
      "Epoch 27, Batch 424/462, Loss: 0.6434558629989624\n",
      "Epoch 27, Batch 425/462, Loss: 0.5942686796188354\n",
      "Epoch 27, Batch 426/462, Loss: 0.7285762429237366\n",
      "Epoch 27, Batch 427/462, Loss: 0.786068320274353\n",
      "Epoch 27, Batch 428/462, Loss: 0.820978581905365\n",
      "Epoch 27, Batch 429/462, Loss: 0.5665045380592346\n",
      "Epoch 27, Batch 430/462, Loss: 0.8143779635429382\n",
      "Epoch 27, Batch 431/462, Loss: 0.7646357417106628\n",
      "Epoch 27, Batch 432/462, Loss: 0.6564407348632812\n",
      "Epoch 27, Batch 433/462, Loss: 0.8016945123672485\n",
      "Epoch 27, Batch 434/462, Loss: 0.5770523548126221\n",
      "Epoch 27, Batch 435/462, Loss: 0.7224500775337219\n",
      "Epoch 27, Batch 436/462, Loss: 0.8566569089889526\n",
      "Epoch 27, Batch 437/462, Loss: 0.7088226675987244\n",
      "Epoch 27, Batch 438/462, Loss: 0.6340369582176208\n",
      "Epoch 27, Batch 439/462, Loss: 0.9084208011627197\n",
      "Epoch 27, Batch 440/462, Loss: 0.7259092926979065\n",
      "Epoch 27, Batch 441/462, Loss: 0.5565916895866394\n",
      "Epoch 27, Batch 442/462, Loss: 0.6568124294281006\n",
      "Epoch 27, Batch 443/462, Loss: 0.7313511371612549\n",
      "Epoch 27, Batch 444/462, Loss: 0.868431568145752\n",
      "Epoch 27, Batch 445/462, Loss: 0.6814233660697937\n",
      "Epoch 27, Batch 446/462, Loss: 0.6641464829444885\n",
      "Epoch 27, Batch 447/462, Loss: 0.6554702520370483\n",
      "Epoch 27, Batch 448/462, Loss: 0.5738294124603271\n",
      "Epoch 27, Batch 449/462, Loss: 0.7595646381378174\n",
      "Epoch 27, Batch 450/462, Loss: 0.6499113440513611\n",
      "Epoch 27, Batch 451/462, Loss: 0.7819958329200745\n",
      "Epoch 27, Batch 452/462, Loss: 0.8134782314300537\n",
      "Epoch 27, Batch 453/462, Loss: 0.7903093099594116\n",
      "Epoch 27, Batch 454/462, Loss: 0.8098905682563782\n",
      "Epoch 27, Batch 455/462, Loss: 0.6669489145278931\n",
      "Epoch 27, Batch 456/462, Loss: 0.7097041606903076\n",
      "Epoch 27, Batch 457/462, Loss: 0.6235483884811401\n",
      "Epoch 27, Batch 458/462, Loss: 0.6936390399932861\n",
      "Epoch 27, Batch 459/462, Loss: 0.7554421424865723\n",
      "Epoch 27, Batch 460/462, Loss: 0.7399783134460449\n",
      "Epoch 27, Batch 461/462, Loss: 0.7903084754943848\n",
      "Epoch 27, Batch 462/462, Loss: 0.7417582273483276\n",
      "Epoch 27, Loss: 341.05968421697617\n",
      "Epoch 28, Batch 1/462, Loss: 0.6696168780326843\n",
      "Epoch 28, Batch 2/462, Loss: 0.6199787259101868\n",
      "Epoch 28, Batch 3/462, Loss: 0.6598308682441711\n",
      "Epoch 28, Batch 4/462, Loss: 0.6076050400733948\n",
      "Epoch 28, Batch 5/462, Loss: 0.7079072594642639\n",
      "Epoch 28, Batch 6/462, Loss: 0.7856367826461792\n",
      "Epoch 28, Batch 7/462, Loss: 0.7923079133033752\n",
      "Epoch 28, Batch 8/462, Loss: 0.6330450177192688\n",
      "Epoch 28, Batch 9/462, Loss: 0.7329285144805908\n",
      "Epoch 28, Batch 10/462, Loss: 0.8853437900543213\n",
      "Epoch 28, Batch 11/462, Loss: 0.7621147632598877\n",
      "Epoch 28, Batch 12/462, Loss: 0.6630798578262329\n",
      "Epoch 28, Batch 13/462, Loss: 0.8249465227127075\n",
      "Epoch 28, Batch 14/462, Loss: 0.6839956641197205\n",
      "Epoch 28, Batch 15/462, Loss: 0.7500030994415283\n",
      "Epoch 28, Batch 16/462, Loss: 0.7341616749763489\n",
      "Epoch 28, Batch 17/462, Loss: 0.8038239479064941\n",
      "Epoch 28, Batch 18/462, Loss: 0.7125904560089111\n",
      "Epoch 28, Batch 19/462, Loss: 0.7381195425987244\n",
      "Epoch 28, Batch 20/462, Loss: 0.7550272941589355\n",
      "Epoch 28, Batch 21/462, Loss: 0.7717007398605347\n",
      "Epoch 28, Batch 22/462, Loss: 0.621232271194458\n",
      "Epoch 28, Batch 23/462, Loss: 0.570635199546814\n",
      "Epoch 28, Batch 24/462, Loss: 0.696488618850708\n",
      "Epoch 28, Batch 25/462, Loss: 0.7692645192146301\n",
      "Epoch 28, Batch 26/462, Loss: 0.8440790176391602\n",
      "Epoch 28, Batch 27/462, Loss: 0.6744928359985352\n",
      "Epoch 28, Batch 28/462, Loss: 0.7294483780860901\n",
      "Epoch 28, Batch 29/462, Loss: 0.6984458565711975\n",
      "Epoch 28, Batch 30/462, Loss: 0.5433635711669922\n",
      "Epoch 28, Batch 31/462, Loss: 0.6658433079719543\n",
      "Epoch 28, Batch 32/462, Loss: 0.6444962024688721\n",
      "Epoch 28, Batch 33/462, Loss: 0.7156025171279907\n",
      "Epoch 28, Batch 34/462, Loss: 0.6550382971763611\n",
      "Epoch 28, Batch 35/462, Loss: 0.6384990215301514\n",
      "Epoch 28, Batch 36/462, Loss: 0.769291341304779\n",
      "Epoch 28, Batch 37/462, Loss: 0.6091652512550354\n",
      "Epoch 28, Batch 38/462, Loss: 0.6916185021400452\n",
      "Epoch 28, Batch 39/462, Loss: 0.7974140644073486\n",
      "Epoch 28, Batch 40/462, Loss: 0.6550537347793579\n",
      "Epoch 28, Batch 41/462, Loss: 0.6538336277008057\n",
      "Epoch 28, Batch 42/462, Loss: 0.6687167882919312\n",
      "Epoch 28, Batch 43/462, Loss: 0.6421335935592651\n",
      "Epoch 28, Batch 44/462, Loss: 0.7452818155288696\n",
      "Epoch 28, Batch 45/462, Loss: 0.6539613008499146\n",
      "Epoch 28, Batch 46/462, Loss: 0.6866670846939087\n",
      "Epoch 28, Batch 47/462, Loss: 0.7448852062225342\n",
      "Epoch 28, Batch 48/462, Loss: 0.761679470539093\n",
      "Epoch 28, Batch 49/462, Loss: 0.8136917948722839\n",
      "Epoch 28, Batch 50/462, Loss: 0.8609176278114319\n",
      "Epoch 28, Batch 51/462, Loss: 0.7940325140953064\n",
      "Epoch 28, Batch 52/462, Loss: 0.685026228427887\n",
      "Epoch 28, Batch 53/462, Loss: 0.678607165813446\n",
      "Epoch 28, Batch 54/462, Loss: 0.7402264475822449\n",
      "Epoch 28, Batch 55/462, Loss: 0.7794512510299683\n",
      "Epoch 28, Batch 56/462, Loss: 0.6592894196510315\n",
      "Epoch 28, Batch 57/462, Loss: 0.8404565453529358\n",
      "Epoch 28, Batch 58/462, Loss: 0.7461358904838562\n",
      "Epoch 28, Batch 59/462, Loss: 0.6257562637329102\n",
      "Epoch 28, Batch 60/462, Loss: 0.9539801478385925\n",
      "Epoch 28, Batch 61/462, Loss: 0.8795729875564575\n",
      "Epoch 28, Batch 62/462, Loss: 0.7351714968681335\n",
      "Epoch 28, Batch 63/462, Loss: 0.7576786279678345\n",
      "Epoch 28, Batch 64/462, Loss: 0.635208249092102\n",
      "Epoch 28, Batch 65/462, Loss: 0.6943268775939941\n",
      "Epoch 28, Batch 66/462, Loss: 0.6385682225227356\n",
      "Epoch 28, Batch 67/462, Loss: 0.7191102504730225\n",
      "Epoch 28, Batch 68/462, Loss: 0.7671471238136292\n",
      "Epoch 28, Batch 69/462, Loss: 0.804208517074585\n",
      "Epoch 28, Batch 70/462, Loss: 0.9517804384231567\n",
      "Epoch 28, Batch 71/462, Loss: 0.6770181655883789\n",
      "Epoch 28, Batch 72/462, Loss: 0.5703327059745789\n",
      "Epoch 28, Batch 73/462, Loss: 0.8512418866157532\n",
      "Epoch 28, Batch 74/462, Loss: 0.7318498492240906\n",
      "Epoch 28, Batch 75/462, Loss: 0.6532052755355835\n",
      "Epoch 28, Batch 76/462, Loss: 0.7370956540107727\n",
      "Epoch 28, Batch 77/462, Loss: 0.8475984334945679\n",
      "Epoch 28, Batch 78/462, Loss: 0.8155617117881775\n",
      "Epoch 28, Batch 79/462, Loss: 0.5384942889213562\n",
      "Epoch 28, Batch 80/462, Loss: 0.7594225406646729\n",
      "Epoch 28, Batch 81/462, Loss: 0.7489622831344604\n",
      "Epoch 28, Batch 82/462, Loss: 0.6057010293006897\n",
      "Epoch 28, Batch 83/462, Loss: 0.6758273839950562\n",
      "Epoch 28, Batch 84/462, Loss: 0.7723321914672852\n",
      "Epoch 28, Batch 85/462, Loss: 0.666897714138031\n",
      "Epoch 28, Batch 86/462, Loss: 0.7459024786949158\n",
      "Epoch 28, Batch 87/462, Loss: 0.5663246512413025\n",
      "Epoch 28, Batch 88/462, Loss: 0.7604681253433228\n",
      "Epoch 28, Batch 89/462, Loss: 0.8037204742431641\n",
      "Epoch 28, Batch 90/462, Loss: 0.7451921701431274\n",
      "Epoch 28, Batch 91/462, Loss: 0.6719011664390564\n",
      "Epoch 28, Batch 92/462, Loss: 0.6603494882583618\n",
      "Epoch 28, Batch 93/462, Loss: 0.858370304107666\n",
      "Epoch 28, Batch 94/462, Loss: 0.5952971577644348\n",
      "Epoch 28, Batch 95/462, Loss: 1.0058059692382812\n",
      "Epoch 28, Batch 96/462, Loss: 0.6668364405632019\n",
      "Epoch 28, Batch 97/462, Loss: 0.6146060824394226\n",
      "Epoch 28, Batch 98/462, Loss: 0.690659761428833\n",
      "Epoch 28, Batch 99/462, Loss: 0.8787052035331726\n",
      "Epoch 28, Batch 100/462, Loss: 0.7029061913490295\n",
      "Epoch 28, Batch 101/462, Loss: 0.7522856593132019\n",
      "Epoch 28, Batch 102/462, Loss: 0.7065389752388\n",
      "Epoch 28, Batch 103/462, Loss: 0.736711859703064\n",
      "Epoch 28, Batch 104/462, Loss: 0.7471507787704468\n",
      "Epoch 28, Batch 105/462, Loss: 0.8093791007995605\n",
      "Epoch 28, Batch 106/462, Loss: 0.7934315204620361\n",
      "Epoch 28, Batch 107/462, Loss: 0.8304296731948853\n",
      "Epoch 28, Batch 108/462, Loss: 0.6648426055908203\n",
      "Epoch 28, Batch 109/462, Loss: 0.6051098704338074\n",
      "Epoch 28, Batch 110/462, Loss: 0.7329946756362915\n",
      "Epoch 28, Batch 111/462, Loss: 0.7689474821090698\n",
      "Epoch 28, Batch 112/462, Loss: 0.6867019534111023\n",
      "Epoch 28, Batch 113/462, Loss: 0.8312957882881165\n",
      "Epoch 28, Batch 114/462, Loss: 0.6844242215156555\n",
      "Epoch 28, Batch 115/462, Loss: 0.7366959452629089\n",
      "Epoch 28, Batch 116/462, Loss: 0.8365419507026672\n",
      "Epoch 28, Batch 117/462, Loss: 0.7372964024543762\n",
      "Epoch 28, Batch 118/462, Loss: 0.7001641988754272\n",
      "Epoch 28, Batch 119/462, Loss: 0.8624363541603088\n",
      "Epoch 28, Batch 120/462, Loss: 0.7313373684883118\n",
      "Epoch 28, Batch 121/462, Loss: 0.7334861159324646\n",
      "Epoch 28, Batch 122/462, Loss: 0.7818763852119446\n",
      "Epoch 28, Batch 123/462, Loss: 0.7249962687492371\n",
      "Epoch 28, Batch 124/462, Loss: 0.8327934145927429\n",
      "Epoch 28, Batch 125/462, Loss: 0.7158302068710327\n",
      "Epoch 28, Batch 126/462, Loss: 0.6776317358016968\n",
      "Epoch 28, Batch 127/462, Loss: 0.7991955876350403\n",
      "Epoch 28, Batch 128/462, Loss: 0.8257923126220703\n",
      "Epoch 28, Batch 129/462, Loss: 0.7886916399002075\n",
      "Epoch 28, Batch 130/462, Loss: 1.021420955657959\n",
      "Epoch 28, Batch 131/462, Loss: 0.6021575331687927\n",
      "Epoch 28, Batch 132/462, Loss: 0.9195274710655212\n",
      "Epoch 28, Batch 133/462, Loss: 0.7624192237854004\n",
      "Epoch 28, Batch 134/462, Loss: 0.619691789150238\n",
      "Epoch 28, Batch 135/462, Loss: 0.9592348337173462\n",
      "Epoch 28, Batch 136/462, Loss: 0.5598287582397461\n",
      "Epoch 28, Batch 137/462, Loss: 0.6876682043075562\n",
      "Epoch 28, Batch 138/462, Loss: 0.6497268080711365\n",
      "Epoch 28, Batch 139/462, Loss: 0.762356698513031\n",
      "Epoch 28, Batch 140/462, Loss: 0.6954871416091919\n",
      "Epoch 28, Batch 141/462, Loss: 0.6407096982002258\n",
      "Epoch 28, Batch 142/462, Loss: 0.6306833624839783\n",
      "Epoch 28, Batch 143/462, Loss: 0.7448686957359314\n",
      "Epoch 28, Batch 144/462, Loss: 0.8787974715232849\n",
      "Epoch 28, Batch 145/462, Loss: 0.6645424365997314\n",
      "Epoch 28, Batch 146/462, Loss: 0.6835273504257202\n",
      "Epoch 28, Batch 147/462, Loss: 0.648252546787262\n",
      "Epoch 28, Batch 148/462, Loss: 0.7597824931144714\n",
      "Epoch 28, Batch 149/462, Loss: 0.6666012406349182\n",
      "Epoch 28, Batch 150/462, Loss: 0.7892272472381592\n",
      "Epoch 28, Batch 151/462, Loss: 0.6633868217468262\n",
      "Epoch 28, Batch 152/462, Loss: 0.6526197195053101\n",
      "Epoch 28, Batch 153/462, Loss: 0.7230502963066101\n",
      "Epoch 28, Batch 154/462, Loss: 0.7958133816719055\n",
      "Epoch 28, Batch 155/462, Loss: 0.9163720011711121\n",
      "Epoch 28, Batch 156/462, Loss: 0.7147307991981506\n",
      "Epoch 28, Batch 157/462, Loss: 0.8706330060958862\n",
      "Epoch 28, Batch 158/462, Loss: 0.7878348231315613\n",
      "Epoch 28, Batch 159/462, Loss: 0.6451976299285889\n",
      "Epoch 28, Batch 160/462, Loss: 0.6330047845840454\n",
      "Epoch 28, Batch 161/462, Loss: 0.8832264542579651\n",
      "Epoch 28, Batch 162/462, Loss: 0.6536086201667786\n",
      "Epoch 28, Batch 163/462, Loss: 0.7667495608329773\n",
      "Epoch 28, Batch 164/462, Loss: 0.8414846062660217\n",
      "Epoch 28, Batch 165/462, Loss: 0.6915422081947327\n",
      "Epoch 28, Batch 166/462, Loss: 0.7408973574638367\n",
      "Epoch 28, Batch 167/462, Loss: 0.7166532278060913\n",
      "Epoch 28, Batch 168/462, Loss: 0.9147555232048035\n",
      "Epoch 28, Batch 169/462, Loss: 0.7726166248321533\n",
      "Epoch 28, Batch 170/462, Loss: 0.7929636240005493\n",
      "Epoch 28, Batch 171/462, Loss: 0.7238035798072815\n",
      "Epoch 28, Batch 172/462, Loss: 0.750603973865509\n",
      "Epoch 28, Batch 173/462, Loss: 0.6347607374191284\n",
      "Epoch 28, Batch 174/462, Loss: 0.5612133145332336\n",
      "Epoch 28, Batch 175/462, Loss: 0.8728235363960266\n",
      "Epoch 28, Batch 176/462, Loss: 0.7668247222900391\n",
      "Epoch 28, Batch 177/462, Loss: 0.5645760297775269\n",
      "Epoch 28, Batch 178/462, Loss: 0.7071053981781006\n",
      "Epoch 28, Batch 179/462, Loss: 0.7077375054359436\n",
      "Epoch 28, Batch 180/462, Loss: 0.6190620064735413\n",
      "Epoch 28, Batch 181/462, Loss: 0.7809338569641113\n",
      "Epoch 28, Batch 182/462, Loss: 0.7759901285171509\n",
      "Epoch 28, Batch 183/462, Loss: 0.5954563021659851\n",
      "Epoch 28, Batch 184/462, Loss: 0.8085408806800842\n",
      "Epoch 28, Batch 185/462, Loss: 0.7672817707061768\n",
      "Epoch 28, Batch 186/462, Loss: 0.6584095358848572\n",
      "Epoch 28, Batch 187/462, Loss: 0.733667254447937\n",
      "Epoch 28, Batch 188/462, Loss: 0.7317832112312317\n",
      "Epoch 28, Batch 189/462, Loss: 0.6262291669845581\n",
      "Epoch 28, Batch 190/462, Loss: 0.8238013386726379\n",
      "Epoch 28, Batch 191/462, Loss: 0.8341870307922363\n",
      "Epoch 28, Batch 192/462, Loss: 0.80659419298172\n",
      "Epoch 28, Batch 193/462, Loss: 0.7109180092811584\n",
      "Epoch 28, Batch 194/462, Loss: 0.6701573133468628\n",
      "Epoch 28, Batch 195/462, Loss: 0.65473473072052\n",
      "Epoch 28, Batch 196/462, Loss: 0.8214956521987915\n",
      "Epoch 28, Batch 197/462, Loss: 0.7520522475242615\n",
      "Epoch 28, Batch 198/462, Loss: 0.5822145342826843\n",
      "Epoch 28, Batch 199/462, Loss: 0.7208281755447388\n",
      "Epoch 28, Batch 200/462, Loss: 0.6453648805618286\n",
      "Epoch 28, Batch 201/462, Loss: 0.8021745085716248\n",
      "Epoch 28, Batch 202/462, Loss: 0.5898036956787109\n",
      "Epoch 28, Batch 203/462, Loss: 0.6617125272750854\n",
      "Epoch 28, Batch 204/462, Loss: 0.6820380687713623\n",
      "Epoch 28, Batch 205/462, Loss: 0.5885335803031921\n",
      "Epoch 28, Batch 206/462, Loss: 0.7109350562095642\n",
      "Epoch 28, Batch 207/462, Loss: 0.7219029664993286\n",
      "Epoch 28, Batch 208/462, Loss: 0.8568698167800903\n",
      "Epoch 28, Batch 209/462, Loss: 0.9548656344413757\n",
      "Epoch 28, Batch 210/462, Loss: 0.6064023971557617\n",
      "Epoch 28, Batch 211/462, Loss: 0.5374674797058105\n",
      "Epoch 28, Batch 212/462, Loss: 0.700334370136261\n",
      "Epoch 28, Batch 213/462, Loss: 0.7869900465011597\n",
      "Epoch 28, Batch 214/462, Loss: 0.7681987285614014\n",
      "Epoch 28, Batch 215/462, Loss: 0.6999484896659851\n",
      "Epoch 28, Batch 216/462, Loss: 0.8332372903823853\n",
      "Epoch 28, Batch 217/462, Loss: 0.6117671132087708\n",
      "Epoch 28, Batch 218/462, Loss: 0.6761543154716492\n",
      "Epoch 28, Batch 219/462, Loss: 0.6626500487327576\n",
      "Epoch 28, Batch 220/462, Loss: 0.7798194289207458\n",
      "Epoch 28, Batch 221/462, Loss: 0.6548518538475037\n",
      "Epoch 28, Batch 222/462, Loss: 0.6201375722885132\n",
      "Epoch 28, Batch 223/462, Loss: 0.7868274450302124\n",
      "Epoch 28, Batch 224/462, Loss: 0.698560357093811\n",
      "Epoch 28, Batch 225/462, Loss: 0.7365710735321045\n",
      "Epoch 28, Batch 226/462, Loss: 0.6664996147155762\n",
      "Epoch 28, Batch 227/462, Loss: 0.887749433517456\n",
      "Epoch 28, Batch 228/462, Loss: 0.7625637054443359\n",
      "Epoch 28, Batch 229/462, Loss: 0.5268625617027283\n",
      "Epoch 28, Batch 230/462, Loss: 0.7650324702262878\n",
      "Epoch 28, Batch 231/462, Loss: 0.6448512077331543\n",
      "Epoch 28, Batch 232/462, Loss: 0.6890775561332703\n",
      "Epoch 28, Batch 233/462, Loss: 0.7468987107276917\n",
      "Epoch 28, Batch 234/462, Loss: 0.8316401243209839\n",
      "Epoch 28, Batch 235/462, Loss: 0.698350727558136\n",
      "Epoch 28, Batch 236/462, Loss: 0.755520224571228\n",
      "Epoch 28, Batch 237/462, Loss: 0.7557176351547241\n",
      "Epoch 28, Batch 238/462, Loss: 0.7492560744285583\n",
      "Epoch 28, Batch 239/462, Loss: 0.6441419124603271\n",
      "Epoch 28, Batch 240/462, Loss: 0.7767643332481384\n",
      "Epoch 28, Batch 241/462, Loss: 0.7733629941940308\n",
      "Epoch 28, Batch 242/462, Loss: 1.0262125730514526\n",
      "Epoch 28, Batch 243/462, Loss: 0.9241561889648438\n",
      "Epoch 28, Batch 244/462, Loss: 0.6189994215965271\n",
      "Epoch 28, Batch 245/462, Loss: 0.6806198954582214\n",
      "Epoch 28, Batch 246/462, Loss: 0.853947103023529\n",
      "Epoch 28, Batch 247/462, Loss: 0.7784166932106018\n",
      "Epoch 28, Batch 248/462, Loss: 0.7813444137573242\n",
      "Epoch 28, Batch 249/462, Loss: 0.6106847524642944\n",
      "Epoch 28, Batch 250/462, Loss: 0.7538168430328369\n",
      "Epoch 28, Batch 251/462, Loss: 0.8027907609939575\n",
      "Epoch 28, Batch 252/462, Loss: 0.6419517993927002\n",
      "Epoch 28, Batch 253/462, Loss: 0.6849088668823242\n",
      "Epoch 28, Batch 254/462, Loss: 0.6932015419006348\n",
      "Epoch 28, Batch 255/462, Loss: 0.787601888179779\n",
      "Epoch 28, Batch 256/462, Loss: 0.9098415374755859\n",
      "Epoch 28, Batch 257/462, Loss: 0.6472355127334595\n",
      "Epoch 28, Batch 258/462, Loss: 0.6180272102355957\n",
      "Epoch 28, Batch 259/462, Loss: 0.7414206862449646\n",
      "Epoch 28, Batch 260/462, Loss: 0.7373961806297302\n",
      "Epoch 28, Batch 261/462, Loss: 0.857189416885376\n",
      "Epoch 28, Batch 262/462, Loss: 0.5958537459373474\n",
      "Epoch 28, Batch 263/462, Loss: 0.8495590686798096\n",
      "Epoch 28, Batch 264/462, Loss: 0.6141591668128967\n",
      "Epoch 28, Batch 265/462, Loss: 0.6532949209213257\n",
      "Epoch 28, Batch 266/462, Loss: 0.7381949424743652\n",
      "Epoch 28, Batch 267/462, Loss: 0.7905600666999817\n",
      "Epoch 28, Batch 268/462, Loss: 0.7960554361343384\n",
      "Epoch 28, Batch 269/462, Loss: 0.7971249222755432\n",
      "Epoch 28, Batch 270/462, Loss: 0.6152994632720947\n",
      "Epoch 28, Batch 271/462, Loss: 0.6757924556732178\n",
      "Epoch 28, Batch 272/462, Loss: 0.8290390372276306\n",
      "Epoch 28, Batch 273/462, Loss: 0.9292219281196594\n",
      "Epoch 28, Batch 274/462, Loss: 0.9164235591888428\n",
      "Epoch 28, Batch 275/462, Loss: 0.6088436245918274\n",
      "Epoch 28, Batch 276/462, Loss: 0.6864649057388306\n",
      "Epoch 28, Batch 277/462, Loss: 0.610447883605957\n",
      "Epoch 28, Batch 278/462, Loss: 0.7451380491256714\n",
      "Epoch 28, Batch 279/462, Loss: 0.696570634841919\n",
      "Epoch 28, Batch 280/462, Loss: 0.7839428782463074\n",
      "Epoch 28, Batch 281/462, Loss: 0.8251755833625793\n",
      "Epoch 28, Batch 282/462, Loss: 0.6614443063735962\n",
      "Epoch 28, Batch 283/462, Loss: 0.7160019278526306\n",
      "Epoch 28, Batch 284/462, Loss: 0.6649974584579468\n",
      "Epoch 28, Batch 285/462, Loss: 0.5819705724716187\n",
      "Epoch 28, Batch 286/462, Loss: 0.8519935607910156\n",
      "Epoch 28, Batch 287/462, Loss: 0.7893834114074707\n",
      "Epoch 28, Batch 288/462, Loss: 0.5596513748168945\n",
      "Epoch 28, Batch 289/462, Loss: 0.6365512013435364\n",
      "Epoch 28, Batch 290/462, Loss: 0.7598226070404053\n",
      "Epoch 28, Batch 291/462, Loss: 0.794548511505127\n",
      "Epoch 28, Batch 292/462, Loss: 0.8108426332473755\n",
      "Epoch 28, Batch 293/462, Loss: 0.6603302359580994\n",
      "Epoch 28, Batch 294/462, Loss: 0.548685610294342\n",
      "Epoch 28, Batch 295/462, Loss: 0.7899180054664612\n",
      "Epoch 28, Batch 296/462, Loss: 0.8545922040939331\n",
      "Epoch 28, Batch 297/462, Loss: 0.718680202960968\n",
      "Epoch 28, Batch 298/462, Loss: 0.9986207485198975\n",
      "Epoch 28, Batch 299/462, Loss: 0.6620286703109741\n",
      "Epoch 28, Batch 300/462, Loss: 0.7122830152511597\n",
      "Epoch 28, Batch 301/462, Loss: 0.9047389030456543\n",
      "Epoch 28, Batch 302/462, Loss: 0.6427599787712097\n",
      "Epoch 28, Batch 303/462, Loss: 0.6240118145942688\n",
      "Epoch 28, Batch 304/462, Loss: 0.7390261292457581\n",
      "Epoch 28, Batch 305/462, Loss: 0.7510413527488708\n",
      "Epoch 28, Batch 306/462, Loss: 0.8956745266914368\n",
      "Epoch 28, Batch 307/462, Loss: 0.8224995136260986\n",
      "Epoch 28, Batch 308/462, Loss: 0.7643387317657471\n",
      "Epoch 28, Batch 309/462, Loss: 0.7860738039016724\n",
      "Epoch 28, Batch 310/462, Loss: 0.6117856502532959\n",
      "Epoch 28, Batch 311/462, Loss: 0.6114529371261597\n",
      "Epoch 28, Batch 312/462, Loss: 0.6079592704772949\n",
      "Epoch 28, Batch 313/462, Loss: 0.7914798259735107\n",
      "Epoch 28, Batch 314/462, Loss: 0.6736705899238586\n",
      "Epoch 28, Batch 315/462, Loss: 0.7828036546707153\n",
      "Epoch 28, Batch 316/462, Loss: 0.6035971641540527\n",
      "Epoch 28, Batch 317/462, Loss: 0.6693709492683411\n",
      "Epoch 28, Batch 318/462, Loss: 0.7145927548408508\n",
      "Epoch 28, Batch 319/462, Loss: 0.7800828218460083\n",
      "Epoch 28, Batch 320/462, Loss: 0.7344308495521545\n",
      "Epoch 28, Batch 321/462, Loss: 0.9295139312744141\n",
      "Epoch 28, Batch 322/462, Loss: 0.8821625709533691\n",
      "Epoch 28, Batch 323/462, Loss: 0.70418381690979\n",
      "Epoch 28, Batch 324/462, Loss: 0.7361968755722046\n",
      "Epoch 28, Batch 325/462, Loss: 0.7925371527671814\n",
      "Epoch 28, Batch 326/462, Loss: 0.7500054240226746\n",
      "Epoch 28, Batch 327/462, Loss: 0.8655309081077576\n",
      "Epoch 28, Batch 328/462, Loss: 0.786534309387207\n",
      "Epoch 28, Batch 329/462, Loss: 0.7062162756919861\n",
      "Epoch 28, Batch 330/462, Loss: 0.756342887878418\n",
      "Epoch 28, Batch 331/462, Loss: 0.6129114627838135\n",
      "Epoch 28, Batch 332/462, Loss: 0.9260542392730713\n",
      "Epoch 28, Batch 333/462, Loss: 0.7172108292579651\n",
      "Epoch 28, Batch 334/462, Loss: 0.5477650165557861\n",
      "Epoch 28, Batch 335/462, Loss: 0.8097801208496094\n",
      "Epoch 28, Batch 336/462, Loss: 0.836285412311554\n",
      "Epoch 28, Batch 337/462, Loss: 0.845483124256134\n",
      "Epoch 28, Batch 338/462, Loss: 0.7681792378425598\n",
      "Epoch 28, Batch 339/462, Loss: 0.8049991130828857\n",
      "Epoch 28, Batch 340/462, Loss: 0.8457664847373962\n",
      "Epoch 28, Batch 341/462, Loss: 0.6741461753845215\n",
      "Epoch 28, Batch 342/462, Loss: 0.7811873555183411\n",
      "Epoch 28, Batch 343/462, Loss: 0.7680070996284485\n",
      "Epoch 28, Batch 344/462, Loss: 0.7574700117111206\n",
      "Epoch 28, Batch 345/462, Loss: 0.5324670672416687\n",
      "Epoch 28, Batch 346/462, Loss: 0.784035325050354\n",
      "Epoch 28, Batch 347/462, Loss: 0.6383001208305359\n",
      "Epoch 28, Batch 348/462, Loss: 0.7143842577934265\n",
      "Epoch 28, Batch 349/462, Loss: 0.7121160626411438\n",
      "Epoch 28, Batch 350/462, Loss: 0.6276224255561829\n",
      "Epoch 28, Batch 351/462, Loss: 0.7006863951683044\n",
      "Epoch 28, Batch 352/462, Loss: 0.6626421213150024\n",
      "Epoch 28, Batch 353/462, Loss: 0.6608811616897583\n",
      "Epoch 28, Batch 354/462, Loss: 0.9584106802940369\n",
      "Epoch 28, Batch 355/462, Loss: 0.7259648442268372\n",
      "Epoch 28, Batch 356/462, Loss: 0.6712422370910645\n",
      "Epoch 28, Batch 357/462, Loss: 0.6249849200248718\n",
      "Epoch 28, Batch 358/462, Loss: 0.7545360922813416\n",
      "Epoch 28, Batch 359/462, Loss: 0.6080873012542725\n",
      "Epoch 28, Batch 360/462, Loss: 0.6404569149017334\n",
      "Epoch 28, Batch 361/462, Loss: 0.7861658334732056\n",
      "Epoch 28, Batch 362/462, Loss: 0.7001887559890747\n",
      "Epoch 28, Batch 363/462, Loss: 0.7634211182594299\n",
      "Epoch 28, Batch 364/462, Loss: 0.6311877369880676\n",
      "Epoch 28, Batch 365/462, Loss: 0.8335917592048645\n",
      "Epoch 28, Batch 366/462, Loss: 0.7608678936958313\n",
      "Epoch 28, Batch 367/462, Loss: 0.8055956363677979\n",
      "Epoch 28, Batch 368/462, Loss: 0.8097046613693237\n",
      "Epoch 28, Batch 369/462, Loss: 0.6902031302452087\n",
      "Epoch 28, Batch 370/462, Loss: 0.8973801732063293\n",
      "Epoch 28, Batch 371/462, Loss: 0.7786946296691895\n",
      "Epoch 28, Batch 372/462, Loss: 0.6474490761756897\n",
      "Epoch 28, Batch 373/462, Loss: 0.854041576385498\n",
      "Epoch 28, Batch 374/462, Loss: 0.6888366341590881\n",
      "Epoch 28, Batch 375/462, Loss: 0.6906209588050842\n",
      "Epoch 28, Batch 376/462, Loss: 0.6419927477836609\n",
      "Epoch 28, Batch 377/462, Loss: 0.70522540807724\n",
      "Epoch 28, Batch 378/462, Loss: 0.8465349078178406\n",
      "Epoch 28, Batch 379/462, Loss: 0.6278188228607178\n",
      "Epoch 28, Batch 380/462, Loss: 0.6684084534645081\n",
      "Epoch 28, Batch 381/462, Loss: 0.641619086265564\n",
      "Epoch 28, Batch 382/462, Loss: 0.8009805083274841\n",
      "Epoch 28, Batch 383/462, Loss: 0.854481041431427\n",
      "Epoch 28, Batch 384/462, Loss: 0.5148733854293823\n",
      "Epoch 28, Batch 385/462, Loss: 0.6848259568214417\n",
      "Epoch 28, Batch 386/462, Loss: 0.7825164794921875\n",
      "Epoch 28, Batch 387/462, Loss: 0.6290220618247986\n",
      "Epoch 28, Batch 388/462, Loss: 0.6504871249198914\n",
      "Epoch 28, Batch 389/462, Loss: 0.7810393571853638\n",
      "Epoch 28, Batch 390/462, Loss: 0.6156818866729736\n",
      "Epoch 28, Batch 391/462, Loss: 0.9663878679275513\n",
      "Epoch 28, Batch 392/462, Loss: 0.596572995185852\n",
      "Epoch 28, Batch 393/462, Loss: 0.8461511135101318\n",
      "Epoch 28, Batch 394/462, Loss: 0.5486348867416382\n",
      "Epoch 28, Batch 395/462, Loss: 0.7003203630447388\n",
      "Epoch 28, Batch 396/462, Loss: 0.7060661911964417\n",
      "Epoch 28, Batch 397/462, Loss: 0.7747307419776917\n",
      "Epoch 28, Batch 398/462, Loss: 0.7652691006660461\n",
      "Epoch 28, Batch 399/462, Loss: 0.6882792115211487\n",
      "Epoch 28, Batch 400/462, Loss: 0.6824688911437988\n",
      "Epoch 28, Batch 401/462, Loss: 0.8707231283187866\n",
      "Epoch 28, Batch 402/462, Loss: 0.5564242005348206\n",
      "Epoch 28, Batch 403/462, Loss: 0.7551015019416809\n",
      "Epoch 28, Batch 404/462, Loss: 0.6468200087547302\n",
      "Epoch 28, Batch 405/462, Loss: 0.5524261593818665\n",
      "Epoch 28, Batch 406/462, Loss: 0.8759298920631409\n",
      "Epoch 28, Batch 407/462, Loss: 0.8672642707824707\n",
      "Epoch 28, Batch 408/462, Loss: 0.6652713418006897\n",
      "Epoch 28, Batch 409/462, Loss: 0.7750499844551086\n",
      "Epoch 28, Batch 410/462, Loss: 0.8193435072898865\n",
      "Epoch 28, Batch 411/462, Loss: 0.7669960856437683\n",
      "Epoch 28, Batch 412/462, Loss: 0.6443089246749878\n",
      "Epoch 28, Batch 413/462, Loss: 0.5543574690818787\n",
      "Epoch 28, Batch 414/462, Loss: 0.7040852308273315\n",
      "Epoch 28, Batch 415/462, Loss: 0.5547588467597961\n",
      "Epoch 28, Batch 416/462, Loss: 0.7180045247077942\n",
      "Epoch 28, Batch 417/462, Loss: 0.6384207606315613\n",
      "Epoch 28, Batch 418/462, Loss: 0.7296267151832581\n",
      "Epoch 28, Batch 419/462, Loss: 0.6778551936149597\n",
      "Epoch 28, Batch 420/462, Loss: 0.6197351217269897\n",
      "Epoch 28, Batch 421/462, Loss: 0.7574907541275024\n",
      "Epoch 28, Batch 422/462, Loss: 0.7481310963630676\n",
      "Epoch 28, Batch 423/462, Loss: 0.8350998163223267\n",
      "Epoch 28, Batch 424/462, Loss: 0.7018491625785828\n",
      "Epoch 28, Batch 425/462, Loss: 0.7541228532791138\n",
      "Epoch 28, Batch 426/462, Loss: 0.7877647876739502\n",
      "Epoch 28, Batch 427/462, Loss: 0.6360924243927002\n",
      "Epoch 28, Batch 428/462, Loss: 0.7821555733680725\n",
      "Epoch 28, Batch 429/462, Loss: 0.5967995524406433\n",
      "Epoch 28, Batch 430/462, Loss: 0.7074254751205444\n",
      "Epoch 28, Batch 431/462, Loss: 0.6964067816734314\n",
      "Epoch 28, Batch 432/462, Loss: 0.7615562081336975\n",
      "Epoch 28, Batch 433/462, Loss: 0.7204241752624512\n",
      "Epoch 28, Batch 434/462, Loss: 0.5698150992393494\n",
      "Epoch 28, Batch 435/462, Loss: 0.750171422958374\n",
      "Epoch 28, Batch 436/462, Loss: 0.7164183855056763\n",
      "Epoch 28, Batch 437/462, Loss: 0.7780612707138062\n",
      "Epoch 28, Batch 438/462, Loss: 0.6112346649169922\n",
      "Epoch 28, Batch 439/462, Loss: 0.7205376029014587\n",
      "Epoch 28, Batch 440/462, Loss: 0.7099205851554871\n",
      "Epoch 28, Batch 441/462, Loss: 0.6917089223861694\n",
      "Epoch 28, Batch 442/462, Loss: 0.6810137629508972\n",
      "Epoch 28, Batch 443/462, Loss: 0.8196850419044495\n",
      "Epoch 28, Batch 444/462, Loss: 0.5768583416938782\n",
      "Epoch 28, Batch 445/462, Loss: 0.8018360733985901\n",
      "Epoch 28, Batch 446/462, Loss: 0.7512997984886169\n",
      "Epoch 28, Batch 447/462, Loss: 0.7605165243148804\n",
      "Epoch 28, Batch 448/462, Loss: 0.6518334746360779\n",
      "Epoch 28, Batch 449/462, Loss: 0.5613216161727905\n",
      "Epoch 28, Batch 450/462, Loss: 0.824752688407898\n",
      "Epoch 28, Batch 451/462, Loss: 0.8734830617904663\n",
      "Epoch 28, Batch 452/462, Loss: 0.8632683157920837\n",
      "Epoch 28, Batch 453/462, Loss: 0.7634156346321106\n",
      "Epoch 28, Batch 454/462, Loss: 0.6037933230400085\n",
      "Epoch 28, Batch 455/462, Loss: 0.7505092620849609\n",
      "Epoch 28, Batch 456/462, Loss: 0.7264074683189392\n",
      "Epoch 28, Batch 457/462, Loss: 0.5583871603012085\n",
      "Epoch 28, Batch 458/462, Loss: 0.7207579016685486\n",
      "Epoch 28, Batch 459/462, Loss: 0.7461186647415161\n",
      "Epoch 28, Batch 460/462, Loss: 0.8315057754516602\n",
      "Epoch 28, Batch 461/462, Loss: 0.7884734272956848\n",
      "Epoch 28, Batch 462/462, Loss: 0.5763023495674133\n",
      "Epoch 28, Loss: 336.51770198345184\n",
      "Epoch 29, Batch 1/462, Loss: 0.5600274801254272\n",
      "Epoch 29, Batch 2/462, Loss: 0.7519417405128479\n",
      "Epoch 29, Batch 3/462, Loss: 0.7778850197792053\n",
      "Epoch 29, Batch 4/462, Loss: 0.7605413794517517\n",
      "Epoch 29, Batch 5/462, Loss: 0.7650812268257141\n",
      "Epoch 29, Batch 6/462, Loss: 0.7565694451332092\n",
      "Epoch 29, Batch 7/462, Loss: 0.790553629398346\n",
      "Epoch 29, Batch 8/462, Loss: 0.743958055973053\n",
      "Epoch 29, Batch 9/462, Loss: 0.8055645227432251\n",
      "Epoch 29, Batch 10/462, Loss: 0.5866889953613281\n",
      "Epoch 29, Batch 11/462, Loss: 0.6857692003250122\n",
      "Epoch 29, Batch 12/462, Loss: 0.8094096183776855\n",
      "Epoch 29, Batch 13/462, Loss: 0.618645966053009\n",
      "Epoch 29, Batch 14/462, Loss: 0.7341241836547852\n",
      "Epoch 29, Batch 15/462, Loss: 0.8126557469367981\n",
      "Epoch 29, Batch 16/462, Loss: 0.7274377346038818\n",
      "Epoch 29, Batch 17/462, Loss: 0.6608702540397644\n",
      "Epoch 29, Batch 18/462, Loss: 0.807058572769165\n",
      "Epoch 29, Batch 19/462, Loss: 0.6261925101280212\n",
      "Epoch 29, Batch 20/462, Loss: 0.5934358239173889\n",
      "Epoch 29, Batch 21/462, Loss: 0.7409398555755615\n",
      "Epoch 29, Batch 22/462, Loss: 0.7376291155815125\n",
      "Epoch 29, Batch 23/462, Loss: 0.8016682267189026\n",
      "Epoch 29, Batch 24/462, Loss: 0.6549707651138306\n",
      "Epoch 29, Batch 25/462, Loss: 0.8034729957580566\n",
      "Epoch 29, Batch 26/462, Loss: 1.0276198387145996\n",
      "Epoch 29, Batch 27/462, Loss: 0.8672747015953064\n",
      "Epoch 29, Batch 28/462, Loss: 0.7992882132530212\n",
      "Epoch 29, Batch 29/462, Loss: 0.5667223930358887\n",
      "Epoch 29, Batch 30/462, Loss: 0.8062956929206848\n",
      "Epoch 29, Batch 31/462, Loss: 0.7011882066726685\n",
      "Epoch 29, Batch 32/462, Loss: 0.8328942656517029\n",
      "Epoch 29, Batch 33/462, Loss: 0.843556821346283\n",
      "Epoch 29, Batch 34/462, Loss: 0.816455602645874\n",
      "Epoch 29, Batch 35/462, Loss: 0.6916185021400452\n",
      "Epoch 29, Batch 36/462, Loss: 0.7191793322563171\n",
      "Epoch 29, Batch 37/462, Loss: 0.6355729699134827\n",
      "Epoch 29, Batch 38/462, Loss: 0.694008469581604\n",
      "Epoch 29, Batch 39/462, Loss: 0.746379554271698\n",
      "Epoch 29, Batch 40/462, Loss: 0.7154430747032166\n",
      "Epoch 29, Batch 41/462, Loss: 0.7953504323959351\n",
      "Epoch 29, Batch 42/462, Loss: 0.6044440865516663\n",
      "Epoch 29, Batch 43/462, Loss: 0.687061607837677\n",
      "Epoch 29, Batch 44/462, Loss: 0.6018884778022766\n",
      "Epoch 29, Batch 45/462, Loss: 0.7818377614021301\n",
      "Epoch 29, Batch 46/462, Loss: 0.8143936991691589\n",
      "Epoch 29, Batch 47/462, Loss: 0.7678797841072083\n",
      "Epoch 29, Batch 48/462, Loss: 0.9861279726028442\n",
      "Epoch 29, Batch 49/462, Loss: 0.6658721566200256\n",
      "Epoch 29, Batch 50/462, Loss: 0.8704357743263245\n",
      "Epoch 29, Batch 51/462, Loss: 0.7243866324424744\n",
      "Epoch 29, Batch 52/462, Loss: 0.8238576054573059\n",
      "Epoch 29, Batch 53/462, Loss: 0.5469006299972534\n",
      "Epoch 29, Batch 54/462, Loss: 1.0151697397232056\n",
      "Epoch 29, Batch 55/462, Loss: 0.6071885824203491\n",
      "Epoch 29, Batch 56/462, Loss: 0.6811689734458923\n",
      "Epoch 29, Batch 57/462, Loss: 0.7176936864852905\n",
      "Epoch 29, Batch 58/462, Loss: 0.7071413397789001\n",
      "Epoch 29, Batch 59/462, Loss: 0.6671627163887024\n",
      "Epoch 29, Batch 60/462, Loss: 0.7994632124900818\n",
      "Epoch 29, Batch 61/462, Loss: 0.7586483955383301\n",
      "Epoch 29, Batch 62/462, Loss: 0.5251993536949158\n",
      "Epoch 29, Batch 63/462, Loss: 0.8234599828720093\n",
      "Epoch 29, Batch 64/462, Loss: 0.6139239072799683\n",
      "Epoch 29, Batch 65/462, Loss: 0.7544335722923279\n",
      "Epoch 29, Batch 66/462, Loss: 0.7869061231613159\n",
      "Epoch 29, Batch 67/462, Loss: 0.723644495010376\n",
      "Epoch 29, Batch 68/462, Loss: 0.854537844657898\n",
      "Epoch 29, Batch 69/462, Loss: 0.7763276100158691\n",
      "Epoch 29, Batch 70/462, Loss: 0.7288157343864441\n",
      "Epoch 29, Batch 71/462, Loss: 0.7252737879753113\n",
      "Epoch 29, Batch 72/462, Loss: 0.6990049481391907\n",
      "Epoch 29, Batch 73/462, Loss: 0.7028605341911316\n",
      "Epoch 29, Batch 74/462, Loss: 0.841232419013977\n",
      "Epoch 29, Batch 75/462, Loss: 0.7614341378211975\n",
      "Epoch 29, Batch 76/462, Loss: 0.6788915395736694\n",
      "Epoch 29, Batch 77/462, Loss: 0.858110249042511\n",
      "Epoch 29, Batch 78/462, Loss: 0.7258107662200928\n",
      "Epoch 29, Batch 79/462, Loss: 0.7657529711723328\n",
      "Epoch 29, Batch 80/462, Loss: 0.7552710771560669\n",
      "Epoch 29, Batch 81/462, Loss: 0.7105090022087097\n",
      "Epoch 29, Batch 82/462, Loss: 0.7518375515937805\n",
      "Epoch 29, Batch 83/462, Loss: 0.7125377058982849\n",
      "Epoch 29, Batch 84/462, Loss: 0.7771766185760498\n",
      "Epoch 29, Batch 85/462, Loss: 0.893908679485321\n",
      "Epoch 29, Batch 86/462, Loss: 0.7063071727752686\n",
      "Epoch 29, Batch 87/462, Loss: 0.6099950075149536\n",
      "Epoch 29, Batch 88/462, Loss: 0.7467246055603027\n",
      "Epoch 29, Batch 89/462, Loss: 0.6846027374267578\n",
      "Epoch 29, Batch 90/462, Loss: 0.7903938293457031\n",
      "Epoch 29, Batch 91/462, Loss: 0.8132486343383789\n",
      "Epoch 29, Batch 92/462, Loss: 0.84307461977005\n",
      "Epoch 29, Batch 93/462, Loss: 0.5916285514831543\n",
      "Epoch 29, Batch 94/462, Loss: 0.6999110579490662\n",
      "Epoch 29, Batch 95/462, Loss: 0.5570638179779053\n",
      "Epoch 29, Batch 96/462, Loss: 0.8328237533569336\n",
      "Epoch 29, Batch 97/462, Loss: 0.6079703569412231\n",
      "Epoch 29, Batch 98/462, Loss: 0.7784791588783264\n",
      "Epoch 29, Batch 99/462, Loss: 0.6950139999389648\n",
      "Epoch 29, Batch 100/462, Loss: 0.732175350189209\n",
      "Epoch 29, Batch 101/462, Loss: 0.6821143627166748\n",
      "Epoch 29, Batch 102/462, Loss: 0.7493962645530701\n",
      "Epoch 29, Batch 103/462, Loss: 0.7253718376159668\n",
      "Epoch 29, Batch 104/462, Loss: 0.8957379460334778\n",
      "Epoch 29, Batch 105/462, Loss: 0.6273503303527832\n",
      "Epoch 29, Batch 106/462, Loss: 0.6501578688621521\n",
      "Epoch 29, Batch 107/462, Loss: 0.8739224076271057\n",
      "Epoch 29, Batch 108/462, Loss: 0.9023180603981018\n",
      "Epoch 29, Batch 109/462, Loss: 0.7534934282302856\n",
      "Epoch 29, Batch 110/462, Loss: 0.6499603390693665\n",
      "Epoch 29, Batch 111/462, Loss: 0.7982158660888672\n",
      "Epoch 29, Batch 112/462, Loss: 0.6973309516906738\n",
      "Epoch 29, Batch 113/462, Loss: 0.7313823103904724\n",
      "Epoch 29, Batch 114/462, Loss: 0.7516630291938782\n",
      "Epoch 29, Batch 115/462, Loss: 0.7984969019889832\n",
      "Epoch 29, Batch 116/462, Loss: 0.6845123171806335\n",
      "Epoch 29, Batch 117/462, Loss: 0.6933824419975281\n",
      "Epoch 29, Batch 118/462, Loss: 0.6808405518531799\n",
      "Epoch 29, Batch 119/462, Loss: 0.5320553779602051\n",
      "Epoch 29, Batch 120/462, Loss: 0.588720440864563\n",
      "Epoch 29, Batch 121/462, Loss: 0.8481737971305847\n",
      "Epoch 29, Batch 122/462, Loss: 0.5769630670547485\n",
      "Epoch 29, Batch 123/462, Loss: 0.7628247737884521\n",
      "Epoch 29, Batch 124/462, Loss: 0.6188486814498901\n",
      "Epoch 29, Batch 125/462, Loss: 0.7985683083534241\n",
      "Epoch 29, Batch 126/462, Loss: 0.5300971865653992\n",
      "Epoch 29, Batch 127/462, Loss: 0.6862858533859253\n",
      "Epoch 29, Batch 128/462, Loss: 0.5353670120239258\n",
      "Epoch 29, Batch 129/462, Loss: 0.8090438842773438\n",
      "Epoch 29, Batch 130/462, Loss: 0.9021111130714417\n",
      "Epoch 29, Batch 131/462, Loss: 0.629505455493927\n",
      "Epoch 29, Batch 132/462, Loss: 0.696316123008728\n",
      "Epoch 29, Batch 133/462, Loss: 0.8323900103569031\n",
      "Epoch 29, Batch 134/462, Loss: 0.5385981202125549\n",
      "Epoch 29, Batch 135/462, Loss: 0.7199926376342773\n",
      "Epoch 29, Batch 136/462, Loss: 0.7378730177879333\n",
      "Epoch 29, Batch 137/462, Loss: 0.8499866724014282\n",
      "Epoch 29, Batch 138/462, Loss: 0.840338408946991\n",
      "Epoch 29, Batch 139/462, Loss: 0.8731698989868164\n",
      "Epoch 29, Batch 140/462, Loss: 0.7378350496292114\n",
      "Epoch 29, Batch 141/462, Loss: 0.8118320107460022\n",
      "Epoch 29, Batch 142/462, Loss: 0.8247570991516113\n",
      "Epoch 29, Batch 143/462, Loss: 0.6874576210975647\n",
      "Epoch 29, Batch 144/462, Loss: 0.5949791669845581\n",
      "Epoch 29, Batch 145/462, Loss: 0.8074873089790344\n",
      "Epoch 29, Batch 146/462, Loss: 0.7859068512916565\n",
      "Epoch 29, Batch 147/462, Loss: 0.7907854914665222\n",
      "Epoch 29, Batch 148/462, Loss: 0.7185201048851013\n",
      "Epoch 29, Batch 149/462, Loss: 0.8223066926002502\n",
      "Epoch 29, Batch 150/462, Loss: 0.7248615026473999\n",
      "Epoch 29, Batch 151/462, Loss: 0.7731305956840515\n",
      "Epoch 29, Batch 152/462, Loss: 0.76408451795578\n",
      "Epoch 29, Batch 153/462, Loss: 0.6291256546974182\n",
      "Epoch 29, Batch 154/462, Loss: 0.7938461899757385\n",
      "Epoch 29, Batch 155/462, Loss: 0.6117326021194458\n",
      "Epoch 29, Batch 156/462, Loss: 0.7028161883354187\n",
      "Epoch 29, Batch 157/462, Loss: 0.7844365239143372\n",
      "Epoch 29, Batch 158/462, Loss: 0.8256418108940125\n",
      "Epoch 29, Batch 159/462, Loss: 0.8313472270965576\n",
      "Epoch 29, Batch 160/462, Loss: 0.68122398853302\n",
      "Epoch 29, Batch 161/462, Loss: 0.8631238341331482\n",
      "Epoch 29, Batch 162/462, Loss: 0.7242849469184875\n",
      "Epoch 29, Batch 163/462, Loss: 0.7515509128570557\n",
      "Epoch 29, Batch 164/462, Loss: 0.7025598287582397\n",
      "Epoch 29, Batch 165/462, Loss: 0.6949213147163391\n",
      "Epoch 29, Batch 166/462, Loss: 0.8103572726249695\n",
      "Epoch 29, Batch 167/462, Loss: 0.8311565518379211\n",
      "Epoch 29, Batch 168/462, Loss: 0.6880538463592529\n",
      "Epoch 29, Batch 169/462, Loss: 0.778924286365509\n",
      "Epoch 29, Batch 170/462, Loss: 0.8073986172676086\n",
      "Epoch 29, Batch 171/462, Loss: 0.8523327112197876\n",
      "Epoch 29, Batch 172/462, Loss: 0.8248700499534607\n",
      "Epoch 29, Batch 173/462, Loss: 0.5163216590881348\n",
      "Epoch 29, Batch 174/462, Loss: 0.711617648601532\n",
      "Epoch 29, Batch 175/462, Loss: 0.8337434530258179\n",
      "Epoch 29, Batch 176/462, Loss: 0.6916302442550659\n",
      "Epoch 29, Batch 177/462, Loss: 0.5983168482780457\n",
      "Epoch 29, Batch 178/462, Loss: 0.6727110743522644\n",
      "Epoch 29, Batch 179/462, Loss: 0.8160570859909058\n",
      "Epoch 29, Batch 180/462, Loss: 0.6678772568702698\n",
      "Epoch 29, Batch 181/462, Loss: 0.7901098132133484\n",
      "Epoch 29, Batch 182/462, Loss: 0.8691437244415283\n",
      "Epoch 29, Batch 183/462, Loss: 0.7977601289749146\n",
      "Epoch 29, Batch 184/462, Loss: 0.7604634165763855\n",
      "Epoch 29, Batch 185/462, Loss: 0.6934881210327148\n",
      "Epoch 29, Batch 186/462, Loss: 0.8487889170646667\n",
      "Epoch 29, Batch 187/462, Loss: 0.901336669921875\n",
      "Epoch 29, Batch 188/462, Loss: 0.6795225739479065\n",
      "Epoch 29, Batch 189/462, Loss: 0.6778932809829712\n",
      "Epoch 29, Batch 190/462, Loss: 0.929483950138092\n",
      "Epoch 29, Batch 191/462, Loss: 0.7676188945770264\n",
      "Epoch 29, Batch 192/462, Loss: 0.6242974400520325\n",
      "Epoch 29, Batch 193/462, Loss: 0.8163995146751404\n",
      "Epoch 29, Batch 194/462, Loss: 0.6166985034942627\n",
      "Epoch 29, Batch 195/462, Loss: 0.5656155943870544\n",
      "Epoch 29, Batch 196/462, Loss: 0.7705090045928955\n",
      "Epoch 29, Batch 197/462, Loss: 0.9274810552597046\n",
      "Epoch 29, Batch 198/462, Loss: 0.6758716106414795\n",
      "Epoch 29, Batch 199/462, Loss: 0.8136951923370361\n",
      "Epoch 29, Batch 200/462, Loss: 0.8133594393730164\n",
      "Epoch 29, Batch 201/462, Loss: 0.7421900033950806\n",
      "Epoch 29, Batch 202/462, Loss: 0.8200381994247437\n",
      "Epoch 29, Batch 203/462, Loss: 0.7144781947135925\n",
      "Epoch 29, Batch 204/462, Loss: 0.6889768838882446\n",
      "Epoch 29, Batch 205/462, Loss: 0.7175698280334473\n",
      "Epoch 29, Batch 206/462, Loss: 0.645176887512207\n",
      "Epoch 29, Batch 207/462, Loss: 0.8497536778450012\n",
      "Epoch 29, Batch 208/462, Loss: 0.77718186378479\n",
      "Epoch 29, Batch 209/462, Loss: 0.6125507354736328\n",
      "Epoch 29, Batch 210/462, Loss: 0.6416618227958679\n",
      "Epoch 29, Batch 211/462, Loss: 0.6031798124313354\n",
      "Epoch 29, Batch 212/462, Loss: 0.819441020488739\n",
      "Epoch 29, Batch 213/462, Loss: 0.6466452479362488\n",
      "Epoch 29, Batch 214/462, Loss: 0.7794772982597351\n",
      "Epoch 29, Batch 215/462, Loss: 0.6954078078269958\n",
      "Epoch 29, Batch 216/462, Loss: 0.6129570603370667\n",
      "Epoch 29, Batch 217/462, Loss: 0.7593885064125061\n",
      "Epoch 29, Batch 218/462, Loss: 0.8408700823783875\n",
      "Epoch 29, Batch 219/462, Loss: 0.7677457928657532\n",
      "Epoch 29, Batch 220/462, Loss: 0.6424293518066406\n",
      "Epoch 29, Batch 221/462, Loss: 0.8102176785469055\n",
      "Epoch 29, Batch 222/462, Loss: 0.7638925313949585\n",
      "Epoch 29, Batch 223/462, Loss: 0.7528269290924072\n",
      "Epoch 29, Batch 224/462, Loss: 0.6804161071777344\n",
      "Epoch 29, Batch 225/462, Loss: 0.8118559718132019\n",
      "Epoch 29, Batch 226/462, Loss: 0.8596935272216797\n",
      "Epoch 29, Batch 227/462, Loss: 0.82583087682724\n",
      "Epoch 29, Batch 228/462, Loss: 0.6863060593605042\n",
      "Epoch 29, Batch 229/462, Loss: 0.6824787259101868\n",
      "Epoch 29, Batch 230/462, Loss: 0.7603091597557068\n",
      "Epoch 29, Batch 231/462, Loss: 0.8197474479675293\n",
      "Epoch 29, Batch 232/462, Loss: 0.816003143787384\n",
      "Epoch 29, Batch 233/462, Loss: 0.682915210723877\n",
      "Epoch 29, Batch 234/462, Loss: 0.7768946886062622\n",
      "Epoch 29, Batch 235/462, Loss: 0.7232668399810791\n",
      "Epoch 29, Batch 236/462, Loss: 0.8486191630363464\n",
      "Epoch 29, Batch 237/462, Loss: 0.7433587312698364\n",
      "Epoch 29, Batch 238/462, Loss: 0.8010416030883789\n",
      "Epoch 29, Batch 239/462, Loss: 0.851998507976532\n",
      "Epoch 29, Batch 240/462, Loss: 0.7267557382583618\n",
      "Epoch 29, Batch 241/462, Loss: 0.615875244140625\n",
      "Epoch 29, Batch 242/462, Loss: 0.5653732419013977\n",
      "Epoch 29, Batch 243/462, Loss: 0.9043370485305786\n",
      "Epoch 29, Batch 244/462, Loss: 0.6878622174263\n",
      "Epoch 29, Batch 245/462, Loss: 0.6871647238731384\n",
      "Epoch 29, Batch 246/462, Loss: 0.7612553834915161\n",
      "Epoch 29, Batch 247/462, Loss: 0.9355814456939697\n",
      "Epoch 29, Batch 248/462, Loss: 0.7884130477905273\n",
      "Epoch 29, Batch 249/462, Loss: 0.7012614607810974\n",
      "Epoch 29, Batch 250/462, Loss: 0.7063062191009521\n",
      "Epoch 29, Batch 251/462, Loss: 0.770691990852356\n",
      "Epoch 29, Batch 252/462, Loss: 0.6026501059532166\n",
      "Epoch 29, Batch 253/462, Loss: 1.0475419759750366\n",
      "Epoch 29, Batch 254/462, Loss: 0.5844244956970215\n",
      "Epoch 29, Batch 255/462, Loss: 0.7459954619407654\n",
      "Epoch 29, Batch 256/462, Loss: 0.8099005818367004\n",
      "Epoch 29, Batch 257/462, Loss: 0.6436544060707092\n",
      "Epoch 29, Batch 258/462, Loss: 0.5601102113723755\n",
      "Epoch 29, Batch 259/462, Loss: 0.7381266355514526\n",
      "Epoch 29, Batch 260/462, Loss: 0.7536828517913818\n",
      "Epoch 29, Batch 261/462, Loss: 0.8389389514923096\n",
      "Epoch 29, Batch 262/462, Loss: 0.6624658703804016\n",
      "Epoch 29, Batch 263/462, Loss: 0.576123833656311\n",
      "Epoch 29, Batch 264/462, Loss: 0.6896538734436035\n",
      "Epoch 29, Batch 265/462, Loss: 0.7322421073913574\n",
      "Epoch 29, Batch 266/462, Loss: 0.6748863458633423\n",
      "Epoch 29, Batch 267/462, Loss: 0.8013764023780823\n",
      "Epoch 29, Batch 268/462, Loss: 0.6530416011810303\n",
      "Epoch 29, Batch 269/462, Loss: 0.7091081142425537\n",
      "Epoch 29, Batch 270/462, Loss: 0.7957271933555603\n",
      "Epoch 29, Batch 271/462, Loss: 0.5558674335479736\n",
      "Epoch 29, Batch 272/462, Loss: 0.6429733633995056\n",
      "Epoch 29, Batch 273/462, Loss: 0.6240500211715698\n",
      "Epoch 29, Batch 274/462, Loss: 0.7353354692459106\n",
      "Epoch 29, Batch 275/462, Loss: 0.804688036441803\n",
      "Epoch 29, Batch 276/462, Loss: 0.5405861139297485\n",
      "Epoch 29, Batch 277/462, Loss: 0.9212273955345154\n",
      "Epoch 29, Batch 278/462, Loss: 0.7840336561203003\n",
      "Epoch 29, Batch 279/462, Loss: 0.6256139874458313\n",
      "Epoch 29, Batch 280/462, Loss: 0.5858429074287415\n",
      "Epoch 29, Batch 281/462, Loss: 0.6967281103134155\n",
      "Epoch 29, Batch 282/462, Loss: 0.7473804354667664\n",
      "Epoch 29, Batch 283/462, Loss: 0.6923185586929321\n",
      "Epoch 29, Batch 284/462, Loss: 0.7224167585372925\n",
      "Epoch 29, Batch 285/462, Loss: 0.7698438167572021\n",
      "Epoch 29, Batch 286/462, Loss: 0.6550970077514648\n",
      "Epoch 29, Batch 287/462, Loss: 0.6328718066215515\n",
      "Epoch 29, Batch 288/462, Loss: 0.8827303647994995\n",
      "Epoch 29, Batch 289/462, Loss: 0.7308757901191711\n",
      "Epoch 29, Batch 290/462, Loss: 0.782496988773346\n",
      "Epoch 29, Batch 291/462, Loss: 0.6406326293945312\n",
      "Epoch 29, Batch 292/462, Loss: 0.5507497787475586\n",
      "Epoch 29, Batch 293/462, Loss: 0.7690921425819397\n",
      "Epoch 29, Batch 294/462, Loss: 0.8404138684272766\n",
      "Epoch 29, Batch 295/462, Loss: 0.7282084226608276\n",
      "Epoch 29, Batch 296/462, Loss: 0.6749680638313293\n",
      "Epoch 29, Batch 297/462, Loss: 0.7571694254875183\n",
      "Epoch 29, Batch 298/462, Loss: 0.847578227519989\n",
      "Epoch 29, Batch 299/462, Loss: 0.7134116291999817\n",
      "Epoch 29, Batch 300/462, Loss: 0.6064696907997131\n",
      "Epoch 29, Batch 301/462, Loss: 0.7220637798309326\n",
      "Epoch 29, Batch 302/462, Loss: 0.8875703811645508\n",
      "Epoch 29, Batch 303/462, Loss: 0.8119875192642212\n",
      "Epoch 29, Batch 304/462, Loss: 0.708143413066864\n",
      "Epoch 29, Batch 305/462, Loss: 0.72100830078125\n",
      "Epoch 29, Batch 306/462, Loss: 0.8064522743225098\n",
      "Epoch 29, Batch 307/462, Loss: 0.6676449179649353\n",
      "Epoch 29, Batch 308/462, Loss: 0.7231152653694153\n",
      "Epoch 29, Batch 309/462, Loss: 0.6868147850036621\n",
      "Epoch 29, Batch 310/462, Loss: 0.6458433866500854\n",
      "Epoch 29, Batch 311/462, Loss: 0.7535066604614258\n",
      "Epoch 29, Batch 312/462, Loss: 0.7456837892532349\n",
      "Epoch 29, Batch 313/462, Loss: 0.8544794321060181\n",
      "Epoch 29, Batch 314/462, Loss: 0.685714066028595\n",
      "Epoch 29, Batch 315/462, Loss: 0.8077072501182556\n",
      "Epoch 29, Batch 316/462, Loss: 0.7710750102996826\n",
      "Epoch 29, Batch 317/462, Loss: 0.586992621421814\n",
      "Epoch 29, Batch 318/462, Loss: 0.6311250329017639\n",
      "Epoch 29, Batch 319/462, Loss: 0.7425425052642822\n",
      "Epoch 29, Batch 320/462, Loss: 0.6394275426864624\n",
      "Epoch 29, Batch 321/462, Loss: 0.7022907733917236\n",
      "Epoch 29, Batch 322/462, Loss: 0.7721339464187622\n",
      "Epoch 29, Batch 323/462, Loss: 0.584425151348114\n",
      "Epoch 29, Batch 324/462, Loss: 0.8704143762588501\n",
      "Epoch 29, Batch 325/462, Loss: 0.6995262503623962\n",
      "Epoch 29, Batch 326/462, Loss: 0.8313966989517212\n",
      "Epoch 29, Batch 327/462, Loss: 0.6661220192909241\n",
      "Epoch 29, Batch 328/462, Loss: 0.8152844905853271\n",
      "Epoch 29, Batch 329/462, Loss: 0.7026311755180359\n",
      "Epoch 29, Batch 330/462, Loss: 0.6118164658546448\n",
      "Epoch 29, Batch 331/462, Loss: 0.8049800992012024\n",
      "Epoch 29, Batch 332/462, Loss: 0.7442304491996765\n",
      "Epoch 29, Batch 333/462, Loss: 0.722610592842102\n",
      "Epoch 29, Batch 334/462, Loss: 0.6545440554618835\n",
      "Epoch 29, Batch 335/462, Loss: 0.7512163519859314\n",
      "Epoch 29, Batch 336/462, Loss: 0.9134712219238281\n",
      "Epoch 29, Batch 337/462, Loss: 0.835623025894165\n",
      "Epoch 29, Batch 338/462, Loss: 0.8288923501968384\n",
      "Epoch 29, Batch 339/462, Loss: 0.7650337219238281\n",
      "Epoch 29, Batch 340/462, Loss: 0.7064772248268127\n",
      "Epoch 29, Batch 341/462, Loss: 0.6351150274276733\n",
      "Epoch 29, Batch 342/462, Loss: 0.6454677581787109\n",
      "Epoch 29, Batch 343/462, Loss: 0.5648201107978821\n",
      "Epoch 29, Batch 344/462, Loss: 0.6259520053863525\n",
      "Epoch 29, Batch 345/462, Loss: 0.49737516045570374\n",
      "Epoch 29, Batch 346/462, Loss: 0.7770169377326965\n",
      "Epoch 29, Batch 347/462, Loss: 0.6543576121330261\n",
      "Epoch 29, Batch 348/462, Loss: 0.6655614376068115\n",
      "Epoch 29, Batch 349/462, Loss: 0.7795396447181702\n",
      "Epoch 29, Batch 350/462, Loss: 0.6478785872459412\n",
      "Epoch 29, Batch 351/462, Loss: 0.6543020009994507\n",
      "Epoch 29, Batch 352/462, Loss: 0.7483148574829102\n",
      "Epoch 29, Batch 353/462, Loss: 0.7701760530471802\n",
      "Epoch 29, Batch 354/462, Loss: 0.7329230904579163\n",
      "Epoch 29, Batch 355/462, Loss: 0.6351349353790283\n",
      "Epoch 29, Batch 356/462, Loss: 0.5799252390861511\n",
      "Epoch 29, Batch 357/462, Loss: 0.7715837955474854\n",
      "Epoch 29, Batch 358/462, Loss: 0.7290064096450806\n",
      "Epoch 29, Batch 359/462, Loss: 0.7451902627944946\n",
      "Epoch 29, Batch 360/462, Loss: 0.6578251719474792\n",
      "Epoch 29, Batch 361/462, Loss: 0.557051420211792\n",
      "Epoch 29, Batch 362/462, Loss: 0.7297157645225525\n",
      "Epoch 29, Batch 363/462, Loss: 0.7457104921340942\n",
      "Epoch 29, Batch 364/462, Loss: 0.7051666378974915\n",
      "Epoch 29, Batch 365/462, Loss: 0.6816354393959045\n",
      "Epoch 29, Batch 366/462, Loss: 0.7990907430648804\n",
      "Epoch 29, Batch 367/462, Loss: 0.6157106161117554\n",
      "Epoch 29, Batch 368/462, Loss: 0.7821619510650635\n",
      "Epoch 29, Batch 369/462, Loss: 0.7027910351753235\n",
      "Epoch 29, Batch 370/462, Loss: 0.9667118787765503\n",
      "Epoch 29, Batch 371/462, Loss: 0.5452932715415955\n",
      "Epoch 29, Batch 372/462, Loss: 0.6472727656364441\n",
      "Epoch 29, Batch 373/462, Loss: 0.6704270839691162\n",
      "Epoch 29, Batch 374/462, Loss: 0.6414737701416016\n",
      "Epoch 29, Batch 375/462, Loss: 0.7334573268890381\n",
      "Epoch 29, Batch 376/462, Loss: 0.687393307685852\n",
      "Epoch 29, Batch 377/462, Loss: 0.7246037125587463\n",
      "Epoch 29, Batch 378/462, Loss: 0.728512167930603\n",
      "Epoch 29, Batch 379/462, Loss: 0.9222539067268372\n",
      "Epoch 29, Batch 380/462, Loss: 0.4771904945373535\n",
      "Epoch 29, Batch 381/462, Loss: 0.5484781265258789\n",
      "Epoch 29, Batch 382/462, Loss: 0.7390341758728027\n",
      "Epoch 29, Batch 383/462, Loss: 0.6396763324737549\n",
      "Epoch 29, Batch 384/462, Loss: 0.8252535462379456\n",
      "Epoch 29, Batch 385/462, Loss: 0.6690461039543152\n",
      "Epoch 29, Batch 386/462, Loss: 0.5505192875862122\n",
      "Epoch 29, Batch 387/462, Loss: 0.8404363989830017\n",
      "Epoch 29, Batch 388/462, Loss: 0.6711574792861938\n",
      "Epoch 29, Batch 389/462, Loss: 0.6810218095779419\n",
      "Epoch 29, Batch 390/462, Loss: 0.66064453125\n",
      "Epoch 29, Batch 391/462, Loss: 0.71535325050354\n",
      "Epoch 29, Batch 392/462, Loss: 0.6685693264007568\n",
      "Epoch 29, Batch 393/462, Loss: 0.5762711763381958\n",
      "Epoch 29, Batch 394/462, Loss: 0.668515145778656\n",
      "Epoch 29, Batch 395/462, Loss: 0.8228408694267273\n",
      "Epoch 29, Batch 396/462, Loss: 0.9249086380004883\n",
      "Epoch 29, Batch 397/462, Loss: 0.8024303317070007\n",
      "Epoch 29, Batch 398/462, Loss: 0.7944745421409607\n",
      "Epoch 29, Batch 399/462, Loss: 0.8100998401641846\n",
      "Epoch 29, Batch 400/462, Loss: 0.7662426233291626\n",
      "Epoch 29, Batch 401/462, Loss: 0.6645818948745728\n",
      "Epoch 29, Batch 402/462, Loss: 0.5393649935722351\n",
      "Epoch 29, Batch 403/462, Loss: 0.8946040868759155\n",
      "Epoch 29, Batch 404/462, Loss: 0.8303371071815491\n",
      "Epoch 29, Batch 405/462, Loss: 1.0274574756622314\n",
      "Epoch 29, Batch 406/462, Loss: 0.9078969955444336\n",
      "Epoch 29, Batch 407/462, Loss: 0.6125959157943726\n",
      "Epoch 29, Batch 408/462, Loss: 0.5734319090843201\n",
      "Epoch 29, Batch 409/462, Loss: 0.8075278401374817\n",
      "Epoch 29, Batch 410/462, Loss: 0.7920265197753906\n",
      "Epoch 29, Batch 411/462, Loss: 0.8369970917701721\n",
      "Epoch 29, Batch 412/462, Loss: 0.8256456851959229\n",
      "Epoch 29, Batch 413/462, Loss: 0.6185257434844971\n",
      "Epoch 29, Batch 414/462, Loss: 0.7929492592811584\n",
      "Epoch 29, Batch 415/462, Loss: 0.7243475914001465\n",
      "Epoch 29, Batch 416/462, Loss: 0.8271070718765259\n",
      "Epoch 29, Batch 417/462, Loss: 0.6273233294487\n",
      "Epoch 29, Batch 418/462, Loss: 0.6214006543159485\n",
      "Epoch 29, Batch 419/462, Loss: 0.8906936645507812\n",
      "Epoch 29, Batch 420/462, Loss: 0.7207547426223755\n",
      "Epoch 29, Batch 421/462, Loss: 0.9013658761978149\n",
      "Epoch 29, Batch 422/462, Loss: 0.5829935073852539\n",
      "Epoch 29, Batch 423/462, Loss: 0.9138557314872742\n",
      "Epoch 29, Batch 424/462, Loss: 0.9110800623893738\n",
      "Epoch 29, Batch 425/462, Loss: 0.7312649488449097\n",
      "Epoch 29, Batch 426/462, Loss: 0.71819007396698\n",
      "Epoch 29, Batch 427/462, Loss: 0.8521681427955627\n",
      "Epoch 29, Batch 428/462, Loss: 0.704943835735321\n",
      "Epoch 29, Batch 429/462, Loss: 0.7307252287864685\n",
      "Epoch 29, Batch 430/462, Loss: 0.7666974663734436\n",
      "Epoch 29, Batch 431/462, Loss: 0.872971773147583\n",
      "Epoch 29, Batch 432/462, Loss: 0.5925875306129456\n",
      "Epoch 29, Batch 433/462, Loss: 0.63941490650177\n",
      "Epoch 29, Batch 434/462, Loss: 0.7262019515037537\n",
      "Epoch 29, Batch 435/462, Loss: 0.9787442684173584\n",
      "Epoch 29, Batch 436/462, Loss: 0.8200078010559082\n",
      "Epoch 29, Batch 437/462, Loss: 0.5993742942810059\n",
      "Epoch 29, Batch 438/462, Loss: 0.6148298978805542\n",
      "Epoch 29, Batch 439/462, Loss: 0.7313560843467712\n",
      "Epoch 29, Batch 440/462, Loss: 0.5729509592056274\n",
      "Epoch 29, Batch 441/462, Loss: 0.5507444739341736\n",
      "Epoch 29, Batch 442/462, Loss: 0.7888624668121338\n",
      "Epoch 29, Batch 443/462, Loss: 0.6665489077568054\n",
      "Epoch 29, Batch 444/462, Loss: 0.7577320337295532\n",
      "Epoch 29, Batch 445/462, Loss: 0.722809910774231\n",
      "Epoch 29, Batch 446/462, Loss: 0.7256937026977539\n",
      "Epoch 29, Batch 447/462, Loss: 0.5690861940383911\n",
      "Epoch 29, Batch 448/462, Loss: 0.7910270690917969\n",
      "Epoch 29, Batch 449/462, Loss: 0.9613716006278992\n",
      "Epoch 29, Batch 450/462, Loss: 0.8082928657531738\n",
      "Epoch 29, Batch 451/462, Loss: 0.7249612212181091\n",
      "Epoch 29, Batch 452/462, Loss: 0.7499853372573853\n",
      "Epoch 29, Batch 453/462, Loss: 0.6943095922470093\n",
      "Epoch 29, Batch 454/462, Loss: 0.8616156578063965\n",
      "Epoch 29, Batch 455/462, Loss: 0.6542286276817322\n",
      "Epoch 29, Batch 456/462, Loss: 0.720623791217804\n",
      "Epoch 29, Batch 457/462, Loss: 0.8234835267066956\n",
      "Epoch 29, Batch 458/462, Loss: 0.6428534388542175\n",
      "Epoch 29, Batch 459/462, Loss: 0.7199800610542297\n",
      "Epoch 29, Batch 460/462, Loss: 0.8002365827560425\n",
      "Epoch 29, Batch 461/462, Loss: 0.7957984805107117\n",
      "Epoch 29, Batch 462/462, Loss: 0.9868226051330566\n",
      "Epoch 29, Loss: 338.930711299181\n",
      "Epoch 30, Batch 1/462, Loss: 0.7287046909332275\n",
      "Epoch 30, Batch 2/462, Loss: 0.842440664768219\n",
      "Epoch 30, Batch 3/462, Loss: 0.5876478552818298\n",
      "Epoch 30, Batch 4/462, Loss: 0.6276229023933411\n",
      "Epoch 30, Batch 5/462, Loss: 0.5978115200996399\n",
      "Epoch 30, Batch 6/462, Loss: 0.7425501346588135\n",
      "Epoch 30, Batch 7/462, Loss: 0.7123783826828003\n",
      "Epoch 30, Batch 8/462, Loss: 0.7608698606491089\n",
      "Epoch 30, Batch 9/462, Loss: 0.6452068090438843\n",
      "Epoch 30, Batch 10/462, Loss: 0.69631427526474\n",
      "Epoch 30, Batch 11/462, Loss: 0.7346575856208801\n",
      "Epoch 30, Batch 12/462, Loss: 0.6047796607017517\n",
      "Epoch 30, Batch 13/462, Loss: 0.6388853788375854\n",
      "Epoch 30, Batch 14/462, Loss: 0.7062899470329285\n",
      "Epoch 30, Batch 15/462, Loss: 0.6161547303199768\n",
      "Epoch 30, Batch 16/462, Loss: 0.6715883016586304\n",
      "Epoch 30, Batch 17/462, Loss: 0.7358732223510742\n",
      "Epoch 30, Batch 18/462, Loss: 0.9823673963546753\n",
      "Epoch 30, Batch 19/462, Loss: 0.6375068426132202\n",
      "Epoch 30, Batch 20/462, Loss: 0.8149681687355042\n",
      "Epoch 30, Batch 21/462, Loss: 0.6574625968933105\n",
      "Epoch 30, Batch 22/462, Loss: 0.7387528419494629\n",
      "Epoch 30, Batch 23/462, Loss: 0.7904011011123657\n",
      "Epoch 30, Batch 24/462, Loss: 0.6643853783607483\n",
      "Epoch 30, Batch 25/462, Loss: 0.9380853772163391\n",
      "Epoch 30, Batch 26/462, Loss: 0.8114209175109863\n",
      "Epoch 30, Batch 27/462, Loss: 0.8496145009994507\n",
      "Epoch 30, Batch 28/462, Loss: 0.71528160572052\n",
      "Epoch 30, Batch 29/462, Loss: 0.6152503490447998\n",
      "Epoch 30, Batch 30/462, Loss: 0.6847264170646667\n",
      "Epoch 30, Batch 31/462, Loss: 0.8024497628211975\n",
      "Epoch 30, Batch 32/462, Loss: 0.689275860786438\n",
      "Epoch 30, Batch 33/462, Loss: 0.7867735624313354\n",
      "Epoch 30, Batch 34/462, Loss: 0.5697622299194336\n",
      "Epoch 30, Batch 35/462, Loss: 0.6384158134460449\n",
      "Epoch 30, Batch 36/462, Loss: 0.6893712282180786\n",
      "Epoch 30, Batch 37/462, Loss: 0.9293790459632874\n",
      "Epoch 30, Batch 38/462, Loss: 0.8074268102645874\n",
      "Epoch 30, Batch 39/462, Loss: 0.7035630345344543\n",
      "Epoch 30, Batch 40/462, Loss: 0.865964412689209\n",
      "Epoch 30, Batch 41/462, Loss: 0.7096332907676697\n",
      "Epoch 30, Batch 42/462, Loss: 0.7293803691864014\n",
      "Epoch 30, Batch 43/462, Loss: 0.8227505683898926\n",
      "Epoch 30, Batch 44/462, Loss: 0.8565605282783508\n",
      "Epoch 30, Batch 45/462, Loss: 0.7361021041870117\n",
      "Epoch 30, Batch 46/462, Loss: 0.6167211532592773\n",
      "Epoch 30, Batch 47/462, Loss: 0.892244815826416\n",
      "Epoch 30, Batch 48/462, Loss: 0.754812479019165\n",
      "Epoch 30, Batch 49/462, Loss: 0.6609804034233093\n",
      "Epoch 30, Batch 50/462, Loss: 0.884253203868866\n",
      "Epoch 30, Batch 51/462, Loss: 0.6933454275131226\n",
      "Epoch 30, Batch 52/462, Loss: 0.6322402954101562\n",
      "Epoch 30, Batch 53/462, Loss: 0.888520359992981\n",
      "Epoch 30, Batch 54/462, Loss: 0.7082275748252869\n",
      "Epoch 30, Batch 55/462, Loss: 0.6628457903862\n",
      "Epoch 30, Batch 56/462, Loss: 0.5749465227127075\n",
      "Epoch 30, Batch 57/462, Loss: 0.7003396153450012\n",
      "Epoch 30, Batch 58/462, Loss: 0.6778283715248108\n",
      "Epoch 30, Batch 59/462, Loss: 0.8159393668174744\n",
      "Epoch 30, Batch 60/462, Loss: 0.579020619392395\n",
      "Epoch 30, Batch 61/462, Loss: 0.7760690450668335\n",
      "Epoch 30, Batch 62/462, Loss: 0.671347975730896\n",
      "Epoch 30, Batch 63/462, Loss: 0.8134253621101379\n",
      "Epoch 30, Batch 64/462, Loss: 0.6270576119422913\n",
      "Epoch 30, Batch 65/462, Loss: 0.6436718106269836\n",
      "Epoch 30, Batch 66/462, Loss: 0.6827243566513062\n",
      "Epoch 30, Batch 67/462, Loss: 0.7741243839263916\n",
      "Epoch 30, Batch 68/462, Loss: 0.6695175170898438\n",
      "Epoch 30, Batch 69/462, Loss: 0.8911007046699524\n",
      "Epoch 30, Batch 70/462, Loss: 0.7039480209350586\n",
      "Epoch 30, Batch 71/462, Loss: 0.676316499710083\n",
      "Epoch 30, Batch 72/462, Loss: 0.5939446687698364\n",
      "Epoch 30, Batch 73/462, Loss: 0.7481395602226257\n",
      "Epoch 30, Batch 74/462, Loss: 0.6382197737693787\n",
      "Epoch 30, Batch 75/462, Loss: 0.6590065956115723\n",
      "Epoch 30, Batch 76/462, Loss: 0.8982638120651245\n",
      "Epoch 30, Batch 77/462, Loss: 0.6978564262390137\n",
      "Epoch 30, Batch 78/462, Loss: 0.5018314123153687\n",
      "Epoch 30, Batch 79/462, Loss: 0.7091798782348633\n",
      "Epoch 30, Batch 80/462, Loss: 0.7482465505599976\n",
      "Epoch 30, Batch 81/462, Loss: 0.5987094640731812\n",
      "Epoch 30, Batch 82/462, Loss: 0.648250937461853\n",
      "Epoch 30, Batch 83/462, Loss: 0.6766751408576965\n",
      "Epoch 30, Batch 84/462, Loss: 0.6366325616836548\n",
      "Epoch 30, Batch 85/462, Loss: 0.6892066597938538\n",
      "Epoch 30, Batch 86/462, Loss: 0.8203555941581726\n",
      "Epoch 30, Batch 87/462, Loss: 0.7590004801750183\n",
      "Epoch 30, Batch 88/462, Loss: 0.8940234780311584\n",
      "Epoch 30, Batch 89/462, Loss: 0.6638748049736023\n",
      "Epoch 30, Batch 90/462, Loss: 0.5907633900642395\n",
      "Epoch 30, Batch 91/462, Loss: 0.5762364864349365\n",
      "Epoch 30, Batch 92/462, Loss: 0.8725597262382507\n",
      "Epoch 30, Batch 93/462, Loss: 0.5593454837799072\n",
      "Epoch 30, Batch 94/462, Loss: 0.7236981391906738\n",
      "Epoch 30, Batch 95/462, Loss: 0.7047216892242432\n",
      "Epoch 30, Batch 96/462, Loss: 0.7252635359764099\n",
      "Epoch 30, Batch 97/462, Loss: 0.5359840393066406\n",
      "Epoch 30, Batch 98/462, Loss: 0.6104035377502441\n",
      "Epoch 30, Batch 99/462, Loss: 0.7447617650032043\n",
      "Epoch 30, Batch 100/462, Loss: 0.7638558745384216\n",
      "Epoch 30, Batch 101/462, Loss: 0.6892624497413635\n",
      "Epoch 30, Batch 102/462, Loss: 0.7099852561950684\n",
      "Epoch 30, Batch 103/462, Loss: 0.6631167531013489\n",
      "Epoch 30, Batch 104/462, Loss: 0.7229759097099304\n",
      "Epoch 30, Batch 105/462, Loss: 0.7487464547157288\n",
      "Epoch 30, Batch 106/462, Loss: 0.6788402199745178\n",
      "Epoch 30, Batch 107/462, Loss: 0.8421240448951721\n",
      "Epoch 30, Batch 108/462, Loss: 0.6794560551643372\n",
      "Epoch 30, Batch 109/462, Loss: 0.8434791564941406\n",
      "Epoch 30, Batch 110/462, Loss: 0.7830251455307007\n",
      "Epoch 30, Batch 111/462, Loss: 0.7263123393058777\n",
      "Epoch 30, Batch 112/462, Loss: 0.8010616898536682\n",
      "Epoch 30, Batch 113/462, Loss: 0.7387162446975708\n",
      "Epoch 30, Batch 114/462, Loss: 0.7509909868240356\n",
      "Epoch 30, Batch 115/462, Loss: 0.6937077045440674\n",
      "Epoch 30, Batch 116/462, Loss: 0.7371689677238464\n",
      "Epoch 30, Batch 117/462, Loss: 0.8459956645965576\n",
      "Epoch 30, Batch 118/462, Loss: 0.8828942775726318\n",
      "Epoch 30, Batch 119/462, Loss: 0.7742655277252197\n",
      "Epoch 30, Batch 120/462, Loss: 0.7304089665412903\n",
      "Epoch 30, Batch 121/462, Loss: 0.6089236736297607\n",
      "Epoch 30, Batch 122/462, Loss: 0.6922300457954407\n",
      "Epoch 30, Batch 123/462, Loss: 0.8000357747077942\n",
      "Epoch 30, Batch 124/462, Loss: 0.7077094912528992\n",
      "Epoch 30, Batch 125/462, Loss: 0.7989292740821838\n",
      "Epoch 30, Batch 126/462, Loss: 0.591896653175354\n",
      "Epoch 30, Batch 127/462, Loss: 0.7970596551895142\n",
      "Epoch 30, Batch 128/462, Loss: 0.7020940780639648\n",
      "Epoch 30, Batch 129/462, Loss: 0.7968404293060303\n",
      "Epoch 30, Batch 130/462, Loss: 0.6045809984207153\n",
      "Epoch 30, Batch 131/462, Loss: 0.7711412310600281\n",
      "Epoch 30, Batch 132/462, Loss: 0.6445001363754272\n",
      "Epoch 30, Batch 133/462, Loss: 0.6602374315261841\n",
      "Epoch 30, Batch 134/462, Loss: 0.675316572189331\n",
      "Epoch 30, Batch 135/462, Loss: 0.6077240109443665\n",
      "Epoch 30, Batch 136/462, Loss: 0.7030016183853149\n",
      "Epoch 30, Batch 137/462, Loss: 0.8263845443725586\n",
      "Epoch 30, Batch 138/462, Loss: 0.45833373069763184\n",
      "Epoch 30, Batch 139/462, Loss: 0.7878397703170776\n",
      "Epoch 30, Batch 140/462, Loss: 0.8182528018951416\n",
      "Epoch 30, Batch 141/462, Loss: 0.7849035859107971\n",
      "Epoch 30, Batch 142/462, Loss: 0.8704426884651184\n",
      "Epoch 30, Batch 143/462, Loss: 0.658061146736145\n",
      "Epoch 30, Batch 144/462, Loss: 0.5863478779792786\n",
      "Epoch 30, Batch 145/462, Loss: 0.5972999334335327\n",
      "Epoch 30, Batch 146/462, Loss: 0.791693925857544\n",
      "Epoch 30, Batch 147/462, Loss: 0.7186928391456604\n",
      "Epoch 30, Batch 148/462, Loss: 0.7726587057113647\n",
      "Epoch 30, Batch 149/462, Loss: 0.75408536195755\n",
      "Epoch 30, Batch 150/462, Loss: 0.8716188073158264\n",
      "Epoch 30, Batch 151/462, Loss: 0.8014912605285645\n",
      "Epoch 30, Batch 152/462, Loss: 0.6163455247879028\n",
      "Epoch 30, Batch 153/462, Loss: 0.8216223120689392\n",
      "Epoch 30, Batch 154/462, Loss: 0.8097584247589111\n",
      "Epoch 30, Batch 155/462, Loss: 0.7409250140190125\n",
      "Epoch 30, Batch 156/462, Loss: 0.6367828249931335\n",
      "Epoch 30, Batch 157/462, Loss: 0.5016473531723022\n",
      "Epoch 30, Batch 158/462, Loss: 0.6270509362220764\n",
      "Epoch 30, Batch 159/462, Loss: 0.8293807506561279\n",
      "Epoch 30, Batch 160/462, Loss: 0.7123088836669922\n",
      "Epoch 30, Batch 161/462, Loss: 0.6301395893096924\n",
      "Epoch 30, Batch 162/462, Loss: 0.691347599029541\n",
      "Epoch 30, Batch 163/462, Loss: 0.6799256205558777\n",
      "Epoch 30, Batch 164/462, Loss: 0.8457911014556885\n",
      "Epoch 30, Batch 165/462, Loss: 0.8242865800857544\n",
      "Epoch 30, Batch 166/462, Loss: 0.6922650933265686\n",
      "Epoch 30, Batch 167/462, Loss: 0.7575666308403015\n",
      "Epoch 30, Batch 168/462, Loss: 0.6368703842163086\n",
      "Epoch 30, Batch 169/462, Loss: 0.7673275470733643\n",
      "Epoch 30, Batch 170/462, Loss: 0.5463578701019287\n",
      "Epoch 30, Batch 171/462, Loss: 0.7603150606155396\n",
      "Epoch 30, Batch 172/462, Loss: 0.7461938261985779\n",
      "Epoch 30, Batch 173/462, Loss: 0.71626216173172\n",
      "Epoch 30, Batch 174/462, Loss: 0.8046197891235352\n",
      "Epoch 30, Batch 175/462, Loss: 0.7682600021362305\n",
      "Epoch 30, Batch 176/462, Loss: 0.8344162702560425\n",
      "Epoch 30, Batch 177/462, Loss: 0.7401577234268188\n",
      "Epoch 30, Batch 178/462, Loss: 0.8394293785095215\n",
      "Epoch 30, Batch 179/462, Loss: 0.6592675447463989\n",
      "Epoch 30, Batch 180/462, Loss: 0.7199637293815613\n",
      "Epoch 30, Batch 181/462, Loss: 0.7481560707092285\n",
      "Epoch 30, Batch 182/462, Loss: 0.7062829732894897\n",
      "Epoch 30, Batch 183/462, Loss: 0.6317440867424011\n",
      "Epoch 30, Batch 184/462, Loss: 0.6459612846374512\n",
      "Epoch 30, Batch 185/462, Loss: 0.7067088484764099\n",
      "Epoch 30, Batch 186/462, Loss: 0.8056952953338623\n",
      "Epoch 30, Batch 187/462, Loss: 0.8336045145988464\n",
      "Epoch 30, Batch 188/462, Loss: 0.7545821666717529\n",
      "Epoch 30, Batch 189/462, Loss: 0.9104173183441162\n",
      "Epoch 30, Batch 190/462, Loss: 0.6586304903030396\n",
      "Epoch 30, Batch 191/462, Loss: 0.6463111042976379\n",
      "Epoch 30, Batch 192/462, Loss: 0.822598397731781\n",
      "Epoch 30, Batch 193/462, Loss: 0.5187874436378479\n",
      "Epoch 30, Batch 194/462, Loss: 0.9389280676841736\n",
      "Epoch 30, Batch 195/462, Loss: 0.8335500955581665\n",
      "Epoch 30, Batch 196/462, Loss: 0.7053602933883667\n",
      "Epoch 30, Batch 197/462, Loss: 0.7822975516319275\n",
      "Epoch 30, Batch 198/462, Loss: 0.620510995388031\n",
      "Epoch 30, Batch 199/462, Loss: 0.8286272287368774\n",
      "Epoch 30, Batch 200/462, Loss: 0.7588145136833191\n",
      "Epoch 30, Batch 201/462, Loss: 0.9139687418937683\n",
      "Epoch 30, Batch 202/462, Loss: 0.7517597079277039\n",
      "Epoch 30, Batch 203/462, Loss: 0.801646888256073\n",
      "Epoch 30, Batch 204/462, Loss: 0.5595598220825195\n",
      "Epoch 30, Batch 205/462, Loss: 0.7368381023406982\n",
      "Epoch 30, Batch 206/462, Loss: 0.7489299178123474\n",
      "Epoch 30, Batch 207/462, Loss: 0.7279143929481506\n",
      "Epoch 30, Batch 208/462, Loss: 0.9028988480567932\n",
      "Epoch 30, Batch 209/462, Loss: 0.7680180668830872\n",
      "Epoch 30, Batch 210/462, Loss: 0.8382838368415833\n",
      "Epoch 30, Batch 211/462, Loss: 0.8082619309425354\n",
      "Epoch 30, Batch 212/462, Loss: 0.7054354548454285\n",
      "Epoch 30, Batch 213/462, Loss: 0.6654676198959351\n",
      "Epoch 30, Batch 214/462, Loss: 0.6356689929962158\n",
      "Epoch 30, Batch 215/462, Loss: 0.5782160758972168\n",
      "Epoch 30, Batch 216/462, Loss: 0.9187321066856384\n",
      "Epoch 30, Batch 217/462, Loss: 0.8193589448928833\n",
      "Epoch 30, Batch 218/462, Loss: 0.7441807985305786\n",
      "Epoch 30, Batch 219/462, Loss: 0.9162489175796509\n",
      "Epoch 30, Batch 220/462, Loss: 0.7056745290756226\n",
      "Epoch 30, Batch 221/462, Loss: 0.7438797354698181\n",
      "Epoch 30, Batch 222/462, Loss: 0.7513706684112549\n",
      "Epoch 30, Batch 223/462, Loss: 0.6251527667045593\n",
      "Epoch 30, Batch 224/462, Loss: 0.7643824815750122\n",
      "Epoch 30, Batch 225/462, Loss: 0.8718329071998596\n",
      "Epoch 30, Batch 226/462, Loss: 0.595050573348999\n",
      "Epoch 30, Batch 227/462, Loss: 0.6693070530891418\n",
      "Epoch 30, Batch 228/462, Loss: 0.9090644121170044\n",
      "Epoch 30, Batch 229/462, Loss: 0.763015866279602\n",
      "Epoch 30, Batch 230/462, Loss: 0.8540965914726257\n",
      "Epoch 30, Batch 231/462, Loss: 0.6518887877464294\n",
      "Epoch 30, Batch 232/462, Loss: 0.7676467299461365\n",
      "Epoch 30, Batch 233/462, Loss: 0.7503235340118408\n",
      "Epoch 30, Batch 234/462, Loss: 0.8780054450035095\n",
      "Epoch 30, Batch 235/462, Loss: 0.7029521465301514\n",
      "Epoch 30, Batch 236/462, Loss: 0.6374934315681458\n",
      "Epoch 30, Batch 237/462, Loss: 0.606281578540802\n",
      "Epoch 30, Batch 238/462, Loss: 0.8300660848617554\n",
      "Epoch 30, Batch 239/462, Loss: 0.6647658944129944\n",
      "Epoch 30, Batch 240/462, Loss: 0.7567351460456848\n",
      "Epoch 30, Batch 241/462, Loss: 0.6669055223464966\n",
      "Epoch 30, Batch 242/462, Loss: 0.7686563730239868\n",
      "Epoch 30, Batch 243/462, Loss: 0.7098165154457092\n",
      "Epoch 30, Batch 244/462, Loss: 0.8159552216529846\n",
      "Epoch 30, Batch 245/462, Loss: 0.8625638484954834\n",
      "Epoch 30, Batch 246/462, Loss: 0.7289437055587769\n",
      "Epoch 30, Batch 247/462, Loss: 0.6782403588294983\n",
      "Epoch 30, Batch 248/462, Loss: 0.8693786859512329\n",
      "Epoch 30, Batch 249/462, Loss: 0.8407045006752014\n",
      "Epoch 30, Batch 250/462, Loss: 0.7081131935119629\n",
      "Epoch 30, Batch 251/462, Loss: 0.878098726272583\n",
      "Epoch 30, Batch 252/462, Loss: 0.8208038210868835\n",
      "Epoch 30, Batch 253/462, Loss: 0.8012667298316956\n",
      "Epoch 30, Batch 254/462, Loss: 0.8363145589828491\n",
      "Epoch 30, Batch 255/462, Loss: 0.7875471115112305\n",
      "Epoch 30, Batch 256/462, Loss: 0.7181521654129028\n",
      "Epoch 30, Batch 257/462, Loss: 0.6058022379875183\n",
      "Epoch 30, Batch 258/462, Loss: 0.766086757183075\n",
      "Epoch 30, Batch 259/462, Loss: 0.7872761487960815\n",
      "Epoch 30, Batch 260/462, Loss: 0.780756413936615\n",
      "Epoch 30, Batch 261/462, Loss: 0.7456175684928894\n",
      "Epoch 30, Batch 262/462, Loss: 0.6367067098617554\n",
      "Epoch 30, Batch 263/462, Loss: 0.6644566655158997\n",
      "Epoch 30, Batch 264/462, Loss: 0.7621575593948364\n",
      "Epoch 30, Batch 265/462, Loss: 0.8089127540588379\n",
      "Epoch 30, Batch 266/462, Loss: 0.9522031545639038\n",
      "Epoch 30, Batch 267/462, Loss: 0.7719348669052124\n",
      "Epoch 30, Batch 268/462, Loss: 0.7675254940986633\n",
      "Epoch 30, Batch 269/462, Loss: 0.5533207654953003\n",
      "Epoch 30, Batch 270/462, Loss: 0.6815930008888245\n",
      "Epoch 30, Batch 271/462, Loss: 0.7318221926689148\n",
      "Epoch 30, Batch 272/462, Loss: 0.7257645130157471\n",
      "Epoch 30, Batch 273/462, Loss: 0.7492049336433411\n",
      "Epoch 30, Batch 274/462, Loss: 0.7767085433006287\n",
      "Epoch 30, Batch 275/462, Loss: 0.7579025626182556\n",
      "Epoch 30, Batch 276/462, Loss: 0.6355332732200623\n",
      "Epoch 30, Batch 277/462, Loss: 0.7760513424873352\n",
      "Epoch 30, Batch 278/462, Loss: 0.6936807036399841\n",
      "Epoch 30, Batch 279/462, Loss: 0.6278219223022461\n",
      "Epoch 30, Batch 280/462, Loss: 0.778311550617218\n",
      "Epoch 30, Batch 281/462, Loss: 0.6818637847900391\n",
      "Epoch 30, Batch 282/462, Loss: 0.6401385068893433\n",
      "Epoch 30, Batch 283/462, Loss: 0.5710433125495911\n",
      "Epoch 30, Batch 284/462, Loss: 0.7266694903373718\n",
      "Epoch 30, Batch 285/462, Loss: 0.6786141395568848\n",
      "Epoch 30, Batch 286/462, Loss: 0.6516788601875305\n",
      "Epoch 30, Batch 287/462, Loss: 0.6457559466362\n",
      "Epoch 30, Batch 288/462, Loss: 0.650443971157074\n",
      "Epoch 30, Batch 289/462, Loss: 0.6867895126342773\n",
      "Epoch 30, Batch 290/462, Loss: 0.7093299031257629\n",
      "Epoch 30, Batch 291/462, Loss: 0.9397053122520447\n",
      "Epoch 30, Batch 292/462, Loss: 1.0103504657745361\n",
      "Epoch 30, Batch 293/462, Loss: 0.7480466961860657\n",
      "Epoch 30, Batch 294/462, Loss: 0.7709213495254517\n",
      "Epoch 30, Batch 295/462, Loss: 0.579337477684021\n",
      "Epoch 30, Batch 296/462, Loss: 0.6956846714019775\n",
      "Epoch 30, Batch 297/462, Loss: 0.8362768888473511\n",
      "Epoch 30, Batch 298/462, Loss: 0.6435826420783997\n",
      "Epoch 30, Batch 299/462, Loss: 0.8523623943328857\n",
      "Epoch 30, Batch 300/462, Loss: 0.8720430135726929\n",
      "Epoch 30, Batch 301/462, Loss: 0.6921806931495667\n",
      "Epoch 30, Batch 302/462, Loss: 0.7351653575897217\n",
      "Epoch 30, Batch 303/462, Loss: 0.8595961928367615\n",
      "Epoch 30, Batch 304/462, Loss: 0.6832817792892456\n",
      "Epoch 30, Batch 305/462, Loss: 0.5354188084602356\n",
      "Epoch 30, Batch 306/462, Loss: 0.7263267636299133\n",
      "Epoch 30, Batch 307/462, Loss: 0.7384088635444641\n",
      "Epoch 30, Batch 308/462, Loss: 0.6704617738723755\n",
      "Epoch 30, Batch 309/462, Loss: 0.6267403960227966\n",
      "Epoch 30, Batch 310/462, Loss: 0.9290192127227783\n",
      "Epoch 30, Batch 311/462, Loss: 0.7383392453193665\n",
      "Epoch 30, Batch 312/462, Loss: 0.742780864238739\n",
      "Epoch 30, Batch 313/462, Loss: 0.9135738611221313\n",
      "Epoch 30, Batch 314/462, Loss: 0.8834130764007568\n",
      "Epoch 30, Batch 315/462, Loss: 0.5847394466400146\n",
      "Epoch 30, Batch 316/462, Loss: 0.6407081484794617\n",
      "Epoch 30, Batch 317/462, Loss: 0.7819201946258545\n",
      "Epoch 30, Batch 318/462, Loss: 0.6338533163070679\n",
      "Epoch 30, Batch 319/462, Loss: 0.7161720395088196\n",
      "Epoch 30, Batch 320/462, Loss: 0.7972638010978699\n",
      "Epoch 30, Batch 321/462, Loss: 0.6896122097969055\n",
      "Epoch 30, Batch 322/462, Loss: 0.6362526416778564\n",
      "Epoch 30, Batch 323/462, Loss: 0.5464588403701782\n",
      "Epoch 30, Batch 324/462, Loss: 0.6867808103561401\n",
      "Epoch 30, Batch 325/462, Loss: 0.7264061570167542\n",
      "Epoch 30, Batch 326/462, Loss: 0.765325129032135\n",
      "Epoch 30, Batch 327/462, Loss: 0.7319416403770447\n",
      "Epoch 30, Batch 328/462, Loss: 0.69942307472229\n",
      "Epoch 30, Batch 329/462, Loss: 0.7274246215820312\n",
      "Epoch 30, Batch 330/462, Loss: 0.8690673112869263\n",
      "Epoch 30, Batch 331/462, Loss: 0.7001914978027344\n",
      "Epoch 30, Batch 332/462, Loss: 0.8250778317451477\n",
      "Epoch 30, Batch 333/462, Loss: 0.7081572413444519\n",
      "Epoch 30, Batch 334/462, Loss: 0.6210089921951294\n",
      "Epoch 30, Batch 335/462, Loss: 0.6417769193649292\n",
      "Epoch 30, Batch 336/462, Loss: 0.711577832698822\n",
      "Epoch 30, Batch 337/462, Loss: 0.663907527923584\n",
      "Epoch 30, Batch 338/462, Loss: 0.6691840887069702\n",
      "Epoch 30, Batch 339/462, Loss: 0.6538539528846741\n",
      "Epoch 30, Batch 340/462, Loss: 0.8102225065231323\n",
      "Epoch 30, Batch 341/462, Loss: 0.7590327262878418\n",
      "Epoch 30, Batch 342/462, Loss: 0.7433817982673645\n",
      "Epoch 30, Batch 343/462, Loss: 0.7901629209518433\n",
      "Epoch 30, Batch 344/462, Loss: 0.8262708783149719\n",
      "Epoch 30, Batch 345/462, Loss: 0.7272317409515381\n",
      "Epoch 30, Batch 346/462, Loss: 0.7365031242370605\n",
      "Epoch 30, Batch 347/462, Loss: 0.8317714929580688\n",
      "Epoch 30, Batch 348/462, Loss: 0.6513528227806091\n",
      "Epoch 30, Batch 349/462, Loss: 0.62489914894104\n",
      "Epoch 30, Batch 350/462, Loss: 0.614338755607605\n",
      "Epoch 30, Batch 351/462, Loss: 0.6662446856498718\n",
      "Epoch 30, Batch 352/462, Loss: 0.7093119621276855\n",
      "Epoch 30, Batch 353/462, Loss: 0.7289641499519348\n",
      "Epoch 30, Batch 354/462, Loss: 0.5494492650032043\n",
      "Epoch 30, Batch 355/462, Loss: 0.6478719115257263\n",
      "Epoch 30, Batch 356/462, Loss: 0.7395963668823242\n",
      "Epoch 30, Batch 357/462, Loss: 0.792589008808136\n",
      "Epoch 30, Batch 358/462, Loss: 0.6171936392784119\n",
      "Epoch 30, Batch 359/462, Loss: 0.7683736085891724\n",
      "Epoch 30, Batch 360/462, Loss: 0.9020392298698425\n",
      "Epoch 30, Batch 361/462, Loss: 0.6045937538146973\n",
      "Epoch 30, Batch 362/462, Loss: 0.5884010195732117\n",
      "Epoch 30, Batch 363/462, Loss: 0.6754984855651855\n",
      "Epoch 30, Batch 364/462, Loss: 0.679121196269989\n",
      "Epoch 30, Batch 365/462, Loss: 0.6349079012870789\n",
      "Epoch 30, Batch 366/462, Loss: 0.7112485766410828\n",
      "Epoch 30, Batch 367/462, Loss: 0.6447014212608337\n",
      "Epoch 30, Batch 368/462, Loss: 0.7889153957366943\n",
      "Epoch 30, Batch 369/462, Loss: 0.7116632461547852\n",
      "Epoch 30, Batch 370/462, Loss: 0.8213568329811096\n",
      "Epoch 30, Batch 371/462, Loss: 0.7294374108314514\n",
      "Epoch 30, Batch 372/462, Loss: 0.5694536566734314\n",
      "Epoch 30, Batch 373/462, Loss: 0.7665489315986633\n",
      "Epoch 30, Batch 374/462, Loss: 0.7729671001434326\n",
      "Epoch 30, Batch 375/462, Loss: 0.6168608069419861\n",
      "Epoch 30, Batch 376/462, Loss: 0.7373351454734802\n",
      "Epoch 30, Batch 377/462, Loss: 0.609600305557251\n",
      "Epoch 30, Batch 378/462, Loss: 0.7574983835220337\n",
      "Epoch 30, Batch 379/462, Loss: 0.5330559611320496\n",
      "Epoch 30, Batch 380/462, Loss: 0.683129608631134\n",
      "Epoch 30, Batch 381/462, Loss: 0.6561061143875122\n",
      "Epoch 30, Batch 382/462, Loss: 0.6970133781433105\n",
      "Epoch 30, Batch 383/462, Loss: 0.7248640656471252\n",
      "Epoch 30, Batch 384/462, Loss: 0.807283878326416\n",
      "Epoch 30, Batch 385/462, Loss: 0.8694243431091309\n",
      "Epoch 30, Batch 386/462, Loss: 0.7766278982162476\n",
      "Epoch 30, Batch 387/462, Loss: 0.7002745270729065\n",
      "Epoch 30, Batch 388/462, Loss: 0.7895030975341797\n",
      "Epoch 30, Batch 389/462, Loss: 0.7487707734107971\n",
      "Epoch 30, Batch 390/462, Loss: 0.8469764590263367\n",
      "Epoch 30, Batch 391/462, Loss: 0.9738940596580505\n",
      "Epoch 30, Batch 392/462, Loss: 0.7038018107414246\n",
      "Epoch 30, Batch 393/462, Loss: 0.7635387778282166\n",
      "Epoch 30, Batch 394/462, Loss: 0.8181462287902832\n",
      "Epoch 30, Batch 395/462, Loss: 0.6157445907592773\n",
      "Epoch 30, Batch 396/462, Loss: 0.649056077003479\n",
      "Epoch 30, Batch 397/462, Loss: 0.784157395362854\n",
      "Epoch 30, Batch 398/462, Loss: 0.5822141170501709\n",
      "Epoch 30, Batch 399/462, Loss: 0.8261256217956543\n",
      "Epoch 30, Batch 400/462, Loss: 0.7189995646476746\n",
      "Epoch 30, Batch 401/462, Loss: 0.6347864866256714\n",
      "Epoch 30, Batch 402/462, Loss: 0.7862634658813477\n",
      "Epoch 30, Batch 403/462, Loss: 0.5744605660438538\n",
      "Epoch 30, Batch 404/462, Loss: 0.8018388748168945\n",
      "Epoch 30, Batch 405/462, Loss: 0.6372236013412476\n",
      "Epoch 30, Batch 406/462, Loss: 0.9442616701126099\n",
      "Epoch 30, Batch 407/462, Loss: 0.5768513679504395\n",
      "Epoch 30, Batch 408/462, Loss: 0.7617496252059937\n",
      "Epoch 30, Batch 409/462, Loss: 0.6669208407402039\n",
      "Epoch 30, Batch 410/462, Loss: 0.6381698250770569\n",
      "Epoch 30, Batch 411/462, Loss: 0.9295376539230347\n",
      "Epoch 30, Batch 412/462, Loss: 0.6286941170692444\n",
      "Epoch 30, Batch 413/462, Loss: 0.8394878506660461\n",
      "Epoch 30, Batch 414/462, Loss: 0.7727354764938354\n",
      "Epoch 30, Batch 415/462, Loss: 0.7691304087638855\n",
      "Epoch 30, Batch 416/462, Loss: 0.6572439074516296\n",
      "Epoch 30, Batch 417/462, Loss: 0.6582196950912476\n",
      "Epoch 30, Batch 418/462, Loss: 0.7568749189376831\n",
      "Epoch 30, Batch 419/462, Loss: 0.6599973440170288\n",
      "Epoch 30, Batch 420/462, Loss: 0.7251282930374146\n",
      "Epoch 30, Batch 421/462, Loss: 0.7766703367233276\n",
      "Epoch 30, Batch 422/462, Loss: 0.5717846751213074\n",
      "Epoch 30, Batch 423/462, Loss: 0.6410229802131653\n",
      "Epoch 30, Batch 424/462, Loss: 0.8738251328468323\n",
      "Epoch 30, Batch 425/462, Loss: 0.5344690680503845\n",
      "Epoch 30, Batch 426/462, Loss: 0.7172619104385376\n",
      "Epoch 30, Batch 427/462, Loss: 0.8715901374816895\n",
      "Epoch 30, Batch 428/462, Loss: 0.7910984754562378\n",
      "Epoch 30, Batch 429/462, Loss: 0.7558253407478333\n",
      "Epoch 30, Batch 430/462, Loss: 0.5926871299743652\n",
      "Epoch 30, Batch 431/462, Loss: 0.7742517590522766\n",
      "Epoch 30, Batch 432/462, Loss: 0.7997342348098755\n",
      "Epoch 30, Batch 433/462, Loss: 0.5983554720878601\n",
      "Epoch 30, Batch 434/462, Loss: 0.6953010559082031\n",
      "Epoch 30, Batch 435/462, Loss: 0.5379120111465454\n",
      "Epoch 30, Batch 436/462, Loss: 0.8361541032791138\n",
      "Epoch 30, Batch 437/462, Loss: 0.7626116275787354\n",
      "Epoch 30, Batch 438/462, Loss: 0.8967193961143494\n",
      "Epoch 30, Batch 439/462, Loss: 0.6117100119590759\n",
      "Epoch 30, Batch 440/462, Loss: 0.7818596959114075\n",
      "Epoch 30, Batch 441/462, Loss: 0.7779477834701538\n",
      "Epoch 30, Batch 442/462, Loss: 0.6642583608627319\n",
      "Epoch 30, Batch 443/462, Loss: 0.7178182005882263\n",
      "Epoch 30, Batch 444/462, Loss: 1.0003817081451416\n",
      "Epoch 30, Batch 445/462, Loss: 0.6321618556976318\n",
      "Epoch 30, Batch 446/462, Loss: 0.9431002736091614\n",
      "Epoch 30, Batch 447/462, Loss: 0.814411461353302\n",
      "Epoch 30, Batch 448/462, Loss: 0.8268628120422363\n",
      "Epoch 30, Batch 449/462, Loss: 0.7383521795272827\n",
      "Epoch 30, Batch 450/462, Loss: 0.6008233428001404\n",
      "Epoch 30, Batch 451/462, Loss: 0.8055927753448486\n",
      "Epoch 30, Batch 452/462, Loss: 0.7798715829849243\n",
      "Epoch 30, Batch 453/462, Loss: 0.7205840945243835\n",
      "Epoch 30, Batch 454/462, Loss: 0.8405695557594299\n",
      "Epoch 30, Batch 455/462, Loss: 0.775383472442627\n",
      "Epoch 30, Batch 456/462, Loss: 0.6571203470230103\n",
      "Epoch 30, Batch 457/462, Loss: 0.7272308468818665\n",
      "Epoch 30, Batch 458/462, Loss: 0.5754983425140381\n",
      "Epoch 30, Batch 459/462, Loss: 0.6023849248886108\n",
      "Epoch 30, Batch 460/462, Loss: 0.8192301392555237\n",
      "Epoch 30, Batch 461/462, Loss: 0.7289144992828369\n",
      "Epoch 30, Batch 462/462, Loss: 0.7536038756370544\n",
      "Epoch 30, Loss: 336.49496626853943\n",
      "Epoch 31, Batch 1/462, Loss: 0.6249967813491821\n",
      "Epoch 31, Batch 2/462, Loss: 0.8772552609443665\n",
      "Epoch 31, Batch 3/462, Loss: 0.6717227697372437\n",
      "Epoch 31, Batch 4/462, Loss: 0.6342895030975342\n",
      "Epoch 31, Batch 5/462, Loss: 0.6818943023681641\n",
      "Epoch 31, Batch 6/462, Loss: 0.650012195110321\n",
      "Epoch 31, Batch 7/462, Loss: 0.7273668050765991\n",
      "Epoch 31, Batch 8/462, Loss: 0.833264946937561\n",
      "Epoch 31, Batch 9/462, Loss: 0.7417008280754089\n",
      "Epoch 31, Batch 10/462, Loss: 0.6762036681175232\n",
      "Epoch 31, Batch 11/462, Loss: 0.6522205471992493\n",
      "Epoch 31, Batch 12/462, Loss: 0.660521924495697\n",
      "Epoch 31, Batch 13/462, Loss: 0.6614364385604858\n",
      "Epoch 31, Batch 14/462, Loss: 0.6777463555335999\n",
      "Epoch 31, Batch 15/462, Loss: 0.6203135251998901\n",
      "Epoch 31, Batch 16/462, Loss: 0.7568370699882507\n",
      "Epoch 31, Batch 17/462, Loss: 0.9640579223632812\n",
      "Epoch 31, Batch 18/462, Loss: 0.7782785296440125\n",
      "Epoch 31, Batch 19/462, Loss: 0.7673094868659973\n",
      "Epoch 31, Batch 20/462, Loss: 0.8007382154464722\n",
      "Epoch 31, Batch 21/462, Loss: 0.7777050137519836\n",
      "Epoch 31, Batch 22/462, Loss: 0.643160343170166\n",
      "Epoch 31, Batch 23/462, Loss: 0.7370907068252563\n",
      "Epoch 31, Batch 24/462, Loss: 0.7809941172599792\n",
      "Epoch 31, Batch 25/462, Loss: 0.6151791214942932\n",
      "Epoch 31, Batch 26/462, Loss: 0.8698635101318359\n",
      "Epoch 31, Batch 27/462, Loss: 0.6969912052154541\n",
      "Epoch 31, Batch 28/462, Loss: 0.7977608442306519\n",
      "Epoch 31, Batch 29/462, Loss: 0.580048680305481\n",
      "Epoch 31, Batch 30/462, Loss: 0.753416121006012\n",
      "Epoch 31, Batch 31/462, Loss: 0.6859915256500244\n",
      "Epoch 31, Batch 32/462, Loss: 0.7093153595924377\n",
      "Epoch 31, Batch 33/462, Loss: 0.7207848429679871\n",
      "Epoch 31, Batch 34/462, Loss: 0.7980287671089172\n",
      "Epoch 31, Batch 35/462, Loss: 0.8237913846969604\n",
      "Epoch 31, Batch 36/462, Loss: 0.7370439171791077\n",
      "Epoch 31, Batch 37/462, Loss: 0.7078273296356201\n",
      "Epoch 31, Batch 38/462, Loss: 0.7691635489463806\n",
      "Epoch 31, Batch 39/462, Loss: 0.7584038972854614\n",
      "Epoch 31, Batch 40/462, Loss: 0.625977635383606\n",
      "Epoch 31, Batch 41/462, Loss: 0.8521222472190857\n",
      "Epoch 31, Batch 42/462, Loss: 0.7634008526802063\n",
      "Epoch 31, Batch 43/462, Loss: 0.6876737475395203\n",
      "Epoch 31, Batch 44/462, Loss: 0.563449501991272\n",
      "Epoch 31, Batch 45/462, Loss: 0.8168250322341919\n",
      "Epoch 31, Batch 46/462, Loss: 0.7595838904380798\n",
      "Epoch 31, Batch 47/462, Loss: 0.8199414014816284\n",
      "Epoch 31, Batch 48/462, Loss: 0.7772495150566101\n",
      "Epoch 31, Batch 49/462, Loss: 0.7049728631973267\n",
      "Epoch 31, Batch 50/462, Loss: 0.7075902223587036\n",
      "Epoch 31, Batch 51/462, Loss: 0.8436954617500305\n",
      "Epoch 31, Batch 52/462, Loss: 0.5290601253509521\n",
      "Epoch 31, Batch 53/462, Loss: 0.5533558130264282\n",
      "Epoch 31, Batch 54/462, Loss: 0.6215147376060486\n",
      "Epoch 31, Batch 55/462, Loss: 0.6133968234062195\n",
      "Epoch 31, Batch 56/462, Loss: 0.6938222646713257\n",
      "Epoch 31, Batch 57/462, Loss: 0.8088648319244385\n",
      "Epoch 31, Batch 58/462, Loss: 0.7151821255683899\n",
      "Epoch 31, Batch 59/462, Loss: 0.7164496183395386\n",
      "Epoch 31, Batch 60/462, Loss: 0.6256523132324219\n",
      "Epoch 31, Batch 61/462, Loss: 0.6069228053092957\n",
      "Epoch 31, Batch 62/462, Loss: 0.8965266942977905\n",
      "Epoch 31, Batch 63/462, Loss: 0.7742557525634766\n",
      "Epoch 31, Batch 64/462, Loss: 0.8121887445449829\n",
      "Epoch 31, Batch 65/462, Loss: 0.7487475275993347\n",
      "Epoch 31, Batch 66/462, Loss: 0.7966992855072021\n",
      "Epoch 31, Batch 67/462, Loss: 0.7733446955680847\n",
      "Epoch 31, Batch 68/462, Loss: 0.8697284460067749\n",
      "Epoch 31, Batch 69/462, Loss: 0.7628985047340393\n",
      "Epoch 31, Batch 70/462, Loss: 0.7542425990104675\n",
      "Epoch 31, Batch 71/462, Loss: 0.608981192111969\n",
      "Epoch 31, Batch 72/462, Loss: 0.8180654048919678\n",
      "Epoch 31, Batch 73/462, Loss: 0.6597351431846619\n",
      "Epoch 31, Batch 74/462, Loss: 0.742609441280365\n",
      "Epoch 31, Batch 75/462, Loss: 0.7691264748573303\n",
      "Epoch 31, Batch 76/462, Loss: 0.7003061771392822\n",
      "Epoch 31, Batch 77/462, Loss: 0.9018619656562805\n",
      "Epoch 31, Batch 78/462, Loss: 0.7378943562507629\n",
      "Epoch 31, Batch 79/462, Loss: 0.7287535667419434\n",
      "Epoch 31, Batch 80/462, Loss: 0.5223909020423889\n",
      "Epoch 31, Batch 81/462, Loss: 0.7849063873291016\n",
      "Epoch 31, Batch 82/462, Loss: 0.6385764479637146\n",
      "Epoch 31, Batch 83/462, Loss: 0.6184830069541931\n",
      "Epoch 31, Batch 84/462, Loss: 0.7721045613288879\n",
      "Epoch 31, Batch 85/462, Loss: 0.6673417091369629\n",
      "Epoch 31, Batch 86/462, Loss: 0.6690000295639038\n",
      "Epoch 31, Batch 87/462, Loss: 0.6514589786529541\n",
      "Epoch 31, Batch 88/462, Loss: 0.8945281505584717\n",
      "Epoch 31, Batch 89/462, Loss: 0.6728311777114868\n",
      "Epoch 31, Batch 90/462, Loss: 0.5953810214996338\n",
      "Epoch 31, Batch 91/462, Loss: 0.5678019523620605\n",
      "Epoch 31, Batch 92/462, Loss: 0.7384567856788635\n",
      "Epoch 31, Batch 93/462, Loss: 0.832635760307312\n",
      "Epoch 31, Batch 94/462, Loss: 0.6308545470237732\n",
      "Epoch 31, Batch 95/462, Loss: 0.6203133463859558\n",
      "Epoch 31, Batch 96/462, Loss: 0.7891407608985901\n",
      "Epoch 31, Batch 97/462, Loss: 0.9909586310386658\n",
      "Epoch 31, Batch 98/462, Loss: 0.7437927722930908\n",
      "Epoch 31, Batch 99/462, Loss: 0.8350836038589478\n",
      "Epoch 31, Batch 100/462, Loss: 0.6658942103385925\n",
      "Epoch 31, Batch 101/462, Loss: 0.695235013961792\n",
      "Epoch 31, Batch 102/462, Loss: 0.718368411064148\n",
      "Epoch 31, Batch 103/462, Loss: 0.652311384677887\n",
      "Epoch 31, Batch 104/462, Loss: 0.7743638753890991\n",
      "Epoch 31, Batch 105/462, Loss: 0.7889457941055298\n",
      "Epoch 31, Batch 106/462, Loss: 0.6938223838806152\n",
      "Epoch 31, Batch 107/462, Loss: 0.8167216777801514\n",
      "Epoch 31, Batch 108/462, Loss: 0.6355584859848022\n",
      "Epoch 31, Batch 109/462, Loss: 0.6923670768737793\n",
      "Epoch 31, Batch 110/462, Loss: 0.9659285545349121\n",
      "Epoch 31, Batch 111/462, Loss: 0.6384559869766235\n",
      "Epoch 31, Batch 112/462, Loss: 0.6431683301925659\n",
      "Epoch 31, Batch 113/462, Loss: 0.7841344475746155\n",
      "Epoch 31, Batch 114/462, Loss: 0.8427735567092896\n",
      "Epoch 31, Batch 115/462, Loss: 0.6868824362754822\n",
      "Epoch 31, Batch 116/462, Loss: 0.6562860608100891\n",
      "Epoch 31, Batch 117/462, Loss: 0.7321799397468567\n",
      "Epoch 31, Batch 118/462, Loss: 0.718514084815979\n",
      "Epoch 31, Batch 119/462, Loss: 0.9062638878822327\n",
      "Epoch 31, Batch 120/462, Loss: 0.6674189567565918\n",
      "Epoch 31, Batch 121/462, Loss: 0.7196123003959656\n",
      "Epoch 31, Batch 122/462, Loss: 0.8337113261222839\n",
      "Epoch 31, Batch 123/462, Loss: 0.8895218968391418\n",
      "Epoch 31, Batch 124/462, Loss: 0.7397943735122681\n",
      "Epoch 31, Batch 125/462, Loss: 0.6555914878845215\n",
      "Epoch 31, Batch 126/462, Loss: 0.6901814937591553\n",
      "Epoch 31, Batch 127/462, Loss: 0.6936163902282715\n",
      "Epoch 31, Batch 128/462, Loss: 0.7619094848632812\n",
      "Epoch 31, Batch 129/462, Loss: 0.7248032093048096\n",
      "Epoch 31, Batch 130/462, Loss: 0.7065906524658203\n",
      "Epoch 31, Batch 131/462, Loss: 0.8698756694793701\n",
      "Epoch 31, Batch 132/462, Loss: 0.6786336302757263\n",
      "Epoch 31, Batch 133/462, Loss: 0.7707880735397339\n",
      "Epoch 31, Batch 134/462, Loss: 0.785161554813385\n",
      "Epoch 31, Batch 135/462, Loss: 0.6979846358299255\n",
      "Epoch 31, Batch 136/462, Loss: 0.7753639221191406\n",
      "Epoch 31, Batch 137/462, Loss: 0.6223146915435791\n",
      "Epoch 31, Batch 138/462, Loss: 0.8593437671661377\n",
      "Epoch 31, Batch 139/462, Loss: 0.827135443687439\n",
      "Epoch 31, Batch 140/462, Loss: 0.7193899750709534\n",
      "Epoch 31, Batch 141/462, Loss: 0.751848042011261\n",
      "Epoch 31, Batch 142/462, Loss: 0.789114236831665\n",
      "Epoch 31, Batch 143/462, Loss: 0.7417051196098328\n",
      "Epoch 31, Batch 144/462, Loss: 0.5401060581207275\n",
      "Epoch 31, Batch 145/462, Loss: 0.5376790165901184\n",
      "Epoch 31, Batch 146/462, Loss: 0.5666645169258118\n",
      "Epoch 31, Batch 147/462, Loss: 0.78306645154953\n",
      "Epoch 31, Batch 148/462, Loss: 0.5599933862686157\n",
      "Epoch 31, Batch 149/462, Loss: 0.6575536727905273\n",
      "Epoch 31, Batch 150/462, Loss: 0.9292998313903809\n",
      "Epoch 31, Batch 151/462, Loss: 0.67991042137146\n",
      "Epoch 31, Batch 152/462, Loss: 0.6947036981582642\n",
      "Epoch 31, Batch 153/462, Loss: 0.7894105315208435\n",
      "Epoch 31, Batch 154/462, Loss: 0.7866288423538208\n",
      "Epoch 31, Batch 155/462, Loss: 0.6715348362922668\n",
      "Epoch 31, Batch 156/462, Loss: 0.6894068717956543\n",
      "Epoch 31, Batch 157/462, Loss: 0.7173141837120056\n",
      "Epoch 31, Batch 158/462, Loss: 0.7432137131690979\n",
      "Epoch 31, Batch 159/462, Loss: 0.6759504079818726\n",
      "Epoch 31, Batch 160/462, Loss: 0.8198925256729126\n",
      "Epoch 31, Batch 161/462, Loss: 0.5516910552978516\n",
      "Epoch 31, Batch 162/462, Loss: 0.9406720399856567\n",
      "Epoch 31, Batch 163/462, Loss: 0.7602676749229431\n",
      "Epoch 31, Batch 164/462, Loss: 0.9851422309875488\n",
      "Epoch 31, Batch 165/462, Loss: 0.8008084893226624\n",
      "Epoch 31, Batch 166/462, Loss: 0.6515324115753174\n",
      "Epoch 31, Batch 167/462, Loss: 1.0271822214126587\n",
      "Epoch 31, Batch 168/462, Loss: 0.6107971668243408\n",
      "Epoch 31, Batch 169/462, Loss: 0.7443469762802124\n",
      "Epoch 31, Batch 170/462, Loss: 0.8100088238716125\n",
      "Epoch 31, Batch 171/462, Loss: 0.7334602475166321\n",
      "Epoch 31, Batch 172/462, Loss: 0.7954939603805542\n",
      "Epoch 31, Batch 173/462, Loss: 0.9283305406570435\n",
      "Epoch 31, Batch 174/462, Loss: 0.6481287479400635\n",
      "Epoch 31, Batch 175/462, Loss: 0.7476104497909546\n",
      "Epoch 31, Batch 176/462, Loss: 0.7002546787261963\n",
      "Epoch 31, Batch 177/462, Loss: 0.691850483417511\n",
      "Epoch 31, Batch 178/462, Loss: 0.6139473915100098\n",
      "Epoch 31, Batch 179/462, Loss: 0.6768972873687744\n",
      "Epoch 31, Batch 180/462, Loss: 0.8398222327232361\n",
      "Epoch 31, Batch 181/462, Loss: 0.8446250557899475\n",
      "Epoch 31, Batch 182/462, Loss: 0.7443628311157227\n",
      "Epoch 31, Batch 183/462, Loss: 0.7543932199478149\n",
      "Epoch 31, Batch 184/462, Loss: 0.6662097573280334\n",
      "Epoch 31, Batch 185/462, Loss: 0.6905325055122375\n",
      "Epoch 31, Batch 186/462, Loss: 0.7799053192138672\n",
      "Epoch 31, Batch 187/462, Loss: 0.7452723979949951\n",
      "Epoch 31, Batch 188/462, Loss: 0.751660168170929\n",
      "Epoch 31, Batch 189/462, Loss: 0.6915174126625061\n",
      "Epoch 31, Batch 190/462, Loss: 0.6373896598815918\n",
      "Epoch 31, Batch 191/462, Loss: 0.660152792930603\n",
      "Epoch 31, Batch 192/462, Loss: 0.7325413823127747\n",
      "Epoch 31, Batch 193/462, Loss: 0.5202387571334839\n",
      "Epoch 31, Batch 194/462, Loss: 0.6137564778327942\n",
      "Epoch 31, Batch 195/462, Loss: 0.756283164024353\n",
      "Epoch 31, Batch 196/462, Loss: 0.8910418748855591\n",
      "Epoch 31, Batch 197/462, Loss: 0.7383784651756287\n",
      "Epoch 31, Batch 198/462, Loss: 0.8308247923851013\n",
      "Epoch 31, Batch 199/462, Loss: 0.7045770287513733\n",
      "Epoch 31, Batch 200/462, Loss: 0.6827319264411926\n",
      "Epoch 31, Batch 201/462, Loss: 0.7020699977874756\n",
      "Epoch 31, Batch 202/462, Loss: 0.7296968698501587\n",
      "Epoch 31, Batch 203/462, Loss: 0.6405589580535889\n",
      "Epoch 31, Batch 204/462, Loss: 0.7175948619842529\n",
      "Epoch 31, Batch 205/462, Loss: 0.7308929562568665\n",
      "Epoch 31, Batch 206/462, Loss: 0.7122138738632202\n",
      "Epoch 31, Batch 207/462, Loss: 0.7920256853103638\n",
      "Epoch 31, Batch 208/462, Loss: 0.7244027256965637\n",
      "Epoch 31, Batch 209/462, Loss: 0.6521456241607666\n",
      "Epoch 31, Batch 210/462, Loss: 0.7736085057258606\n",
      "Epoch 31, Batch 211/462, Loss: 0.6502878665924072\n",
      "Epoch 31, Batch 212/462, Loss: 0.6774629950523376\n",
      "Epoch 31, Batch 213/462, Loss: 0.7616261839866638\n",
      "Epoch 31, Batch 214/462, Loss: 0.8343511819839478\n",
      "Epoch 31, Batch 215/462, Loss: 0.7733110189437866\n",
      "Epoch 31, Batch 216/462, Loss: 0.7394589185714722\n",
      "Epoch 31, Batch 217/462, Loss: 0.7475982904434204\n",
      "Epoch 31, Batch 218/462, Loss: 0.7512471079826355\n",
      "Epoch 31, Batch 219/462, Loss: 0.6786430478096008\n",
      "Epoch 31, Batch 220/462, Loss: 0.8678638339042664\n",
      "Epoch 31, Batch 221/462, Loss: 0.8689237833023071\n",
      "Epoch 31, Batch 222/462, Loss: 0.8048226833343506\n",
      "Epoch 31, Batch 223/462, Loss: 0.5694541931152344\n",
      "Epoch 31, Batch 224/462, Loss: 0.8363705277442932\n",
      "Epoch 31, Batch 225/462, Loss: 0.7352784872055054\n",
      "Epoch 31, Batch 226/462, Loss: 0.6533930897712708\n",
      "Epoch 31, Batch 227/462, Loss: 0.7115442156791687\n",
      "Epoch 31, Batch 228/462, Loss: 0.7243930101394653\n",
      "Epoch 31, Batch 229/462, Loss: 0.6961312890052795\n",
      "Epoch 31, Batch 230/462, Loss: 0.7098721265792847\n",
      "Epoch 31, Batch 231/462, Loss: 0.6549134850502014\n",
      "Epoch 31, Batch 232/462, Loss: 0.895301103591919\n",
      "Epoch 31, Batch 233/462, Loss: 0.8064801692962646\n",
      "Epoch 31, Batch 234/462, Loss: 0.7409756183624268\n",
      "Epoch 31, Batch 235/462, Loss: 0.7172207832336426\n",
      "Epoch 31, Batch 236/462, Loss: 0.8766935467720032\n",
      "Epoch 31, Batch 237/462, Loss: 0.7575655579566956\n",
      "Epoch 31, Batch 238/462, Loss: 0.7326591610908508\n",
      "Epoch 31, Batch 239/462, Loss: 0.661961019039154\n",
      "Epoch 31, Batch 240/462, Loss: 0.7908364534378052\n",
      "Epoch 31, Batch 241/462, Loss: 0.6230790615081787\n",
      "Epoch 31, Batch 242/462, Loss: 0.6228641271591187\n",
      "Epoch 31, Batch 243/462, Loss: 0.6370636224746704\n",
      "Epoch 31, Batch 244/462, Loss: 0.8453122973442078\n",
      "Epoch 31, Batch 245/462, Loss: 0.8165386915206909\n",
      "Epoch 31, Batch 246/462, Loss: 0.9261541962623596\n",
      "Epoch 31, Batch 247/462, Loss: 0.705511212348938\n",
      "Epoch 31, Batch 248/462, Loss: 0.7459918260574341\n",
      "Epoch 31, Batch 249/462, Loss: 0.5518420934677124\n",
      "Epoch 31, Batch 250/462, Loss: 0.7276509404182434\n",
      "Epoch 31, Batch 251/462, Loss: 0.7566543221473694\n",
      "Epoch 31, Batch 252/462, Loss: 0.5419813394546509\n",
      "Epoch 31, Batch 253/462, Loss: 0.5628308057785034\n",
      "Epoch 31, Batch 254/462, Loss: 0.7249035835266113\n",
      "Epoch 31, Batch 255/462, Loss: 0.7114443778991699\n",
      "Epoch 31, Batch 256/462, Loss: 0.6754227876663208\n",
      "Epoch 31, Batch 257/462, Loss: 0.749247670173645\n",
      "Epoch 31, Batch 258/462, Loss: 0.8071625828742981\n",
      "Epoch 31, Batch 259/462, Loss: 0.7609993815422058\n",
      "Epoch 31, Batch 260/462, Loss: 0.6778114438056946\n",
      "Epoch 31, Batch 261/462, Loss: 0.8198716044425964\n",
      "Epoch 31, Batch 262/462, Loss: 0.8029040098190308\n",
      "Epoch 31, Batch 263/462, Loss: 0.6415457129478455\n",
      "Epoch 31, Batch 264/462, Loss: 0.5898227691650391\n",
      "Epoch 31, Batch 265/462, Loss: 0.592702329158783\n",
      "Epoch 31, Batch 266/462, Loss: 0.8623074889183044\n",
      "Epoch 31, Batch 267/462, Loss: 0.7869630455970764\n",
      "Epoch 31, Batch 268/462, Loss: 0.7626006603240967\n",
      "Epoch 31, Batch 269/462, Loss: 0.7274829149246216\n",
      "Epoch 31, Batch 270/462, Loss: 0.5652635097503662\n",
      "Epoch 31, Batch 271/462, Loss: 0.5447196960449219\n",
      "Epoch 31, Batch 272/462, Loss: 0.8163377046585083\n",
      "Epoch 31, Batch 273/462, Loss: 0.5793189406394958\n",
      "Epoch 31, Batch 274/462, Loss: 0.8229559063911438\n",
      "Epoch 31, Batch 275/462, Loss: 0.6789587736129761\n",
      "Epoch 31, Batch 276/462, Loss: 0.5397132039070129\n",
      "Epoch 31, Batch 277/462, Loss: 0.6789296865463257\n",
      "Epoch 31, Batch 278/462, Loss: 0.7376009225845337\n",
      "Epoch 31, Batch 279/462, Loss: 0.5665331482887268\n",
      "Epoch 31, Batch 280/462, Loss: 0.8513334393501282\n",
      "Epoch 31, Batch 281/462, Loss: 0.7542412281036377\n",
      "Epoch 31, Batch 282/462, Loss: 0.831567645072937\n",
      "Epoch 31, Batch 283/462, Loss: 0.532401442527771\n",
      "Epoch 31, Batch 284/462, Loss: 0.7021967768669128\n",
      "Epoch 31, Batch 285/462, Loss: 0.8055384755134583\n",
      "Epoch 31, Batch 286/462, Loss: 0.6648242473602295\n",
      "Epoch 31, Batch 287/462, Loss: 0.6344629526138306\n",
      "Epoch 31, Batch 288/462, Loss: 0.7181827425956726\n",
      "Epoch 31, Batch 289/462, Loss: 0.7583953142166138\n",
      "Epoch 31, Batch 290/462, Loss: 0.551582396030426\n",
      "Epoch 31, Batch 291/462, Loss: 0.5904934406280518\n",
      "Epoch 31, Batch 292/462, Loss: 0.7502421736717224\n",
      "Epoch 31, Batch 293/462, Loss: 0.6812140941619873\n",
      "Epoch 31, Batch 294/462, Loss: 0.7015676498413086\n",
      "Epoch 31, Batch 295/462, Loss: 0.7748100757598877\n",
      "Epoch 31, Batch 296/462, Loss: 0.5598127841949463\n",
      "Epoch 31, Batch 297/462, Loss: 0.539551854133606\n",
      "Epoch 31, Batch 298/462, Loss: 0.8405495882034302\n",
      "Epoch 31, Batch 299/462, Loss: 0.6450569033622742\n",
      "Epoch 31, Batch 300/462, Loss: 0.6878331899642944\n",
      "Epoch 31, Batch 301/462, Loss: 0.7862657308578491\n",
      "Epoch 31, Batch 302/462, Loss: 0.7454516887664795\n",
      "Epoch 31, Batch 303/462, Loss: 0.7611554861068726\n",
      "Epoch 31, Batch 304/462, Loss: 0.7733668088912964\n",
      "Epoch 31, Batch 305/462, Loss: 0.7672940492630005\n",
      "Epoch 31, Batch 306/462, Loss: 0.5808742046356201\n",
      "Epoch 31, Batch 307/462, Loss: 0.8277746438980103\n",
      "Epoch 31, Batch 308/462, Loss: 0.870930016040802\n",
      "Epoch 31, Batch 309/462, Loss: 0.6504095196723938\n",
      "Epoch 31, Batch 310/462, Loss: 0.6891298294067383\n",
      "Epoch 31, Batch 311/462, Loss: 0.662226140499115\n",
      "Epoch 31, Batch 312/462, Loss: 0.8389959335327148\n",
      "Epoch 31, Batch 313/462, Loss: 0.6712740063667297\n",
      "Epoch 31, Batch 314/462, Loss: 0.6057095527648926\n",
      "Epoch 31, Batch 315/462, Loss: 0.9086272120475769\n",
      "Epoch 31, Batch 316/462, Loss: 0.6519013047218323\n",
      "Epoch 31, Batch 317/462, Loss: 0.7120373845100403\n",
      "Epoch 31, Batch 318/462, Loss: 0.6012328267097473\n",
      "Epoch 31, Batch 319/462, Loss: 0.7345263361930847\n",
      "Epoch 31, Batch 320/462, Loss: 0.5922133922576904\n",
      "Epoch 31, Batch 321/462, Loss: 0.7458229660987854\n",
      "Epoch 31, Batch 322/462, Loss: 0.6988579630851746\n",
      "Epoch 31, Batch 323/462, Loss: 0.5224410891532898\n",
      "Epoch 31, Batch 324/462, Loss: 0.7066903114318848\n",
      "Epoch 31, Batch 325/462, Loss: 0.744343101978302\n",
      "Epoch 31, Batch 326/462, Loss: 0.9137735366821289\n",
      "Epoch 31, Batch 327/462, Loss: 0.6843935251235962\n",
      "Epoch 31, Batch 328/462, Loss: 0.8462267518043518\n",
      "Epoch 31, Batch 329/462, Loss: 0.7821453809738159\n",
      "Epoch 31, Batch 330/462, Loss: 0.9055701494216919\n",
      "Epoch 31, Batch 331/462, Loss: 0.6853348612785339\n",
      "Epoch 31, Batch 332/462, Loss: 0.7789543867111206\n",
      "Epoch 31, Batch 333/462, Loss: 0.7518006563186646\n",
      "Epoch 31, Batch 334/462, Loss: 0.7118312120437622\n",
      "Epoch 31, Batch 335/462, Loss: 0.6579986214637756\n",
      "Epoch 31, Batch 336/462, Loss: 0.7222431302070618\n",
      "Epoch 31, Batch 337/462, Loss: 0.7546558380126953\n",
      "Epoch 31, Batch 338/462, Loss: 0.8928458094596863\n",
      "Epoch 31, Batch 339/462, Loss: 0.7498788833618164\n",
      "Epoch 31, Batch 340/462, Loss: 0.6999331116676331\n",
      "Epoch 31, Batch 341/462, Loss: 0.7783131003379822\n",
      "Epoch 31, Batch 342/462, Loss: 0.8740529417991638\n",
      "Epoch 31, Batch 343/462, Loss: 0.7217721343040466\n",
      "Epoch 31, Batch 344/462, Loss: 0.8983012437820435\n",
      "Epoch 31, Batch 345/462, Loss: 0.6652397513389587\n",
      "Epoch 31, Batch 346/462, Loss: 0.7159116864204407\n",
      "Epoch 31, Batch 347/462, Loss: 0.6810858845710754\n",
      "Epoch 31, Batch 348/462, Loss: 0.7512260675430298\n",
      "Epoch 31, Batch 349/462, Loss: 0.7847561836242676\n",
      "Epoch 31, Batch 350/462, Loss: 0.7940061688423157\n",
      "Epoch 31, Batch 351/462, Loss: 0.8133392333984375\n",
      "Epoch 31, Batch 352/462, Loss: 0.7269249558448792\n",
      "Epoch 31, Batch 353/462, Loss: 0.6435542106628418\n",
      "Epoch 31, Batch 354/462, Loss: 0.7013924717903137\n",
      "Epoch 31, Batch 355/462, Loss: 0.6039267182350159\n",
      "Epoch 31, Batch 356/462, Loss: 0.6761736273765564\n",
      "Epoch 31, Batch 357/462, Loss: 0.78160160779953\n",
      "Epoch 31, Batch 358/462, Loss: 0.6187414526939392\n",
      "Epoch 31, Batch 359/462, Loss: 0.6958296895027161\n",
      "Epoch 31, Batch 360/462, Loss: 0.7286871671676636\n",
      "Epoch 31, Batch 361/462, Loss: 0.6328202486038208\n",
      "Epoch 31, Batch 362/462, Loss: 0.6629123687744141\n",
      "Epoch 31, Batch 363/462, Loss: 0.677331805229187\n",
      "Epoch 31, Batch 364/462, Loss: 0.6686058640480042\n",
      "Epoch 31, Batch 365/462, Loss: 0.6893677115440369\n",
      "Epoch 31, Batch 366/462, Loss: 0.7941713929176331\n",
      "Epoch 31, Batch 367/462, Loss: 0.7430500388145447\n",
      "Epoch 31, Batch 368/462, Loss: 0.917851448059082\n",
      "Epoch 31, Batch 369/462, Loss: 0.8909104466438293\n",
      "Epoch 31, Batch 370/462, Loss: 0.8074597120285034\n",
      "Epoch 31, Batch 371/462, Loss: 0.7849419713020325\n",
      "Epoch 31, Batch 372/462, Loss: 0.6757754683494568\n",
      "Epoch 31, Batch 373/462, Loss: 0.6533589363098145\n",
      "Epoch 31, Batch 374/462, Loss: 0.8130313754081726\n",
      "Epoch 31, Batch 375/462, Loss: 0.7807439565658569\n",
      "Epoch 31, Batch 376/462, Loss: 0.9732583165168762\n",
      "Epoch 31, Batch 377/462, Loss: 0.7280884981155396\n",
      "Epoch 31, Batch 378/462, Loss: 0.6487182974815369\n",
      "Epoch 31, Batch 379/462, Loss: 0.654768168926239\n",
      "Epoch 31, Batch 380/462, Loss: 0.5811993479728699\n",
      "Epoch 31, Batch 381/462, Loss: 0.6569202542304993\n",
      "Epoch 31, Batch 382/462, Loss: 0.6251505613327026\n",
      "Epoch 31, Batch 383/462, Loss: 0.8050743341445923\n",
      "Epoch 31, Batch 384/462, Loss: 0.803821861743927\n",
      "Epoch 31, Batch 385/462, Loss: 0.7803440690040588\n",
      "Epoch 31, Batch 386/462, Loss: 0.8311004042625427\n",
      "Epoch 31, Batch 387/462, Loss: 0.8560933470726013\n",
      "Epoch 31, Batch 388/462, Loss: 0.7265281677246094\n",
      "Epoch 31, Batch 389/462, Loss: 0.7286279201507568\n",
      "Epoch 31, Batch 390/462, Loss: 0.8821858167648315\n",
      "Epoch 31, Batch 391/462, Loss: 0.6079406142234802\n",
      "Epoch 31, Batch 392/462, Loss: 0.7549812197685242\n",
      "Epoch 31, Batch 393/462, Loss: 0.8059206008911133\n",
      "Epoch 31, Batch 394/462, Loss: 0.8609288930892944\n",
      "Epoch 31, Batch 395/462, Loss: 0.6539279222488403\n",
      "Epoch 31, Batch 396/462, Loss: 0.6839599609375\n",
      "Epoch 31, Batch 397/462, Loss: 0.6102237701416016\n",
      "Epoch 31, Batch 398/462, Loss: 0.6613080501556396\n",
      "Epoch 31, Batch 399/462, Loss: 0.6618297696113586\n",
      "Epoch 31, Batch 400/462, Loss: 0.7383102178573608\n",
      "Epoch 31, Batch 401/462, Loss: 0.5950509905815125\n",
      "Epoch 31, Batch 402/462, Loss: 0.5932973027229309\n",
      "Epoch 31, Batch 403/462, Loss: 0.7542362213134766\n",
      "Epoch 31, Batch 404/462, Loss: 0.566976010799408\n",
      "Epoch 31, Batch 405/462, Loss: 0.7618708610534668\n",
      "Epoch 31, Batch 406/462, Loss: 0.6064624190330505\n",
      "Epoch 31, Batch 407/462, Loss: 0.8406631350517273\n",
      "Epoch 31, Batch 408/462, Loss: 0.723075270652771\n",
      "Epoch 31, Batch 409/462, Loss: 0.8604494333267212\n",
      "Epoch 31, Batch 410/462, Loss: 0.8330211043357849\n",
      "Epoch 31, Batch 411/462, Loss: 0.5854847431182861\n",
      "Epoch 31, Batch 412/462, Loss: 0.7152130007743835\n",
      "Epoch 31, Batch 413/462, Loss: 0.6867870092391968\n",
      "Epoch 31, Batch 414/462, Loss: 0.7446619868278503\n",
      "Epoch 31, Batch 415/462, Loss: 0.9128267168998718\n",
      "Epoch 31, Batch 416/462, Loss: 0.7169820666313171\n",
      "Epoch 31, Batch 417/462, Loss: 0.7750602960586548\n",
      "Epoch 31, Batch 418/462, Loss: 0.6474752426147461\n",
      "Epoch 31, Batch 419/462, Loss: 0.7475298643112183\n",
      "Epoch 31, Batch 420/462, Loss: 0.6757028698921204\n",
      "Epoch 31, Batch 421/462, Loss: 0.6609256863594055\n",
      "Epoch 31, Batch 422/462, Loss: 0.8329159021377563\n",
      "Epoch 31, Batch 423/462, Loss: 0.6288296580314636\n",
      "Epoch 31, Batch 424/462, Loss: 0.5731559991836548\n",
      "Epoch 31, Batch 425/462, Loss: 0.7386084794998169\n",
      "Epoch 31, Batch 426/462, Loss: 0.700308084487915\n",
      "Epoch 31, Batch 427/462, Loss: 0.5539463758468628\n",
      "Epoch 31, Batch 428/462, Loss: 0.6075206398963928\n",
      "Epoch 31, Batch 429/462, Loss: 0.6894494295120239\n",
      "Epoch 31, Batch 430/462, Loss: 0.683103084564209\n",
      "Epoch 31, Batch 431/462, Loss: 0.66187584400177\n",
      "Epoch 31, Batch 432/462, Loss: 0.7964478731155396\n",
      "Epoch 31, Batch 433/462, Loss: 0.7838770151138306\n",
      "Epoch 31, Batch 434/462, Loss: 0.6699342131614685\n",
      "Epoch 31, Batch 435/462, Loss: 0.8405323028564453\n",
      "Epoch 31, Batch 436/462, Loss: 0.5323264598846436\n",
      "Epoch 31, Batch 437/462, Loss: 0.8871514201164246\n",
      "Epoch 31, Batch 438/462, Loss: 0.7307212948799133\n",
      "Epoch 31, Batch 439/462, Loss: 0.5821835398674011\n",
      "Epoch 31, Batch 440/462, Loss: 0.6439291834831238\n",
      "Epoch 31, Batch 441/462, Loss: 0.6810358166694641\n",
      "Epoch 31, Batch 442/462, Loss: 0.5657668709754944\n",
      "Epoch 31, Batch 443/462, Loss: 0.619846522808075\n",
      "Epoch 31, Batch 444/462, Loss: 0.9223483800888062\n",
      "Epoch 31, Batch 445/462, Loss: 0.617456316947937\n",
      "Epoch 31, Batch 446/462, Loss: 0.5588849782943726\n",
      "Epoch 31, Batch 447/462, Loss: 0.6605842709541321\n",
      "Epoch 31, Batch 448/462, Loss: 0.8694493174552917\n",
      "Epoch 31, Batch 449/462, Loss: 0.6089092493057251\n",
      "Epoch 31, Batch 450/462, Loss: 0.7551366686820984\n",
      "Epoch 31, Batch 451/462, Loss: 0.6477494835853577\n",
      "Epoch 31, Batch 452/462, Loss: 0.7451650500297546\n",
      "Epoch 31, Batch 453/462, Loss: 0.7061042189598083\n",
      "Epoch 31, Batch 454/462, Loss: 0.6807966232299805\n",
      "Epoch 31, Batch 455/462, Loss: 0.7453556060791016\n",
      "Epoch 31, Batch 456/462, Loss: 0.5809326171875\n",
      "Epoch 31, Batch 457/462, Loss: 0.8816750645637512\n",
      "Epoch 31, Batch 458/462, Loss: 0.8121635317802429\n",
      "Epoch 31, Batch 459/462, Loss: 0.5285242199897766\n",
      "Epoch 31, Batch 460/462, Loss: 0.5915930867195129\n",
      "Epoch 31, Batch 461/462, Loss: 0.7169003486633301\n",
      "Epoch 31, Batch 462/462, Loss: 0.8058772087097168\n",
      "Epoch 31, Loss: 334.5990415215492\n",
      "Epoch 32, Batch 1/462, Loss: 0.6865600347518921\n",
      "Epoch 32, Batch 2/462, Loss: 0.7045251727104187\n",
      "Epoch 32, Batch 3/462, Loss: 0.5527084469795227\n",
      "Epoch 32, Batch 4/462, Loss: 0.7184441685676575\n",
      "Epoch 32, Batch 5/462, Loss: 0.6433858871459961\n",
      "Epoch 32, Batch 6/462, Loss: 0.7388928532600403\n",
      "Epoch 32, Batch 7/462, Loss: 0.632181704044342\n",
      "Epoch 32, Batch 8/462, Loss: 0.6276336908340454\n",
      "Epoch 32, Batch 9/462, Loss: 0.7032185196876526\n",
      "Epoch 32, Batch 10/462, Loss: 0.7922180891036987\n",
      "Epoch 32, Batch 11/462, Loss: 0.6988049149513245\n",
      "Epoch 32, Batch 12/462, Loss: 0.8508241176605225\n",
      "Epoch 32, Batch 13/462, Loss: 0.8493226766586304\n",
      "Epoch 32, Batch 14/462, Loss: 0.6323337554931641\n",
      "Epoch 32, Batch 15/462, Loss: 0.6534782648086548\n",
      "Epoch 32, Batch 16/462, Loss: 0.7104176878929138\n",
      "Epoch 32, Batch 17/462, Loss: 0.6776478290557861\n",
      "Epoch 32, Batch 18/462, Loss: 0.5973536968231201\n",
      "Epoch 32, Batch 19/462, Loss: 1.0514800548553467\n",
      "Epoch 32, Batch 20/462, Loss: 0.6280213594436646\n",
      "Epoch 32, Batch 21/462, Loss: 0.7492916584014893\n",
      "Epoch 32, Batch 22/462, Loss: 0.691831648349762\n",
      "Epoch 32, Batch 23/462, Loss: 0.7912907004356384\n",
      "Epoch 32, Batch 24/462, Loss: 0.9870935678482056\n",
      "Epoch 32, Batch 25/462, Loss: 0.6216298341751099\n",
      "Epoch 32, Batch 26/462, Loss: 0.6764342188835144\n",
      "Epoch 32, Batch 27/462, Loss: 0.7130861282348633\n",
      "Epoch 32, Batch 28/462, Loss: 0.7630372047424316\n",
      "Epoch 32, Batch 29/462, Loss: 0.7650458812713623\n",
      "Epoch 32, Batch 30/462, Loss: 0.6371184587478638\n",
      "Epoch 32, Batch 31/462, Loss: 0.7089986801147461\n",
      "Epoch 32, Batch 32/462, Loss: 0.6834758520126343\n",
      "Epoch 32, Batch 33/462, Loss: 0.6315697431564331\n",
      "Epoch 32, Batch 34/462, Loss: 0.6676574349403381\n",
      "Epoch 32, Batch 35/462, Loss: 0.867790937423706\n",
      "Epoch 32, Batch 36/462, Loss: 0.5797349810600281\n",
      "Epoch 32, Batch 37/462, Loss: 0.715355396270752\n",
      "Epoch 32, Batch 38/462, Loss: 0.6243430376052856\n",
      "Epoch 32, Batch 39/462, Loss: 0.6996927857398987\n",
      "Epoch 32, Batch 40/462, Loss: 0.7582828402519226\n",
      "Epoch 32, Batch 41/462, Loss: 0.6333982348442078\n",
      "Epoch 32, Batch 42/462, Loss: 0.4973466396331787\n",
      "Epoch 32, Batch 43/462, Loss: 0.7475494146347046\n",
      "Epoch 32, Batch 44/462, Loss: 0.6858192086219788\n",
      "Epoch 32, Batch 45/462, Loss: 0.749032735824585\n",
      "Epoch 32, Batch 46/462, Loss: 0.7302765250205994\n",
      "Epoch 32, Batch 47/462, Loss: 0.6808193922042847\n",
      "Epoch 32, Batch 48/462, Loss: 0.5214661359786987\n",
      "Epoch 32, Batch 49/462, Loss: 0.8409578204154968\n",
      "Epoch 32, Batch 50/462, Loss: 0.6677873134613037\n",
      "Epoch 32, Batch 51/462, Loss: 0.679018497467041\n",
      "Epoch 32, Batch 52/462, Loss: 0.7582464218139648\n",
      "Epoch 32, Batch 53/462, Loss: 0.7527235746383667\n",
      "Epoch 32, Batch 54/462, Loss: 0.6486049890518188\n",
      "Epoch 32, Batch 55/462, Loss: 0.7225165367126465\n",
      "Epoch 32, Batch 56/462, Loss: 0.6466308832168579\n",
      "Epoch 32, Batch 57/462, Loss: 0.5664243698120117\n",
      "Epoch 32, Batch 58/462, Loss: 0.8635134696960449\n",
      "Epoch 32, Batch 59/462, Loss: 0.7312484383583069\n",
      "Epoch 32, Batch 60/462, Loss: 0.6543751358985901\n",
      "Epoch 32, Batch 61/462, Loss: 0.6912210583686829\n",
      "Epoch 32, Batch 62/462, Loss: 0.679280161857605\n",
      "Epoch 32, Batch 63/462, Loss: 0.6296217441558838\n",
      "Epoch 32, Batch 64/462, Loss: 0.8097632527351379\n",
      "Epoch 32, Batch 65/462, Loss: 0.7197558879852295\n",
      "Epoch 32, Batch 66/462, Loss: 0.4785006046295166\n",
      "Epoch 32, Batch 67/462, Loss: 0.5780198574066162\n",
      "Epoch 32, Batch 68/462, Loss: 0.6984930038452148\n",
      "Epoch 32, Batch 69/462, Loss: 0.6523395776748657\n",
      "Epoch 32, Batch 70/462, Loss: 0.8676163554191589\n",
      "Epoch 32, Batch 71/462, Loss: 0.8554337024688721\n",
      "Epoch 32, Batch 72/462, Loss: 0.726068377494812\n",
      "Epoch 32, Batch 73/462, Loss: 0.6488132476806641\n",
      "Epoch 32, Batch 74/462, Loss: 0.9010021090507507\n",
      "Epoch 32, Batch 75/462, Loss: 0.8020243644714355\n",
      "Epoch 32, Batch 76/462, Loss: 0.798773467540741\n",
      "Epoch 32, Batch 77/462, Loss: 0.7424349784851074\n",
      "Epoch 32, Batch 78/462, Loss: 0.7002572417259216\n",
      "Epoch 32, Batch 79/462, Loss: 0.7710694670677185\n",
      "Epoch 32, Batch 80/462, Loss: 0.7731641530990601\n",
      "Epoch 32, Batch 81/462, Loss: 0.6725379228591919\n",
      "Epoch 32, Batch 82/462, Loss: 0.6377132534980774\n",
      "Epoch 32, Batch 83/462, Loss: 0.7387288808822632\n",
      "Epoch 32, Batch 84/462, Loss: 0.7425597906112671\n",
      "Epoch 32, Batch 85/462, Loss: 0.7474138140678406\n",
      "Epoch 32, Batch 86/462, Loss: 0.8611662983894348\n",
      "Epoch 32, Batch 87/462, Loss: 0.5728033781051636\n",
      "Epoch 32, Batch 88/462, Loss: 1.032065510749817\n",
      "Epoch 32, Batch 89/462, Loss: 0.6393106579780579\n",
      "Epoch 32, Batch 90/462, Loss: 0.742158055305481\n",
      "Epoch 32, Batch 91/462, Loss: 0.7337971329689026\n",
      "Epoch 32, Batch 92/462, Loss: 0.7917178273200989\n",
      "Epoch 32, Batch 93/462, Loss: 0.8343645334243774\n",
      "Epoch 32, Batch 94/462, Loss: 0.7803320288658142\n",
      "Epoch 32, Batch 95/462, Loss: 0.5902363657951355\n",
      "Epoch 32, Batch 96/462, Loss: 0.7280698418617249\n",
      "Epoch 32, Batch 97/462, Loss: 0.5991477966308594\n",
      "Epoch 32, Batch 98/462, Loss: 0.6300367116928101\n",
      "Epoch 32, Batch 99/462, Loss: 0.7335529923439026\n",
      "Epoch 32, Batch 100/462, Loss: 0.8572906255722046\n",
      "Epoch 32, Batch 101/462, Loss: 0.6866815090179443\n",
      "Epoch 32, Batch 102/462, Loss: 0.8182092905044556\n",
      "Epoch 32, Batch 103/462, Loss: 0.775881290435791\n",
      "Epoch 32, Batch 104/462, Loss: 0.7301449775695801\n",
      "Epoch 32, Batch 105/462, Loss: 0.7663690447807312\n",
      "Epoch 32, Batch 106/462, Loss: 0.7573691010475159\n",
      "Epoch 32, Batch 107/462, Loss: 0.6681420803070068\n",
      "Epoch 32, Batch 108/462, Loss: 0.73540860414505\n",
      "Epoch 32, Batch 109/462, Loss: 0.5495456457138062\n",
      "Epoch 32, Batch 110/462, Loss: 0.6371057033538818\n",
      "Epoch 32, Batch 111/462, Loss: 0.7821215391159058\n",
      "Epoch 32, Batch 112/462, Loss: 0.7413786053657532\n",
      "Epoch 32, Batch 113/462, Loss: 0.8271629810333252\n",
      "Epoch 32, Batch 114/462, Loss: 0.629117488861084\n",
      "Epoch 32, Batch 115/462, Loss: 0.8908994197845459\n",
      "Epoch 32, Batch 116/462, Loss: 0.7301174402236938\n",
      "Epoch 32, Batch 117/462, Loss: 0.6362904906272888\n",
      "Epoch 32, Batch 118/462, Loss: 0.6977605819702148\n",
      "Epoch 32, Batch 119/462, Loss: 0.9195382595062256\n",
      "Epoch 32, Batch 120/462, Loss: 0.6063699722290039\n",
      "Epoch 32, Batch 121/462, Loss: 0.7467904090881348\n",
      "Epoch 32, Batch 122/462, Loss: 0.6077038645744324\n",
      "Epoch 32, Batch 123/462, Loss: 0.8165873885154724\n",
      "Epoch 32, Batch 124/462, Loss: 0.7110427618026733\n",
      "Epoch 32, Batch 125/462, Loss: 0.8698577880859375\n",
      "Epoch 32, Batch 126/462, Loss: 0.6687688231468201\n",
      "Epoch 32, Batch 127/462, Loss: 0.8327754139900208\n",
      "Epoch 32, Batch 128/462, Loss: 0.7591358423233032\n",
      "Epoch 32, Batch 129/462, Loss: 0.6756587624549866\n",
      "Epoch 32, Batch 130/462, Loss: 0.7094032168388367\n",
      "Epoch 32, Batch 131/462, Loss: 0.6252561211585999\n",
      "Epoch 32, Batch 132/462, Loss: 0.6431189179420471\n",
      "Epoch 32, Batch 133/462, Loss: 0.7043886184692383\n",
      "Epoch 32, Batch 134/462, Loss: 0.515309751033783\n",
      "Epoch 32, Batch 135/462, Loss: 0.7651551365852356\n",
      "Epoch 32, Batch 136/462, Loss: 0.8366142511367798\n",
      "Epoch 32, Batch 137/462, Loss: 0.5366228818893433\n",
      "Epoch 32, Batch 138/462, Loss: 0.7032128572463989\n",
      "Epoch 32, Batch 139/462, Loss: 0.7654913067817688\n",
      "Epoch 32, Batch 140/462, Loss: 0.7001845836639404\n",
      "Epoch 32, Batch 141/462, Loss: 0.6723196506500244\n",
      "Epoch 32, Batch 142/462, Loss: 0.938265323638916\n",
      "Epoch 32, Batch 143/462, Loss: 0.7180086374282837\n",
      "Epoch 32, Batch 144/462, Loss: 0.7166524529457092\n",
      "Epoch 32, Batch 145/462, Loss: 0.6906766295433044\n",
      "Epoch 32, Batch 146/462, Loss: 0.6734998822212219\n",
      "Epoch 32, Batch 147/462, Loss: 0.5914431810379028\n",
      "Epoch 32, Batch 148/462, Loss: 0.6285982131958008\n",
      "Epoch 32, Batch 149/462, Loss: 0.5338993072509766\n",
      "Epoch 32, Batch 150/462, Loss: 0.7426139116287231\n",
      "Epoch 32, Batch 151/462, Loss: 0.8669067621231079\n",
      "Epoch 32, Batch 152/462, Loss: 0.7985563278198242\n",
      "Epoch 32, Batch 153/462, Loss: 0.7273881435394287\n",
      "Epoch 32, Batch 154/462, Loss: 0.9699305891990662\n",
      "Epoch 32, Batch 155/462, Loss: 0.5948935151100159\n",
      "Epoch 32, Batch 156/462, Loss: 0.8961113691329956\n",
      "Epoch 32, Batch 157/462, Loss: 0.7415853142738342\n",
      "Epoch 32, Batch 158/462, Loss: 0.7774891257286072\n",
      "Epoch 32, Batch 159/462, Loss: 0.7258434295654297\n",
      "Epoch 32, Batch 160/462, Loss: 0.8628259897232056\n",
      "Epoch 32, Batch 161/462, Loss: 0.8210431337356567\n",
      "Epoch 32, Batch 162/462, Loss: 0.5683091282844543\n",
      "Epoch 32, Batch 163/462, Loss: 0.6123583316802979\n",
      "Epoch 32, Batch 164/462, Loss: 0.7947420477867126\n",
      "Epoch 32, Batch 165/462, Loss: 0.632379412651062\n",
      "Epoch 32, Batch 166/462, Loss: 0.7226380109786987\n",
      "Epoch 32, Batch 167/462, Loss: 0.7900207042694092\n",
      "Epoch 32, Batch 168/462, Loss: 0.7492290735244751\n",
      "Epoch 32, Batch 169/462, Loss: 0.7267231941223145\n",
      "Epoch 32, Batch 170/462, Loss: 0.6769278049468994\n",
      "Epoch 32, Batch 171/462, Loss: 0.8144341707229614\n",
      "Epoch 32, Batch 172/462, Loss: 0.5565204620361328\n",
      "Epoch 32, Batch 173/462, Loss: 0.786902129650116\n",
      "Epoch 32, Batch 174/462, Loss: 0.6390408277511597\n",
      "Epoch 32, Batch 175/462, Loss: 0.800385594367981\n",
      "Epoch 32, Batch 176/462, Loss: 0.7843726873397827\n",
      "Epoch 32, Batch 177/462, Loss: 0.6896407008171082\n",
      "Epoch 32, Batch 178/462, Loss: 0.6702848672866821\n",
      "Epoch 32, Batch 179/462, Loss: 0.7062089443206787\n",
      "Epoch 32, Batch 180/462, Loss: 0.6591963768005371\n",
      "Epoch 32, Batch 181/462, Loss: 0.7522072792053223\n",
      "Epoch 32, Batch 182/462, Loss: 0.6285353899002075\n",
      "Epoch 32, Batch 183/462, Loss: 0.7877514958381653\n",
      "Epoch 32, Batch 184/462, Loss: 0.7410754561424255\n",
      "Epoch 32, Batch 185/462, Loss: 0.7802146673202515\n",
      "Epoch 32, Batch 186/462, Loss: 0.7018134593963623\n",
      "Epoch 32, Batch 187/462, Loss: 0.8312996029853821\n",
      "Epoch 32, Batch 188/462, Loss: 0.685494601726532\n",
      "Epoch 32, Batch 189/462, Loss: 0.6936992406845093\n",
      "Epoch 32, Batch 190/462, Loss: 0.6312198638916016\n",
      "Epoch 32, Batch 191/462, Loss: 0.782123327255249\n",
      "Epoch 32, Batch 192/462, Loss: 0.782931923866272\n",
      "Epoch 32, Batch 193/462, Loss: 0.6671062707901001\n",
      "Epoch 32, Batch 194/462, Loss: 0.6411791443824768\n",
      "Epoch 32, Batch 195/462, Loss: 0.7937843799591064\n",
      "Epoch 32, Batch 196/462, Loss: 0.694008469581604\n",
      "Epoch 32, Batch 197/462, Loss: 0.8287947773933411\n",
      "Epoch 32, Batch 198/462, Loss: 0.6807612180709839\n",
      "Epoch 32, Batch 199/462, Loss: 0.7571171522140503\n",
      "Epoch 32, Batch 200/462, Loss: 0.7680434584617615\n",
      "Epoch 32, Batch 201/462, Loss: 0.7841247916221619\n",
      "Epoch 32, Batch 202/462, Loss: 0.6108559370040894\n",
      "Epoch 32, Batch 203/462, Loss: 0.8502539396286011\n",
      "Epoch 32, Batch 204/462, Loss: 0.8274376392364502\n",
      "Epoch 32, Batch 205/462, Loss: 0.6891626715660095\n",
      "Epoch 32, Batch 206/462, Loss: 0.7902522683143616\n",
      "Epoch 32, Batch 207/462, Loss: 0.7749428749084473\n",
      "Epoch 32, Batch 208/462, Loss: 0.9338021278381348\n",
      "Epoch 32, Batch 209/462, Loss: 0.8564209342002869\n",
      "Epoch 32, Batch 210/462, Loss: 0.7818145155906677\n",
      "Epoch 32, Batch 211/462, Loss: 0.6134268045425415\n",
      "Epoch 32, Batch 212/462, Loss: 0.8224717974662781\n",
      "Epoch 32, Batch 213/462, Loss: 0.7454484105110168\n",
      "Epoch 32, Batch 214/462, Loss: 0.7056850790977478\n",
      "Epoch 32, Batch 215/462, Loss: 0.6990036368370056\n",
      "Epoch 32, Batch 216/462, Loss: 0.6599377989768982\n",
      "Epoch 32, Batch 217/462, Loss: 0.7235832214355469\n",
      "Epoch 32, Batch 218/462, Loss: 0.8111825585365295\n",
      "Epoch 32, Batch 219/462, Loss: 0.7216982245445251\n",
      "Epoch 32, Batch 220/462, Loss: 0.7001191973686218\n",
      "Epoch 32, Batch 221/462, Loss: 0.6114999055862427\n",
      "Epoch 32, Batch 222/462, Loss: 0.8492915630340576\n",
      "Epoch 32, Batch 223/462, Loss: 0.6605011820793152\n",
      "Epoch 32, Batch 224/462, Loss: 0.7522006034851074\n",
      "Epoch 32, Batch 225/462, Loss: 0.6547912955284119\n",
      "Epoch 32, Batch 226/462, Loss: 0.6672418117523193\n",
      "Epoch 32, Batch 227/462, Loss: 0.7202883958816528\n",
      "Epoch 32, Batch 228/462, Loss: 0.751681387424469\n",
      "Epoch 32, Batch 229/462, Loss: 0.7573477029800415\n",
      "Epoch 32, Batch 230/462, Loss: 0.8599443435668945\n",
      "Epoch 32, Batch 231/462, Loss: 0.6950922012329102\n",
      "Epoch 32, Batch 232/462, Loss: 0.5635321736335754\n",
      "Epoch 32, Batch 233/462, Loss: 0.7174363732337952\n",
      "Epoch 32, Batch 234/462, Loss: 0.7784212827682495\n",
      "Epoch 32, Batch 235/462, Loss: 0.8247060775756836\n",
      "Epoch 32, Batch 236/462, Loss: 0.6767024993896484\n",
      "Epoch 32, Batch 237/462, Loss: 0.7064381241798401\n",
      "Epoch 32, Batch 238/462, Loss: 0.7571176886558533\n",
      "Epoch 32, Batch 239/462, Loss: 0.7625941038131714\n",
      "Epoch 32, Batch 240/462, Loss: 0.6954021453857422\n",
      "Epoch 32, Batch 241/462, Loss: 0.6816378235816956\n",
      "Epoch 32, Batch 242/462, Loss: 0.7224841713905334\n",
      "Epoch 32, Batch 243/462, Loss: 0.6943947672843933\n",
      "Epoch 32, Batch 244/462, Loss: 0.8331875801086426\n",
      "Epoch 32, Batch 245/462, Loss: 0.7277571558952332\n",
      "Epoch 32, Batch 246/462, Loss: 0.9152234792709351\n",
      "Epoch 32, Batch 247/462, Loss: 0.6701894402503967\n",
      "Epoch 32, Batch 248/462, Loss: 0.7222025990486145\n",
      "Epoch 32, Batch 249/462, Loss: 0.8566856980323792\n",
      "Epoch 32, Batch 250/462, Loss: 0.7585111260414124\n",
      "Epoch 32, Batch 251/462, Loss: 0.7483010292053223\n",
      "Epoch 32, Batch 252/462, Loss: 0.742814838886261\n",
      "Epoch 32, Batch 253/462, Loss: 0.7269330620765686\n",
      "Epoch 32, Batch 254/462, Loss: 0.5381708145141602\n",
      "Epoch 32, Batch 255/462, Loss: 0.7569182515144348\n",
      "Epoch 32, Batch 256/462, Loss: 0.7766657471656799\n",
      "Epoch 32, Batch 257/462, Loss: 0.7560998201370239\n",
      "Epoch 32, Batch 258/462, Loss: 0.7270539999008179\n",
      "Epoch 32, Batch 259/462, Loss: 0.6087533235549927\n",
      "Epoch 32, Batch 260/462, Loss: 1.0061582326889038\n",
      "Epoch 32, Batch 261/462, Loss: 0.7737684845924377\n",
      "Epoch 32, Batch 262/462, Loss: 0.6485485434532166\n",
      "Epoch 32, Batch 263/462, Loss: 0.7394170165061951\n",
      "Epoch 32, Batch 264/462, Loss: 0.7071143388748169\n",
      "Epoch 32, Batch 265/462, Loss: 0.808106005191803\n",
      "Epoch 32, Batch 266/462, Loss: 0.7116302847862244\n",
      "Epoch 32, Batch 267/462, Loss: 0.6536561846733093\n",
      "Epoch 32, Batch 268/462, Loss: 0.776250422000885\n",
      "Epoch 32, Batch 269/462, Loss: 0.8600327968597412\n",
      "Epoch 32, Batch 270/462, Loss: 0.6736153364181519\n",
      "Epoch 32, Batch 271/462, Loss: 0.9006859064102173\n",
      "Epoch 32, Batch 272/462, Loss: 0.8829813003540039\n",
      "Epoch 32, Batch 273/462, Loss: 0.7887463569641113\n",
      "Epoch 32, Batch 274/462, Loss: 0.5973165035247803\n",
      "Epoch 32, Batch 275/462, Loss: 0.6199774146080017\n",
      "Epoch 32, Batch 276/462, Loss: 0.6985912322998047\n",
      "Epoch 32, Batch 277/462, Loss: 0.6564337015151978\n",
      "Epoch 32, Batch 278/462, Loss: 0.686686098575592\n",
      "Epoch 32, Batch 279/462, Loss: 0.6426130533218384\n",
      "Epoch 32, Batch 280/462, Loss: 0.7528907060623169\n",
      "Epoch 32, Batch 281/462, Loss: 0.7314687371253967\n",
      "Epoch 32, Batch 282/462, Loss: 0.893244206905365\n",
      "Epoch 32, Batch 283/462, Loss: 0.890174388885498\n",
      "Epoch 32, Batch 284/462, Loss: 0.7798346877098083\n",
      "Epoch 32, Batch 285/462, Loss: 0.45009949803352356\n",
      "Epoch 32, Batch 286/462, Loss: 0.6532199382781982\n",
      "Epoch 32, Batch 287/462, Loss: 0.7753023505210876\n",
      "Epoch 32, Batch 288/462, Loss: 0.7908591628074646\n",
      "Epoch 32, Batch 289/462, Loss: 0.7316020727157593\n",
      "Epoch 32, Batch 290/462, Loss: 0.5242998600006104\n",
      "Epoch 32, Batch 291/462, Loss: 0.8353881239891052\n",
      "Epoch 32, Batch 292/462, Loss: 0.9166309237480164\n",
      "Epoch 32, Batch 293/462, Loss: 0.8017487525939941\n",
      "Epoch 32, Batch 294/462, Loss: 0.9526270627975464\n",
      "Epoch 32, Batch 295/462, Loss: 0.5460302233695984\n",
      "Epoch 32, Batch 296/462, Loss: 0.7434982657432556\n",
      "Epoch 32, Batch 297/462, Loss: 0.6208701729774475\n",
      "Epoch 32, Batch 298/462, Loss: 0.7055920958518982\n",
      "Epoch 32, Batch 299/462, Loss: 0.7718490362167358\n",
      "Epoch 32, Batch 300/462, Loss: 0.6595356464385986\n",
      "Epoch 32, Batch 301/462, Loss: 0.670772135257721\n",
      "Epoch 32, Batch 302/462, Loss: 0.781916618347168\n",
      "Epoch 32, Batch 303/462, Loss: 0.7545495629310608\n",
      "Epoch 32, Batch 304/462, Loss: 0.6474748849868774\n",
      "Epoch 32, Batch 305/462, Loss: 0.7858046889305115\n",
      "Epoch 32, Batch 306/462, Loss: 0.77915358543396\n",
      "Epoch 32, Batch 307/462, Loss: 0.7216039299964905\n",
      "Epoch 32, Batch 308/462, Loss: 0.6422122120857239\n",
      "Epoch 32, Batch 309/462, Loss: 0.8089499473571777\n",
      "Epoch 32, Batch 310/462, Loss: 0.8796080350875854\n",
      "Epoch 32, Batch 311/462, Loss: 0.7665455937385559\n",
      "Epoch 32, Batch 312/462, Loss: 0.7823116183280945\n",
      "Epoch 32, Batch 313/462, Loss: 0.7233921885490417\n",
      "Epoch 32, Batch 314/462, Loss: 0.7783583998680115\n",
      "Epoch 32, Batch 315/462, Loss: 0.6162649989128113\n",
      "Epoch 32, Batch 316/462, Loss: 0.7261428236961365\n",
      "Epoch 32, Batch 317/462, Loss: 0.6500452160835266\n",
      "Epoch 32, Batch 318/462, Loss: 0.6366930603981018\n",
      "Epoch 32, Batch 319/462, Loss: 0.7575114965438843\n",
      "Epoch 32, Batch 320/462, Loss: 0.7346738576889038\n",
      "Epoch 32, Batch 321/462, Loss: 0.5891447067260742\n",
      "Epoch 32, Batch 322/462, Loss: 0.7709041833877563\n",
      "Epoch 32, Batch 323/462, Loss: 0.8523691296577454\n",
      "Epoch 32, Batch 324/462, Loss: 0.610685408115387\n",
      "Epoch 32, Batch 325/462, Loss: 0.5491217374801636\n",
      "Epoch 32, Batch 326/462, Loss: 0.6337291598320007\n",
      "Epoch 32, Batch 327/462, Loss: 0.7118331789970398\n",
      "Epoch 32, Batch 328/462, Loss: 0.5364953875541687\n",
      "Epoch 32, Batch 329/462, Loss: 0.6523348689079285\n",
      "Epoch 32, Batch 330/462, Loss: 0.7077594995498657\n",
      "Epoch 32, Batch 331/462, Loss: 0.6622158885002136\n",
      "Epoch 32, Batch 332/462, Loss: 0.7974697947502136\n",
      "Epoch 32, Batch 333/462, Loss: 0.7407784461975098\n",
      "Epoch 32, Batch 334/462, Loss: 0.7216950058937073\n",
      "Epoch 32, Batch 335/462, Loss: 0.618735671043396\n",
      "Epoch 32, Batch 336/462, Loss: 0.9169335961341858\n",
      "Epoch 32, Batch 337/462, Loss: 0.7435865998268127\n",
      "Epoch 32, Batch 338/462, Loss: 0.7331293821334839\n",
      "Epoch 32, Batch 339/462, Loss: 0.7033238410949707\n",
      "Epoch 32, Batch 340/462, Loss: 0.8269013166427612\n",
      "Epoch 32, Batch 341/462, Loss: 0.6914212107658386\n",
      "Epoch 32, Batch 342/462, Loss: 0.5539394021034241\n",
      "Epoch 32, Batch 343/462, Loss: 0.591920018196106\n",
      "Epoch 32, Batch 344/462, Loss: 0.6145175099372864\n",
      "Epoch 32, Batch 345/462, Loss: 0.7683509588241577\n",
      "Epoch 32, Batch 346/462, Loss: 0.7136169075965881\n",
      "Epoch 32, Batch 347/462, Loss: 0.9663379192352295\n",
      "Epoch 32, Batch 348/462, Loss: 0.7350627779960632\n",
      "Epoch 32, Batch 349/462, Loss: 0.668049693107605\n",
      "Epoch 32, Batch 350/462, Loss: 0.78522127866745\n",
      "Epoch 32, Batch 351/462, Loss: 0.6427679061889648\n",
      "Epoch 32, Batch 352/462, Loss: 0.7089904546737671\n",
      "Epoch 32, Batch 353/462, Loss: 0.580339252948761\n",
      "Epoch 32, Batch 354/462, Loss: 0.6503603458404541\n",
      "Epoch 32, Batch 355/462, Loss: 0.8231796026229858\n",
      "Epoch 32, Batch 356/462, Loss: 0.8037081360816956\n",
      "Epoch 32, Batch 357/462, Loss: 0.6222406625747681\n",
      "Epoch 32, Batch 358/462, Loss: 0.7560865879058838\n",
      "Epoch 32, Batch 359/462, Loss: 0.6516814827919006\n",
      "Epoch 32, Batch 360/462, Loss: 0.7645661234855652\n",
      "Epoch 32, Batch 361/462, Loss: 0.7028054594993591\n",
      "Epoch 32, Batch 362/462, Loss: 0.8359659910202026\n",
      "Epoch 32, Batch 363/462, Loss: 0.7020949721336365\n",
      "Epoch 32, Batch 364/462, Loss: 0.936784029006958\n",
      "Epoch 32, Batch 365/462, Loss: 0.6548092365264893\n",
      "Epoch 32, Batch 366/462, Loss: 0.6274397373199463\n",
      "Epoch 32, Batch 367/462, Loss: 0.7004039883613586\n",
      "Epoch 32, Batch 368/462, Loss: 0.7248690128326416\n",
      "Epoch 32, Batch 369/462, Loss: 0.6886481642723083\n",
      "Epoch 32, Batch 370/462, Loss: 0.7159134149551392\n",
      "Epoch 32, Batch 371/462, Loss: 0.8243382573127747\n",
      "Epoch 32, Batch 372/462, Loss: 0.6836468577384949\n",
      "Epoch 32, Batch 373/462, Loss: 0.7153924703598022\n",
      "Epoch 32, Batch 374/462, Loss: 0.6769202947616577\n",
      "Epoch 32, Batch 375/462, Loss: 0.7102400660514832\n",
      "Epoch 32, Batch 376/462, Loss: 0.730964720249176\n",
      "Epoch 32, Batch 377/462, Loss: 0.8537086844444275\n",
      "Epoch 32, Batch 378/462, Loss: 0.7888098955154419\n",
      "Epoch 32, Batch 379/462, Loss: 0.8591801524162292\n",
      "Epoch 32, Batch 380/462, Loss: 0.547525942325592\n",
      "Epoch 32, Batch 381/462, Loss: 0.6074264645576477\n",
      "Epoch 32, Batch 382/462, Loss: 0.7405110001564026\n",
      "Epoch 32, Batch 383/462, Loss: 0.878100574016571\n",
      "Epoch 32, Batch 384/462, Loss: 0.7396679520606995\n",
      "Epoch 32, Batch 385/462, Loss: 0.5717114806175232\n",
      "Epoch 32, Batch 386/462, Loss: 0.7336310744285583\n",
      "Epoch 32, Batch 387/462, Loss: 0.8402988314628601\n",
      "Epoch 32, Batch 388/462, Loss: 0.7534228563308716\n",
      "Epoch 32, Batch 389/462, Loss: 0.8347398042678833\n",
      "Epoch 32, Batch 390/462, Loss: 0.9298471212387085\n",
      "Epoch 32, Batch 391/462, Loss: 0.6278449892997742\n",
      "Epoch 32, Batch 392/462, Loss: 0.8526809811592102\n",
      "Epoch 32, Batch 393/462, Loss: 0.8273564577102661\n",
      "Epoch 32, Batch 394/462, Loss: 0.6420895457267761\n",
      "Epoch 32, Batch 395/462, Loss: 0.5820269584655762\n",
      "Epoch 32, Batch 396/462, Loss: 0.7347570657730103\n",
      "Epoch 32, Batch 397/462, Loss: 0.8630209565162659\n",
      "Epoch 32, Batch 398/462, Loss: 0.6901732087135315\n",
      "Epoch 32, Batch 399/462, Loss: 0.7881395816802979\n",
      "Epoch 32, Batch 400/462, Loss: 0.6377902626991272\n",
      "Epoch 32, Batch 401/462, Loss: 0.7085404992103577\n",
      "Epoch 32, Batch 402/462, Loss: 0.8819587230682373\n",
      "Epoch 32, Batch 403/462, Loss: 0.7179149389266968\n",
      "Epoch 32, Batch 404/462, Loss: 0.5972679853439331\n",
      "Epoch 32, Batch 405/462, Loss: 0.7111400961875916\n",
      "Epoch 32, Batch 406/462, Loss: 0.6823013424873352\n",
      "Epoch 32, Batch 407/462, Loss: 0.8742246627807617\n",
      "Epoch 32, Batch 408/462, Loss: 0.7223189473152161\n",
      "Epoch 32, Batch 409/462, Loss: 0.8173810243606567\n",
      "Epoch 32, Batch 410/462, Loss: 0.6193751096725464\n",
      "Epoch 32, Batch 411/462, Loss: 0.6649130582809448\n",
      "Epoch 32, Batch 412/462, Loss: 0.6210792660713196\n",
      "Epoch 32, Batch 413/462, Loss: 0.7361985445022583\n",
      "Epoch 32, Batch 414/462, Loss: 0.6596567630767822\n",
      "Epoch 32, Batch 415/462, Loss: 0.7291433215141296\n",
      "Epoch 32, Batch 416/462, Loss: 0.639794647693634\n",
      "Epoch 32, Batch 417/462, Loss: 0.6521862149238586\n",
      "Epoch 32, Batch 418/462, Loss: 0.7331345081329346\n",
      "Epoch 32, Batch 419/462, Loss: 0.5842537879943848\n",
      "Epoch 32, Batch 420/462, Loss: 0.8154866695404053\n",
      "Epoch 32, Batch 421/462, Loss: 0.6520929336547852\n",
      "Epoch 32, Batch 422/462, Loss: 0.6178942322731018\n",
      "Epoch 32, Batch 423/462, Loss: 0.5758015513420105\n",
      "Epoch 32, Batch 424/462, Loss: 0.888468861579895\n",
      "Epoch 32, Batch 425/462, Loss: 0.7023682594299316\n",
      "Epoch 32, Batch 426/462, Loss: 0.7247315049171448\n",
      "Epoch 32, Batch 427/462, Loss: 0.6506502628326416\n",
      "Epoch 32, Batch 428/462, Loss: 0.7008516788482666\n",
      "Epoch 32, Batch 429/462, Loss: 0.6949900984764099\n",
      "Epoch 32, Batch 430/462, Loss: 0.6995859146118164\n",
      "Epoch 32, Batch 431/462, Loss: 0.8117140531539917\n",
      "Epoch 32, Batch 432/462, Loss: 0.7621058225631714\n",
      "Epoch 32, Batch 433/462, Loss: 0.7610669136047363\n",
      "Epoch 32, Batch 434/462, Loss: 0.6172791123390198\n",
      "Epoch 32, Batch 435/462, Loss: 0.7954739928245544\n",
      "Epoch 32, Batch 436/462, Loss: 0.7553232908248901\n",
      "Epoch 32, Batch 437/462, Loss: 0.7011793851852417\n",
      "Epoch 32, Batch 438/462, Loss: 0.7153676748275757\n",
      "Epoch 32, Batch 439/462, Loss: 0.7684735655784607\n",
      "Epoch 32, Batch 440/462, Loss: 0.7642270922660828\n",
      "Epoch 32, Batch 441/462, Loss: 0.5994019508361816\n",
      "Epoch 32, Batch 442/462, Loss: 0.6997367143630981\n",
      "Epoch 32, Batch 443/462, Loss: 0.7328532934188843\n",
      "Epoch 32, Batch 444/462, Loss: 0.6789385080337524\n",
      "Epoch 32, Batch 445/462, Loss: 0.8188738822937012\n",
      "Epoch 32, Batch 446/462, Loss: 0.8132259249687195\n",
      "Epoch 32, Batch 447/462, Loss: 0.6544751524925232\n",
      "Epoch 32, Batch 448/462, Loss: 0.9481332302093506\n",
      "Epoch 32, Batch 449/462, Loss: 0.7807969450950623\n",
      "Epoch 32, Batch 450/462, Loss: 0.62408447265625\n",
      "Epoch 32, Batch 451/462, Loss: 0.7050520181655884\n",
      "Epoch 32, Batch 452/462, Loss: 0.6775755882263184\n",
      "Epoch 32, Batch 453/462, Loss: 0.7235209941864014\n",
      "Epoch 32, Batch 454/462, Loss: 0.5892168283462524\n",
      "Epoch 32, Batch 455/462, Loss: 0.6603050827980042\n",
      "Epoch 32, Batch 456/462, Loss: 0.7895419001579285\n",
      "Epoch 32, Batch 457/462, Loss: 0.7328574061393738\n",
      "Epoch 32, Batch 458/462, Loss: 0.665183961391449\n",
      "Epoch 32, Batch 459/462, Loss: 0.9085038304328918\n",
      "Epoch 32, Batch 460/462, Loss: 0.8300372362136841\n",
      "Epoch 32, Batch 461/462, Loss: 0.7444879412651062\n",
      "Epoch 32, Batch 462/462, Loss: 0.5963154435157776\n",
      "Epoch 32, Loss: 334.684503108263\n",
      "Epoch 33, Batch 1/462, Loss: 0.57330322265625\n",
      "Epoch 33, Batch 2/462, Loss: 0.7855794429779053\n",
      "Epoch 33, Batch 3/462, Loss: 0.6525955200195312\n",
      "Epoch 33, Batch 4/462, Loss: 0.820724606513977\n",
      "Epoch 33, Batch 5/462, Loss: 0.7053290605545044\n",
      "Epoch 33, Batch 6/462, Loss: 0.6830492615699768\n",
      "Epoch 33, Batch 7/462, Loss: 0.7272484302520752\n",
      "Epoch 33, Batch 8/462, Loss: 0.8966510891914368\n",
      "Epoch 33, Batch 9/462, Loss: 0.5513964891433716\n",
      "Epoch 33, Batch 10/462, Loss: 0.901665449142456\n",
      "Epoch 33, Batch 11/462, Loss: 0.7057876586914062\n",
      "Epoch 33, Batch 12/462, Loss: 0.7652426362037659\n",
      "Epoch 33, Batch 13/462, Loss: 0.7214252352714539\n",
      "Epoch 33, Batch 14/462, Loss: 0.6076893210411072\n",
      "Epoch 33, Batch 15/462, Loss: 0.6051017045974731\n",
      "Epoch 33, Batch 16/462, Loss: 0.8650711178779602\n",
      "Epoch 33, Batch 17/462, Loss: 0.8655162453651428\n",
      "Epoch 33, Batch 18/462, Loss: 0.7266250252723694\n",
      "Epoch 33, Batch 19/462, Loss: 0.6476060152053833\n",
      "Epoch 33, Batch 20/462, Loss: 0.6883373856544495\n",
      "Epoch 33, Batch 21/462, Loss: 0.7087308168411255\n",
      "Epoch 33, Batch 22/462, Loss: 0.7729597091674805\n",
      "Epoch 33, Batch 23/462, Loss: 0.746821939945221\n",
      "Epoch 33, Batch 24/462, Loss: 0.7387635111808777\n",
      "Epoch 33, Batch 25/462, Loss: 0.6124148964881897\n",
      "Epoch 33, Batch 26/462, Loss: 0.595072329044342\n",
      "Epoch 33, Batch 27/462, Loss: 0.7834290266036987\n",
      "Epoch 33, Batch 28/462, Loss: 0.8461136221885681\n",
      "Epoch 33, Batch 29/462, Loss: 0.8629884719848633\n",
      "Epoch 33, Batch 30/462, Loss: 0.7508730292320251\n",
      "Epoch 33, Batch 31/462, Loss: 0.7489402294158936\n",
      "Epoch 33, Batch 32/462, Loss: 0.6554499268531799\n",
      "Epoch 33, Batch 33/462, Loss: 0.6439605355262756\n",
      "Epoch 33, Batch 34/462, Loss: 0.616287350654602\n",
      "Epoch 33, Batch 35/462, Loss: 0.5811801552772522\n",
      "Epoch 33, Batch 36/462, Loss: 0.6939655542373657\n",
      "Epoch 33, Batch 37/462, Loss: 0.6126912236213684\n",
      "Epoch 33, Batch 38/462, Loss: 0.6874349117279053\n",
      "Epoch 33, Batch 39/462, Loss: 0.7221158742904663\n",
      "Epoch 33, Batch 40/462, Loss: 0.5645684003829956\n",
      "Epoch 33, Batch 41/462, Loss: 0.7808598279953003\n",
      "Epoch 33, Batch 42/462, Loss: 0.771358847618103\n",
      "Epoch 33, Batch 43/462, Loss: 0.6036683917045593\n",
      "Epoch 33, Batch 44/462, Loss: 0.6076682209968567\n",
      "Epoch 33, Batch 45/462, Loss: 0.6661868691444397\n",
      "Epoch 33, Batch 46/462, Loss: 0.6788614988327026\n",
      "Epoch 33, Batch 47/462, Loss: 0.589422881603241\n",
      "Epoch 33, Batch 48/462, Loss: 0.7804590463638306\n",
      "Epoch 33, Batch 49/462, Loss: 0.6421318650245667\n",
      "Epoch 33, Batch 50/462, Loss: 0.7148172855377197\n",
      "Epoch 33, Batch 51/462, Loss: 0.8308899402618408\n",
      "Epoch 33, Batch 52/462, Loss: 0.9405267238616943\n",
      "Epoch 33, Batch 53/462, Loss: 0.7796096801757812\n",
      "Epoch 33, Batch 54/462, Loss: 0.5959761738777161\n",
      "Epoch 33, Batch 55/462, Loss: 0.6931171417236328\n",
      "Epoch 33, Batch 56/462, Loss: 0.809869110584259\n",
      "Epoch 33, Batch 57/462, Loss: 0.7199474573135376\n",
      "Epoch 33, Batch 58/462, Loss: 0.7016363143920898\n",
      "Epoch 33, Batch 59/462, Loss: 0.7700482606887817\n",
      "Epoch 33, Batch 60/462, Loss: 0.6776662468910217\n",
      "Epoch 33, Batch 61/462, Loss: 0.7712066173553467\n",
      "Epoch 33, Batch 62/462, Loss: 0.730108380317688\n",
      "Epoch 33, Batch 63/462, Loss: 0.6044642925262451\n",
      "Epoch 33, Batch 64/462, Loss: 0.7741608023643494\n",
      "Epoch 33, Batch 65/462, Loss: 0.7518976330757141\n",
      "Epoch 33, Batch 66/462, Loss: 0.7563334703445435\n",
      "Epoch 33, Batch 67/462, Loss: 0.6890265941619873\n",
      "Epoch 33, Batch 68/462, Loss: 0.8318462371826172\n",
      "Epoch 33, Batch 69/462, Loss: 0.5682628154754639\n",
      "Epoch 33, Batch 70/462, Loss: 0.8165180683135986\n",
      "Epoch 33, Batch 71/462, Loss: 0.6099308133125305\n",
      "Epoch 33, Batch 72/462, Loss: 0.8326660990715027\n",
      "Epoch 33, Batch 73/462, Loss: 0.6245095133781433\n",
      "Epoch 33, Batch 74/462, Loss: 0.9318063259124756\n",
      "Epoch 33, Batch 75/462, Loss: 0.6930074691772461\n",
      "Epoch 33, Batch 76/462, Loss: 0.7401089668273926\n",
      "Epoch 33, Batch 77/462, Loss: 0.6837934255599976\n",
      "Epoch 33, Batch 78/462, Loss: 0.6083552241325378\n",
      "Epoch 33, Batch 79/462, Loss: 0.6948700547218323\n",
      "Epoch 33, Batch 80/462, Loss: 0.7783259749412537\n",
      "Epoch 33, Batch 81/462, Loss: 0.6968741416931152\n",
      "Epoch 33, Batch 82/462, Loss: 0.9269515872001648\n",
      "Epoch 33, Batch 83/462, Loss: 0.8737043142318726\n",
      "Epoch 33, Batch 84/462, Loss: 0.7464933395385742\n",
      "Epoch 33, Batch 85/462, Loss: 0.8535239100456238\n",
      "Epoch 33, Batch 86/462, Loss: 0.6203976273536682\n",
      "Epoch 33, Batch 87/462, Loss: 0.8148335218429565\n",
      "Epoch 33, Batch 88/462, Loss: 0.7093838453292847\n",
      "Epoch 33, Batch 89/462, Loss: 0.7528930306434631\n",
      "Epoch 33, Batch 90/462, Loss: 0.7058188319206238\n",
      "Epoch 33, Batch 91/462, Loss: 0.7795804142951965\n",
      "Epoch 33, Batch 92/462, Loss: 0.739025890827179\n",
      "Epoch 33, Batch 93/462, Loss: 0.6398701667785645\n",
      "Epoch 33, Batch 94/462, Loss: 0.6015791296958923\n",
      "Epoch 33, Batch 95/462, Loss: 0.8774632811546326\n",
      "Epoch 33, Batch 96/462, Loss: 0.7788320183753967\n",
      "Epoch 33, Batch 97/462, Loss: 0.6484996676445007\n",
      "Epoch 33, Batch 98/462, Loss: 0.6718834042549133\n",
      "Epoch 33, Batch 99/462, Loss: 0.752118706703186\n",
      "Epoch 33, Batch 100/462, Loss: 0.8626070022583008\n",
      "Epoch 33, Batch 101/462, Loss: 0.6865662932395935\n",
      "Epoch 33, Batch 102/462, Loss: 0.6595985293388367\n",
      "Epoch 33, Batch 103/462, Loss: 0.624809980392456\n",
      "Epoch 33, Batch 104/462, Loss: 0.7291719913482666\n",
      "Epoch 33, Batch 105/462, Loss: 0.583526611328125\n",
      "Epoch 33, Batch 106/462, Loss: 0.7880052924156189\n",
      "Epoch 33, Batch 107/462, Loss: 0.632314920425415\n",
      "Epoch 33, Batch 108/462, Loss: 0.6901824474334717\n",
      "Epoch 33, Batch 109/462, Loss: 0.7862664461135864\n",
      "Epoch 33, Batch 110/462, Loss: 0.9107866883277893\n",
      "Epoch 33, Batch 111/462, Loss: 0.6973080039024353\n",
      "Epoch 33, Batch 112/462, Loss: 0.6997970342636108\n",
      "Epoch 33, Batch 113/462, Loss: 0.7230066061019897\n",
      "Epoch 33, Batch 114/462, Loss: 0.7100693583488464\n",
      "Epoch 33, Batch 115/462, Loss: 1.0973378419876099\n",
      "Epoch 33, Batch 116/462, Loss: 0.9306884407997131\n",
      "Epoch 33, Batch 117/462, Loss: 0.8469974398612976\n",
      "Epoch 33, Batch 118/462, Loss: 0.7179089188575745\n",
      "Epoch 33, Batch 119/462, Loss: 0.6890177130699158\n",
      "Epoch 33, Batch 120/462, Loss: 0.8293120861053467\n",
      "Epoch 33, Batch 121/462, Loss: 0.6893596053123474\n",
      "Epoch 33, Batch 122/462, Loss: 0.7216752171516418\n",
      "Epoch 33, Batch 123/462, Loss: 0.7576155662536621\n",
      "Epoch 33, Batch 124/462, Loss: 0.840721070766449\n",
      "Epoch 33, Batch 125/462, Loss: 0.6977135539054871\n",
      "Epoch 33, Batch 126/462, Loss: 0.7622307538986206\n",
      "Epoch 33, Batch 127/462, Loss: 0.7049443125724792\n",
      "Epoch 33, Batch 128/462, Loss: 0.7662113308906555\n",
      "Epoch 33, Batch 129/462, Loss: 0.7151899337768555\n",
      "Epoch 33, Batch 130/462, Loss: 0.6870335340499878\n",
      "Epoch 33, Batch 131/462, Loss: 0.6519756317138672\n",
      "Epoch 33, Batch 132/462, Loss: 0.5777730345726013\n",
      "Epoch 33, Batch 133/462, Loss: 0.7544232606887817\n",
      "Epoch 33, Batch 134/462, Loss: 0.858691394329071\n",
      "Epoch 33, Batch 135/462, Loss: 0.6288396716117859\n",
      "Epoch 33, Batch 136/462, Loss: 0.7949694395065308\n",
      "Epoch 33, Batch 137/462, Loss: 0.6279469132423401\n",
      "Epoch 33, Batch 138/462, Loss: 0.7241014242172241\n",
      "Epoch 33, Batch 139/462, Loss: 0.6829749345779419\n",
      "Epoch 33, Batch 140/462, Loss: 0.7571390867233276\n",
      "Epoch 33, Batch 141/462, Loss: 0.7909919023513794\n",
      "Epoch 33, Batch 142/462, Loss: 0.6590551733970642\n",
      "Epoch 33, Batch 143/462, Loss: 0.6470579504966736\n",
      "Epoch 33, Batch 144/462, Loss: 0.7151990532875061\n",
      "Epoch 33, Batch 145/462, Loss: 0.7414218187332153\n",
      "Epoch 33, Batch 146/462, Loss: 0.8231588006019592\n",
      "Epoch 33, Batch 147/462, Loss: 0.7376665472984314\n",
      "Epoch 33, Batch 148/462, Loss: 0.7052813172340393\n",
      "Epoch 33, Batch 149/462, Loss: 0.7911492586135864\n",
      "Epoch 33, Batch 150/462, Loss: 0.7319279909133911\n",
      "Epoch 33, Batch 151/462, Loss: 0.6945320963859558\n",
      "Epoch 33, Batch 152/462, Loss: 0.7770119905471802\n",
      "Epoch 33, Batch 153/462, Loss: 0.8185316920280457\n",
      "Epoch 33, Batch 154/462, Loss: 0.7309821248054504\n",
      "Epoch 33, Batch 155/462, Loss: 0.7082512378692627\n",
      "Epoch 33, Batch 156/462, Loss: 0.6632179021835327\n",
      "Epoch 33, Batch 157/462, Loss: 0.7209594249725342\n",
      "Epoch 33, Batch 158/462, Loss: 0.6385849118232727\n",
      "Epoch 33, Batch 159/462, Loss: 0.6539469957351685\n",
      "Epoch 33, Batch 160/462, Loss: 0.7542970180511475\n",
      "Epoch 33, Batch 161/462, Loss: 0.797120988368988\n",
      "Epoch 33, Batch 162/462, Loss: 0.7568124532699585\n",
      "Epoch 33, Batch 163/462, Loss: 0.685235321521759\n",
      "Epoch 33, Batch 164/462, Loss: 0.8550341725349426\n",
      "Epoch 33, Batch 165/462, Loss: 0.7067508101463318\n",
      "Epoch 33, Batch 166/462, Loss: 0.6474790573120117\n",
      "Epoch 33, Batch 167/462, Loss: 0.7063707709312439\n",
      "Epoch 33, Batch 168/462, Loss: 0.7197228074073792\n",
      "Epoch 33, Batch 169/462, Loss: 0.835236668586731\n",
      "Epoch 33, Batch 170/462, Loss: 0.9139630794525146\n",
      "Epoch 33, Batch 171/462, Loss: 0.8602475523948669\n",
      "Epoch 33, Batch 172/462, Loss: 0.6171669363975525\n",
      "Epoch 33, Batch 173/462, Loss: 0.6377347707748413\n",
      "Epoch 33, Batch 174/462, Loss: 0.7854864597320557\n",
      "Epoch 33, Batch 175/462, Loss: 0.6438701152801514\n",
      "Epoch 33, Batch 176/462, Loss: 0.7856958508491516\n",
      "Epoch 33, Batch 177/462, Loss: 0.8461348414421082\n",
      "Epoch 33, Batch 178/462, Loss: 0.6418346762657166\n",
      "Epoch 33, Batch 179/462, Loss: 0.7579976916313171\n",
      "Epoch 33, Batch 180/462, Loss: 0.6618047952651978\n",
      "Epoch 33, Batch 181/462, Loss: 0.7673602104187012\n",
      "Epoch 33, Batch 182/462, Loss: 0.9607326984405518\n",
      "Epoch 33, Batch 183/462, Loss: 0.7628486752510071\n",
      "Epoch 33, Batch 184/462, Loss: 0.7779054641723633\n",
      "Epoch 33, Batch 185/462, Loss: 0.6260457634925842\n",
      "Epoch 33, Batch 186/462, Loss: 0.6409541368484497\n",
      "Epoch 33, Batch 187/462, Loss: 0.8307619690895081\n",
      "Epoch 33, Batch 188/462, Loss: 0.7681212425231934\n",
      "Epoch 33, Batch 189/462, Loss: 0.6390023231506348\n",
      "Epoch 33, Batch 190/462, Loss: 0.8165952563285828\n",
      "Epoch 33, Batch 191/462, Loss: 0.7376753091812134\n",
      "Epoch 33, Batch 192/462, Loss: 0.5941908359527588\n",
      "Epoch 33, Batch 193/462, Loss: 0.7907043695449829\n",
      "Epoch 33, Batch 194/462, Loss: 0.627393364906311\n",
      "Epoch 33, Batch 195/462, Loss: 0.8621638417243958\n",
      "Epoch 33, Batch 196/462, Loss: 0.7890865802764893\n",
      "Epoch 33, Batch 197/462, Loss: 0.7700900435447693\n",
      "Epoch 33, Batch 198/462, Loss: 0.6512090563774109\n",
      "Epoch 33, Batch 199/462, Loss: 0.7095664739608765\n",
      "Epoch 33, Batch 200/462, Loss: 0.7820852398872375\n",
      "Epoch 33, Batch 201/462, Loss: 0.6600822806358337\n",
      "Epoch 33, Batch 202/462, Loss: 0.7067742943763733\n",
      "Epoch 33, Batch 203/462, Loss: 0.7938193082809448\n",
      "Epoch 33, Batch 204/462, Loss: 0.5685004591941833\n",
      "Epoch 33, Batch 205/462, Loss: 0.8198999166488647\n",
      "Epoch 33, Batch 206/462, Loss: 0.8056760430335999\n",
      "Epoch 33, Batch 207/462, Loss: 0.7036303281784058\n",
      "Epoch 33, Batch 208/462, Loss: 0.8537662029266357\n",
      "Epoch 33, Batch 209/462, Loss: 0.8293344974517822\n",
      "Epoch 33, Batch 210/462, Loss: 0.6398501396179199\n",
      "Epoch 33, Batch 211/462, Loss: 0.6390926241874695\n",
      "Epoch 33, Batch 212/462, Loss: 0.8009286522865295\n",
      "Epoch 33, Batch 213/462, Loss: 0.879986584186554\n",
      "Epoch 33, Batch 214/462, Loss: 0.5519991517066956\n",
      "Epoch 33, Batch 215/462, Loss: 0.6441264152526855\n",
      "Epoch 33, Batch 216/462, Loss: 0.6054982542991638\n",
      "Epoch 33, Batch 217/462, Loss: 0.8368205428123474\n",
      "Epoch 33, Batch 218/462, Loss: 0.6461069583892822\n",
      "Epoch 33, Batch 219/462, Loss: 0.7185275554656982\n",
      "Epoch 33, Batch 220/462, Loss: 0.7727190256118774\n",
      "Epoch 33, Batch 221/462, Loss: 0.6431453824043274\n",
      "Epoch 33, Batch 222/462, Loss: 0.6399227380752563\n",
      "Epoch 33, Batch 223/462, Loss: 0.7383159399032593\n",
      "Epoch 33, Batch 224/462, Loss: 0.7540165185928345\n",
      "Epoch 33, Batch 225/462, Loss: 0.7096624970436096\n",
      "Epoch 33, Batch 226/462, Loss: 0.6261790990829468\n",
      "Epoch 33, Batch 227/462, Loss: 0.5566743612289429\n",
      "Epoch 33, Batch 228/462, Loss: 0.8604565262794495\n",
      "Epoch 33, Batch 229/462, Loss: 0.6372341513633728\n",
      "Epoch 33, Batch 230/462, Loss: 0.8591247200965881\n",
      "Epoch 33, Batch 231/462, Loss: 0.6017878651618958\n",
      "Epoch 33, Batch 232/462, Loss: 0.5963677167892456\n",
      "Epoch 33, Batch 233/462, Loss: 0.8560174703598022\n",
      "Epoch 33, Batch 234/462, Loss: 0.8365158438682556\n",
      "Epoch 33, Batch 235/462, Loss: 0.6339583396911621\n",
      "Epoch 33, Batch 236/462, Loss: 0.6957701444625854\n",
      "Epoch 33, Batch 237/462, Loss: 0.6136271357536316\n",
      "Epoch 33, Batch 238/462, Loss: 0.7459258437156677\n",
      "Epoch 33, Batch 239/462, Loss: 0.6913176774978638\n",
      "Epoch 33, Batch 240/462, Loss: 0.8195878267288208\n",
      "Epoch 33, Batch 241/462, Loss: 0.6897027492523193\n",
      "Epoch 33, Batch 242/462, Loss: 0.5520917773246765\n",
      "Epoch 33, Batch 243/462, Loss: 0.6693085432052612\n",
      "Epoch 33, Batch 244/462, Loss: 0.6613786816596985\n",
      "Epoch 33, Batch 245/462, Loss: 0.7985692024230957\n",
      "Epoch 33, Batch 246/462, Loss: 0.6555439233779907\n",
      "Epoch 33, Batch 247/462, Loss: 0.7074385285377502\n",
      "Epoch 33, Batch 248/462, Loss: 0.6871705651283264\n",
      "Epoch 33, Batch 249/462, Loss: 0.6882903575897217\n",
      "Epoch 33, Batch 250/462, Loss: 0.5546720027923584\n",
      "Epoch 33, Batch 251/462, Loss: 0.6870183944702148\n",
      "Epoch 33, Batch 252/462, Loss: 0.7667786478996277\n",
      "Epoch 33, Batch 253/462, Loss: 0.7920563817024231\n",
      "Epoch 33, Batch 254/462, Loss: 0.7044588327407837\n",
      "Epoch 33, Batch 255/462, Loss: 0.7688976526260376\n",
      "Epoch 33, Batch 256/462, Loss: 0.8213778138160706\n",
      "Epoch 33, Batch 257/462, Loss: 0.7167394161224365\n",
      "Epoch 33, Batch 258/462, Loss: 0.6878620386123657\n",
      "Epoch 33, Batch 259/462, Loss: 0.7783761620521545\n",
      "Epoch 33, Batch 260/462, Loss: 0.7634673714637756\n",
      "Epoch 33, Batch 261/462, Loss: 0.5761928558349609\n",
      "Epoch 33, Batch 262/462, Loss: 0.7246201038360596\n",
      "Epoch 33, Batch 263/462, Loss: 0.6320793032646179\n",
      "Epoch 33, Batch 264/462, Loss: 0.7245160341262817\n",
      "Epoch 33, Batch 265/462, Loss: 0.6783425211906433\n",
      "Epoch 33, Batch 266/462, Loss: 0.9618412852287292\n",
      "Epoch 33, Batch 267/462, Loss: 0.8750405311584473\n",
      "Epoch 33, Batch 268/462, Loss: 0.7667087912559509\n",
      "Epoch 33, Batch 269/462, Loss: 0.6809008717536926\n",
      "Epoch 33, Batch 270/462, Loss: 0.8261628150939941\n",
      "Epoch 33, Batch 271/462, Loss: 0.7332040667533875\n",
      "Epoch 33, Batch 272/462, Loss: 0.8544130921363831\n",
      "Epoch 33, Batch 273/462, Loss: 0.8414516448974609\n",
      "Epoch 33, Batch 274/462, Loss: 0.529412031173706\n",
      "Epoch 33, Batch 275/462, Loss: 0.6627932190895081\n",
      "Epoch 33, Batch 276/462, Loss: 0.6725972890853882\n",
      "Epoch 33, Batch 277/462, Loss: 0.8317426443099976\n",
      "Epoch 33, Batch 278/462, Loss: 0.5945543646812439\n",
      "Epoch 33, Batch 279/462, Loss: 0.7313985228538513\n",
      "Epoch 33, Batch 280/462, Loss: 0.6665929555892944\n",
      "Epoch 33, Batch 281/462, Loss: 0.7987986207008362\n",
      "Epoch 33, Batch 282/462, Loss: 0.7250155210494995\n",
      "Epoch 33, Batch 283/462, Loss: 0.5660505890846252\n",
      "Epoch 33, Batch 284/462, Loss: 0.7746748924255371\n",
      "Epoch 33, Batch 285/462, Loss: 0.720626711845398\n",
      "Epoch 33, Batch 286/462, Loss: 0.7153642177581787\n",
      "Epoch 33, Batch 287/462, Loss: 0.7003605365753174\n",
      "Epoch 33, Batch 288/462, Loss: 0.8870656490325928\n",
      "Epoch 33, Batch 289/462, Loss: 0.5857542753219604\n",
      "Epoch 33, Batch 290/462, Loss: 0.5451157093048096\n",
      "Epoch 33, Batch 291/462, Loss: 0.515242874622345\n",
      "Epoch 33, Batch 292/462, Loss: 0.6734715104103088\n",
      "Epoch 33, Batch 293/462, Loss: 0.6390709280967712\n",
      "Epoch 33, Batch 294/462, Loss: 0.7002673149108887\n",
      "Epoch 33, Batch 295/462, Loss: 0.6447761654853821\n",
      "Epoch 33, Batch 296/462, Loss: 0.8203557729721069\n",
      "Epoch 33, Batch 297/462, Loss: 0.7711430191993713\n",
      "Epoch 33, Batch 298/462, Loss: 0.7265003323554993\n",
      "Epoch 33, Batch 299/462, Loss: 0.8413565754890442\n",
      "Epoch 33, Batch 300/462, Loss: 0.6751483678817749\n",
      "Epoch 33, Batch 301/462, Loss: 0.573476254940033\n",
      "Epoch 33, Batch 302/462, Loss: 0.5595038533210754\n",
      "Epoch 33, Batch 303/462, Loss: 0.5270959138870239\n",
      "Epoch 33, Batch 304/462, Loss: 0.6703828573226929\n",
      "Epoch 33, Batch 305/462, Loss: 0.8492788672447205\n",
      "Epoch 33, Batch 306/462, Loss: 0.6958175897598267\n",
      "Epoch 33, Batch 307/462, Loss: 0.7122189402580261\n",
      "Epoch 33, Batch 308/462, Loss: 0.6293849349021912\n",
      "Epoch 33, Batch 309/462, Loss: 0.73015958070755\n",
      "Epoch 33, Batch 310/462, Loss: 0.8401426672935486\n",
      "Epoch 33, Batch 311/462, Loss: 0.6874380707740784\n",
      "Epoch 33, Batch 312/462, Loss: 0.6323668360710144\n",
      "Epoch 33, Batch 313/462, Loss: 0.6626019477844238\n",
      "Epoch 33, Batch 314/462, Loss: 0.8340452313423157\n",
      "Epoch 33, Batch 315/462, Loss: 0.618151843547821\n",
      "Epoch 33, Batch 316/462, Loss: 0.65827876329422\n",
      "Epoch 33, Batch 317/462, Loss: 0.8339855074882507\n",
      "Epoch 33, Batch 318/462, Loss: 0.8051543831825256\n",
      "Epoch 33, Batch 319/462, Loss: 0.8696200847625732\n",
      "Epoch 33, Batch 320/462, Loss: 0.6867086887359619\n",
      "Epoch 33, Batch 321/462, Loss: 0.7669898271560669\n",
      "Epoch 33, Batch 322/462, Loss: 0.9287052154541016\n",
      "Epoch 33, Batch 323/462, Loss: 0.9839809536933899\n",
      "Epoch 33, Batch 324/462, Loss: 0.6164793372154236\n",
      "Epoch 33, Batch 325/462, Loss: 0.8213639855384827\n",
      "Epoch 33, Batch 326/462, Loss: 0.6303744912147522\n",
      "Epoch 33, Batch 327/462, Loss: 0.8009809255599976\n",
      "Epoch 33, Batch 328/462, Loss: 0.6690435409545898\n",
      "Epoch 33, Batch 329/462, Loss: 0.7340074777603149\n",
      "Epoch 33, Batch 330/462, Loss: 0.8444321751594543\n",
      "Epoch 33, Batch 331/462, Loss: 0.8116427659988403\n",
      "Epoch 33, Batch 332/462, Loss: 0.637474775314331\n",
      "Epoch 33, Batch 333/462, Loss: 0.6281589269638062\n",
      "Epoch 33, Batch 334/462, Loss: 0.6813682317733765\n",
      "Epoch 33, Batch 335/462, Loss: 0.7000135779380798\n",
      "Epoch 33, Batch 336/462, Loss: 0.7270131707191467\n",
      "Epoch 33, Batch 337/462, Loss: 0.7619637846946716\n",
      "Epoch 33, Batch 338/462, Loss: 0.648124098777771\n",
      "Epoch 33, Batch 339/462, Loss: 0.6912844181060791\n",
      "Epoch 33, Batch 340/462, Loss: 0.6589767932891846\n",
      "Epoch 33, Batch 341/462, Loss: 0.8052992224693298\n",
      "Epoch 33, Batch 342/462, Loss: 0.8196955919265747\n",
      "Epoch 33, Batch 343/462, Loss: 0.7865965366363525\n",
      "Epoch 33, Batch 344/462, Loss: 0.5362746715545654\n",
      "Epoch 33, Batch 345/462, Loss: 0.6817479729652405\n",
      "Epoch 33, Batch 346/462, Loss: 0.660546064376831\n",
      "Epoch 33, Batch 347/462, Loss: 0.5196447372436523\n",
      "Epoch 33, Batch 348/462, Loss: 0.7173662185668945\n",
      "Epoch 33, Batch 349/462, Loss: 0.6633737683296204\n",
      "Epoch 33, Batch 350/462, Loss: 0.7216442227363586\n",
      "Epoch 33, Batch 351/462, Loss: 0.8155919909477234\n",
      "Epoch 33, Batch 352/462, Loss: 0.9761889576911926\n",
      "Epoch 33, Batch 353/462, Loss: 0.8081176280975342\n",
      "Epoch 33, Batch 354/462, Loss: 0.7439926862716675\n",
      "Epoch 33, Batch 355/462, Loss: 0.6904348134994507\n",
      "Epoch 33, Batch 356/462, Loss: 0.5913689732551575\n",
      "Epoch 33, Batch 357/462, Loss: 0.6785291433334351\n",
      "Epoch 33, Batch 358/462, Loss: 0.8716985583305359\n",
      "Epoch 33, Batch 359/462, Loss: 0.7949482202529907\n",
      "Epoch 33, Batch 360/462, Loss: 0.6533337831497192\n",
      "Epoch 33, Batch 361/462, Loss: 0.6854389905929565\n",
      "Epoch 33, Batch 362/462, Loss: 0.6961038708686829\n",
      "Epoch 33, Batch 363/462, Loss: 0.8020156621932983\n",
      "Epoch 33, Batch 364/462, Loss: 0.8690485954284668\n",
      "Epoch 33, Batch 365/462, Loss: 0.7484442591667175\n",
      "Epoch 33, Batch 366/462, Loss: 0.5880356431007385\n",
      "Epoch 33, Batch 367/462, Loss: 0.7030154466629028\n",
      "Epoch 33, Batch 368/462, Loss: 0.7260572910308838\n",
      "Epoch 33, Batch 369/462, Loss: 0.6999714970588684\n",
      "Epoch 33, Batch 370/462, Loss: 0.6828367114067078\n",
      "Epoch 33, Batch 371/462, Loss: 0.5803287625312805\n",
      "Epoch 33, Batch 372/462, Loss: 0.857314944267273\n",
      "Epoch 33, Batch 373/462, Loss: 0.7979893088340759\n",
      "Epoch 33, Batch 374/462, Loss: 0.6860637664794922\n",
      "Epoch 33, Batch 375/462, Loss: 0.7506759166717529\n",
      "Epoch 33, Batch 376/462, Loss: 0.6510905027389526\n",
      "Epoch 33, Batch 377/462, Loss: 0.7095712423324585\n",
      "Epoch 33, Batch 378/462, Loss: 0.6862083673477173\n",
      "Epoch 33, Batch 379/462, Loss: 0.7412098050117493\n",
      "Epoch 33, Batch 380/462, Loss: 0.8297391533851624\n",
      "Epoch 33, Batch 381/462, Loss: 0.6519091129302979\n",
      "Epoch 33, Batch 382/462, Loss: 0.8343532681465149\n",
      "Epoch 33, Batch 383/462, Loss: 0.8381595015525818\n",
      "Epoch 33, Batch 384/462, Loss: 0.6365909576416016\n",
      "Epoch 33, Batch 385/462, Loss: 0.6792831420898438\n",
      "Epoch 33, Batch 386/462, Loss: 0.8057355880737305\n",
      "Epoch 33, Batch 387/462, Loss: 0.6194616556167603\n",
      "Epoch 33, Batch 388/462, Loss: 0.696146547794342\n",
      "Epoch 33, Batch 389/462, Loss: 0.7259467244148254\n",
      "Epoch 33, Batch 390/462, Loss: 0.7409745454788208\n",
      "Epoch 33, Batch 391/462, Loss: 0.6360434293746948\n",
      "Epoch 33, Batch 392/462, Loss: 0.6642464995384216\n",
      "Epoch 33, Batch 393/462, Loss: 0.6197085976600647\n",
      "Epoch 33, Batch 394/462, Loss: 0.6601258516311646\n",
      "Epoch 33, Batch 395/462, Loss: 0.6149994134902954\n",
      "Epoch 33, Batch 396/462, Loss: 0.7868872284889221\n",
      "Epoch 33, Batch 397/462, Loss: 0.8065223097801208\n",
      "Epoch 33, Batch 398/462, Loss: 0.6914787292480469\n",
      "Epoch 33, Batch 399/462, Loss: 0.7576117515563965\n",
      "Epoch 33, Batch 400/462, Loss: 0.7197967767715454\n",
      "Epoch 33, Batch 401/462, Loss: 0.7887348532676697\n",
      "Epoch 33, Batch 402/462, Loss: 0.6950916647911072\n",
      "Epoch 33, Batch 403/462, Loss: 0.8568541407585144\n",
      "Epoch 33, Batch 404/462, Loss: 0.5325932502746582\n",
      "Epoch 33, Batch 405/462, Loss: 0.6643097400665283\n",
      "Epoch 33, Batch 406/462, Loss: 0.48384514451026917\n",
      "Epoch 33, Batch 407/462, Loss: 0.6477999687194824\n",
      "Epoch 33, Batch 408/462, Loss: 0.759791910648346\n",
      "Epoch 33, Batch 409/462, Loss: 0.7689746022224426\n",
      "Epoch 33, Batch 410/462, Loss: 0.7215623259544373\n",
      "Epoch 33, Batch 411/462, Loss: 0.7188438177108765\n",
      "Epoch 33, Batch 412/462, Loss: 0.6789730191230774\n",
      "Epoch 33, Batch 413/462, Loss: 0.7755520343780518\n",
      "Epoch 33, Batch 414/462, Loss: 0.7455471754074097\n",
      "Epoch 33, Batch 415/462, Loss: 0.6777597665786743\n",
      "Epoch 33, Batch 416/462, Loss: 0.8175660967826843\n",
      "Epoch 33, Batch 417/462, Loss: 0.7072517275810242\n",
      "Epoch 33, Batch 418/462, Loss: 0.5957531332969666\n",
      "Epoch 33, Batch 419/462, Loss: 0.6867392063140869\n",
      "Epoch 33, Batch 420/462, Loss: 0.7110159397125244\n",
      "Epoch 33, Batch 421/462, Loss: 0.7722436189651489\n",
      "Epoch 33, Batch 422/462, Loss: 0.7016086578369141\n",
      "Epoch 33, Batch 423/462, Loss: 0.46317100524902344\n",
      "Epoch 33, Batch 424/462, Loss: 0.6982581615447998\n",
      "Epoch 33, Batch 425/462, Loss: 0.7784775495529175\n",
      "Epoch 33, Batch 426/462, Loss: 0.6975452899932861\n",
      "Epoch 33, Batch 427/462, Loss: 0.7363290786743164\n",
      "Epoch 33, Batch 428/462, Loss: 0.7217933535575867\n",
      "Epoch 33, Batch 429/462, Loss: 0.7611073851585388\n",
      "Epoch 33, Batch 430/462, Loss: 0.6292485594749451\n",
      "Epoch 33, Batch 431/462, Loss: 0.7656477689743042\n",
      "Epoch 33, Batch 432/462, Loss: 0.5683927536010742\n",
      "Epoch 33, Batch 433/462, Loss: 0.7112133502960205\n",
      "Epoch 33, Batch 434/462, Loss: 0.6188253164291382\n",
      "Epoch 33, Batch 435/462, Loss: 0.8267671465873718\n",
      "Epoch 33, Batch 436/462, Loss: 0.6775178909301758\n",
      "Epoch 33, Batch 437/462, Loss: 0.6964200735092163\n",
      "Epoch 33, Batch 438/462, Loss: 0.4645349979400635\n",
      "Epoch 33, Batch 439/462, Loss: 0.8326376676559448\n",
      "Epoch 33, Batch 440/462, Loss: 0.6334245800971985\n",
      "Epoch 33, Batch 441/462, Loss: 0.47190454602241516\n",
      "Epoch 33, Batch 442/462, Loss: 0.8820968866348267\n",
      "Epoch 33, Batch 443/462, Loss: 0.6995035409927368\n",
      "Epoch 33, Batch 444/462, Loss: 0.613240659236908\n",
      "Epoch 33, Batch 445/462, Loss: 0.747736930847168\n",
      "Epoch 33, Batch 446/462, Loss: 0.8362695574760437\n",
      "Epoch 33, Batch 447/462, Loss: 0.7066317200660706\n",
      "Epoch 33, Batch 448/462, Loss: 0.6252980828285217\n",
      "Epoch 33, Batch 449/462, Loss: 0.4399552345275879\n",
      "Epoch 33, Batch 450/462, Loss: 0.7113799452781677\n",
      "Epoch 33, Batch 451/462, Loss: 0.7490132451057434\n",
      "Epoch 33, Batch 452/462, Loss: 0.6404068470001221\n",
      "Epoch 33, Batch 453/462, Loss: 0.7612734436988831\n",
      "Epoch 33, Batch 454/462, Loss: 0.9352723360061646\n",
      "Epoch 33, Batch 455/462, Loss: 0.8971221446990967\n",
      "Epoch 33, Batch 456/462, Loss: 0.73377525806427\n",
      "Epoch 33, Batch 457/462, Loss: 0.8365988731384277\n",
      "Epoch 33, Batch 458/462, Loss: 0.5960456728935242\n",
      "Epoch 33, Batch 459/462, Loss: 0.5471621751785278\n",
      "Epoch 33, Batch 460/462, Loss: 0.6153702735900879\n",
      "Epoch 33, Batch 461/462, Loss: 0.687547504901886\n",
      "Epoch 33, Batch 462/462, Loss: 0.8715199828147888\n",
      "Epoch 33, Loss: 332.8038266301155\n",
      "Epoch 34, Batch 1/462, Loss: 0.687139093875885\n",
      "Epoch 34, Batch 2/462, Loss: 0.6940797567367554\n",
      "Epoch 34, Batch 3/462, Loss: 0.6320905685424805\n",
      "Epoch 34, Batch 4/462, Loss: 0.7763901352882385\n",
      "Epoch 34, Batch 5/462, Loss: 0.6565740704536438\n",
      "Epoch 34, Batch 6/462, Loss: 0.9243161678314209\n",
      "Epoch 34, Batch 7/462, Loss: 0.6460629105567932\n",
      "Epoch 34, Batch 8/462, Loss: 0.6233275532722473\n",
      "Epoch 34, Batch 9/462, Loss: 0.562976598739624\n",
      "Epoch 34, Batch 10/462, Loss: 0.6775109171867371\n",
      "Epoch 34, Batch 11/462, Loss: 0.6041709184646606\n",
      "Epoch 34, Batch 12/462, Loss: 0.6208005547523499\n",
      "Epoch 34, Batch 13/462, Loss: 0.6942092776298523\n",
      "Epoch 34, Batch 14/462, Loss: 0.6669974327087402\n",
      "Epoch 34, Batch 15/462, Loss: 0.818453311920166\n",
      "Epoch 34, Batch 16/462, Loss: 0.66246497631073\n",
      "Epoch 34, Batch 17/462, Loss: 0.6096445918083191\n",
      "Epoch 34, Batch 18/462, Loss: 0.6629911661148071\n",
      "Epoch 34, Batch 19/462, Loss: 0.8025985360145569\n",
      "Epoch 34, Batch 20/462, Loss: 0.8302175998687744\n",
      "Epoch 34, Batch 21/462, Loss: 0.7599097490310669\n",
      "Epoch 34, Batch 22/462, Loss: 0.8472068905830383\n",
      "Epoch 34, Batch 23/462, Loss: 0.7609328031539917\n",
      "Epoch 34, Batch 24/462, Loss: 0.5687190294265747\n",
      "Epoch 34, Batch 25/462, Loss: 0.6152111291885376\n",
      "Epoch 34, Batch 26/462, Loss: 0.6248277425765991\n",
      "Epoch 34, Batch 27/462, Loss: 0.751165509223938\n",
      "Epoch 34, Batch 28/462, Loss: 0.6262632012367249\n",
      "Epoch 34, Batch 29/462, Loss: 0.6406036019325256\n",
      "Epoch 34, Batch 30/462, Loss: 0.6911592483520508\n",
      "Epoch 34, Batch 31/462, Loss: 0.6569905281066895\n",
      "Epoch 34, Batch 32/462, Loss: 0.872442901134491\n",
      "Epoch 34, Batch 33/462, Loss: 0.6603728532791138\n",
      "Epoch 34, Batch 34/462, Loss: 0.7986096739768982\n",
      "Epoch 34, Batch 35/462, Loss: 0.8724997043609619\n",
      "Epoch 34, Batch 36/462, Loss: 0.6198248863220215\n",
      "Epoch 34, Batch 37/462, Loss: 0.7531580328941345\n",
      "Epoch 34, Batch 38/462, Loss: 0.7304727435112\n",
      "Epoch 34, Batch 39/462, Loss: 0.7825308442115784\n",
      "Epoch 34, Batch 40/462, Loss: 0.849814772605896\n",
      "Epoch 34, Batch 41/462, Loss: 0.6357853412628174\n",
      "Epoch 34, Batch 42/462, Loss: 0.850212037563324\n",
      "Epoch 34, Batch 43/462, Loss: 0.9859239459037781\n",
      "Epoch 34, Batch 44/462, Loss: 0.6826199889183044\n",
      "Epoch 34, Batch 45/462, Loss: 0.5949488282203674\n",
      "Epoch 34, Batch 46/462, Loss: 0.6962683200836182\n",
      "Epoch 34, Batch 47/462, Loss: 0.873500645160675\n",
      "Epoch 34, Batch 48/462, Loss: 0.8808935880661011\n",
      "Epoch 34, Batch 49/462, Loss: 0.9037312269210815\n",
      "Epoch 34, Batch 50/462, Loss: 0.6919949650764465\n",
      "Epoch 34, Batch 51/462, Loss: 0.748715341091156\n",
      "Epoch 34, Batch 52/462, Loss: 0.6614301800727844\n",
      "Epoch 34, Batch 53/462, Loss: 0.5872690081596375\n",
      "Epoch 34, Batch 54/462, Loss: 0.8438063859939575\n",
      "Epoch 34, Batch 55/462, Loss: 0.7655045986175537\n",
      "Epoch 34, Batch 56/462, Loss: 0.5685499906539917\n",
      "Epoch 34, Batch 57/462, Loss: 0.8994768261909485\n",
      "Epoch 34, Batch 58/462, Loss: 0.636774480342865\n",
      "Epoch 34, Batch 59/462, Loss: 0.7708332538604736\n",
      "Epoch 34, Batch 60/462, Loss: 0.7236963510513306\n",
      "Epoch 34, Batch 61/462, Loss: 0.7156828045845032\n",
      "Epoch 34, Batch 62/462, Loss: 0.7222278118133545\n",
      "Epoch 34, Batch 63/462, Loss: 0.8686014413833618\n",
      "Epoch 34, Batch 64/462, Loss: 0.6475887298583984\n",
      "Epoch 34, Batch 65/462, Loss: 0.7713760733604431\n",
      "Epoch 34, Batch 66/462, Loss: 0.813972532749176\n",
      "Epoch 34, Batch 67/462, Loss: 0.7514586448669434\n",
      "Epoch 34, Batch 68/462, Loss: 0.6693684458732605\n",
      "Epoch 34, Batch 69/462, Loss: 0.7958465218544006\n",
      "Epoch 34, Batch 70/462, Loss: 0.6984826326370239\n",
      "Epoch 34, Batch 71/462, Loss: 0.728629469871521\n",
      "Epoch 34, Batch 72/462, Loss: 0.8250831365585327\n",
      "Epoch 34, Batch 73/462, Loss: 0.6118264198303223\n",
      "Epoch 34, Batch 74/462, Loss: 1.2997242212295532\n",
      "Epoch 34, Batch 75/462, Loss: 0.709118664264679\n",
      "Epoch 34, Batch 76/462, Loss: 0.6339187026023865\n",
      "Epoch 34, Batch 77/462, Loss: 0.681682288646698\n",
      "Epoch 34, Batch 78/462, Loss: 0.6179593205451965\n",
      "Epoch 34, Batch 79/462, Loss: 0.9359250068664551\n",
      "Epoch 34, Batch 80/462, Loss: 0.7857950329780579\n",
      "Epoch 34, Batch 81/462, Loss: 0.8747400045394897\n",
      "Epoch 34, Batch 82/462, Loss: 0.5974445939064026\n",
      "Epoch 34, Batch 83/462, Loss: 0.6301082372665405\n",
      "Epoch 34, Batch 84/462, Loss: 0.6776285767555237\n",
      "Epoch 34, Batch 85/462, Loss: 0.6167638301849365\n",
      "Epoch 34, Batch 86/462, Loss: 0.722156822681427\n",
      "Epoch 34, Batch 87/462, Loss: 0.7841002345085144\n",
      "Epoch 34, Batch 88/462, Loss: 0.8186792135238647\n",
      "Epoch 34, Batch 89/462, Loss: 0.7195337414741516\n",
      "Epoch 34, Batch 90/462, Loss: 0.8511836528778076\n",
      "Epoch 34, Batch 91/462, Loss: 0.8157475590705872\n",
      "Epoch 34, Batch 92/462, Loss: 0.8836414217948914\n",
      "Epoch 34, Batch 93/462, Loss: 0.6384217143058777\n",
      "Epoch 34, Batch 94/462, Loss: 0.7974935173988342\n",
      "Epoch 34, Batch 95/462, Loss: 0.9392619132995605\n",
      "Epoch 34, Batch 96/462, Loss: 0.6034396886825562\n",
      "Epoch 34, Batch 97/462, Loss: 0.5473290681838989\n",
      "Epoch 34, Batch 98/462, Loss: 0.7690456509590149\n",
      "Epoch 34, Batch 99/462, Loss: 0.8041873574256897\n",
      "Epoch 34, Batch 100/462, Loss: 0.609595775604248\n",
      "Epoch 34, Batch 101/462, Loss: 0.816519558429718\n",
      "Epoch 34, Batch 102/462, Loss: 0.6739040613174438\n",
      "Epoch 34, Batch 103/462, Loss: 0.6195870637893677\n",
      "Epoch 34, Batch 104/462, Loss: 0.8832917213439941\n",
      "Epoch 34, Batch 105/462, Loss: 0.6774005889892578\n",
      "Epoch 34, Batch 106/462, Loss: 0.6820198893547058\n",
      "Epoch 34, Batch 107/462, Loss: 0.5929461717605591\n",
      "Epoch 34, Batch 108/462, Loss: 0.7030842900276184\n",
      "Epoch 34, Batch 109/462, Loss: 0.578742504119873\n",
      "Epoch 34, Batch 110/462, Loss: 0.7379221320152283\n",
      "Epoch 34, Batch 111/462, Loss: 0.8749637603759766\n",
      "Epoch 34, Batch 112/462, Loss: 0.6342039108276367\n",
      "Epoch 34, Batch 113/462, Loss: 0.7919688820838928\n",
      "Epoch 34, Batch 114/462, Loss: 0.8724581599235535\n",
      "Epoch 34, Batch 115/462, Loss: 0.7526648640632629\n",
      "Epoch 34, Batch 116/462, Loss: 0.6795884966850281\n",
      "Epoch 34, Batch 117/462, Loss: 0.6540994644165039\n",
      "Epoch 34, Batch 118/462, Loss: 0.8310741186141968\n",
      "Epoch 34, Batch 119/462, Loss: 0.6458261609077454\n",
      "Epoch 34, Batch 120/462, Loss: 0.6585168242454529\n",
      "Epoch 34, Batch 121/462, Loss: 0.7177186608314514\n",
      "Epoch 34, Batch 122/462, Loss: 0.7662928700447083\n",
      "Epoch 34, Batch 123/462, Loss: 0.7695145010948181\n",
      "Epoch 34, Batch 124/462, Loss: 0.7855759263038635\n",
      "Epoch 34, Batch 125/462, Loss: 0.8506277799606323\n",
      "Epoch 34, Batch 126/462, Loss: 0.5179119110107422\n",
      "Epoch 34, Batch 127/462, Loss: 0.7206695675849915\n",
      "Epoch 34, Batch 128/462, Loss: 0.6958204507827759\n",
      "Epoch 34, Batch 129/462, Loss: 0.7635117173194885\n",
      "Epoch 34, Batch 130/462, Loss: 0.6897997856140137\n",
      "Epoch 34, Batch 131/462, Loss: 0.7829915881156921\n",
      "Epoch 34, Batch 132/462, Loss: 0.7011616826057434\n",
      "Epoch 34, Batch 133/462, Loss: 0.5629541873931885\n",
      "Epoch 34, Batch 134/462, Loss: 0.6618137955665588\n",
      "Epoch 34, Batch 135/462, Loss: 0.7149613499641418\n",
      "Epoch 34, Batch 136/462, Loss: 0.7918220162391663\n",
      "Epoch 34, Batch 137/462, Loss: 0.8210097551345825\n",
      "Epoch 34, Batch 138/462, Loss: 0.6859919428825378\n",
      "Epoch 34, Batch 139/462, Loss: 0.6541486382484436\n",
      "Epoch 34, Batch 140/462, Loss: 0.7107468247413635\n",
      "Epoch 34, Batch 141/462, Loss: 0.675706148147583\n",
      "Epoch 34, Batch 142/462, Loss: 0.6121220588684082\n",
      "Epoch 34, Batch 143/462, Loss: 0.8903733491897583\n",
      "Epoch 34, Batch 144/462, Loss: 0.7454813122749329\n",
      "Epoch 34, Batch 145/462, Loss: 0.981378436088562\n",
      "Epoch 34, Batch 146/462, Loss: 0.6811028122901917\n",
      "Epoch 34, Batch 147/462, Loss: 0.6869908571243286\n",
      "Epoch 34, Batch 148/462, Loss: 0.6342785358428955\n",
      "Epoch 34, Batch 149/462, Loss: 0.5786958336830139\n",
      "Epoch 34, Batch 150/462, Loss: 0.7010904550552368\n",
      "Epoch 34, Batch 151/462, Loss: 0.7486267685890198\n",
      "Epoch 34, Batch 152/462, Loss: 0.8168345093727112\n",
      "Epoch 34, Batch 153/462, Loss: 0.6896464824676514\n",
      "Epoch 34, Batch 154/462, Loss: 0.7860663533210754\n",
      "Epoch 34, Batch 155/462, Loss: 0.7290853261947632\n",
      "Epoch 34, Batch 156/462, Loss: 0.810597836971283\n",
      "Epoch 34, Batch 157/462, Loss: 0.7414805889129639\n",
      "Epoch 34, Batch 158/462, Loss: 0.8195624947547913\n",
      "Epoch 34, Batch 159/462, Loss: 0.7436644434928894\n",
      "Epoch 34, Batch 160/462, Loss: 0.6804083585739136\n",
      "Epoch 34, Batch 161/462, Loss: 0.8248944282531738\n",
      "Epoch 34, Batch 162/462, Loss: 0.5257738828659058\n",
      "Epoch 34, Batch 163/462, Loss: 0.6209281086921692\n",
      "Epoch 34, Batch 164/462, Loss: 0.6522064805030823\n",
      "Epoch 34, Batch 165/462, Loss: 0.8605228662490845\n",
      "Epoch 34, Batch 166/462, Loss: 0.8260202407836914\n",
      "Epoch 34, Batch 167/462, Loss: 0.6733841300010681\n",
      "Epoch 34, Batch 168/462, Loss: 0.5755809545516968\n",
      "Epoch 34, Batch 169/462, Loss: 0.6271665692329407\n",
      "Epoch 34, Batch 170/462, Loss: 0.7683050632476807\n",
      "Epoch 34, Batch 171/462, Loss: 0.8045675754547119\n",
      "Epoch 34, Batch 172/462, Loss: 0.662459135055542\n",
      "Epoch 34, Batch 173/462, Loss: 0.8993750810623169\n",
      "Epoch 34, Batch 174/462, Loss: 0.8439613580703735\n",
      "Epoch 34, Batch 175/462, Loss: 0.5097549557685852\n",
      "Epoch 34, Batch 176/462, Loss: 0.8429341316223145\n",
      "Epoch 34, Batch 177/462, Loss: 0.6616759896278381\n",
      "Epoch 34, Batch 178/462, Loss: 0.6149763464927673\n",
      "Epoch 34, Batch 179/462, Loss: 0.5507038235664368\n",
      "Epoch 34, Batch 180/462, Loss: 0.8021073937416077\n",
      "Epoch 34, Batch 181/462, Loss: 0.6881815195083618\n",
      "Epoch 34, Batch 182/462, Loss: 0.5309621095657349\n",
      "Epoch 34, Batch 183/462, Loss: 0.6248870491981506\n",
      "Epoch 34, Batch 184/462, Loss: 0.49947232007980347\n",
      "Epoch 34, Batch 185/462, Loss: 0.6630333065986633\n",
      "Epoch 34, Batch 186/462, Loss: 0.6490290760993958\n",
      "Epoch 34, Batch 187/462, Loss: 0.7750341892242432\n",
      "Epoch 34, Batch 188/462, Loss: 0.7498387098312378\n",
      "Epoch 34, Batch 189/462, Loss: 0.7760995030403137\n",
      "Epoch 34, Batch 190/462, Loss: 0.6659232378005981\n",
      "Epoch 34, Batch 191/462, Loss: 0.7319477200508118\n",
      "Epoch 34, Batch 192/462, Loss: 0.5998693704605103\n",
      "Epoch 34, Batch 193/462, Loss: 0.7962087988853455\n",
      "Epoch 34, Batch 194/462, Loss: 0.6929039359092712\n",
      "Epoch 34, Batch 195/462, Loss: 0.7258037328720093\n",
      "Epoch 34, Batch 196/462, Loss: 0.7053362727165222\n",
      "Epoch 34, Batch 197/462, Loss: 0.663087010383606\n",
      "Epoch 34, Batch 198/462, Loss: 0.8419178128242493\n",
      "Epoch 34, Batch 199/462, Loss: 0.6537677049636841\n",
      "Epoch 34, Batch 200/462, Loss: 0.7864829897880554\n",
      "Epoch 34, Batch 201/462, Loss: 0.7393065094947815\n",
      "Epoch 34, Batch 202/462, Loss: 0.9202747941017151\n",
      "Epoch 34, Batch 203/462, Loss: 0.6676998734474182\n",
      "Epoch 34, Batch 204/462, Loss: 0.7371777296066284\n",
      "Epoch 34, Batch 205/462, Loss: 0.9120557904243469\n",
      "Epoch 34, Batch 206/462, Loss: 0.8350184559822083\n",
      "Epoch 34, Batch 207/462, Loss: 0.7818180918693542\n",
      "Epoch 34, Batch 208/462, Loss: 0.7547461986541748\n",
      "Epoch 34, Batch 209/462, Loss: 0.589989960193634\n",
      "Epoch 34, Batch 210/462, Loss: 0.667456328868866\n",
      "Epoch 34, Batch 211/462, Loss: 0.6557669043540955\n",
      "Epoch 34, Batch 212/462, Loss: 0.6648455262184143\n",
      "Epoch 34, Batch 213/462, Loss: 0.6046527624130249\n",
      "Epoch 34, Batch 214/462, Loss: 0.6155164241790771\n",
      "Epoch 34, Batch 215/462, Loss: 0.5979904532432556\n",
      "Epoch 34, Batch 216/462, Loss: 0.6686080694198608\n",
      "Epoch 34, Batch 217/462, Loss: 0.5022420287132263\n",
      "Epoch 34, Batch 218/462, Loss: 0.8103159666061401\n",
      "Epoch 34, Batch 219/462, Loss: 0.6417834162712097\n",
      "Epoch 34, Batch 220/462, Loss: 0.6800171136856079\n",
      "Epoch 34, Batch 221/462, Loss: 0.6041707396507263\n",
      "Epoch 34, Batch 222/462, Loss: 0.9007037281990051\n",
      "Epoch 34, Batch 223/462, Loss: 0.8477460741996765\n",
      "Epoch 34, Batch 224/462, Loss: 0.7061457633972168\n",
      "Epoch 34, Batch 225/462, Loss: 0.6527422666549683\n",
      "Epoch 34, Batch 226/462, Loss: 0.6398577094078064\n",
      "Epoch 34, Batch 227/462, Loss: 0.5986168384552002\n",
      "Epoch 34, Batch 228/462, Loss: 0.7882483005523682\n",
      "Epoch 34, Batch 229/462, Loss: 0.8584461808204651\n",
      "Epoch 34, Batch 230/462, Loss: 0.7433124780654907\n",
      "Epoch 34, Batch 231/462, Loss: 0.8425620794296265\n",
      "Epoch 34, Batch 232/462, Loss: 0.8526549935340881\n",
      "Epoch 34, Batch 233/462, Loss: 0.6920297741889954\n",
      "Epoch 34, Batch 234/462, Loss: 0.6377942562103271\n",
      "Epoch 34, Batch 235/462, Loss: 0.6458714604377747\n",
      "Epoch 34, Batch 236/462, Loss: 0.781150758266449\n",
      "Epoch 34, Batch 237/462, Loss: 0.5324111580848694\n",
      "Epoch 34, Batch 238/462, Loss: 0.8057623505592346\n",
      "Epoch 34, Batch 239/462, Loss: 1.0017632246017456\n",
      "Epoch 34, Batch 240/462, Loss: 0.7217569351196289\n",
      "Epoch 34, Batch 241/462, Loss: 0.7283400893211365\n",
      "Epoch 34, Batch 242/462, Loss: 0.7810754776000977\n",
      "Epoch 34, Batch 243/462, Loss: 0.5787103176116943\n",
      "Epoch 34, Batch 244/462, Loss: 0.7583415508270264\n",
      "Epoch 34, Batch 245/462, Loss: 0.663372278213501\n",
      "Epoch 34, Batch 246/462, Loss: 0.7726932764053345\n",
      "Epoch 34, Batch 247/462, Loss: 0.7207143306732178\n",
      "Epoch 34, Batch 248/462, Loss: 0.8457719683647156\n",
      "Epoch 34, Batch 249/462, Loss: 0.6924129128456116\n",
      "Epoch 34, Batch 250/462, Loss: 0.74144047498703\n",
      "Epoch 34, Batch 251/462, Loss: 0.6995262503623962\n",
      "Epoch 34, Batch 252/462, Loss: 0.671085000038147\n",
      "Epoch 34, Batch 253/462, Loss: 0.8816871643066406\n",
      "Epoch 34, Batch 254/462, Loss: 0.8058916926383972\n",
      "Epoch 34, Batch 255/462, Loss: 0.7643324136734009\n",
      "Epoch 34, Batch 256/462, Loss: 0.8696185946464539\n",
      "Epoch 34, Batch 257/462, Loss: 0.6814630627632141\n",
      "Epoch 34, Batch 258/462, Loss: 0.7286591529846191\n",
      "Epoch 34, Batch 259/462, Loss: 0.6818552613258362\n",
      "Epoch 34, Batch 260/462, Loss: 0.6779335737228394\n",
      "Epoch 34, Batch 261/462, Loss: 0.8265124559402466\n",
      "Epoch 34, Batch 262/462, Loss: 0.7639289498329163\n",
      "Epoch 34, Batch 263/462, Loss: 0.8053919076919556\n",
      "Epoch 34, Batch 264/462, Loss: 0.6148005723953247\n",
      "Epoch 34, Batch 265/462, Loss: 0.6367560029029846\n",
      "Epoch 34, Batch 266/462, Loss: 0.6332942247390747\n",
      "Epoch 34, Batch 267/462, Loss: 0.8511236906051636\n",
      "Epoch 34, Batch 268/462, Loss: 0.6188134551048279\n",
      "Epoch 34, Batch 269/462, Loss: 0.853256106376648\n",
      "Epoch 34, Batch 270/462, Loss: 0.6917390823364258\n",
      "Epoch 34, Batch 271/462, Loss: 0.5843178033828735\n",
      "Epoch 34, Batch 272/462, Loss: 0.8533750772476196\n",
      "Epoch 34, Batch 273/462, Loss: 0.6455637216567993\n",
      "Epoch 34, Batch 274/462, Loss: 0.6487694978713989\n",
      "Epoch 34, Batch 275/462, Loss: 0.6656577587127686\n",
      "Epoch 34, Batch 276/462, Loss: 0.7507386803627014\n",
      "Epoch 34, Batch 277/462, Loss: 0.8403096199035645\n",
      "Epoch 34, Batch 278/462, Loss: 0.5854253768920898\n",
      "Epoch 34, Batch 279/462, Loss: 0.7453749179840088\n",
      "Epoch 34, Batch 280/462, Loss: 0.8446710109710693\n",
      "Epoch 34, Batch 281/462, Loss: 0.6288318037986755\n",
      "Epoch 34, Batch 282/462, Loss: 0.7294728755950928\n",
      "Epoch 34, Batch 283/462, Loss: 0.9187823534011841\n",
      "Epoch 34, Batch 284/462, Loss: 0.7101069688796997\n",
      "Epoch 34, Batch 285/462, Loss: 0.6144620776176453\n",
      "Epoch 34, Batch 286/462, Loss: 0.6561486124992371\n",
      "Epoch 34, Batch 287/462, Loss: 0.5577714443206787\n",
      "Epoch 34, Batch 288/462, Loss: 0.7703973650932312\n",
      "Epoch 34, Batch 289/462, Loss: 0.6063321232795715\n",
      "Epoch 34, Batch 290/462, Loss: 0.7542807459831238\n",
      "Epoch 34, Batch 291/462, Loss: 0.7087010741233826\n",
      "Epoch 34, Batch 292/462, Loss: 0.8706523776054382\n",
      "Epoch 34, Batch 293/462, Loss: 0.5499507188796997\n",
      "Epoch 34, Batch 294/462, Loss: 0.5884657502174377\n",
      "Epoch 34, Batch 295/462, Loss: 0.6152794361114502\n",
      "Epoch 34, Batch 296/462, Loss: 0.9198729395866394\n",
      "Epoch 34, Batch 297/462, Loss: 0.5757257342338562\n",
      "Epoch 34, Batch 298/462, Loss: 0.7137271165847778\n",
      "Epoch 34, Batch 299/462, Loss: 0.7702770233154297\n",
      "Epoch 34, Batch 300/462, Loss: 0.7347292900085449\n",
      "Epoch 34, Batch 301/462, Loss: 0.6638261675834656\n",
      "Epoch 34, Batch 302/462, Loss: 0.8365841507911682\n",
      "Epoch 34, Batch 303/462, Loss: 0.7707599997520447\n",
      "Epoch 34, Batch 304/462, Loss: 0.6300700902938843\n",
      "Epoch 34, Batch 305/462, Loss: 0.7021552324295044\n",
      "Epoch 34, Batch 306/462, Loss: 0.9440780282020569\n",
      "Epoch 34, Batch 307/462, Loss: 0.5713239312171936\n",
      "Epoch 34, Batch 308/462, Loss: 0.5652746558189392\n",
      "Epoch 34, Batch 309/462, Loss: 0.7860466837882996\n",
      "Epoch 34, Batch 310/462, Loss: 0.5363195538520813\n",
      "Epoch 34, Batch 311/462, Loss: 0.6468737125396729\n",
      "Epoch 34, Batch 312/462, Loss: 0.7418765425682068\n",
      "Epoch 34, Batch 313/462, Loss: 0.7290767431259155\n",
      "Epoch 34, Batch 314/462, Loss: 0.7834540009498596\n",
      "Epoch 34, Batch 315/462, Loss: 0.6423165202140808\n",
      "Epoch 34, Batch 316/462, Loss: 0.6241399049758911\n",
      "Epoch 34, Batch 317/462, Loss: 0.7292996048927307\n",
      "Epoch 34, Batch 318/462, Loss: 0.699161171913147\n",
      "Epoch 34, Batch 319/462, Loss: 0.9322941303253174\n",
      "Epoch 34, Batch 320/462, Loss: 0.8103693127632141\n",
      "Epoch 34, Batch 321/462, Loss: 0.8959450721740723\n",
      "Epoch 34, Batch 322/462, Loss: 0.8675654530525208\n",
      "Epoch 34, Batch 323/462, Loss: 0.7111902832984924\n",
      "Epoch 34, Batch 324/462, Loss: 0.7309300303459167\n",
      "Epoch 34, Batch 325/462, Loss: 0.7682504057884216\n",
      "Epoch 34, Batch 326/462, Loss: 0.6960228681564331\n",
      "Epoch 34, Batch 327/462, Loss: 0.7998295426368713\n",
      "Epoch 34, Batch 328/462, Loss: 0.6798403859138489\n",
      "Epoch 34, Batch 329/462, Loss: 0.6712542176246643\n",
      "Epoch 34, Batch 330/462, Loss: 0.660302460193634\n",
      "Epoch 34, Batch 331/462, Loss: 0.6823330521583557\n",
      "Epoch 34, Batch 332/462, Loss: 0.6369823813438416\n",
      "Epoch 34, Batch 333/462, Loss: 0.7989530563354492\n",
      "Epoch 34, Batch 334/462, Loss: 0.8143137693405151\n",
      "Epoch 34, Batch 335/462, Loss: 0.9319521188735962\n",
      "Epoch 34, Batch 336/462, Loss: 0.6491749882698059\n",
      "Epoch 34, Batch 337/462, Loss: 0.8237419724464417\n",
      "Epoch 34, Batch 338/462, Loss: 0.6330211162567139\n",
      "Epoch 34, Batch 339/462, Loss: 0.7248473763465881\n",
      "Epoch 34, Batch 340/462, Loss: 0.7923239469528198\n",
      "Epoch 34, Batch 341/462, Loss: 0.6439104080200195\n",
      "Epoch 34, Batch 342/462, Loss: 0.534263014793396\n",
      "Epoch 34, Batch 343/462, Loss: 0.6406018733978271\n",
      "Epoch 34, Batch 344/462, Loss: 0.8158698081970215\n",
      "Epoch 34, Batch 345/462, Loss: 0.7141804695129395\n",
      "Epoch 34, Batch 346/462, Loss: 0.6554641127586365\n",
      "Epoch 34, Batch 347/462, Loss: 0.6249492168426514\n",
      "Epoch 34, Batch 348/462, Loss: 0.6419486999511719\n",
      "Epoch 34, Batch 349/462, Loss: 0.7682106494903564\n",
      "Epoch 34, Batch 350/462, Loss: 0.6806322932243347\n",
      "Epoch 34, Batch 351/462, Loss: 0.7122988700866699\n",
      "Epoch 34, Batch 352/462, Loss: 0.79416424036026\n",
      "Epoch 34, Batch 353/462, Loss: 0.7724939584732056\n",
      "Epoch 34, Batch 354/462, Loss: 0.7367826700210571\n",
      "Epoch 34, Batch 355/462, Loss: 0.810904324054718\n",
      "Epoch 34, Batch 356/462, Loss: 0.6481636762619019\n",
      "Epoch 34, Batch 357/462, Loss: 0.7696183323860168\n",
      "Epoch 34, Batch 358/462, Loss: 0.7426544427871704\n",
      "Epoch 34, Batch 359/462, Loss: 0.7664164900779724\n",
      "Epoch 34, Batch 360/462, Loss: 0.7113645672798157\n",
      "Epoch 34, Batch 361/462, Loss: 0.7173460125923157\n",
      "Epoch 34, Batch 362/462, Loss: 0.702165424823761\n",
      "Epoch 34, Batch 363/462, Loss: 0.7415938973426819\n",
      "Epoch 34, Batch 364/462, Loss: 0.6636077761650085\n",
      "Epoch 34, Batch 365/462, Loss: 0.6223494410514832\n",
      "Epoch 34, Batch 366/462, Loss: 0.8397367000579834\n",
      "Epoch 34, Batch 367/462, Loss: 0.5631812810897827\n",
      "Epoch 34, Batch 368/462, Loss: 0.7572373747825623\n",
      "Epoch 34, Batch 369/462, Loss: 0.7237932682037354\n",
      "Epoch 34, Batch 370/462, Loss: 0.7617136240005493\n",
      "Epoch 34, Batch 371/462, Loss: 0.724378228187561\n",
      "Epoch 34, Batch 372/462, Loss: 0.7055060863494873\n",
      "Epoch 34, Batch 373/462, Loss: 0.5958325266838074\n",
      "Epoch 34, Batch 374/462, Loss: 0.6795875430107117\n",
      "Epoch 34, Batch 375/462, Loss: 0.9062036871910095\n",
      "Epoch 34, Batch 376/462, Loss: 0.8024516701698303\n",
      "Epoch 34, Batch 377/462, Loss: 0.691856324672699\n",
      "Epoch 34, Batch 378/462, Loss: 0.7767412662506104\n",
      "Epoch 34, Batch 379/462, Loss: 0.7706475853919983\n",
      "Epoch 34, Batch 380/462, Loss: 0.7175257205963135\n",
      "Epoch 34, Batch 381/462, Loss: 0.789454996585846\n",
      "Epoch 34, Batch 382/462, Loss: 0.5044193267822266\n",
      "Epoch 34, Batch 383/462, Loss: 0.6957404613494873\n",
      "Epoch 34, Batch 384/462, Loss: 0.5912293195724487\n",
      "Epoch 34, Batch 385/462, Loss: 0.547737717628479\n",
      "Epoch 34, Batch 386/462, Loss: 0.5458700656890869\n",
      "Epoch 34, Batch 387/462, Loss: 0.6413695812225342\n",
      "Epoch 34, Batch 388/462, Loss: 0.7211605310440063\n",
      "Epoch 34, Batch 389/462, Loss: 0.7437474727630615\n",
      "Epoch 34, Batch 390/462, Loss: 0.7393044233322144\n",
      "Epoch 34, Batch 391/462, Loss: 0.6820362210273743\n",
      "Epoch 34, Batch 392/462, Loss: 0.7059096097946167\n",
      "Epoch 34, Batch 393/462, Loss: 0.7744690775871277\n",
      "Epoch 34, Batch 394/462, Loss: 0.7564707398414612\n",
      "Epoch 34, Batch 395/462, Loss: 0.7483594417572021\n",
      "Epoch 34, Batch 396/462, Loss: 0.7085989713668823\n",
      "Epoch 34, Batch 397/462, Loss: 0.9067933559417725\n",
      "Epoch 34, Batch 398/462, Loss: 0.6796854734420776\n",
      "Epoch 34, Batch 399/462, Loss: 0.6563106775283813\n",
      "Epoch 34, Batch 400/462, Loss: 0.6702206134796143\n",
      "Epoch 34, Batch 401/462, Loss: 0.8036172389984131\n",
      "Epoch 34, Batch 402/462, Loss: 0.7127791047096252\n",
      "Epoch 34, Batch 403/462, Loss: 0.7474072575569153\n",
      "Epoch 34, Batch 404/462, Loss: 0.5633518695831299\n",
      "Epoch 34, Batch 405/462, Loss: 0.7287796139717102\n",
      "Epoch 34, Batch 406/462, Loss: 0.7580444812774658\n",
      "Epoch 34, Batch 407/462, Loss: 0.6027288436889648\n",
      "Epoch 34, Batch 408/462, Loss: 0.6740226745605469\n",
      "Epoch 34, Batch 409/462, Loss: 0.684363067150116\n",
      "Epoch 34, Batch 410/462, Loss: 0.6966124176979065\n",
      "Epoch 34, Batch 411/462, Loss: 0.7626066207885742\n",
      "Epoch 34, Batch 412/462, Loss: 0.5600071549415588\n",
      "Epoch 34, Batch 413/462, Loss: 0.7522869110107422\n",
      "Epoch 34, Batch 414/462, Loss: 0.7210352420806885\n",
      "Epoch 34, Batch 415/462, Loss: 0.7754225730895996\n",
      "Epoch 34, Batch 416/462, Loss: 0.8080592155456543\n",
      "Epoch 34, Batch 417/462, Loss: 0.8215863704681396\n",
      "Epoch 34, Batch 418/462, Loss: 0.6922250986099243\n",
      "Epoch 34, Batch 419/462, Loss: 0.6738424301147461\n",
      "Epoch 34, Batch 420/462, Loss: 0.7194996476173401\n",
      "Epoch 34, Batch 421/462, Loss: 0.6488139629364014\n",
      "Epoch 34, Batch 422/462, Loss: 0.6828889846801758\n",
      "Epoch 34, Batch 423/462, Loss: 0.7958847880363464\n",
      "Epoch 34, Batch 424/462, Loss: 0.722557008266449\n",
      "Epoch 34, Batch 425/462, Loss: 0.8496018052101135\n",
      "Epoch 34, Batch 426/462, Loss: 0.8069925904273987\n",
      "Epoch 34, Batch 427/462, Loss: 0.75881427526474\n",
      "Epoch 34, Batch 428/462, Loss: 0.7442839741706848\n",
      "Epoch 34, Batch 429/462, Loss: 0.8324580192565918\n",
      "Epoch 34, Batch 430/462, Loss: 0.7136715650558472\n",
      "Epoch 34, Batch 431/462, Loss: 0.5999488234519958\n",
      "Epoch 34, Batch 432/462, Loss: 0.7641840577125549\n",
      "Epoch 34, Batch 433/462, Loss: 0.8139774203300476\n",
      "Epoch 34, Batch 434/462, Loss: 0.6427522301673889\n",
      "Epoch 34, Batch 435/462, Loss: 0.5205541849136353\n",
      "Epoch 34, Batch 436/462, Loss: 0.8244335055351257\n",
      "Epoch 34, Batch 437/462, Loss: 0.9238075017929077\n",
      "Epoch 34, Batch 438/462, Loss: 0.720464289188385\n",
      "Epoch 34, Batch 439/462, Loss: 0.7001519799232483\n",
      "Epoch 34, Batch 440/462, Loss: 0.5793166160583496\n",
      "Epoch 34, Batch 441/462, Loss: 0.6042200922966003\n",
      "Epoch 34, Batch 442/462, Loss: 0.8449351191520691\n",
      "Epoch 34, Batch 443/462, Loss: 0.7162744998931885\n",
      "Epoch 34, Batch 444/462, Loss: 0.63172447681427\n",
      "Epoch 34, Batch 445/462, Loss: 0.6694222688674927\n",
      "Epoch 34, Batch 446/462, Loss: 0.7418084144592285\n",
      "Epoch 34, Batch 447/462, Loss: 0.7375043630599976\n",
      "Epoch 34, Batch 448/462, Loss: 0.8313421607017517\n",
      "Epoch 34, Batch 449/462, Loss: 0.6425948143005371\n",
      "Epoch 34, Batch 450/462, Loss: 0.6876737475395203\n",
      "Epoch 34, Batch 451/462, Loss: 0.7968840003013611\n",
      "Epoch 34, Batch 452/462, Loss: 0.7546727657318115\n",
      "Epoch 34, Batch 453/462, Loss: 0.9670852422714233\n",
      "Epoch 34, Batch 454/462, Loss: 0.8159921169281006\n",
      "Epoch 34, Batch 455/462, Loss: 0.8006366491317749\n",
      "Epoch 34, Batch 456/462, Loss: 0.7527734041213989\n",
      "Epoch 34, Batch 457/462, Loss: 0.7277362942695618\n",
      "Epoch 34, Batch 458/462, Loss: 0.8484276533126831\n",
      "Epoch 34, Batch 459/462, Loss: 0.6652478575706482\n",
      "Epoch 34, Batch 460/462, Loss: 0.7780543565750122\n",
      "Epoch 34, Batch 461/462, Loss: 0.6918972134590149\n",
      "Epoch 34, Batch 462/462, Loss: 0.8190184831619263\n",
      "Epoch 34, Loss: 334.06935691833496\n",
      "Epoch 35, Batch 1/462, Loss: 0.6975568532943726\n",
      "Epoch 35, Batch 2/462, Loss: 0.7964850068092346\n",
      "Epoch 35, Batch 3/462, Loss: 0.9075576066970825\n",
      "Epoch 35, Batch 4/462, Loss: 0.7864131331443787\n",
      "Epoch 35, Batch 5/462, Loss: 0.811436116695404\n",
      "Epoch 35, Batch 6/462, Loss: 0.706937849521637\n",
      "Epoch 35, Batch 7/462, Loss: 0.8180049061775208\n",
      "Epoch 35, Batch 8/462, Loss: 0.7964432239532471\n",
      "Epoch 35, Batch 9/462, Loss: 0.8182629346847534\n",
      "Epoch 35, Batch 10/462, Loss: 0.8057447075843811\n",
      "Epoch 35, Batch 11/462, Loss: 0.6175845861434937\n",
      "Epoch 35, Batch 12/462, Loss: 0.689730167388916\n",
      "Epoch 35, Batch 13/462, Loss: 0.7992356419563293\n",
      "Epoch 35, Batch 14/462, Loss: 0.7106651663780212\n",
      "Epoch 35, Batch 15/462, Loss: 0.8113771080970764\n",
      "Epoch 35, Batch 16/462, Loss: 0.690912127494812\n",
      "Epoch 35, Batch 17/462, Loss: 0.8454517126083374\n",
      "Epoch 35, Batch 18/462, Loss: 0.5721964240074158\n",
      "Epoch 35, Batch 19/462, Loss: 0.9724071025848389\n",
      "Epoch 35, Batch 20/462, Loss: 0.7582128643989563\n",
      "Epoch 35, Batch 21/462, Loss: 0.6810938119888306\n",
      "Epoch 35, Batch 22/462, Loss: 0.7856985330581665\n",
      "Epoch 35, Batch 23/462, Loss: 0.7363533973693848\n",
      "Epoch 35, Batch 24/462, Loss: 0.6741645336151123\n",
      "Epoch 35, Batch 25/462, Loss: 0.7795285582542419\n",
      "Epoch 35, Batch 26/462, Loss: 0.6588857173919678\n",
      "Epoch 35, Batch 27/462, Loss: 0.6863403916358948\n",
      "Epoch 35, Batch 28/462, Loss: 0.6699720621109009\n",
      "Epoch 35, Batch 29/462, Loss: 0.7363247275352478\n",
      "Epoch 35, Batch 30/462, Loss: 0.6263138055801392\n",
      "Epoch 35, Batch 31/462, Loss: 0.5658655166625977\n",
      "Epoch 35, Batch 32/462, Loss: 0.6942446231842041\n",
      "Epoch 35, Batch 33/462, Loss: 0.8376895785331726\n",
      "Epoch 35, Batch 34/462, Loss: 0.7546590566635132\n",
      "Epoch 35, Batch 35/462, Loss: 0.6175456643104553\n",
      "Epoch 35, Batch 36/462, Loss: 0.7832754254341125\n",
      "Epoch 35, Batch 37/462, Loss: 0.7289287447929382\n",
      "Epoch 35, Batch 38/462, Loss: 0.7419229745864868\n",
      "Epoch 35, Batch 39/462, Loss: 0.6331194639205933\n",
      "Epoch 35, Batch 40/462, Loss: 0.7510744333267212\n",
      "Epoch 35, Batch 41/462, Loss: 0.6920074820518494\n",
      "Epoch 35, Batch 42/462, Loss: 0.8250228762626648\n",
      "Epoch 35, Batch 43/462, Loss: 0.6584759950637817\n",
      "Epoch 35, Batch 44/462, Loss: 0.6810656785964966\n",
      "Epoch 35, Batch 45/462, Loss: 0.6070253849029541\n",
      "Epoch 35, Batch 46/462, Loss: 0.5634300708770752\n",
      "Epoch 35, Batch 47/462, Loss: 0.6771965622901917\n",
      "Epoch 35, Batch 48/462, Loss: 0.7815364599227905\n",
      "Epoch 35, Batch 49/462, Loss: 0.6753818392753601\n",
      "Epoch 35, Batch 50/462, Loss: 0.6988601684570312\n",
      "Epoch 35, Batch 51/462, Loss: 0.7371118664741516\n",
      "Epoch 35, Batch 52/462, Loss: 0.8021290302276611\n",
      "Epoch 35, Batch 53/462, Loss: 0.7255464792251587\n",
      "Epoch 35, Batch 54/462, Loss: 0.7917866110801697\n",
      "Epoch 35, Batch 55/462, Loss: 0.7881847620010376\n",
      "Epoch 35, Batch 56/462, Loss: 0.7481905221939087\n",
      "Epoch 35, Batch 57/462, Loss: 0.6941244006156921\n",
      "Epoch 35, Batch 58/462, Loss: 0.66658616065979\n",
      "Epoch 35, Batch 59/462, Loss: 0.8730108737945557\n",
      "Epoch 35, Batch 60/462, Loss: 0.7592270970344543\n",
      "Epoch 35, Batch 61/462, Loss: 0.6284769773483276\n",
      "Epoch 35, Batch 62/462, Loss: 0.7905594110488892\n",
      "Epoch 35, Batch 63/462, Loss: 0.6047510504722595\n",
      "Epoch 35, Batch 64/462, Loss: 0.7399573922157288\n",
      "Epoch 35, Batch 65/462, Loss: 0.7381849884986877\n",
      "Epoch 35, Batch 66/462, Loss: 0.884345293045044\n",
      "Epoch 35, Batch 67/462, Loss: 0.7671048045158386\n",
      "Epoch 35, Batch 68/462, Loss: 0.7235308885574341\n",
      "Epoch 35, Batch 69/462, Loss: 0.7189743518829346\n",
      "Epoch 35, Batch 70/462, Loss: 0.7585805058479309\n",
      "Epoch 35, Batch 71/462, Loss: 0.7409884333610535\n",
      "Epoch 35, Batch 72/462, Loss: 0.9301072359085083\n",
      "Epoch 35, Batch 73/462, Loss: 0.7200848460197449\n",
      "Epoch 35, Batch 74/462, Loss: 0.6486400961875916\n",
      "Epoch 35, Batch 75/462, Loss: 0.8328269720077515\n",
      "Epoch 35, Batch 76/462, Loss: 0.9813746213912964\n",
      "Epoch 35, Batch 77/462, Loss: 0.7790156006813049\n",
      "Epoch 35, Batch 78/462, Loss: 0.7207666039466858\n",
      "Epoch 35, Batch 79/462, Loss: 0.7545709013938904\n",
      "Epoch 35, Batch 80/462, Loss: 0.6861987113952637\n",
      "Epoch 35, Batch 81/462, Loss: 0.9032580852508545\n",
      "Epoch 35, Batch 82/462, Loss: 0.6726087331771851\n",
      "Epoch 35, Batch 83/462, Loss: 0.5670506954193115\n",
      "Epoch 35, Batch 84/462, Loss: 0.7566900849342346\n",
      "Epoch 35, Batch 85/462, Loss: 0.7559565305709839\n",
      "Epoch 35, Batch 86/462, Loss: 0.6615340113639832\n",
      "Epoch 35, Batch 87/462, Loss: 0.6848906874656677\n",
      "Epoch 35, Batch 88/462, Loss: 0.7775319218635559\n",
      "Epoch 35, Batch 89/462, Loss: 0.8146169185638428\n",
      "Epoch 35, Batch 90/462, Loss: 0.7159678936004639\n",
      "Epoch 35, Batch 91/462, Loss: 0.5982179641723633\n",
      "Epoch 35, Batch 92/462, Loss: 0.7264554500579834\n",
      "Epoch 35, Batch 93/462, Loss: 0.5955197215080261\n",
      "Epoch 35, Batch 94/462, Loss: 0.7049770951271057\n",
      "Epoch 35, Batch 95/462, Loss: 0.6570447087287903\n",
      "Epoch 35, Batch 96/462, Loss: 0.6250009536743164\n",
      "Epoch 35, Batch 97/462, Loss: 0.7660397887229919\n",
      "Epoch 35, Batch 98/462, Loss: 0.786777138710022\n",
      "Epoch 35, Batch 99/462, Loss: 0.6548529863357544\n",
      "Epoch 35, Batch 100/462, Loss: 0.66915363073349\n",
      "Epoch 35, Batch 101/462, Loss: 0.7705910801887512\n",
      "Epoch 35, Batch 102/462, Loss: 0.7884597182273865\n",
      "Epoch 35, Batch 103/462, Loss: 0.7733228206634521\n",
      "Epoch 35, Batch 104/462, Loss: 0.6546775698661804\n",
      "Epoch 35, Batch 105/462, Loss: 0.6380544900894165\n",
      "Epoch 35, Batch 106/462, Loss: 0.6562343239784241\n",
      "Epoch 35, Batch 107/462, Loss: 0.5747432708740234\n",
      "Epoch 35, Batch 108/462, Loss: 0.7536779046058655\n",
      "Epoch 35, Batch 109/462, Loss: 0.7250025868415833\n",
      "Epoch 35, Batch 110/462, Loss: 0.6756503582000732\n",
      "Epoch 35, Batch 111/462, Loss: 0.7080559134483337\n",
      "Epoch 35, Batch 112/462, Loss: 0.5648593306541443\n",
      "Epoch 35, Batch 113/462, Loss: 0.6505226492881775\n",
      "Epoch 35, Batch 114/462, Loss: 0.7790724039077759\n",
      "Epoch 35, Batch 115/462, Loss: 0.9820191860198975\n",
      "Epoch 35, Batch 116/462, Loss: 0.5786601305007935\n",
      "Epoch 35, Batch 117/462, Loss: 0.7930915355682373\n",
      "Epoch 35, Batch 118/462, Loss: 0.7336106896400452\n",
      "Epoch 35, Batch 119/462, Loss: 0.7612162828445435\n",
      "Epoch 35, Batch 120/462, Loss: 0.6550439596176147\n",
      "Epoch 35, Batch 121/462, Loss: 0.8364813923835754\n",
      "Epoch 35, Batch 122/462, Loss: 0.7276533842086792\n",
      "Epoch 35, Batch 123/462, Loss: 0.6195993423461914\n",
      "Epoch 35, Batch 124/462, Loss: 0.7112302780151367\n",
      "Epoch 35, Batch 125/462, Loss: 0.7136877775192261\n",
      "Epoch 35, Batch 126/462, Loss: 0.7816062569618225\n",
      "Epoch 35, Batch 127/462, Loss: 0.6126425862312317\n",
      "Epoch 35, Batch 128/462, Loss: 0.6617634296417236\n",
      "Epoch 35, Batch 129/462, Loss: 0.6965063214302063\n",
      "Epoch 35, Batch 130/462, Loss: 0.67044597864151\n",
      "Epoch 35, Batch 131/462, Loss: 0.8304744958877563\n",
      "Epoch 35, Batch 132/462, Loss: 0.6121117472648621\n",
      "Epoch 35, Batch 133/462, Loss: 0.7484145164489746\n",
      "Epoch 35, Batch 134/462, Loss: 0.7484589219093323\n",
      "Epoch 35, Batch 135/462, Loss: 0.6210637092590332\n",
      "Epoch 35, Batch 136/462, Loss: 0.9303541779518127\n",
      "Epoch 35, Batch 137/462, Loss: 0.7071753144264221\n",
      "Epoch 35, Batch 138/462, Loss: 0.8797233700752258\n",
      "Epoch 35, Batch 139/462, Loss: 0.6603214144706726\n",
      "Epoch 35, Batch 140/462, Loss: 0.7772241234779358\n",
      "Epoch 35, Batch 141/462, Loss: 0.6995570659637451\n",
      "Epoch 35, Batch 142/462, Loss: 0.67933189868927\n",
      "Epoch 35, Batch 143/462, Loss: 0.6272684931755066\n",
      "Epoch 35, Batch 144/462, Loss: 0.7261791825294495\n",
      "Epoch 35, Batch 145/462, Loss: 0.6618335843086243\n",
      "Epoch 35, Batch 146/462, Loss: 0.6437996029853821\n",
      "Epoch 35, Batch 147/462, Loss: 0.7551345229148865\n",
      "Epoch 35, Batch 148/462, Loss: 0.7817991971969604\n",
      "Epoch 35, Batch 149/462, Loss: 0.795547604560852\n",
      "Epoch 35, Batch 150/462, Loss: 0.6315141320228577\n",
      "Epoch 35, Batch 151/462, Loss: 0.6852951645851135\n",
      "Epoch 35, Batch 152/462, Loss: 0.7395351529121399\n",
      "Epoch 35, Batch 153/462, Loss: 0.6619454026222229\n",
      "Epoch 35, Batch 154/462, Loss: 0.7316427230834961\n",
      "Epoch 35, Batch 155/462, Loss: 0.9185291528701782\n",
      "Epoch 35, Batch 156/462, Loss: 0.734363317489624\n",
      "Epoch 35, Batch 157/462, Loss: 0.8490762114524841\n",
      "Epoch 35, Batch 158/462, Loss: 0.7514665126800537\n",
      "Epoch 35, Batch 159/462, Loss: 0.7789562344551086\n",
      "Epoch 35, Batch 160/462, Loss: 0.7522992491722107\n",
      "Epoch 35, Batch 161/462, Loss: 0.683965265750885\n",
      "Epoch 35, Batch 162/462, Loss: 0.6711463332176208\n",
      "Epoch 35, Batch 163/462, Loss: 0.5930264592170715\n",
      "Epoch 35, Batch 164/462, Loss: 0.683987021446228\n",
      "Epoch 35, Batch 165/462, Loss: 0.5855093002319336\n",
      "Epoch 35, Batch 166/462, Loss: 0.7540367841720581\n",
      "Epoch 35, Batch 167/462, Loss: 0.6957718729972839\n",
      "Epoch 35, Batch 168/462, Loss: 0.8579598665237427\n",
      "Epoch 35, Batch 169/462, Loss: 0.6766265034675598\n",
      "Epoch 35, Batch 170/462, Loss: 0.8913336992263794\n",
      "Epoch 35, Batch 171/462, Loss: 0.7665233612060547\n",
      "Epoch 35, Batch 172/462, Loss: 0.6297405362129211\n",
      "Epoch 35, Batch 173/462, Loss: 0.8807029724121094\n",
      "Epoch 35, Batch 174/462, Loss: 0.7059777975082397\n",
      "Epoch 35, Batch 175/462, Loss: 0.6524579524993896\n",
      "Epoch 35, Batch 176/462, Loss: 0.7744893431663513\n",
      "Epoch 35, Batch 177/462, Loss: 0.6574798226356506\n",
      "Epoch 35, Batch 178/462, Loss: 0.8041052222251892\n",
      "Epoch 35, Batch 179/462, Loss: 0.8002768158912659\n",
      "Epoch 35, Batch 180/462, Loss: 0.6608601808547974\n",
      "Epoch 35, Batch 181/462, Loss: 0.6635000705718994\n",
      "Epoch 35, Batch 182/462, Loss: 0.7788997292518616\n",
      "Epoch 35, Batch 183/462, Loss: 0.8276739716529846\n",
      "Epoch 35, Batch 184/462, Loss: 0.7626062035560608\n",
      "Epoch 35, Batch 185/462, Loss: 0.7596431374549866\n",
      "Epoch 35, Batch 186/462, Loss: 0.8378086090087891\n",
      "Epoch 35, Batch 187/462, Loss: 0.6209389567375183\n",
      "Epoch 35, Batch 188/462, Loss: 0.591921329498291\n",
      "Epoch 35, Batch 189/462, Loss: 0.5809534788131714\n",
      "Epoch 35, Batch 190/462, Loss: 0.94264817237854\n",
      "Epoch 35, Batch 191/462, Loss: 0.677882969379425\n",
      "Epoch 35, Batch 192/462, Loss: 0.7247461080551147\n",
      "Epoch 35, Batch 193/462, Loss: 0.6642490029335022\n",
      "Epoch 35, Batch 194/462, Loss: 0.5539404153823853\n",
      "Epoch 35, Batch 195/462, Loss: 0.6301175355911255\n",
      "Epoch 35, Batch 196/462, Loss: 0.6455656290054321\n",
      "Epoch 35, Batch 197/462, Loss: 0.7019984722137451\n",
      "Epoch 35, Batch 198/462, Loss: 0.7304081916809082\n",
      "Epoch 35, Batch 199/462, Loss: 0.6669233441352844\n",
      "Epoch 35, Batch 200/462, Loss: 0.726341962814331\n",
      "Epoch 35, Batch 201/462, Loss: 0.646881639957428\n",
      "Epoch 35, Batch 202/462, Loss: 0.7145216464996338\n",
      "Epoch 35, Batch 203/462, Loss: 0.4892733693122864\n",
      "Epoch 35, Batch 204/462, Loss: 0.7780302166938782\n",
      "Epoch 35, Batch 205/462, Loss: 0.7487107515335083\n",
      "Epoch 35, Batch 206/462, Loss: 0.8230968713760376\n",
      "Epoch 35, Batch 207/462, Loss: 0.5188839435577393\n",
      "Epoch 35, Batch 208/462, Loss: 0.8267214298248291\n",
      "Epoch 35, Batch 209/462, Loss: 0.7464580535888672\n",
      "Epoch 35, Batch 210/462, Loss: 0.6313799619674683\n",
      "Epoch 35, Batch 211/462, Loss: 0.5643687844276428\n",
      "Epoch 35, Batch 212/462, Loss: 0.6265110969543457\n",
      "Epoch 35, Batch 213/462, Loss: 0.77704256772995\n",
      "Epoch 35, Batch 214/462, Loss: 0.8648760318756104\n",
      "Epoch 35, Batch 215/462, Loss: 0.7808935642242432\n",
      "Epoch 35, Batch 216/462, Loss: 0.8069251775741577\n",
      "Epoch 35, Batch 217/462, Loss: 0.6497087478637695\n",
      "Epoch 35, Batch 218/462, Loss: 0.7589880228042603\n",
      "Epoch 35, Batch 219/462, Loss: 0.9083408117294312\n",
      "Epoch 35, Batch 220/462, Loss: 0.7468635439872742\n",
      "Epoch 35, Batch 221/462, Loss: 0.526848316192627\n",
      "Epoch 35, Batch 222/462, Loss: 0.7550721764564514\n",
      "Epoch 35, Batch 223/462, Loss: 0.6232773065567017\n",
      "Epoch 35, Batch 224/462, Loss: 0.6972500681877136\n",
      "Epoch 35, Batch 225/462, Loss: 0.7895827889442444\n",
      "Epoch 35, Batch 226/462, Loss: 0.6338816285133362\n",
      "Epoch 35, Batch 227/462, Loss: 0.6684231162071228\n",
      "Epoch 35, Batch 228/462, Loss: 0.6941158175468445\n",
      "Epoch 35, Batch 229/462, Loss: 0.6649096608161926\n",
      "Epoch 35, Batch 230/462, Loss: 0.673346996307373\n",
      "Epoch 35, Batch 231/462, Loss: 0.5663668513298035\n",
      "Epoch 35, Batch 232/462, Loss: 0.767688512802124\n",
      "Epoch 35, Batch 233/462, Loss: 0.6184694766998291\n",
      "Epoch 35, Batch 234/462, Loss: 0.7415270805358887\n",
      "Epoch 35, Batch 235/462, Loss: 0.690820574760437\n",
      "Epoch 35, Batch 236/462, Loss: 0.6156522631645203\n",
      "Epoch 35, Batch 237/462, Loss: 0.8001827597618103\n",
      "Epoch 35, Batch 238/462, Loss: 0.7636544108390808\n",
      "Epoch 35, Batch 239/462, Loss: 0.6341944932937622\n",
      "Epoch 35, Batch 240/462, Loss: 0.768575131893158\n",
      "Epoch 35, Batch 241/462, Loss: 0.596096932888031\n",
      "Epoch 35, Batch 242/462, Loss: 0.7390669584274292\n",
      "Epoch 35, Batch 243/462, Loss: 0.9210212230682373\n",
      "Epoch 35, Batch 244/462, Loss: 0.8320396542549133\n",
      "Epoch 35, Batch 245/462, Loss: 0.5915214419364929\n",
      "Epoch 35, Batch 246/462, Loss: 0.7796451449394226\n",
      "Epoch 35, Batch 247/462, Loss: 0.7855903506278992\n",
      "Epoch 35, Batch 248/462, Loss: 0.67426997423172\n",
      "Epoch 35, Batch 249/462, Loss: 0.8860479593276978\n",
      "Epoch 35, Batch 250/462, Loss: 0.783092737197876\n",
      "Epoch 35, Batch 251/462, Loss: 0.7516836524009705\n",
      "Epoch 35, Batch 252/462, Loss: 0.8336120247840881\n",
      "Epoch 35, Batch 253/462, Loss: 0.689882755279541\n",
      "Epoch 35, Batch 254/462, Loss: 0.8643276691436768\n",
      "Epoch 35, Batch 255/462, Loss: 0.7557350397109985\n",
      "Epoch 35, Batch 256/462, Loss: 0.6314291954040527\n",
      "Epoch 35, Batch 257/462, Loss: 0.6714672446250916\n",
      "Epoch 35, Batch 258/462, Loss: 0.847163736820221\n",
      "Epoch 35, Batch 259/462, Loss: 0.6454352140426636\n",
      "Epoch 35, Batch 260/462, Loss: 0.6667755246162415\n",
      "Epoch 35, Batch 261/462, Loss: 0.8395959734916687\n",
      "Epoch 35, Batch 262/462, Loss: 0.7675772309303284\n",
      "Epoch 35, Batch 263/462, Loss: 0.5862475037574768\n",
      "Epoch 35, Batch 264/462, Loss: 0.6865867376327515\n",
      "Epoch 35, Batch 265/462, Loss: 0.6374955177307129\n",
      "Epoch 35, Batch 266/462, Loss: 0.6003217697143555\n",
      "Epoch 35, Batch 267/462, Loss: 0.7020142674446106\n",
      "Epoch 35, Batch 268/462, Loss: 0.6359208822250366\n",
      "Epoch 35, Batch 269/462, Loss: 0.7188546061515808\n",
      "Epoch 35, Batch 270/462, Loss: 0.6871737837791443\n",
      "Epoch 35, Batch 271/462, Loss: 0.7389498949050903\n",
      "Epoch 35, Batch 272/462, Loss: 0.8161256313323975\n",
      "Epoch 35, Batch 273/462, Loss: 0.6888502240180969\n",
      "Epoch 35, Batch 274/462, Loss: 0.6354580521583557\n",
      "Epoch 35, Batch 275/462, Loss: 0.7898746728897095\n",
      "Epoch 35, Batch 276/462, Loss: 0.9283017516136169\n",
      "Epoch 35, Batch 277/462, Loss: 0.8061829209327698\n",
      "Epoch 35, Batch 278/462, Loss: 0.5324872732162476\n",
      "Epoch 35, Batch 279/462, Loss: 0.7415286302566528\n",
      "Epoch 35, Batch 280/462, Loss: 0.6593161225318909\n",
      "Epoch 35, Batch 281/462, Loss: 0.6362302303314209\n",
      "Epoch 35, Batch 282/462, Loss: 0.7405574917793274\n",
      "Epoch 35, Batch 283/462, Loss: 0.911918044090271\n",
      "Epoch 35, Batch 284/462, Loss: 0.7094013094902039\n",
      "Epoch 35, Batch 285/462, Loss: 0.6777492165565491\n",
      "Epoch 35, Batch 286/462, Loss: 0.6538440585136414\n",
      "Epoch 35, Batch 287/462, Loss: 0.8980945944786072\n",
      "Epoch 35, Batch 288/462, Loss: 0.560552179813385\n",
      "Epoch 35, Batch 289/462, Loss: 0.6938003301620483\n",
      "Epoch 35, Batch 290/462, Loss: 0.6611689925193787\n",
      "Epoch 35, Batch 291/462, Loss: 0.703009307384491\n",
      "Epoch 35, Batch 292/462, Loss: 0.8875817656517029\n",
      "Epoch 35, Batch 293/462, Loss: 0.8847845792770386\n",
      "Epoch 35, Batch 294/462, Loss: 0.5389623045921326\n",
      "Epoch 35, Batch 295/462, Loss: 0.589780330657959\n",
      "Epoch 35, Batch 296/462, Loss: 0.7787495851516724\n",
      "Epoch 35, Batch 297/462, Loss: 0.6231750249862671\n",
      "Epoch 35, Batch 298/462, Loss: 0.4501270651817322\n",
      "Epoch 35, Batch 299/462, Loss: 0.6868705153465271\n",
      "Epoch 35, Batch 300/462, Loss: 0.7961046099662781\n",
      "Epoch 35, Batch 301/462, Loss: 0.7354860901832581\n",
      "Epoch 35, Batch 302/462, Loss: 0.8023492693901062\n",
      "Epoch 35, Batch 303/462, Loss: 0.6969189643859863\n",
      "Epoch 35, Batch 304/462, Loss: 0.5677621364593506\n",
      "Epoch 35, Batch 305/462, Loss: 0.6564053297042847\n",
      "Epoch 35, Batch 306/462, Loss: 0.7714139819145203\n",
      "Epoch 35, Batch 307/462, Loss: 0.6626911163330078\n",
      "Epoch 35, Batch 308/462, Loss: 0.8157275319099426\n",
      "Epoch 35, Batch 309/462, Loss: 0.8174558281898499\n",
      "Epoch 35, Batch 310/462, Loss: 0.6410889029502869\n",
      "Epoch 35, Batch 311/462, Loss: 0.756795346736908\n",
      "Epoch 35, Batch 312/462, Loss: 0.630047082901001\n",
      "Epoch 35, Batch 313/462, Loss: 0.6974316835403442\n",
      "Epoch 35, Batch 314/462, Loss: 0.8522276282310486\n",
      "Epoch 35, Batch 315/462, Loss: 0.5686923861503601\n",
      "Epoch 35, Batch 316/462, Loss: 0.6520311236381531\n",
      "Epoch 35, Batch 317/462, Loss: 0.6642459034919739\n",
      "Epoch 35, Batch 318/462, Loss: 0.8072165846824646\n",
      "Epoch 35, Batch 319/462, Loss: 0.7732548713684082\n",
      "Epoch 35, Batch 320/462, Loss: 0.688379168510437\n",
      "Epoch 35, Batch 321/462, Loss: 0.6955960392951965\n",
      "Epoch 35, Batch 322/462, Loss: 0.7877313494682312\n",
      "Epoch 35, Batch 323/462, Loss: 0.5862371325492859\n",
      "Epoch 35, Batch 324/462, Loss: 0.7135299444198608\n",
      "Epoch 35, Batch 325/462, Loss: 0.5840688347816467\n",
      "Epoch 35, Batch 326/462, Loss: 0.687314510345459\n",
      "Epoch 35, Batch 327/462, Loss: 0.8625274896621704\n",
      "Epoch 35, Batch 328/462, Loss: 0.6196881532669067\n",
      "Epoch 35, Batch 329/462, Loss: 0.8897088766098022\n",
      "Epoch 35, Batch 330/462, Loss: 0.6886451840400696\n",
      "Epoch 35, Batch 331/462, Loss: 0.6253300309181213\n",
      "Epoch 35, Batch 332/462, Loss: 0.8100368976593018\n",
      "Epoch 35, Batch 333/462, Loss: 0.7934528589248657\n",
      "Epoch 35, Batch 334/462, Loss: 0.8299578428268433\n",
      "Epoch 35, Batch 335/462, Loss: 0.6364821791648865\n",
      "Epoch 35, Batch 336/462, Loss: 0.843113899230957\n",
      "Epoch 35, Batch 337/462, Loss: 0.8603516817092896\n",
      "Epoch 35, Batch 338/462, Loss: 0.7889586091041565\n",
      "Epoch 35, Batch 339/462, Loss: 0.6688013076782227\n",
      "Epoch 35, Batch 340/462, Loss: 0.8222058415412903\n",
      "Epoch 35, Batch 341/462, Loss: 0.6398932933807373\n",
      "Epoch 35, Batch 342/462, Loss: 0.5910413265228271\n",
      "Epoch 35, Batch 343/462, Loss: 0.7865780591964722\n",
      "Epoch 35, Batch 344/462, Loss: 0.7393910884857178\n",
      "Epoch 35, Batch 345/462, Loss: 0.8666114211082458\n",
      "Epoch 35, Batch 346/462, Loss: 0.8177214860916138\n",
      "Epoch 35, Batch 347/462, Loss: 0.6570212244987488\n",
      "Epoch 35, Batch 348/462, Loss: 0.7687301635742188\n",
      "Epoch 35, Batch 349/462, Loss: 0.6778910160064697\n",
      "Epoch 35, Batch 350/462, Loss: 0.6749926805496216\n",
      "Epoch 35, Batch 351/462, Loss: 0.717811644077301\n",
      "Epoch 35, Batch 352/462, Loss: 0.6634352207183838\n",
      "Epoch 35, Batch 353/462, Loss: 0.7977086901664734\n",
      "Epoch 35, Batch 354/462, Loss: 0.7053732872009277\n",
      "Epoch 35, Batch 355/462, Loss: 0.725321352481842\n",
      "Epoch 35, Batch 356/462, Loss: 0.8618160486221313\n",
      "Epoch 35, Batch 357/462, Loss: 0.5389003753662109\n",
      "Epoch 35, Batch 358/462, Loss: 0.7944380044937134\n",
      "Epoch 35, Batch 359/462, Loss: 0.550367534160614\n",
      "Epoch 35, Batch 360/462, Loss: 0.6931278705596924\n",
      "Epoch 35, Batch 361/462, Loss: 0.7130774855613708\n",
      "Epoch 35, Batch 362/462, Loss: 0.8908030986785889\n",
      "Epoch 35, Batch 363/462, Loss: 0.8184472322463989\n",
      "Epoch 35, Batch 364/462, Loss: 0.7343359589576721\n",
      "Epoch 35, Batch 365/462, Loss: 0.7096763253211975\n",
      "Epoch 35, Batch 366/462, Loss: 0.6145821809768677\n",
      "Epoch 35, Batch 367/462, Loss: 0.6880800724029541\n",
      "Epoch 35, Batch 368/462, Loss: 0.8524535298347473\n",
      "Epoch 35, Batch 369/462, Loss: 0.6017183065414429\n",
      "Epoch 35, Batch 370/462, Loss: 0.6417058706283569\n",
      "Epoch 35, Batch 371/462, Loss: 0.6887874603271484\n",
      "Epoch 35, Batch 372/462, Loss: 0.5411074161529541\n",
      "Epoch 35, Batch 373/462, Loss: 0.5832164883613586\n",
      "Epoch 35, Batch 374/462, Loss: 0.8274266123771667\n",
      "Epoch 35, Batch 375/462, Loss: 0.7295683026313782\n",
      "Epoch 35, Batch 376/462, Loss: 0.6357681155204773\n",
      "Epoch 35, Batch 377/462, Loss: 0.6963849663734436\n",
      "Epoch 35, Batch 378/462, Loss: 0.7541062235832214\n",
      "Epoch 35, Batch 379/462, Loss: 0.7376159429550171\n",
      "Epoch 35, Batch 380/462, Loss: 0.6394590139389038\n",
      "Epoch 35, Batch 381/462, Loss: 0.7536362409591675\n",
      "Epoch 35, Batch 382/462, Loss: 0.7271668910980225\n",
      "Epoch 35, Batch 383/462, Loss: 0.617423951625824\n",
      "Epoch 35, Batch 384/462, Loss: 0.8569431304931641\n",
      "Epoch 35, Batch 385/462, Loss: 0.6723881959915161\n",
      "Epoch 35, Batch 386/462, Loss: 0.7566614151000977\n",
      "Epoch 35, Batch 387/462, Loss: 0.7701561450958252\n",
      "Epoch 35, Batch 388/462, Loss: 0.6393272876739502\n",
      "Epoch 35, Batch 389/462, Loss: 0.6193456053733826\n",
      "Epoch 35, Batch 390/462, Loss: 0.8478172421455383\n",
      "Epoch 35, Batch 391/462, Loss: 0.5899011492729187\n",
      "Epoch 35, Batch 392/462, Loss: 0.7683716416358948\n",
      "Epoch 35, Batch 393/462, Loss: 0.8556174635887146\n",
      "Epoch 35, Batch 394/462, Loss: 0.662456750869751\n",
      "Epoch 35, Batch 395/462, Loss: 0.7607629895210266\n",
      "Epoch 35, Batch 396/462, Loss: 0.7318130731582642\n",
      "Epoch 35, Batch 397/462, Loss: 0.7725216150283813\n",
      "Epoch 35, Batch 398/462, Loss: 0.8211634159088135\n",
      "Epoch 35, Batch 399/462, Loss: 0.7002182602882385\n",
      "Epoch 35, Batch 400/462, Loss: 0.7560724020004272\n",
      "Epoch 35, Batch 401/462, Loss: 0.7390367388725281\n",
      "Epoch 35, Batch 402/462, Loss: 0.692611038684845\n",
      "Epoch 35, Batch 403/462, Loss: 0.8593130707740784\n",
      "Epoch 35, Batch 404/462, Loss: 0.6282607316970825\n",
      "Epoch 35, Batch 405/462, Loss: 0.7152459621429443\n",
      "Epoch 35, Batch 406/462, Loss: 0.6443244814872742\n",
      "Epoch 35, Batch 407/462, Loss: 0.7711634635925293\n",
      "Epoch 35, Batch 408/462, Loss: 0.8047015070915222\n",
      "Epoch 35, Batch 409/462, Loss: 0.7659212350845337\n",
      "Epoch 35, Batch 410/462, Loss: 0.7842236161231995\n",
      "Epoch 35, Batch 411/462, Loss: 0.7468072772026062\n",
      "Epoch 35, Batch 412/462, Loss: 0.7056421041488647\n",
      "Epoch 35, Batch 413/462, Loss: 0.735827624797821\n",
      "Epoch 35, Batch 414/462, Loss: 0.744954526424408\n",
      "Epoch 35, Batch 415/462, Loss: 0.9336256980895996\n",
      "Epoch 35, Batch 416/462, Loss: 0.8981212973594666\n",
      "Epoch 35, Batch 417/462, Loss: 0.8340648412704468\n",
      "Epoch 35, Batch 418/462, Loss: 0.6593799591064453\n",
      "Epoch 35, Batch 419/462, Loss: 0.7420914769172668\n",
      "Epoch 35, Batch 420/462, Loss: 0.9253357648849487\n",
      "Epoch 35, Batch 421/462, Loss: 0.6784327626228333\n",
      "Epoch 35, Batch 422/462, Loss: 0.6007434129714966\n",
      "Epoch 35, Batch 423/462, Loss: 0.684087336063385\n",
      "Epoch 35, Batch 424/462, Loss: 0.7356317639350891\n",
      "Epoch 35, Batch 425/462, Loss: 0.7782590985298157\n",
      "Epoch 35, Batch 426/462, Loss: 0.7606326341629028\n",
      "Epoch 35, Batch 427/462, Loss: 0.5898666381835938\n",
      "Epoch 35, Batch 428/462, Loss: 0.727982759475708\n",
      "Epoch 35, Batch 429/462, Loss: 0.778666079044342\n",
      "Epoch 35, Batch 430/462, Loss: 0.6851351261138916\n",
      "Epoch 35, Batch 431/462, Loss: 0.6645401120185852\n",
      "Epoch 35, Batch 432/462, Loss: 0.5949411392211914\n",
      "Epoch 35, Batch 433/462, Loss: 0.5771064758300781\n",
      "Epoch 35, Batch 434/462, Loss: 0.6140844821929932\n",
      "Epoch 35, Batch 435/462, Loss: 0.6871457695960999\n",
      "Epoch 35, Batch 436/462, Loss: 0.7136451005935669\n",
      "Epoch 35, Batch 437/462, Loss: 0.8821299076080322\n",
      "Epoch 35, Batch 438/462, Loss: 0.615707278251648\n",
      "Epoch 35, Batch 439/462, Loss: 0.6840210556983948\n",
      "Epoch 35, Batch 440/462, Loss: 0.8899058103561401\n",
      "Epoch 35, Batch 441/462, Loss: 0.5253792405128479\n",
      "Epoch 35, Batch 442/462, Loss: 0.7004355192184448\n",
      "Epoch 35, Batch 443/462, Loss: 0.5947709083557129\n",
      "Epoch 35, Batch 444/462, Loss: 0.8012523055076599\n",
      "Epoch 35, Batch 445/462, Loss: 0.7213447690010071\n",
      "Epoch 35, Batch 446/462, Loss: 0.567959189414978\n",
      "Epoch 35, Batch 447/462, Loss: 0.7095285058021545\n",
      "Epoch 35, Batch 448/462, Loss: 0.5265398025512695\n",
      "Epoch 35, Batch 449/462, Loss: 0.707046389579773\n",
      "Epoch 35, Batch 450/462, Loss: 0.7378869652748108\n",
      "Epoch 35, Batch 451/462, Loss: 1.0477237701416016\n",
      "Epoch 35, Batch 452/462, Loss: 0.63837069272995\n",
      "Epoch 35, Batch 453/462, Loss: 0.6157048940658569\n",
      "Epoch 35, Batch 454/462, Loss: 0.8167783617973328\n",
      "Epoch 35, Batch 455/462, Loss: 0.8063551187515259\n",
      "Epoch 35, Batch 456/462, Loss: 0.8757626414299011\n",
      "Epoch 35, Batch 457/462, Loss: 0.6858105063438416\n",
      "Epoch 35, Batch 458/462, Loss: 0.915039598941803\n",
      "Epoch 35, Batch 459/462, Loss: 0.6780376434326172\n",
      "Epoch 35, Batch 460/462, Loss: 0.6585431098937988\n",
      "Epoch 35, Batch 461/462, Loss: 0.644396960735321\n",
      "Epoch 35, Batch 462/462, Loss: 0.755508303642273\n",
      "Epoch 35, Loss: 333.5586791038513\n",
      "Epoch 36, Batch 1/462, Loss: 0.7291032671928406\n",
      "Epoch 36, Batch 2/462, Loss: 0.6055575609207153\n",
      "Epoch 36, Batch 3/462, Loss: 0.8556104898452759\n",
      "Epoch 36, Batch 4/462, Loss: 0.5903035402297974\n",
      "Epoch 36, Batch 5/462, Loss: 0.7394989728927612\n",
      "Epoch 36, Batch 6/462, Loss: 0.6325827240943909\n",
      "Epoch 36, Batch 7/462, Loss: 0.6160477995872498\n",
      "Epoch 36, Batch 8/462, Loss: 0.6121004223823547\n",
      "Epoch 36, Batch 9/462, Loss: 0.849431574344635\n",
      "Epoch 36, Batch 10/462, Loss: 0.5386722683906555\n",
      "Epoch 36, Batch 11/462, Loss: 0.7144291996955872\n",
      "Epoch 36, Batch 12/462, Loss: 0.5682464241981506\n",
      "Epoch 36, Batch 13/462, Loss: 0.62230384349823\n",
      "Epoch 36, Batch 14/462, Loss: 0.5379338264465332\n",
      "Epoch 36, Batch 15/462, Loss: 0.5488404631614685\n",
      "Epoch 36, Batch 16/462, Loss: 0.7527067065238953\n",
      "Epoch 36, Batch 17/462, Loss: 0.7110040187835693\n",
      "Epoch 36, Batch 18/462, Loss: 0.7199322581291199\n",
      "Epoch 36, Batch 19/462, Loss: 0.7799970507621765\n",
      "Epoch 36, Batch 20/462, Loss: 0.7550681233406067\n",
      "Epoch 36, Batch 21/462, Loss: 0.8458662033081055\n",
      "Epoch 36, Batch 22/462, Loss: 0.6335126161575317\n",
      "Epoch 36, Batch 23/462, Loss: 0.7633230090141296\n",
      "Epoch 36, Batch 24/462, Loss: 0.6865902543067932\n",
      "Epoch 36, Batch 25/462, Loss: 0.8449206948280334\n",
      "Epoch 36, Batch 26/462, Loss: 0.7708951830863953\n",
      "Epoch 36, Batch 27/462, Loss: 0.5659928917884827\n",
      "Epoch 36, Batch 28/462, Loss: 0.6919813752174377\n",
      "Epoch 36, Batch 29/462, Loss: 0.7154416441917419\n",
      "Epoch 36, Batch 30/462, Loss: 0.7446661591529846\n",
      "Epoch 36, Batch 31/462, Loss: 0.8354482650756836\n",
      "Epoch 36, Batch 32/462, Loss: 0.6365136504173279\n",
      "Epoch 36, Batch 33/462, Loss: 0.6643497347831726\n",
      "Epoch 36, Batch 34/462, Loss: 0.6944497227668762\n",
      "Epoch 36, Batch 35/462, Loss: 0.685400128364563\n",
      "Epoch 36, Batch 36/462, Loss: 0.5637702345848083\n",
      "Epoch 36, Batch 37/462, Loss: 0.6530470252037048\n",
      "Epoch 36, Batch 38/462, Loss: 0.8274142742156982\n",
      "Epoch 36, Batch 39/462, Loss: 0.6748936176300049\n",
      "Epoch 36, Batch 40/462, Loss: 0.6217747926712036\n",
      "Epoch 36, Batch 41/462, Loss: 0.7198687791824341\n",
      "Epoch 36, Batch 42/462, Loss: 0.7476844787597656\n",
      "Epoch 36, Batch 43/462, Loss: 0.6972376108169556\n",
      "Epoch 36, Batch 44/462, Loss: 0.7444754838943481\n",
      "Epoch 36, Batch 45/462, Loss: 0.6803074479103088\n",
      "Epoch 36, Batch 46/462, Loss: 0.6912367939949036\n",
      "Epoch 36, Batch 47/462, Loss: 0.8398141860961914\n",
      "Epoch 36, Batch 48/462, Loss: 0.8725847601890564\n",
      "Epoch 36, Batch 49/462, Loss: 0.861440896987915\n",
      "Epoch 36, Batch 50/462, Loss: 0.6276555061340332\n",
      "Epoch 36, Batch 51/462, Loss: 0.5484811067581177\n",
      "Epoch 36, Batch 52/462, Loss: 0.5401157736778259\n",
      "Epoch 36, Batch 53/462, Loss: 0.8380333185195923\n",
      "Epoch 36, Batch 54/462, Loss: 0.9255738854408264\n",
      "Epoch 36, Batch 55/462, Loss: 0.5668295621871948\n",
      "Epoch 36, Batch 56/462, Loss: 0.7156206369400024\n",
      "Epoch 36, Batch 57/462, Loss: 0.778508186340332\n",
      "Epoch 36, Batch 58/462, Loss: 0.7935039401054382\n",
      "Epoch 36, Batch 59/462, Loss: 0.7853800058364868\n",
      "Epoch 36, Batch 60/462, Loss: 0.7123846411705017\n",
      "Epoch 36, Batch 61/462, Loss: 0.7923199534416199\n",
      "Epoch 36, Batch 62/462, Loss: 0.7240559458732605\n",
      "Epoch 36, Batch 63/462, Loss: 0.80487459897995\n",
      "Epoch 36, Batch 64/462, Loss: 0.8019380569458008\n",
      "Epoch 36, Batch 65/462, Loss: 0.4714782238006592\n",
      "Epoch 36, Batch 66/462, Loss: 0.6658035516738892\n",
      "Epoch 36, Batch 67/462, Loss: 0.6854361295700073\n",
      "Epoch 36, Batch 68/462, Loss: 0.7120583653450012\n",
      "Epoch 36, Batch 69/462, Loss: 0.7553725838661194\n",
      "Epoch 36, Batch 70/462, Loss: 0.7170695662498474\n",
      "Epoch 36, Batch 71/462, Loss: 0.8206979036331177\n",
      "Epoch 36, Batch 72/462, Loss: 0.7627158164978027\n",
      "Epoch 36, Batch 73/462, Loss: 0.7209769487380981\n",
      "Epoch 36, Batch 74/462, Loss: 1.0956279039382935\n",
      "Epoch 36, Batch 75/462, Loss: 0.6301218867301941\n",
      "Epoch 36, Batch 76/462, Loss: 0.8443112969398499\n",
      "Epoch 36, Batch 77/462, Loss: 0.9213476777076721\n",
      "Epoch 36, Batch 78/462, Loss: 0.6840647459030151\n",
      "Epoch 36, Batch 79/462, Loss: 0.793705403804779\n",
      "Epoch 36, Batch 80/462, Loss: 0.7955906987190247\n",
      "Epoch 36, Batch 81/462, Loss: 0.7586907148361206\n",
      "Epoch 36, Batch 82/462, Loss: 0.5349382758140564\n",
      "Epoch 36, Batch 83/462, Loss: 0.7609734535217285\n",
      "Epoch 36, Batch 84/462, Loss: 0.6776946187019348\n",
      "Epoch 36, Batch 85/462, Loss: 0.561474621295929\n",
      "Epoch 36, Batch 86/462, Loss: 0.5201074481010437\n",
      "Epoch 36, Batch 87/462, Loss: 0.5998456478118896\n",
      "Epoch 36, Batch 88/462, Loss: 0.7382344007492065\n",
      "Epoch 36, Batch 89/462, Loss: 0.7702613472938538\n",
      "Epoch 36, Batch 90/462, Loss: 1.0237194299697876\n",
      "Epoch 36, Batch 91/462, Loss: 0.7898496985435486\n",
      "Epoch 36, Batch 92/462, Loss: 0.6311987042427063\n",
      "Epoch 36, Batch 93/462, Loss: 0.672800600528717\n",
      "Epoch 36, Batch 94/462, Loss: 0.6270880103111267\n",
      "Epoch 36, Batch 95/462, Loss: 0.6813017129898071\n",
      "Epoch 36, Batch 96/462, Loss: 0.7268306612968445\n",
      "Epoch 36, Batch 97/462, Loss: 0.8145489692687988\n",
      "Epoch 36, Batch 98/462, Loss: 0.6349136829376221\n",
      "Epoch 36, Batch 99/462, Loss: 0.5997982621192932\n",
      "Epoch 36, Batch 100/462, Loss: 0.9389212131500244\n",
      "Epoch 36, Batch 101/462, Loss: 0.5265149474143982\n",
      "Epoch 36, Batch 102/462, Loss: 0.7087482810020447\n",
      "Epoch 36, Batch 103/462, Loss: 0.6282837986946106\n",
      "Epoch 36, Batch 104/462, Loss: 0.787587583065033\n",
      "Epoch 36, Batch 105/462, Loss: 0.7354543209075928\n",
      "Epoch 36, Batch 106/462, Loss: 0.763002336025238\n",
      "Epoch 36, Batch 107/462, Loss: 0.6297016143798828\n",
      "Epoch 36, Batch 108/462, Loss: 0.7275263667106628\n",
      "Epoch 36, Batch 109/462, Loss: 0.6326715350151062\n",
      "Epoch 36, Batch 110/462, Loss: 0.9311349391937256\n",
      "Epoch 36, Batch 111/462, Loss: 0.8630673289299011\n",
      "Epoch 36, Batch 112/462, Loss: 0.6993347406387329\n",
      "Epoch 36, Batch 113/462, Loss: 0.6285051703453064\n",
      "Epoch 36, Batch 114/462, Loss: 0.7244742512702942\n",
      "Epoch 36, Batch 115/462, Loss: 0.6341533064842224\n",
      "Epoch 36, Batch 116/462, Loss: 0.7136358022689819\n",
      "Epoch 36, Batch 117/462, Loss: 0.7034002542495728\n",
      "Epoch 36, Batch 118/462, Loss: 0.7243171334266663\n",
      "Epoch 36, Batch 119/462, Loss: 0.7362980246543884\n",
      "Epoch 36, Batch 120/462, Loss: 0.9032806158065796\n",
      "Epoch 36, Batch 121/462, Loss: 0.761406660079956\n",
      "Epoch 36, Batch 122/462, Loss: 0.8320521116256714\n",
      "Epoch 36, Batch 123/462, Loss: 0.7297289967536926\n",
      "Epoch 36, Batch 124/462, Loss: 0.7533005475997925\n",
      "Epoch 36, Batch 125/462, Loss: 0.6797281503677368\n",
      "Epoch 36, Batch 126/462, Loss: 0.6309865117073059\n",
      "Epoch 36, Batch 127/462, Loss: 0.6225976943969727\n",
      "Epoch 36, Batch 128/462, Loss: 0.7881985902786255\n",
      "Epoch 36, Batch 129/462, Loss: 0.6666818261146545\n",
      "Epoch 36, Batch 130/462, Loss: 0.8535025119781494\n",
      "Epoch 36, Batch 131/462, Loss: 0.712745189666748\n",
      "Epoch 36, Batch 132/462, Loss: 0.5928997993469238\n",
      "Epoch 36, Batch 133/462, Loss: 0.7731695771217346\n",
      "Epoch 36, Batch 134/462, Loss: 0.8425289392471313\n",
      "Epoch 36, Batch 135/462, Loss: 0.7227864861488342\n",
      "Epoch 36, Batch 136/462, Loss: 0.8766264915466309\n",
      "Epoch 36, Batch 137/462, Loss: 0.7546437978744507\n",
      "Epoch 36, Batch 138/462, Loss: 0.8703982830047607\n",
      "Epoch 36, Batch 139/462, Loss: 0.7505332231521606\n",
      "Epoch 36, Batch 140/462, Loss: 0.8012677431106567\n",
      "Epoch 36, Batch 141/462, Loss: 0.8090803027153015\n",
      "Epoch 36, Batch 142/462, Loss: 0.6565830111503601\n",
      "Epoch 36, Batch 143/462, Loss: 0.8194617629051208\n",
      "Epoch 36, Batch 144/462, Loss: 0.9611458778381348\n",
      "Epoch 36, Batch 145/462, Loss: 0.6322303414344788\n",
      "Epoch 36, Batch 146/462, Loss: 0.6888116598129272\n",
      "Epoch 36, Batch 147/462, Loss: 0.6294683218002319\n",
      "Epoch 36, Batch 148/462, Loss: 0.6670851111412048\n",
      "Epoch 36, Batch 149/462, Loss: 0.6971069574356079\n",
      "Epoch 36, Batch 150/462, Loss: 0.6630948185920715\n",
      "Epoch 36, Batch 151/462, Loss: 0.7672282457351685\n",
      "Epoch 36, Batch 152/462, Loss: 0.6540827751159668\n",
      "Epoch 36, Batch 153/462, Loss: 0.5711414217948914\n",
      "Epoch 36, Batch 154/462, Loss: 0.7290863394737244\n",
      "Epoch 36, Batch 155/462, Loss: 0.7219042778015137\n",
      "Epoch 36, Batch 156/462, Loss: 0.6191351413726807\n",
      "Epoch 36, Batch 157/462, Loss: 0.6168879866600037\n",
      "Epoch 36, Batch 158/462, Loss: 0.6911300420761108\n",
      "Epoch 36, Batch 159/462, Loss: 0.8656224608421326\n",
      "Epoch 36, Batch 160/462, Loss: 0.7836662530899048\n",
      "Epoch 36, Batch 161/462, Loss: 0.6632184386253357\n",
      "Epoch 36, Batch 162/462, Loss: 0.6718184947967529\n",
      "Epoch 36, Batch 163/462, Loss: 0.7236644625663757\n",
      "Epoch 36, Batch 164/462, Loss: 0.537213146686554\n",
      "Epoch 36, Batch 165/462, Loss: 0.7309269905090332\n",
      "Epoch 36, Batch 166/462, Loss: 0.9426550269126892\n",
      "Epoch 36, Batch 167/462, Loss: 0.6444175839424133\n",
      "Epoch 36, Batch 168/462, Loss: 0.8297348618507385\n",
      "Epoch 36, Batch 169/462, Loss: 0.6723064184188843\n",
      "Epoch 36, Batch 170/462, Loss: 0.5777568221092224\n",
      "Epoch 36, Batch 171/462, Loss: 0.5518548488616943\n",
      "Epoch 36, Batch 172/462, Loss: 0.7529248595237732\n",
      "Epoch 36, Batch 173/462, Loss: 0.7811494469642639\n",
      "Epoch 36, Batch 174/462, Loss: 0.5667535066604614\n",
      "Epoch 36, Batch 175/462, Loss: 0.6172551512718201\n",
      "Epoch 36, Batch 176/462, Loss: 0.9198142290115356\n",
      "Epoch 36, Batch 177/462, Loss: 0.8430182337760925\n",
      "Epoch 36, Batch 178/462, Loss: 0.6154969334602356\n",
      "Epoch 36, Batch 179/462, Loss: 0.6321141123771667\n",
      "Epoch 36, Batch 180/462, Loss: 0.7496510148048401\n",
      "Epoch 36, Batch 181/462, Loss: 0.7835715413093567\n",
      "Epoch 36, Batch 182/462, Loss: 0.6596038341522217\n",
      "Epoch 36, Batch 183/462, Loss: 0.805342972278595\n",
      "Epoch 36, Batch 184/462, Loss: 0.5279368758201599\n",
      "Epoch 36, Batch 185/462, Loss: 0.7023704051971436\n",
      "Epoch 36, Batch 186/462, Loss: 0.7712883353233337\n",
      "Epoch 36, Batch 187/462, Loss: 0.6887087225914001\n",
      "Epoch 36, Batch 188/462, Loss: 0.6231778860092163\n",
      "Epoch 36, Batch 189/462, Loss: 0.5121017694473267\n",
      "Epoch 36, Batch 190/462, Loss: 0.7731084227561951\n",
      "Epoch 36, Batch 191/462, Loss: 0.8169196248054504\n",
      "Epoch 36, Batch 192/462, Loss: 0.7065074443817139\n",
      "Epoch 36, Batch 193/462, Loss: 0.8489429950714111\n",
      "Epoch 36, Batch 194/462, Loss: 0.670818567276001\n",
      "Epoch 36, Batch 195/462, Loss: 0.7389298677444458\n",
      "Epoch 36, Batch 196/462, Loss: 0.7408543229103088\n",
      "Epoch 36, Batch 197/462, Loss: 0.6863811016082764\n",
      "Epoch 36, Batch 198/462, Loss: 0.7570379376411438\n",
      "Epoch 36, Batch 199/462, Loss: 0.7342237830162048\n",
      "Epoch 36, Batch 200/462, Loss: 0.6562375426292419\n",
      "Epoch 36, Batch 201/462, Loss: 0.7386506795883179\n",
      "Epoch 36, Batch 202/462, Loss: 0.7225205302238464\n",
      "Epoch 36, Batch 203/462, Loss: 0.6268382668495178\n",
      "Epoch 36, Batch 204/462, Loss: 0.7729496359825134\n",
      "Epoch 36, Batch 205/462, Loss: 0.8744184970855713\n",
      "Epoch 36, Batch 206/462, Loss: 0.5654134154319763\n",
      "Epoch 36, Batch 207/462, Loss: 0.6634870767593384\n",
      "Epoch 36, Batch 208/462, Loss: 0.718115508556366\n",
      "Epoch 36, Batch 209/462, Loss: 0.6340553760528564\n",
      "Epoch 36, Batch 210/462, Loss: 0.8214526176452637\n",
      "Epoch 36, Batch 211/462, Loss: 0.6549593210220337\n",
      "Epoch 36, Batch 212/462, Loss: 0.8928065896034241\n",
      "Epoch 36, Batch 213/462, Loss: 0.6215800046920776\n",
      "Epoch 36, Batch 214/462, Loss: 0.6475299596786499\n",
      "Epoch 36, Batch 215/462, Loss: 0.70965576171875\n",
      "Epoch 36, Batch 216/462, Loss: 0.6919826865196228\n",
      "Epoch 36, Batch 217/462, Loss: 0.8045622110366821\n",
      "Epoch 36, Batch 218/462, Loss: 0.5974764227867126\n",
      "Epoch 36, Batch 219/462, Loss: 0.7048903703689575\n",
      "Epoch 36, Batch 220/462, Loss: 0.7440176606178284\n",
      "Epoch 36, Batch 221/462, Loss: 0.5502340793609619\n",
      "Epoch 36, Batch 222/462, Loss: 0.6911049485206604\n",
      "Epoch 36, Batch 223/462, Loss: 0.5254042148590088\n",
      "Epoch 36, Batch 224/462, Loss: 0.6230168342590332\n",
      "Epoch 36, Batch 225/462, Loss: 0.8620206713676453\n",
      "Epoch 36, Batch 226/462, Loss: 0.6974236369132996\n",
      "Epoch 36, Batch 227/462, Loss: 0.7867202162742615\n",
      "Epoch 36, Batch 228/462, Loss: 0.7715391516685486\n",
      "Epoch 36, Batch 229/462, Loss: 0.7453098893165588\n",
      "Epoch 36, Batch 230/462, Loss: 0.6805490255355835\n",
      "Epoch 36, Batch 231/462, Loss: 0.7237672209739685\n",
      "Epoch 36, Batch 232/462, Loss: 0.8694907426834106\n",
      "Epoch 36, Batch 233/462, Loss: 0.6201063394546509\n",
      "Epoch 36, Batch 234/462, Loss: 0.5758852958679199\n",
      "Epoch 36, Batch 235/462, Loss: 0.8212486505508423\n",
      "Epoch 36, Batch 236/462, Loss: 0.8087338209152222\n",
      "Epoch 36, Batch 237/462, Loss: 0.6146969795227051\n",
      "Epoch 36, Batch 238/462, Loss: 0.5523291230201721\n",
      "Epoch 36, Batch 239/462, Loss: 0.5702910423278809\n",
      "Epoch 36, Batch 240/462, Loss: 0.7271997928619385\n",
      "Epoch 36, Batch 241/462, Loss: 0.7037093639373779\n",
      "Epoch 36, Batch 242/462, Loss: 0.6853049993515015\n",
      "Epoch 36, Batch 243/462, Loss: 0.9477579593658447\n",
      "Epoch 36, Batch 244/462, Loss: 0.7114061713218689\n",
      "Epoch 36, Batch 245/462, Loss: 0.7615240216255188\n",
      "Epoch 36, Batch 246/462, Loss: 0.6613106727600098\n",
      "Epoch 36, Batch 247/462, Loss: 0.7655861377716064\n",
      "Epoch 36, Batch 248/462, Loss: 0.741358757019043\n",
      "Epoch 36, Batch 249/462, Loss: 0.7761437296867371\n",
      "Epoch 36, Batch 250/462, Loss: 0.6653168797492981\n",
      "Epoch 36, Batch 251/462, Loss: 0.9359130859375\n",
      "Epoch 36, Batch 252/462, Loss: 0.7220818996429443\n",
      "Epoch 36, Batch 253/462, Loss: 0.7866309285163879\n",
      "Epoch 36, Batch 254/462, Loss: 0.7465966939926147\n",
      "Epoch 36, Batch 255/462, Loss: 0.7549561262130737\n",
      "Epoch 36, Batch 256/462, Loss: 0.8044446110725403\n",
      "Epoch 36, Batch 257/462, Loss: 0.6891573667526245\n",
      "Epoch 36, Batch 258/462, Loss: 0.7516997456550598\n",
      "Epoch 36, Batch 259/462, Loss: 0.5678125023841858\n",
      "Epoch 36, Batch 260/462, Loss: 0.7382030487060547\n",
      "Epoch 36, Batch 261/462, Loss: 0.6268357038497925\n",
      "Epoch 36, Batch 262/462, Loss: 0.7352145910263062\n",
      "Epoch 36, Batch 263/462, Loss: 0.6400456428527832\n",
      "Epoch 36, Batch 264/462, Loss: 0.6683959364891052\n",
      "Epoch 36, Batch 265/462, Loss: 0.5478386878967285\n",
      "Epoch 36, Batch 266/462, Loss: 0.8192282915115356\n",
      "Epoch 36, Batch 267/462, Loss: 0.8366727232933044\n",
      "Epoch 36, Batch 268/462, Loss: 0.6480038166046143\n",
      "Epoch 36, Batch 269/462, Loss: 0.7058001160621643\n",
      "Epoch 36, Batch 270/462, Loss: 0.791944146156311\n",
      "Epoch 36, Batch 271/462, Loss: 0.7316685318946838\n",
      "Epoch 36, Batch 272/462, Loss: 0.6611616611480713\n",
      "Epoch 36, Batch 273/462, Loss: 0.7069023847579956\n",
      "Epoch 36, Batch 274/462, Loss: 0.588984489440918\n",
      "Epoch 36, Batch 275/462, Loss: 0.7217581272125244\n",
      "Epoch 36, Batch 276/462, Loss: 0.7096470594406128\n",
      "Epoch 36, Batch 277/462, Loss: 0.6272556185722351\n",
      "Epoch 36, Batch 278/462, Loss: 0.7790603637695312\n",
      "Epoch 36, Batch 279/462, Loss: 0.7807151675224304\n",
      "Epoch 36, Batch 280/462, Loss: 0.7586552500724792\n",
      "Epoch 36, Batch 281/462, Loss: 0.571323812007904\n",
      "Epoch 36, Batch 282/462, Loss: 0.7812888026237488\n",
      "Epoch 36, Batch 283/462, Loss: 0.7930150032043457\n",
      "Epoch 36, Batch 284/462, Loss: 0.7780073285102844\n",
      "Epoch 36, Batch 285/462, Loss: 0.7960299253463745\n",
      "Epoch 36, Batch 286/462, Loss: 0.5860171318054199\n",
      "Epoch 36, Batch 287/462, Loss: 1.0603166818618774\n",
      "Epoch 36, Batch 288/462, Loss: 0.9146933555603027\n",
      "Epoch 36, Batch 289/462, Loss: 0.6974103450775146\n",
      "Epoch 36, Batch 290/462, Loss: 0.8595089912414551\n",
      "Epoch 36, Batch 291/462, Loss: 0.7610183358192444\n",
      "Epoch 36, Batch 292/462, Loss: 0.6573797464370728\n",
      "Epoch 36, Batch 293/462, Loss: 0.7369604706764221\n",
      "Epoch 36, Batch 294/462, Loss: 0.7363847494125366\n",
      "Epoch 36, Batch 295/462, Loss: 0.7612936496734619\n",
      "Epoch 36, Batch 296/462, Loss: 0.7400321364402771\n",
      "Epoch 36, Batch 297/462, Loss: 0.6458818316459656\n",
      "Epoch 36, Batch 298/462, Loss: 0.6815720796585083\n",
      "Epoch 36, Batch 299/462, Loss: 0.7415437698364258\n",
      "Epoch 36, Batch 300/462, Loss: 0.8388930559158325\n",
      "Epoch 36, Batch 301/462, Loss: 0.8501690626144409\n",
      "Epoch 36, Batch 302/462, Loss: 0.6072492599487305\n",
      "Epoch 36, Batch 303/462, Loss: 0.5540757179260254\n",
      "Epoch 36, Batch 304/462, Loss: 0.8021723031997681\n",
      "Epoch 36, Batch 305/462, Loss: 0.7158339023590088\n",
      "Epoch 36, Batch 306/462, Loss: 0.7194271087646484\n",
      "Epoch 36, Batch 307/462, Loss: 0.7175998687744141\n",
      "Epoch 36, Batch 308/462, Loss: 0.8369840383529663\n",
      "Epoch 36, Batch 309/462, Loss: 0.718374490737915\n",
      "Epoch 36, Batch 310/462, Loss: 0.7874318361282349\n",
      "Epoch 36, Batch 311/462, Loss: 0.6793174147605896\n",
      "Epoch 36, Batch 312/462, Loss: 0.9285007119178772\n",
      "Epoch 36, Batch 313/462, Loss: 0.6053681373596191\n",
      "Epoch 36, Batch 314/462, Loss: 0.865058958530426\n",
      "Epoch 36, Batch 315/462, Loss: 0.759460985660553\n",
      "Epoch 36, Batch 316/462, Loss: 0.752285361289978\n",
      "Epoch 36, Batch 317/462, Loss: 0.8962104916572571\n",
      "Epoch 36, Batch 318/462, Loss: 0.7450667023658752\n",
      "Epoch 36, Batch 319/462, Loss: 0.7398116588592529\n",
      "Epoch 36, Batch 320/462, Loss: 0.6155493259429932\n",
      "Epoch 36, Batch 321/462, Loss: 0.6603712439537048\n",
      "Epoch 36, Batch 322/462, Loss: 0.602634847164154\n",
      "Epoch 36, Batch 323/462, Loss: 0.6725045442581177\n",
      "Epoch 36, Batch 324/462, Loss: 0.6082549095153809\n",
      "Epoch 36, Batch 325/462, Loss: 0.8080979585647583\n",
      "Epoch 36, Batch 326/462, Loss: 0.7546517252922058\n",
      "Epoch 36, Batch 327/462, Loss: 0.7360789179801941\n",
      "Epoch 36, Batch 328/462, Loss: 0.7061045169830322\n",
      "Epoch 36, Batch 329/462, Loss: 0.6160435676574707\n",
      "Epoch 36, Batch 330/462, Loss: 0.5144371390342712\n",
      "Epoch 36, Batch 331/462, Loss: 0.8045258522033691\n",
      "Epoch 36, Batch 332/462, Loss: 0.6979551315307617\n",
      "Epoch 36, Batch 333/462, Loss: 0.6367260217666626\n",
      "Epoch 36, Batch 334/462, Loss: 0.694461464881897\n",
      "Epoch 36, Batch 335/462, Loss: 0.8419331908226013\n",
      "Epoch 36, Batch 336/462, Loss: 0.762539267539978\n",
      "Epoch 36, Batch 337/462, Loss: 0.8170329332351685\n",
      "Epoch 36, Batch 338/462, Loss: 0.73365318775177\n",
      "Epoch 36, Batch 339/462, Loss: 0.6594603657722473\n",
      "Epoch 36, Batch 340/462, Loss: 0.614993691444397\n",
      "Epoch 36, Batch 341/462, Loss: 0.7492349147796631\n",
      "Epoch 36, Batch 342/462, Loss: 0.6448343992233276\n",
      "Epoch 36, Batch 343/462, Loss: 0.7392150163650513\n",
      "Epoch 36, Batch 344/462, Loss: 0.8895536065101624\n",
      "Epoch 36, Batch 345/462, Loss: 0.6929494738578796\n",
      "Epoch 36, Batch 346/462, Loss: 0.804034411907196\n",
      "Epoch 36, Batch 347/462, Loss: 0.6764518618583679\n",
      "Epoch 36, Batch 348/462, Loss: 0.735826313495636\n",
      "Epoch 36, Batch 349/462, Loss: 0.8597505688667297\n",
      "Epoch 36, Batch 350/462, Loss: 0.7829955816268921\n",
      "Epoch 36, Batch 351/462, Loss: 0.6495115756988525\n",
      "Epoch 36, Batch 352/462, Loss: 0.9293937683105469\n",
      "Epoch 36, Batch 353/462, Loss: 0.8469722867012024\n",
      "Epoch 36, Batch 354/462, Loss: 0.725105345249176\n",
      "Epoch 36, Batch 355/462, Loss: 0.8185219168663025\n",
      "Epoch 36, Batch 356/462, Loss: 1.0029419660568237\n",
      "Epoch 36, Batch 357/462, Loss: 0.603725016117096\n",
      "Epoch 36, Batch 358/462, Loss: 0.6647294163703918\n",
      "Epoch 36, Batch 359/462, Loss: 0.7136281728744507\n",
      "Epoch 36, Batch 360/462, Loss: 0.6981275081634521\n",
      "Epoch 36, Batch 361/462, Loss: 0.7644262313842773\n",
      "Epoch 36, Batch 362/462, Loss: 0.7965927124023438\n",
      "Epoch 36, Batch 363/462, Loss: 0.9127233624458313\n",
      "Epoch 36, Batch 364/462, Loss: 0.6285236477851868\n",
      "Epoch 36, Batch 365/462, Loss: 0.6493128538131714\n",
      "Epoch 36, Batch 366/462, Loss: 0.7017878293991089\n",
      "Epoch 36, Batch 367/462, Loss: 0.6560698747634888\n",
      "Epoch 36, Batch 368/462, Loss: 0.7949883937835693\n",
      "Epoch 36, Batch 369/462, Loss: 0.6371228694915771\n",
      "Epoch 36, Batch 370/462, Loss: 0.6166719198226929\n",
      "Epoch 36, Batch 371/462, Loss: 0.7362887859344482\n",
      "Epoch 36, Batch 372/462, Loss: 0.6678939461708069\n",
      "Epoch 36, Batch 373/462, Loss: 0.6281406879425049\n",
      "Epoch 36, Batch 374/462, Loss: 0.6691724061965942\n",
      "Epoch 36, Batch 375/462, Loss: 0.6271826028823853\n",
      "Epoch 36, Batch 376/462, Loss: 0.7441087365150452\n",
      "Epoch 36, Batch 377/462, Loss: 0.6207362413406372\n",
      "Epoch 36, Batch 378/462, Loss: 0.471337229013443\n",
      "Epoch 36, Batch 379/462, Loss: 0.8156371712684631\n",
      "Epoch 36, Batch 380/462, Loss: 0.7192554473876953\n",
      "Epoch 36, Batch 381/462, Loss: 0.6687235236167908\n",
      "Epoch 36, Batch 382/462, Loss: 0.6677118539810181\n",
      "Epoch 36, Batch 383/462, Loss: 0.5266672968864441\n",
      "Epoch 36, Batch 384/462, Loss: 0.6816579699516296\n",
      "Epoch 36, Batch 385/462, Loss: 0.7502275109291077\n",
      "Epoch 36, Batch 386/462, Loss: 0.8217955231666565\n",
      "Epoch 36, Batch 387/462, Loss: 0.8025734424591064\n",
      "Epoch 36, Batch 388/462, Loss: 1.0506186485290527\n",
      "Epoch 36, Batch 389/462, Loss: 0.8354204893112183\n",
      "Epoch 36, Batch 390/462, Loss: 0.7225155830383301\n",
      "Epoch 36, Batch 391/462, Loss: 0.6219313144683838\n",
      "Epoch 36, Batch 392/462, Loss: 0.7240532636642456\n",
      "Epoch 36, Batch 393/462, Loss: 0.8555818796157837\n",
      "Epoch 36, Batch 394/462, Loss: 0.7033717036247253\n",
      "Epoch 36, Batch 395/462, Loss: 0.6627856492996216\n",
      "Epoch 36, Batch 396/462, Loss: 0.7217705249786377\n",
      "Epoch 36, Batch 397/462, Loss: 0.7960438132286072\n",
      "Epoch 36, Batch 398/462, Loss: 0.7773512005805969\n",
      "Epoch 36, Batch 399/462, Loss: 0.6583558917045593\n",
      "Epoch 36, Batch 400/462, Loss: 0.7029252648353577\n",
      "Epoch 36, Batch 401/462, Loss: 0.7104403972625732\n",
      "Epoch 36, Batch 402/462, Loss: 0.5582504868507385\n",
      "Epoch 36, Batch 403/462, Loss: 0.8659353256225586\n",
      "Epoch 36, Batch 404/462, Loss: 0.705855667591095\n",
      "Epoch 36, Batch 405/462, Loss: 0.6919134855270386\n",
      "Epoch 36, Batch 406/462, Loss: 0.6719592213630676\n",
      "Epoch 36, Batch 407/462, Loss: 0.7377879619598389\n",
      "Epoch 36, Batch 408/462, Loss: 0.794521152973175\n",
      "Epoch 36, Batch 409/462, Loss: 0.7603310942649841\n",
      "Epoch 36, Batch 410/462, Loss: 0.7135651111602783\n",
      "Epoch 36, Batch 411/462, Loss: 0.667765200138092\n",
      "Epoch 36, Batch 412/462, Loss: 0.7305995225906372\n",
      "Epoch 36, Batch 413/462, Loss: 0.5284647941589355\n",
      "Epoch 36, Batch 414/462, Loss: 0.9514066576957703\n",
      "Epoch 36, Batch 415/462, Loss: 0.5897136926651001\n",
      "Epoch 36, Batch 416/462, Loss: 0.6822205185890198\n",
      "Epoch 36, Batch 417/462, Loss: 0.695269763469696\n",
      "Epoch 36, Batch 418/462, Loss: 0.909076988697052\n",
      "Epoch 36, Batch 419/462, Loss: 0.6074742078781128\n",
      "Epoch 36, Batch 420/462, Loss: 0.7603902220726013\n",
      "Epoch 36, Batch 421/462, Loss: 0.6973902583122253\n",
      "Epoch 36, Batch 422/462, Loss: 0.7706670165061951\n",
      "Epoch 36, Batch 423/462, Loss: 0.6413759589195251\n",
      "Epoch 36, Batch 424/462, Loss: 0.6088671684265137\n",
      "Epoch 36, Batch 425/462, Loss: 0.6874150037765503\n",
      "Epoch 36, Batch 426/462, Loss: 0.8144505023956299\n",
      "Epoch 36, Batch 427/462, Loss: 0.8899298906326294\n",
      "Epoch 36, Batch 428/462, Loss: 0.5642209649085999\n",
      "Epoch 36, Batch 429/462, Loss: 0.7158156037330627\n",
      "Epoch 36, Batch 430/462, Loss: 0.6343846321105957\n",
      "Epoch 36, Batch 431/462, Loss: 0.6779303550720215\n",
      "Epoch 36, Batch 432/462, Loss: 0.647528886795044\n",
      "Epoch 36, Batch 433/462, Loss: 0.5771139860153198\n",
      "Epoch 36, Batch 434/462, Loss: 0.75457763671875\n",
      "Epoch 36, Batch 435/462, Loss: 0.6469956040382385\n",
      "Epoch 36, Batch 436/462, Loss: 0.5960065722465515\n",
      "Epoch 36, Batch 437/462, Loss: 0.7528635263442993\n",
      "Epoch 36, Batch 438/462, Loss: 0.6570912003517151\n",
      "Epoch 36, Batch 439/462, Loss: 0.768492579460144\n",
      "Epoch 36, Batch 440/462, Loss: 0.9360859394073486\n",
      "Epoch 36, Batch 441/462, Loss: 0.7542015910148621\n",
      "Epoch 36, Batch 442/462, Loss: 0.8142808675765991\n",
      "Epoch 36, Batch 443/462, Loss: 0.6090425252914429\n",
      "Epoch 36, Batch 444/462, Loss: 0.5471396446228027\n",
      "Epoch 36, Batch 445/462, Loss: 0.6273291707038879\n",
      "Epoch 36, Batch 446/462, Loss: 0.7872150540351868\n",
      "Epoch 36, Batch 447/462, Loss: 0.6559152603149414\n",
      "Epoch 36, Batch 448/462, Loss: 0.7271063327789307\n",
      "Epoch 36, Batch 449/462, Loss: 0.925184428691864\n",
      "Epoch 36, Batch 450/462, Loss: 0.6859280467033386\n",
      "Epoch 36, Batch 451/462, Loss: 0.7996416091918945\n",
      "Epoch 36, Batch 452/462, Loss: 0.7139372229576111\n",
      "Epoch 36, Batch 453/462, Loss: 0.7119834423065186\n",
      "Epoch 36, Batch 454/462, Loss: 0.9224668145179749\n",
      "Epoch 36, Batch 455/462, Loss: 0.6457392573356628\n",
      "Epoch 36, Batch 456/462, Loss: 0.5113947987556458\n",
      "Epoch 36, Batch 457/462, Loss: 0.6913874745368958\n",
      "Epoch 36, Batch 458/462, Loss: 0.5994699597358704\n",
      "Epoch 36, Batch 459/462, Loss: 0.7105244398117065\n",
      "Epoch 36, Batch 460/462, Loss: 0.5775642991065979\n",
      "Epoch 36, Batch 461/462, Loss: 0.680746853351593\n",
      "Epoch 36, Batch 462/462, Loss: 0.9087198972702026\n",
      "Epoch 36, Loss: 332.25849333405495\n"
     ]
    }
   ],
   "source": [
    "epochs = 36\n",
    "#train_covid(model, optimizer, loss_fn, train_dataloader, epochs)\n",
    "train_covid(model, optimizer, train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.90%\n",
      "Per Class Accuracy: {0: 22.413793103448278, 1: 54.285714285714285, 2: 89.8042414355628}\n",
      "Precision: [32.5        14.96062992 95.57291667]\n",
      "Recall: [22.4137931  54.28571429 89.80424144]\n",
      "F1: [0.26530612 0.2345679  0.92598823]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            tweets, labels = batch\n",
    "            tweets = {key: value.to(device) for key, value in tweets.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # run sequences through CT-BERT\n",
    "            outputs = model(**tweets, labels=labels)\n",
    "            \n",
    "            # highest energy class is our prediction\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    class_labels = [0, 1, 2]\n",
    "    per_class_accuracy = {}\n",
    "    for class_label in class_labels:\n",
    "        # get indices which match current class_label\n",
    "        class_indices = np.where(np.array(all_labels) == class_label)[0]\n",
    "        \n",
    "        # get predictions of current class label\n",
    "        class_preds = np.array(all_preds)[class_indices]\n",
    "        \n",
    "        # calculate accuracy for current class_label\n",
    "        correct_class_preds = np.sum(class_preds == class_label)\n",
    "        total_class_samples = len(class_indices)\n",
    "        \n",
    "        per_class_accuracy[class_label] = (correct_class_preds / total_class_samples) * 100\n",
    "        \n",
    "    accuracy = 100*accuracy_score(all_labels, all_preds)\n",
    "    precision = 100*precision_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "    recall = 100*recall_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, f1, per_class_accuracy\n",
    "\n",
    "\n",
    "# evaluate the model on the test set (unaugmented)\n",
    "accuracy, precision, recall, f1, per_class_accuracy = evaluate_model(model, test_dataloader)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Per Class Accuracy: {per_class_accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./models/model_weights23-{accuracy:.1f}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
