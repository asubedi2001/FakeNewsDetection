{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if PyTorch recognizes GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/covid_lies.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_covid(samples, labels, tokenizer):\n",
    "  tokenized = []\n",
    "  for idx in range(len(samples)):\n",
    "    tokenized_tweet = tokenizer(samples[idx], return_tensors='pt')\n",
    "    \n",
    "    n_inst = {\n",
    "      'tweet_token': tokenized_tweet,\n",
    "      'tweet_origin': samples[idx],\n",
    "      'label': labels[idx], \n",
    "      'idx': idx\n",
    "    }\n",
    "    tokenized.append(n_inst)\n",
    "\n",
    "  return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"digitalepidemiologylab/covid-twitter-bert\")\n",
    "\n",
    "# obtain samples/label pairs from dataset\n",
    "covid_samples = np.array(df['misconception'])\n",
    "covid_labels = np.array(df['label'])\n",
    "\n",
    "tokenized_dataset = tokenize_covid(covid_samples, covid_labels, tokenizer)\n",
    "pprint.pprint(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define PyTorch Datasets (Augmented using SMOTE & Unaugmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "class MisinformationAugmentedDataset(Dataset):\n",
    "    def __init__(self, covid_data):\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "    \n",
    "        self.label_map = {\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 1,\n",
    "            \"na\": 2,\n",
    "        }\n",
    "\n",
    "        # perform upsampling of underrepresented data in our imbalanced dataset\n",
    "        input_ids = [inst['tweet_token']['input_ids'].squeeze(0) for inst in covid_data]\n",
    "        attention_masks = [inst['tweet_token']['attention_mask'].squeeze(0) for inst in covid_data]\n",
    "        token_type_ids = [inst['tweet_token']['token_type_ids'].squeeze(0) for inst in covid_data]\n",
    "        numeric_labels = [self.label_map[inst['label']] for inst in covid_data]\n",
    "        \n",
    "        padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0).numpy()\n",
    "        padded_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0).numpy()\n",
    "        padded_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0).numpy()\n",
    "\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_upsampled, y_upsampled = smote.fit_resample(padded_input_ids, numeric_labels)\n",
    "\n",
    "        # rebuild dictionaries\n",
    "        for idx in range(len(X_upsampled)):\n",
    "            input_ids_tensor = torch.tensor(X_upsampled[idx], dtype=torch.long)\n",
    "            attention_mask_tensor = torch.tensor(\n",
    "                padded_attention_masks[idx % len(padded_attention_masks)], dtype=torch.long\n",
    "            )\n",
    "            token_type_ids_tensor = torch.tensor(\n",
    "                padded_token_type_ids[idx % len(padded_token_type_ids)], dtype=torch.long\n",
    "            )\n",
    "            tweet_token = {\n",
    "                \"input_ids\": input_ids_tensor,\n",
    "                \"attention_mask\": attention_mask_tensor,\n",
    "                \"token_type_ids\": token_type_ids_tensor,\n",
    "            }\n",
    "\n",
    "            self.data.append(tweet_token)\n",
    "            self.labels.append(torch.tensor(y_upsampled[idx], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tweet = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # adjust code according to received UserWarning:\n",
    "        # UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() \n",
    "        # or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
    "        tweet = {key: value.clone().detach() for key, value in tweet.items()}\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return tweet, label\n",
    "    \n",
    "class MisinformationDataset(Dataset):\n",
    "    def __init__(self, covid_data):\n",
    "        self.labels = []\n",
    "        self.data = []\n",
    "    \n",
    "        self.label_map = {\n",
    "            \"pos\": 0,\n",
    "            \"neg\": 1,\n",
    "            \"na\": 2,\n",
    "        }\n",
    "\n",
    "        # perform upsampling of underrepresented data in our imbalanced dataset\n",
    "        input_ids = [inst['tweet_token']['input_ids'].squeeze(0) for inst in covid_data]\n",
    "        attention_masks = [inst['tweet_token']['attention_mask'].squeeze(0) for inst in covid_data]\n",
    "        token_type_ids = [inst['tweet_token']['token_type_ids'].squeeze(0) for inst in covid_data]\n",
    "        numeric_labels = [self.label_map[inst['label']] for inst in covid_data]\n",
    "        \n",
    "        padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0).numpy()\n",
    "        padded_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0).numpy()\n",
    "        padded_token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0).numpy()\n",
    "\n",
    "        # rebuild dictionaries\n",
    "        for idx in range(len(covid_data)):\n",
    "            input_ids_tensor = torch.tensor(padded_input_ids[idx], dtype=torch.long)\n",
    "            attention_mask_tensor = torch.tensor(padded_attention_masks[idx], dtype=torch.long)\n",
    "            token_type_ids_tensor = torch.tensor(padded_token_type_ids[idx], dtype=torch.long)\n",
    "            tweet_token = {\n",
    "                \"input_ids\": input_ids_tensor,\n",
    "                \"attention_mask\": attention_mask_tensor,\n",
    "                \"token_type_ids\": token_type_ids_tensor,\n",
    "            }\n",
    "\n",
    "            self.data.append(tweet_token)\n",
    "            self.labels.append(torch.tensor(numeric_labels[idx], dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the tokenized tweet and label\n",
    "        tweet = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        return tweet, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Dataset into training and test portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_vals = [inst['tweet_token'] for inst in tokenized_dataset]\n",
    "y_vals = [inst['label'] for inst in tokenized_dataset]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vals, y_vals, test_size=0.2, random_state=42)\n",
    "\n",
    "# reconstruct dictionaries using training/test sets\n",
    "train_set = []\n",
    "for tweet_token, label in zip(X_train, y_train):\n",
    "    train_inst = {\n",
    "        'tweet_token': tweet_token,  \n",
    "        'label': label\n",
    "    }\n",
    "    train_set.append(train_inst)\n",
    "\n",
    "test_set = []\n",
    "for tweet_token, label in zip(X_test, y_test):\n",
    "    test_inst = {\n",
    "        'tweet_token': tweet_token,\n",
    "        'label': label\n",
    "    }\n",
    "    test_set.append(test_inst)\n",
    "\n",
    "train_dataset = MisinformationAugmentedDataset(train_set)\n",
    "test_dataset = MisinformationDataset(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, dataloaders, loss function, collate function, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to create collate function to pad variable length sequences for input\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # as per cell 6 output, item[0] will look like this:\n",
    "    # 'tweet_token': {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]]),\n",
    "    #                   'input_ids': tensor([[  101, 21887, 23350,  2003, 19345, 13685,  1012,   102]]),\n",
    "    #                   'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]])}}\n",
    "    # item[1] will be a numeric label according to MisinformationDataset's label_map\n",
    "    input_ids = [item[0]['input_ids'] for item in batch]\n",
    "    attention_masks = [item[0]['attention_mask'] for item in batch]\n",
    "    labels = [item[1] for item in batch]\n",
    "\n",
    "    # pad sequences for input_ids and attention_masks with 0 values\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "    }, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "''' \n",
    "https://huggingface.co/digitalepidemiologylab/covid-twitter-bert-v2\n",
    "\n",
    "CT-BERT: This model was trained on 97M unique tweets (1.2B training examples) \n",
    "collected between January 12 and July 5, 2020 containing at least one of the keywords \n",
    "\"wuhan\", \"ncov\", \"coronavirus\", \"covid\", or \"sars-cov-2\".  \n",
    "These tweets were filtered and preprocessed to reach a final sample of 22.5M tweets \n",
    "(containing 40.7M sentences and 633M tokens) which were used for training.\n",
    "'''\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"digitalepidemiologylab/covid-twitter-bert-v2\", \n",
    "    num_labels=3\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# attempt to use class weights to offset imbalance of dataset\n",
    "# pos_count = np.count_nonzero(covid_labels == 'pos')\n",
    "# neg_count = np.count_nonzero(covid_labels == 'neg')\n",
    "# na_count = np.count_nonzero(covid_labels == 'na')\n",
    "# total_count = len(covid_labels)\n",
    "# pos_weight = total_count / pos_count\n",
    "# neg_weight = total_count / neg_count\n",
    "# na_weight = total_count / na_count\n",
    "# print(f'Weights: \\nPos: {pos_weight}\\nNeg: {neg_weight}\\nNa: {na_weight}')\n",
    "\n",
    "# class_weights = torch.tensor([1.05*pos_weight, neg_weight, na_weight]).to(device)\n",
    "# loss_fn = CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# freeze base model layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# unfreeze last two layers of base model for fine tuning\n",
    "#for param in model.base_model.encoder.layer[-2:]:\n",
    "#    param.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "#dev_dataloader = DataLoader(dev_set, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_covid(model, optim, loss_fn, dataloader, epochs):\n",
    "def train_covid(model, optim, dataloader, epochs):\n",
    "  for epoch in range(epochs):\n",
    "      model.train()\n",
    "      total_loss = 0\n",
    "\n",
    "      for batch_idx, batch in enumerate(dataloader):\n",
    "          optim.zero_grad()\n",
    "          \n",
    "          # unpack batch of form (tweets, labels)\n",
    "          tweets, labels = batch\n",
    "          # send tweets dict's values to device\n",
    "          tweets = {key: value.to(device) for key, value in tweets.items()}\n",
    "          labels = labels.to(device)\n",
    "          \n",
    "          # forward pass on CT-BERT\n",
    "          outputs = model(**tweets, labels=labels)\n",
    "          #logits = outputs.logits\n",
    "          \n",
    "          # class weighted CrossEntropyLoss\n",
    "          #loss = loss_fn(logits, labels)\n",
    "          \n",
    "          # loss provided by model\n",
    "          loss = outputs.loss \n",
    "\n",
    "          # backwards pass on CT-BERT\n",
    "          loss.backward()\n",
    "          optim.step()\n",
    "\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          print(f\"Epoch {epoch + 1}, Batch {batch_idx + 1}/{len(dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "      print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 36\n",
    "#train_covid(model, optimizer, loss_fn, train_dataloader, epochs)\n",
    "train_covid(model, optimizer, train_dataloader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            tweets, labels = batch\n",
    "            tweets = {key: value.to(device) for key, value in tweets.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # run sequences through CT-BERT\n",
    "            outputs = model(**tweets, labels=labels)\n",
    "            \n",
    "            # highest energy class is our prediction\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    class_labels = [0, 1, 2]\n",
    "    per_class_accuracy = {}\n",
    "    for class_label in class_labels:\n",
    "        # get indices which match current class_label\n",
    "        class_indices = np.where(np.array(all_labels) == class_label)[0]\n",
    "        \n",
    "        # get predictions of current class label\n",
    "        class_preds = np.array(all_preds)[class_indices]\n",
    "        \n",
    "        # calculate accuracy for current class_label\n",
    "        correct_class_preds = np.sum(class_preds == class_label)\n",
    "        total_class_samples = len(class_indices)\n",
    "        \n",
    "        per_class_accuracy[class_label] = (correct_class_preds / total_class_samples) * 100\n",
    "        \n",
    "    accuracy = 100*accuracy_score(all_labels, all_preds)\n",
    "    precision = 100*precision_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "    recall = 100*recall_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, labels=class_labels, average=None, zero_division=0)\n",
    "\n",
    "    return accuracy, precision, recall, f1, per_class_accuracy\n",
    "\n",
    "\n",
    "# evaluate the model on the test set (unaugmented)\n",
    "accuracy, precision, recall, f1, per_class_accuracy = evaluate_model(model, test_dataloader)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Per Class Accuracy: {per_class_accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'./models/model_weights-{accuracy:.1f}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmsc673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
